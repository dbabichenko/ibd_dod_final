{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annual Charge Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0_0        0\n0_1        0\n0_2        0\n0_3        0\n0_4        0\n          ..\n3140_54    0\n3140_55    0\n3140_56    0\n3140_57    0\n3140_58    0\nLength: 159967, dtype: int64"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "DS = 'm24p8'\n",
    "chg = pd.Series(utils.pkl_load(f'data/{DS}/annual_charges_label_dict.pkl'))\n",
    "chg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = 'm24p8'\n",
    "chg = pd.Series(utils.pkl_load(f'data/{DS}/annual_charges_label_dict.pkl'))\n",
    "\n",
    "print(f\"Mean: {chg.mean()}. Mean is heavily skewed by disproportionate expenses.\\nMedian:{chg.quantile(0.5)}\")\n",
    "print(\"Distribution of chg between the 10th and 95th %ile:\")\n",
    "sns.distplot(chg[chg.between(chg.quantile(0.1), chg.quantile(0.95))])\n",
    "plt.show()\n",
    "print(\"Distribution of chg between the $1 and $20000:\")\n",
    "sns.distplot(chg[chg.between(1, 20000)])\n",
    "plt.show()\n",
    "print(\"Distribution of chg between the $1 and $5000:\")\n",
    "sns.distplot(chg[chg.between(1, 5000)])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" $ value at 80%ile {chg.quantile(0.8)}\")\n",
    "print([f\"{x}%ile: {chg.quantile(x/10)}\" for x in range(0,10)])\n",
    "print([f\"{x/10}%ile: {chg.quantile(x/100)}\" for x in range(80,100,2)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spike analysis\n",
    "- Spikes are underrepresented in the current m36p12 data. This analysis is not relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.IBDSequenceGenerator(24,8).write_to_disk(self, 'm24p8', tgt_types='spikes', labels_only=True, seq_only=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot spikes/cumulative charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [1,4,9,10,16,24,27,28,51]\n",
    "# arr = chk[chk==3].sample(9).index\n",
    "\n",
    "fig, axes = plt.subplots(3,3, figsize = (12,12))\n",
    "arr = np.array(arr).reshape(3,-1)\n",
    "for r in range(3):\n",
    "    for c in range(3):\n",
    "        df=lib[lib.AUTO_ID==arr[r][c]]\n",
    "        xax = range(len(df))\n",
    "        y = df['charges'].cumsum()\n",
    "        titl = len(df)\n",
    "        axes[r,c].plot(xax, y)\n",
    "        axes[r,c].set_title(f\"AID: {df.AUTO_ID.values[0]}, Months: {titl}\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train.py -b128 -Tannual_charges -Aabnormal_labs,diagnostics,surgery -a0.9 -t0.6 -dm24p8_ds -m2y8m_4 -e300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nohup unbuffer python train.py -b128 -Tannual_charges -Aabnormal_labs,diagnostics,surgery -a0.2 -t0.2 -dm36p12_ds -m3y1y -e300 > models/3y1y.log &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nohup unbuffer python train.py -m 3yAA -dm36p12_AnnualAvg -b128 -e400 > models/3yAA.log &\n",
    "nohup unbuffer python train.py -m 3yAA_Xian -dm36p12_XIAN_AnnualAvg -b192 -e350 > models/3yAA_XIAN.log &\n",
    "nohup unbuffer python train.py -m 2yAA -d m24p8_AnnualAvg -b128 -e400 > models/2yAA.log &\n",
    "nohup unbuffer python train.py -m 2yAA_XIAN -dm24p8_XIAN_AnnualAvg -b192 -e350 > models/2yAA_XIAN.log &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get sequences and their predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "229200\ntn     12939\ntp       941\nfp       393\nfn        76\nName: tag, dtype: int64\n"
    }
   ],
   "source": [
    "import utils \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "_seq/{MODEL}/{i}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get TP/TN/FN/FP sequences\n",
    "dfs = [data(ix) for ix in test_results[test_results.tag=='fn'].index]\n",
    "pd.concat(dfs, axis=0).to_csv(f'models/patient_seq/{MODEL}/FalseNegatives.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: 2y8m_3\nAccuracy:0.9173595049071701\nMultilclass ROC:{0: 0.8892, 1: 0.7774, 2: 0.9364}\n\n                  lt_7k       lt_53k       gt_54k     macro avg  weighted avg\nprecision      0.981789     0.566239     0.665488      0.737839      0.933643\nrecall         0.931668     0.765159     0.879439      0.858755      0.917360\nf1-score       0.956072     0.650839     0.757649      0.788187      0.922975\nsupport    13888.000000  1039.000000  1070.000000  15997.000000  15997.000000\n\n=====================================================================================\n\nModel: 2y8m_4\nAccuracy:0.9173595049071701\nMultilclass ROC:{0: 0.8892, 1: 0.7774, 2: 0.9364}\n\n                  lt_7k       lt_53k       gt_54k     macro avg  weighted avg\nprecision      0.981789     0.566239     0.665488      0.737839      0.933643\nrecall         0.931668     0.765159     0.879439      0.858755      0.917360\nf1-score       0.956072     0.650839     0.757649      0.788187      0.922975\nsupport    13888.000000  1039.000000  1070.000000  15997.000000  15997.000000\n\n=====================================================================================\n\n"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, roc_curve, auc\n",
    "from pprint import pprint\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "\n",
    "for MODEL in ['2y8m_3', '2y8m_4']:\n",
    "    ds = utils.pkl_load('datasets/m24p8_ds/test.dataset')\n",
    "    test_ids = pd.Series(ds.list_IDs)\n",
    "    results = pd.DataFrame(np.array(utils.pkl_load(f'models/{MODEL}/test_eval.pkl')))\n",
    "    results.columns = ['y', 'yhat', 'logit0', 'logit1', 'logit2']\n",
    "    results['seq_id'] = test_ids\n",
    "\n",
    "    report = classification_report(results.y, results.yhat, labels=[0,1,2], target_names=['lt_7k', 'lt_53k', 'gt_54k'], output_dict=True)\n",
    "\n",
    "    y_dum = pd.get_dummies(results.y).values\n",
    "    y_hats = softmax(results[['logit0', 'logit1', 'logit2']].values, axis=0)\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(3):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_dum[:, i], y_hats[:, i])\n",
    "        roc_auc[i] = round(auc(fpr[i], tpr[i]), 4)\n",
    "\n",
    "    print(f\"Model: {MODEL}\")\n",
    "    print(f\"Accuracy:{report['accuracy']}\\nMultilclass ROC:{roc_auc}\\n\")\n",
    "    del report['accuracy']\n",
    "    print(pd.DataFrame.from_dict(report))\n",
    "    print(\"\\n=====================================================================================\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "386.5px",
    "left": "555px",
    "right": "20px",
    "top": "56px",
    "width": "714px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}