 1/1: import random
 1/2: random.random()
 1/3: random.random()
 1/4: random.random()
 1/5: random.random()
 1/6: random.random()
 1/7: [random.random()*random.randint(-4,4) for x in range(30)]
 1/8: [random.random()*random.randint(-4,4) for x in range(30)]
 1/9: start = 100
1/10: vals = [start]
1/11: pc = [random.random()*random.randint(-4,4) for x in range(30)]
1/12: pc = [random.random(-4,4) for x in range(30)]
1/13: random.random()*100
1/14: random.random()*100
1/15: random.random()*100
1/16: random.random()*100
1/17: random.random()*100
1/18: random.random()*100
1/19: random.random()*100
1/20: random.random()*100
1/21: random.random()*100
1/22: random.random()*100
1/23: random.random()*100
1/24: random.random()*100
1/25: random.random()*100
1/26: random.random()*100
1/27: random.random()*100
1/28: random.random()*100
1/29: random.random()*100
1/30: random.random()*100
1/31: random.random()*100
1/32: random.random()*100
1/33: random.random()*100
1/34: random.random()*4
1/35: random.random()*4
1/36: random.random()*4
1/37: random.random()*4
1/38: random.random()*4
1/39: random.random()*4
1/40: random.random()*4
1/41: random.random()*4
1/42: random.random()*4
1/43: random.random()*4
1/44: random.random()*4
1/45: random.random()*4
1/46: random.random()*4
1/47: random.random()*4
1/48: random.random()*4
1/49: random.random()*4
1/50: random.random()*4
1/51: pc = [random.random()*4 for x in range(30)]
1/52: pc
1/53:
for i in pc:
    start.append(start[-1]*(1+i/100))
1/54: start
1/55:
for i in pc:
    vals.append(vals[-1]*(1+i/100))
1/56: vals
1/57: clear
1/58:
for i in pc:
    if random.random()>0.5:
        vals.append(vals[-1]*(1+i/100))
    else:
        vals.append(vals[-1]*(1-i/100))
1/59: vals = [start]
1/60:
for i in pc:
    if random.random()>0.5:
        vals.append(vals[-1]*(1+i/100))
    else:
        vals.append(vals[-1]*(1-i/100))
1/61: vals
1/62: import matplotlib.pyplot as plt
1/63: plt.plot(vals)
1/64:
def nv(start):
    vals = [start]
    pc = [random.random()*4 for x in range(80)]
    for i in pc:
        if random.random()>0.5:
            vals.append(vals[-1]*(1+i/100))
        else:
            vals.append(vals[-1]*(1-i/100))
    plt.plot(vals)
    return vals
1/65: nv(100)
1/66: nv(100)
1/67: clear
1/68:
def nv(price_t0, vol_start=0, vol_range=4):
     vals = [price_t0]
     pc = [vol_start+random.random()*vol_range for x in range(80)]
     for i in pc:
         if random.random()>0.5:
             vals.append(vals[-1]*(1+i/100))
         else:
             vals.append(vals[-1]*(1-i/100))
     plt.plot(vals)
     return vals
1/69: nv(80)
1/70: nv(80, 5,7)
1/71: nv(80, 5,7)
1/72: nv(80, 5,7)
1/73: nv(80, 5,7)
1/74: nv(80, 5,7)
1/75: nv(80, 5,7)
1/76: nv(80, 5,7)
1/77: nv(80, 5,7)
1/78: nv(80, 5,7)
1/79: nv(80, 5,7)
1/80: nv(80, 5,7)
1/81: nv(80, 5,7)
1/82: nv(80, 5,7)
1/83: nv(80, 5,7)
1/84: nv(80, 5,7)
1/85: nv(80, 5,7)
1/86: nv(80, 5,7)
1/87: nv(80, 5,7)
1/88: nv(80)
1/89: nv(80)
1/90: nv(80)
1/91: nv(80)
1/92: nv(80)
 2/1: import random
 2/2: import matplotlib.pyplot as plt
 2/3:
def nv(price_t0, vol_start=0, vol_range=4):
     vals = [price_t0]
     pc = [vol_start+random.random()*vol_range for x in range(80)]
     for i in pc:
         if random.random()>0.5:
             vals.append(vals[-1]*(1+i/100))
         else:
             vals.append(vals[-1]*(1-i/100))
     plt.plot(vals)
     return pc, vals
 2/4: nv(100)
 2/5:
def nv(price_t0, periods, vol_start=0, vol_range=4):
     vals = [price_t0]
     pc = [vol_start+random.random()*vol_range for x in range(periods)]
     for i in pc:
         if random.random()>0.5:
             vals.append(vals[-1]*(1+i/100))
         else:
             vals.append(vals[-1]*(1-i/100))
     plt.plot(vals)
     return pc, vals
 2/6: nv(100, 144)
 2/7: nv(10, 144, 0, 1)
 2/8: nv(10, 144, 0, 1)
 2/9: nv(100, 72, 5, 7)
2/10: nv(100, 72, 5, 7)
2/11: nv(100, 72, 5, 7)
2/12: nv(100, 72, 5, 7)
 3/1: import random
 3/2: import matplotlib.pyplot as plt
 3/3: nv(10, 72, 2,4)
 3/4:
def nv(price_t0, periods, vol_start=0, vol_range=4):
     vals = [price_t0]
     pc = [vol_start+random.random()*vol_range for x in range(periods)]
     for i in pc:
         if random.random()>0.5:
             vals.append(vals[-1]*(1+i/100))
         else:
             vals.append(vals[-1]*(1-i/100))
     plt.plot(vals)
     return pc, vals
 3/5: nv(10, 72, 2,4)
 3/6: nv(10, 72, 0.8,2)
 3/7:
def gen_full(price_t0):
    _, v1 = nv(price_t0, 144)
    _, v2 = nv(v1[-1], 72, 5, 7)
    _, v3 = nv(v2[-1], 144)
    plt.plot(v1+v2+v3)
 3/8: gen_full(100)
 3/9:
def nv(price_t0, periods, vol_start=0, vol_range=4):
     vals = [price_t0]
     pc = [vol_start+random.random()*vol_range for x in range(periods)]
     for i in pc:
         if random.random()>0.5:
             vals.append(vals[-1]*(1+i/100))
         else:
             vals.append(vals[-1]*(1-i/100))
     #plt.plot(vals)
     return pc, vals
3/10: clear
3/11: gen_full(100)
3/12: gen_full(100)
3/13: gen_full(100)
3/14: gen_full(100)
3/15: gen_full(100)
3/16: gen_full(100)
3/17: gen_full(100)
3/18: gen_full(100)
3/19: gen_full(100)
3/20: gen_full(100)
3/21: gen_full(100)
3/22: gen_full(100)
 4/1: import pandas as pd
 4/2: import flask
 4/3: import seaborn as sns
 4/4: import xgboost
 5/1:
import numpy as np
import matplotlib.pyplot as plt

plt.axis([0, 10, 0, 1])

for i in range(10):
    y = np.random.random()
    plt.scatter(i, y)
    plt.pause(0.05)

plt.show()
 5/2:
import numpy as np
import matplotlib.pyplot as plt

plt.axis([0, 10, 0, 1])

for i in range(10):
    y = np.random.random()
    plt.scatter(i, y)
    plt.pause(5)

plt.show()
 5/3:
import numpy as np
import matplotlib.pyplot as plt

plt.axis([0, 10, 0, 1])

for i in range(10):
    y = np.random.random()
    plt.scatter(i, y)
    plt.pause(0.5)

plt.show()
 6/1:
from nltk import word_tokenize
from nltk.corpus import stopwords
import pandas as pd
import re
import numpy as np
from collections import defaultdict
 6/2: l = ['Buccal cellulitis reevaluated We studied children prospectively acute buccal cellulitis The median age months Fifty five percent patients bacteremic three children without meningeal signs symptoms concomitant meningitis Cellulitis aspirate cultures eight positive urine bacterial antigen tests positive useful making etiologic diagnosis Infections due bacteria clinically indistinguishable due Haemophilus influenzae type The right cheek affected often left patients otitis media ipsilateral involved cheek The pathogenesis buccal cellulitis likely involves direct mucous membrane invasion rather spread ipsilateral middle ear', 'Gas absorption middle ear The middle ear middle ear pressure alert anesthetized rhesus monkeys monitored serial tympanograms order determine gas absorption process middle ear cavity During four hour observation period middle ear pressure showed small random fluctuations around zero pressure alert animals whereas anesthetized animals following initial positive pressure phase middle ear pressure reached plateau average pressure mm HO The middle ear gas composition changed air oxygen gas politzerization anesthetized animals determine effect different initial gas compositions rate absorption gas magnitude middle ear pressure Politzerization air oxygen gas resulted average maximum middle ear pressures mm HO respectively The experimental evidence suggested physiological composition middle ear gases nearly surrounding environment rate gas absorption low When gas composition middle ear changed middle ear pressure decreases considerably Based findings possible mechanism development negative middle ear pressure proposed', 'Potential levocetirizine relief nasal congestion Nasal congestion common troublesome symptom allergic rhinitis Because impairs natural human drive nasal breathing addition leads lower self esteem impaired quality life It symptom difficult treat Traditionally intranasal steroids potent anti inflammatory properties vasoconstrictors utilized relieving nasal passages inflamed congested mucosal tissues Recent studies last generation antihistamines demonstrated decongestant properties antihistamines acute seasonal allergic rhinitis chronic lasting perennial allergic rhinitis This study aims review efficacy potent antihistamine levocetirizine relieving nasal congestion reported various studies settings Comparisons placebo antihistamines presented order help general medical practitioners differentiate properties various available antihistamines', 'Placental ischemia induced increases brain water content cerebrovascular permeability role tumor necrosis factor Cerebrovascular complications increased risk encephalopathies characteristic preeclampsia contribute preeclampsia eclampsia related deaths Circulating tumor necrosis factor tumor necrosis factor elevated preeclamptic women infusion tumor necrosis factor pregnant rats mimics characteristics preeclampsia While suggests tumor necrosis factor mechanistic role promote preeclampsia impact tumor necrosis factor cerebral vasculature pregnancy remains unclear We tested hypothesis tumor necrosis factor contributes cerebrovascular abnormalities placental ischemia first infusing tumor necrosis factor pregnant rats ng day ip gestational day levels mimic reported preeclamptic women tumor necrosis factor increased mean arterial pressure mean arterial pressure brain water content anterior cerebrum however tumor necrosis factor infusion effect blood brain barrier blood brain barrier permeability anterior cerebrum posterior cerebrum We assessed role endogenous tumor necrosis factor mediating abnormalities model placental ischemia induced reducing uterine perfusion pressure followed treatment soluble tumor necrosis factor receptor etanercept mg kg sc gestational day Etanercept reduced placental ischemia mediated increases mean arterial pressure anterior brain water content blood brain barrier permeability placental ischemic rats normal pregnant rats Our results indicate tumor necrosis factor mechanistically contributes cerebral edema increasing blood brain barrier permeability underlying factor development cerebrovascular abnormalities associated preeclampsia complicated placental ischemia', 'Recurrent ovarian carcinoma place surgeryThe role cytoreductive surgery well established patients primary ovarian carcinoma Minimal residual disease translates improved response adjuvant treatment prolonged survival For close clinical follow different approaches may helpful detecting recurrent disease including regular physical pelvic examination serial CA levels imaging studies using computerized tomography magnetic resonance imaging positron emission testing At recurrence patients good performance status good response primary therapy macronodular tumor distribution pattern may candidates secondary cytoreductive procedure Data suggests secondary cytoreduction superior chemotherapy alone patients significant disease free interval months Survival secondary cytoreduction optimized cytoreduction microscopic disease yet recognized risk surgical morbidity Therefore strong relationship gynecologic oncology surgeon patient key obtaining appropriate informed consent relaying appropriate outcome expectations', 'Visual loss intoxication The Foster Kennedy Syndrome uncommon The literature cited often confusing This paper presents lucid definition syndrome contemporary diagnostic value tenacious historical roots We attempt sort true Foster Kennedy syndrome congeners discuss misnomers involved An illustrative case presented', 'Mixed cryoglobulinemia chronic hepatitis infection clinicopathologic analysis cases review recent literature We present cases mixed cryoglobulinemia patients infected hepatitis including pertinent clinical serologic pathological data The findings attributable MC appear similar patients HCV infected unknown HCV status The prevalence MC HCV infected patients appears lower region southern Europe although difference due requirement patients included study cryocrit least In patients cryoglobulins shown deposited skin kidney liver The mechanisms HCV MC related remain uncertain Although others evidence enrichment HCV RNA cryoprecipitates patients always case yet clear finding fundamental pathogenic importance Finally appears patients HCV MC may beneficial clinical response vasculitic symptoms therapy alpha interferon well glucocorticoids immunosuppressants In group predictors apparent distinguish responders nonresponders treatment Similarly duration response remains determined', 'Severe venous congestive encephalopathy secondary dialysis arteriovenous graft The clinical presentation imaging venous congestive encephalopathy venous congestive encephalopathy mimic several neurological conditions making diagnosis challenging We report patient end stage renal disease dialysis presented right occipital infarction The patient developed progressive encephalopathy increased intracranial pressure Extensive imaging electroencephalography serum analysis explain cause infarction progressive neurological deterioration Finally cerebral angiography venography demonstrated severe generalized venous congestive encephalopathy due arterial shunting right upper extremity arteriovenous graft extremity arteriovenous graft occluded right innominate venous trunk The right arm shunt resulted severe cerebral venous hypertension due ipsilateral occlusion innominate venous trunk After extremity arteriovenous graft repaired cerebral venous hypertension resolved patient returned baseline', 'The effect pregnancy HIV infected women The diagnosis HIV pregnant women pregnancy HIV positive woman associated affective disorder caring present major challenge physicians health care workers The associated somatic symptoms cause serious morbidity This study aims explore mental health pregnant HIV infected women The study took place University Teaching Hospital Lusaka Zambia satellite clinics Study participants either pregnant women diagnosed HIV course pregnancy women knew HIV positive status prior becoming pregnant questionnaire used assess psychological state women The majority women showed major depressive episodes significant suicidal thoughts About women whose HIV diagnosed pregnancy showed signs somatic illness Those knew HIV status becoming pregnant show severe depressive episodes showed anxiety HIV status babies This suggests women discover HIV status course pregnancy liable develop major depressive illness somatic disorders Physicians dealing women need cognisant strain HIV add pregnancy psychological psychiatric support may require', 'Risk factors cancer oesophagus Kerala India case control study oesophageal cancer carried Trivandrum Kerala involving cases controls Risk factors studied males pan betel tobacco chewing bidi cigarette smoking drinking alcohol taking snuff Only pan tobacco chewing investigated females indulged habits Among males significant associations higher risk observed bidi smoking less bidi plus cigarette smoking greater drinking alcohol less While significant effect duration pan tobacco chewing less observed males significant trend risk first falling rising duration use increased This partly due confounding smoking No effect pan tobacco use observed females step wise model fitted retaining risk factors significant adjusted factors risk factors included duration pan tobacco chewing duration bidi smoking daily frequency bidi cigarette smoking alcohol use yes An adjusted relative risk observed pan tobacco habit years duration years bidi smoking bidis cigarettes per day regular alcohol use category relative baseline never indulging relevant habit']
 6/3:
# l = ['Buccal cellulitis reevaluated We studied children prospectively acute buccal cellulitis The median age months Fifty five percent patients bacteremic three children without meningeal signs symptoms concomitant meningitis Cellulitis aspirate cultures eight positive urine bacterial antigen tests positive useful making etiologic diagnosis Infections due bacteria clinically indistinguishable due Haemophilus influenzae type The right cheek affected often left patients otitis media ipsilateral involved cheek The pathogenesis buccal cellulitis likely involves direct mucous membrane invasion rather spread ipsilateral middle ear', 'Gas absorption middle ear The middle ear middle ear pressure alert anesthetized rhesus monkeys monitored serial tympanograms order determine gas absorption process middle ear cavity During four hour observation period middle ear pressure showed small random fluctuations around zero pressure alert animals whereas anesthetized animals following initial positive pressure phase middle ear pressure reached plateau average pressure mm HO The middle ear gas composition changed air oxygen gas politzerization anesthetized animals determine effect different initial gas compositions rate absorption gas magnitude middle ear pressure Politzerization air oxygen gas resulted average maximum middle ear pressures mm HO respectively The experimental evidence suggested physiological composition middle ear gases nearly surrounding environment rate gas absorption low When gas composition middle ear changed middle ear pressure decreases considerably Based findings possible mechanism development negative middle ear pressure proposed', 'Potential levocetirizine relief nasal congestion Nasal congestion common troublesome symptom allergic rhinitis Because impairs natural human drive nasal breathing addition leads lower self esteem impaired quality life It symptom difficult treat Traditionally intranasal steroids potent anti inflammatory properties vasoconstrictors utilized relieving nasal passages inflamed congested mucosal tissues Recent studies last generation antihistamines demonstrated decongestant properties antihistamines acute seasonal allergic rhinitis chronic lasting perennial allergic rhinitis This study aims review efficacy potent antihistamine levocetirizine relieving nasal congestion reported various studies settings Comparisons placebo antihistamines presented order help general medical practitioners differentiate properties various available antihistamines', 'Placental ischemia induced increases brain water content cerebrovascular permeability role tumor necrosis factor Cerebrovascular complications increased risk encephalopathies characteristic preeclampsia contribute preeclampsia eclampsia related deaths Circulating tumor necrosis factor tumor necrosis factor elevated preeclamptic women infusion tumor necrosis factor pregnant rats mimics characteristics preeclampsia While suggests tumor necrosis factor mechanistic role promote preeclampsia impact tumor necrosis factor cerebral vasculature pregnancy remains unclear We tested hypothesis tumor necrosis factor contributes cerebrovascular abnormalities placental ischemia first infusing tumor necrosis factor pregnant rats ng day ip gestational day levels mimic reported preeclamptic women tumor necrosis factor increased mean arterial pressure mean arterial pressure brain water content anterior cerebrum however tumor necrosis factor infusion effect blood brain barrier blood brain barrier permeability anterior cerebrum posterior cerebrum We assessed role endogenous tumor necrosis factor mediating abnormalities model placental ischemia induced reducing uterine perfusion pressure followed treatment soluble tumor necrosis factor receptor etanercept mg kg sc gestational day Etanercept reduced placental ischemia mediated increases mean arterial pressure anterior brain water content blood brain barrier permeability placental ischemic rats normal pregnant rats Our results indicate tumor necrosis factor mechanistically contributes cerebral edema increasing blood brain barrier permeability underlying factor development cerebrovascular abnormalities associated preeclampsia complicated placental ischemia', 'Recurrent ovarian carcinoma place surgeryThe role cytoreductive surgery well established patients primary ovarian carcinoma Minimal residual disease translates improved response adjuvant treatment prolonged survival For close clinical follow different approaches may helpful detecting recurrent disease including regular physical pelvic examination serial CA levels imaging studies using computerized tomography magnetic resonance imaging positron emission testing At recurrence patients good performance status good response primary therapy macronodular tumor distribution pattern may candidates secondary cytoreductive procedure Data suggests secondary cytoreduction superior chemotherapy alone patients significant disease free interval months Survival secondary cytoreduction optimized cytoreduction microscopic disease yet recognized risk surgical morbidity Therefore strong relationship gynecologic oncology surgeon patient key obtaining appropriate informed consent relaying appropriate outcome expectations', 'Visual loss intoxication The Foster Kennedy Syndrome uncommon The literature cited often confusing This paper presents lucid definition syndrome contemporary diagnostic value tenacious historical roots We attempt sort true Foster Kennedy syndrome congeners discuss misnomers involved An illustrative case presented', 'Mixed cryoglobulinemia chronic hepatitis infection clinicopathologic analysis cases review recent literature We present cases mixed cryoglobulinemia patients infected hepatitis including pertinent clinical serologic pathological data The findings attributable MC appear similar patients HCV infected unknown HCV status The prevalence MC HCV infected patients appears lower region southern Europe although difference due requirement patients included study cryocrit least In patients cryoglobulins shown deposited skin kidney liver The mechanisms HCV MC related remain uncertain Although others evidence enrichment HCV RNA cryoprecipitates patients always case yet clear finding fundamental pathogenic importance Finally appears patients HCV MC may beneficial clinical response vasculitic symptoms therapy alpha interferon well glucocorticoids immunosuppressants In group predictors apparent distinguish responders nonresponders treatment Similarly duration response remains determined', 'Severe venous congestive encephalopathy secondary dialysis arteriovenous graft The clinical presentation imaging venous congestive encephalopathy venous congestive encephalopathy mimic several neurological conditions making diagnosis challenging We report patient end stage renal disease dialysis presented right occipital infarction The patient developed progressive encephalopathy increased intracranial pressure Extensive imaging electroencephalography serum analysis explain cause infarction progressive neurological deterioration Finally cerebral angiography venography demonstrated severe generalized venous congestive encephalopathy due arterial shunting right upper extremity arteriovenous graft extremity arteriovenous graft occluded right innominate venous trunk The right arm shunt resulted severe cerebral venous hypertension due ipsilateral occlusion innominate venous trunk After extremity arteriovenous graft repaired cerebral venous hypertension resolved patient returned baseline', 'The effect pregnancy HIV infected women The diagnosis HIV pregnant women pregnancy HIV positive woman associated affective disorder caring present major challenge physicians health care workers The associated somatic symptoms cause serious morbidity This study aims explore mental health pregnant HIV infected women The study took place University Teaching Hospital Lusaka Zambia satellite clinics Study participants either pregnant women diagnosed HIV course pregnancy women knew HIV positive status prior becoming pregnant questionnaire used assess psychological state women The majority women showed major depressive episodes significant suicidal thoughts About women whose HIV diagnosed pregnancy showed signs somatic illness Those knew HIV status becoming pregnant show severe depressive episodes showed anxiety HIV status babies This suggests women discover HIV status course pregnancy liable develop major depressive illness somatic disorders Physicians dealing women need cognisant strain HIV add pregnancy psychological psychiatric support may require', 'Risk factors cancer oesophagus Kerala India case control study oesophageal cancer carried Trivandrum Kerala involving cases controls Risk factors studied males pan betel tobacco chewing bidi cigarette smoking drinking alcohol taking snuff Only pan tobacco chewing investigated females indulged habits Among males significant associations higher risk observed bidi smoking less bidi plus cigarette smoking greater drinking alcohol less While significant effect duration pan tobacco chewing less observed males significant trend risk first falling rising duration use increased This partly due confounding smoking No effect pan tobacco use observed females step wise model fitted retaining risk factors significant adjusted factors risk factors included duration pan tobacco chewing duration bidi smoking daily frequency bidi cigarette smoking alcohol use yes An adjusted relative risk observed pan tobacco habit years duration years bidi smoking bidis cigarettes per day regular alcohol use category relative baseline never indulging relevant habit']
len(l)
 6/4:
# l = ['Buccal cellulitis reevaluated We studied children prospectively acute buccal cellulitis The median age months Fifty five percent patients bacteremic three children without meningeal signs symptoms concomitant meningitis Cellulitis aspirate cultures eight positive urine bacterial antigen tests positive useful making etiologic diagnosis Infections due bacteria clinically indistinguishable due Haemophilus influenzae type The right cheek affected often left patients otitis media ipsilateral involved cheek The pathogenesis buccal cellulitis likely involves direct mucous membrane invasion rather spread ipsilateral middle ear', 'Gas absorption middle ear The middle ear middle ear pressure alert anesthetized rhesus monkeys monitored serial tympanograms order determine gas absorption process middle ear cavity During four hour observation period middle ear pressure showed small random fluctuations around zero pressure alert animals whereas anesthetized animals following initial positive pressure phase middle ear pressure reached plateau average pressure mm HO The middle ear gas composition changed air oxygen gas politzerization anesthetized animals determine effect different initial gas compositions rate absorption gas magnitude middle ear pressure Politzerization air oxygen gas resulted average maximum middle ear pressures mm HO respectively The experimental evidence suggested physiological composition middle ear gases nearly surrounding environment rate gas absorption low When gas composition middle ear changed middle ear pressure decreases considerably Based findings possible mechanism development negative middle ear pressure proposed', 'Potential levocetirizine relief nasal congestion Nasal congestion common troublesome symptom allergic rhinitis Because impairs natural human drive nasal breathing addition leads lower self esteem impaired quality life It symptom difficult treat Traditionally intranasal steroids potent anti inflammatory properties vasoconstrictors utilized relieving nasal passages inflamed congested mucosal tissues Recent studies last generation antihistamines demonstrated decongestant properties antihistamines acute seasonal allergic rhinitis chronic lasting perennial allergic rhinitis This study aims review efficacy potent antihistamine levocetirizine relieving nasal congestion reported various studies settings Comparisons placebo antihistamines presented order help general medical practitioners differentiate properties various available antihistamines', 'Placental ischemia induced increases brain water content cerebrovascular permeability role tumor necrosis factor Cerebrovascular complications increased risk encephalopathies characteristic preeclampsia contribute preeclampsia eclampsia related deaths Circulating tumor necrosis factor tumor necrosis factor elevated preeclamptic women infusion tumor necrosis factor pregnant rats mimics characteristics preeclampsia While suggests tumor necrosis factor mechanistic role promote preeclampsia impact tumor necrosis factor cerebral vasculature pregnancy remains unclear We tested hypothesis tumor necrosis factor contributes cerebrovascular abnormalities placental ischemia first infusing tumor necrosis factor pregnant rats ng day ip gestational day levels mimic reported preeclamptic women tumor necrosis factor increased mean arterial pressure mean arterial pressure brain water content anterior cerebrum however tumor necrosis factor infusion effect blood brain barrier blood brain barrier permeability anterior cerebrum posterior cerebrum We assessed role endogenous tumor necrosis factor mediating abnormalities model placental ischemia induced reducing uterine perfusion pressure followed treatment soluble tumor necrosis factor receptor etanercept mg kg sc gestational day Etanercept reduced placental ischemia mediated increases mean arterial pressure anterior brain water content blood brain barrier permeability placental ischemic rats normal pregnant rats Our results indicate tumor necrosis factor mechanistically contributes cerebral edema increasing blood brain barrier permeability underlying factor development cerebrovascular abnormalities associated preeclampsia complicated placental ischemia', 'Recurrent ovarian carcinoma place surgeryThe role cytoreductive surgery well established patients primary ovarian carcinoma Minimal residual disease translates improved response adjuvant treatment prolonged survival For close clinical follow different approaches may helpful detecting recurrent disease including regular physical pelvic examination serial CA levels imaging studies using computerized tomography magnetic resonance imaging positron emission testing At recurrence patients good performance status good response primary therapy macronodular tumor distribution pattern may candidates secondary cytoreductive procedure Data suggests secondary cytoreduction superior chemotherapy alone patients significant disease free interval months Survival secondary cytoreduction optimized cytoreduction microscopic disease yet recognized risk surgical morbidity Therefore strong relationship gynecologic oncology surgeon patient key obtaining appropriate informed consent relaying appropriate outcome expectations', 'Visual loss intoxication The Foster Kennedy Syndrome uncommon The literature cited often confusing This paper presents lucid definition syndrome contemporary diagnostic value tenacious historical roots We attempt sort true Foster Kennedy syndrome congeners discuss misnomers involved An illustrative case presented', 'Mixed cryoglobulinemia chronic hepatitis infection clinicopathologic analysis cases review recent literature We present cases mixed cryoglobulinemia patients infected hepatitis including pertinent clinical serologic pathological data The findings attributable MC appear similar patients HCV infected unknown HCV status The prevalence MC HCV infected patients appears lower region southern Europe although difference due requirement patients included study cryocrit least In patients cryoglobulins shown deposited skin kidney liver The mechanisms HCV MC related remain uncertain Although others evidence enrichment HCV RNA cryoprecipitates patients always case yet clear finding fundamental pathogenic importance Finally appears patients HCV MC may beneficial clinical response vasculitic symptoms therapy alpha interferon well glucocorticoids immunosuppressants In group predictors apparent distinguish responders nonresponders treatment Similarly duration response remains determined', 'Severe venous congestive encephalopathy secondary dialysis arteriovenous graft The clinical presentation imaging venous congestive encephalopathy venous congestive encephalopathy mimic several neurological conditions making diagnosis challenging We report patient end stage renal disease dialysis presented right occipital infarction The patient developed progressive encephalopathy increased intracranial pressure Extensive imaging electroencephalography serum analysis explain cause infarction progressive neurological deterioration Finally cerebral angiography venography demonstrated severe generalized venous congestive encephalopathy due arterial shunting right upper extremity arteriovenous graft extremity arteriovenous graft occluded right innominate venous trunk The right arm shunt resulted severe cerebral venous hypertension due ipsilateral occlusion innominate venous trunk After extremity arteriovenous graft repaired cerebral venous hypertension resolved patient returned baseline', 'The effect pregnancy HIV infected women The diagnosis HIV pregnant women pregnancy HIV positive woman associated affective disorder caring present major challenge physicians health care workers The associated somatic symptoms cause serious morbidity This study aims explore mental health pregnant HIV infected women The study took place University Teaching Hospital Lusaka Zambia satellite clinics Study participants either pregnant women diagnosed HIV course pregnancy women knew HIV positive status prior becoming pregnant questionnaire used assess psychological state women The majority women showed major depressive episodes significant suicidal thoughts About women whose HIV diagnosed pregnancy showed signs somatic illness Those knew HIV status becoming pregnant show severe depressive episodes showed anxiety HIV status babies This suggests women discover HIV status course pregnancy liable develop major depressive illness somatic disorders Physicians dealing women need cognisant strain HIV add pregnancy psychological psychiatric support may require', 'Risk factors cancer oesophagus Kerala India case control study oesophageal cancer carried Trivandrum Kerala involving cases controls Risk factors studied males pan betel tobacco chewing bidi cigarette smoking drinking alcohol taking snuff Only pan tobacco chewing investigated females indulged habits Among males significant associations higher risk observed bidi smoking less bidi plus cigarette smoking greater drinking alcohol less While significant effect duration pan tobacco chewing less observed males significant trend risk first falling rising duration use increased This partly due confounding smoking No effect pan tobacco use observed females step wise model fitted retaining risk factors significant adjusted factors risk factors included duration pan tobacco chewing duration bidi smoking daily frequency bidi cigarette smoking alcohol use yes An adjusted relative risk observed pan tobacco habit years duration years bidi smoking bidis cigarettes per day regular alcohol use category relative baseline never indulging relevant habit']

if not data:
    print(1)
 6/5:
# l = ['Buccal cellulitis reevaluated We studied children prospectively acute buccal cellulitis The median age months Fifty five percent patients bacteremic three children without meningeal signs symptoms concomitant meningitis Cellulitis aspirate cultures eight positive urine bacterial antigen tests positive useful making etiologic diagnosis Infections due bacteria clinically indistinguishable due Haemophilus influenzae type The right cheek affected often left patients otitis media ipsilateral involved cheek The pathogenesis buccal cellulitis likely involves direct mucous membrane invasion rather spread ipsilateral middle ear', 'Gas absorption middle ear The middle ear middle ear pressure alert anesthetized rhesus monkeys monitored serial tympanograms order determine gas absorption process middle ear cavity During four hour observation period middle ear pressure showed small random fluctuations around zero pressure alert animals whereas anesthetized animals following initial positive pressure phase middle ear pressure reached plateau average pressure mm HO The middle ear gas composition changed air oxygen gas politzerization anesthetized animals determine effect different initial gas compositions rate absorption gas magnitude middle ear pressure Politzerization air oxygen gas resulted average maximum middle ear pressures mm HO respectively The experimental evidence suggested physiological composition middle ear gases nearly surrounding environment rate gas absorption low When gas composition middle ear changed middle ear pressure decreases considerably Based findings possible mechanism development negative middle ear pressure proposed', 'Potential levocetirizine relief nasal congestion Nasal congestion common troublesome symptom allergic rhinitis Because impairs natural human drive nasal breathing addition leads lower self esteem impaired quality life It symptom difficult treat Traditionally intranasal steroids potent anti inflammatory properties vasoconstrictors utilized relieving nasal passages inflamed congested mucosal tissues Recent studies last generation antihistamines demonstrated decongestant properties antihistamines acute seasonal allergic rhinitis chronic lasting perennial allergic rhinitis This study aims review efficacy potent antihistamine levocetirizine relieving nasal congestion reported various studies settings Comparisons placebo antihistamines presented order help general medical practitioners differentiate properties various available antihistamines', 'Placental ischemia induced increases brain water content cerebrovascular permeability role tumor necrosis factor Cerebrovascular complications increased risk encephalopathies characteristic preeclampsia contribute preeclampsia eclampsia related deaths Circulating tumor necrosis factor tumor necrosis factor elevated preeclamptic women infusion tumor necrosis factor pregnant rats mimics characteristics preeclampsia While suggests tumor necrosis factor mechanistic role promote preeclampsia impact tumor necrosis factor cerebral vasculature pregnancy remains unclear We tested hypothesis tumor necrosis factor contributes cerebrovascular abnormalities placental ischemia first infusing tumor necrosis factor pregnant rats ng day ip gestational day levels mimic reported preeclamptic women tumor necrosis factor increased mean arterial pressure mean arterial pressure brain water content anterior cerebrum however tumor necrosis factor infusion effect blood brain barrier blood brain barrier permeability anterior cerebrum posterior cerebrum We assessed role endogenous tumor necrosis factor mediating abnormalities model placental ischemia induced reducing uterine perfusion pressure followed treatment soluble tumor necrosis factor receptor etanercept mg kg sc gestational day Etanercept reduced placental ischemia mediated increases mean arterial pressure anterior brain water content blood brain barrier permeability placental ischemic rats normal pregnant rats Our results indicate tumor necrosis factor mechanistically contributes cerebral edema increasing blood brain barrier permeability underlying factor development cerebrovascular abnormalities associated preeclampsia complicated placental ischemia', 'Recurrent ovarian carcinoma place surgeryThe role cytoreductive surgery well established patients primary ovarian carcinoma Minimal residual disease translates improved response adjuvant treatment prolonged survival For close clinical follow different approaches may helpful detecting recurrent disease including regular physical pelvic examination serial CA levels imaging studies using computerized tomography magnetic resonance imaging positron emission testing At recurrence patients good performance status good response primary therapy macronodular tumor distribution pattern may candidates secondary cytoreductive procedure Data suggests secondary cytoreduction superior chemotherapy alone patients significant disease free interval months Survival secondary cytoreduction optimized cytoreduction microscopic disease yet recognized risk surgical morbidity Therefore strong relationship gynecologic oncology surgeon patient key obtaining appropriate informed consent relaying appropriate outcome expectations', 'Visual loss intoxication The Foster Kennedy Syndrome uncommon The literature cited often confusing This paper presents lucid definition syndrome contemporary diagnostic value tenacious historical roots We attempt sort true Foster Kennedy syndrome congeners discuss misnomers involved An illustrative case presented', 'Mixed cryoglobulinemia chronic hepatitis infection clinicopathologic analysis cases review recent literature We present cases mixed cryoglobulinemia patients infected hepatitis including pertinent clinical serologic pathological data The findings attributable MC appear similar patients HCV infected unknown HCV status The prevalence MC HCV infected patients appears lower region southern Europe although difference due requirement patients included study cryocrit least In patients cryoglobulins shown deposited skin kidney liver The mechanisms HCV MC related remain uncertain Although others evidence enrichment HCV RNA cryoprecipitates patients always case yet clear finding fundamental pathogenic importance Finally appears patients HCV MC may beneficial clinical response vasculitic symptoms therapy alpha interferon well glucocorticoids immunosuppressants In group predictors apparent distinguish responders nonresponders treatment Similarly duration response remains determined', 'Severe venous congestive encephalopathy secondary dialysis arteriovenous graft The clinical presentation imaging venous congestive encephalopathy venous congestive encephalopathy mimic several neurological conditions making diagnosis challenging We report patient end stage renal disease dialysis presented right occipital infarction The patient developed progressive encephalopathy increased intracranial pressure Extensive imaging electroencephalography serum analysis explain cause infarction progressive neurological deterioration Finally cerebral angiography venography demonstrated severe generalized venous congestive encephalopathy due arterial shunting right upper extremity arteriovenous graft extremity arteriovenous graft occluded right innominate venous trunk The right arm shunt resulted severe cerebral venous hypertension due ipsilateral occlusion innominate venous trunk After extremity arteriovenous graft repaired cerebral venous hypertension resolved patient returned baseline', 'The effect pregnancy HIV infected women The diagnosis HIV pregnant women pregnancy HIV positive woman associated affective disorder caring present major challenge physicians health care workers The associated somatic symptoms cause serious morbidity This study aims explore mental health pregnant HIV infected women The study took place University Teaching Hospital Lusaka Zambia satellite clinics Study participants either pregnant women diagnosed HIV course pregnancy women knew HIV positive status prior becoming pregnant questionnaire used assess psychological state women The majority women showed major depressive episodes significant suicidal thoughts About women whose HIV diagnosed pregnancy showed signs somatic illness Those knew HIV status becoming pregnant show severe depressive episodes showed anxiety HIV status babies This suggests women discover HIV status course pregnancy liable develop major depressive illness somatic disorders Physicians dealing women need cognisant strain HIV add pregnancy psychological psychiatric support may require', 'Risk factors cancer oesophagus Kerala India case control study oesophageal cancer carried Trivandrum Kerala involving cases controls Risk factors studied males pan betel tobacco chewing bidi cigarette smoking drinking alcohol taking snuff Only pan tobacco chewing investigated females indulged habits Among males significant associations higher risk observed bidi smoking less bidi plus cigarette smoking greater drinking alcohol less While significant effect duration pan tobacco chewing less observed males significant trend risk first falling rising duration use increased This partly due confounding smoking No effect pan tobacco use observed females step wise model fitted retaining risk factors significant adjusted factors risk factors included duration pan tobacco chewing duration bidi smoking daily frequency bidi cigarette smoking alcohol use yes An adjusted relative risk observed pan tobacco habit years duration years bidi smoking bidis cigarettes per day regular alcohol use category relative baseline never indulging relevant habit']

stopwords.words('english')
 6/6:
# l = ['Buccal cellulitis reevaluated We studied children prospectively acute buccal cellulitis The median age months Fifty five percent patients bacteremic three children without meningeal signs symptoms concomitant meningitis Cellulitis aspirate cultures eight positive urine bacterial antigen tests positive useful making etiologic diagnosis Infections due bacteria clinically indistinguishable due Haemophilus influenzae type The right cheek affected often left patients otitis media ipsilateral involved cheek The pathogenesis buccal cellulitis likely involves direct mucous membrane invasion rather spread ipsilateral middle ear', 'Gas absorption middle ear The middle ear middle ear pressure alert anesthetized rhesus monkeys monitored serial tympanograms order determine gas absorption process middle ear cavity During four hour observation period middle ear pressure showed small random fluctuations around zero pressure alert animals whereas anesthetized animals following initial positive pressure phase middle ear pressure reached plateau average pressure mm HO The middle ear gas composition changed air oxygen gas politzerization anesthetized animals determine effect different initial gas compositions rate absorption gas magnitude middle ear pressure Politzerization air oxygen gas resulted average maximum middle ear pressures mm HO respectively The experimental evidence suggested physiological composition middle ear gases nearly surrounding environment rate gas absorption low When gas composition middle ear changed middle ear pressure decreases considerably Based findings possible mechanism development negative middle ear pressure proposed', 'Potential levocetirizine relief nasal congestion Nasal congestion common troublesome symptom allergic rhinitis Because impairs natural human drive nasal breathing addition leads lower self esteem impaired quality life It symptom difficult treat Traditionally intranasal steroids potent anti inflammatory properties vasoconstrictors utilized relieving nasal passages inflamed congested mucosal tissues Recent studies last generation antihistamines demonstrated decongestant properties antihistamines acute seasonal allergic rhinitis chronic lasting perennial allergic rhinitis This study aims review efficacy potent antihistamine levocetirizine relieving nasal congestion reported various studies settings Comparisons placebo antihistamines presented order help general medical practitioners differentiate properties various available antihistamines', 'Placental ischemia induced increases brain water content cerebrovascular permeability role tumor necrosis factor Cerebrovascular complications increased risk encephalopathies characteristic preeclampsia contribute preeclampsia eclampsia related deaths Circulating tumor necrosis factor tumor necrosis factor elevated preeclamptic women infusion tumor necrosis factor pregnant rats mimics characteristics preeclampsia While suggests tumor necrosis factor mechanistic role promote preeclampsia impact tumor necrosis factor cerebral vasculature pregnancy remains unclear We tested hypothesis tumor necrosis factor contributes cerebrovascular abnormalities placental ischemia first infusing tumor necrosis factor pregnant rats ng day ip gestational day levels mimic reported preeclamptic women tumor necrosis factor increased mean arterial pressure mean arterial pressure brain water content anterior cerebrum however tumor necrosis factor infusion effect blood brain barrier blood brain barrier permeability anterior cerebrum posterior cerebrum We assessed role endogenous tumor necrosis factor mediating abnormalities model placental ischemia induced reducing uterine perfusion pressure followed treatment soluble tumor necrosis factor receptor etanercept mg kg sc gestational day Etanercept reduced placental ischemia mediated increases mean arterial pressure anterior brain water content blood brain barrier permeability placental ischemic rats normal pregnant rats Our results indicate tumor necrosis factor mechanistically contributes cerebral edema increasing blood brain barrier permeability underlying factor development cerebrovascular abnormalities associated preeclampsia complicated placental ischemia', 'Recurrent ovarian carcinoma place surgeryThe role cytoreductive surgery well established patients primary ovarian carcinoma Minimal residual disease translates improved response adjuvant treatment prolonged survival For close clinical follow different approaches may helpful detecting recurrent disease including regular physical pelvic examination serial CA levels imaging studies using computerized tomography magnetic resonance imaging positron emission testing At recurrence patients good performance status good response primary therapy macronodular tumor distribution pattern may candidates secondary cytoreductive procedure Data suggests secondary cytoreduction superior chemotherapy alone patients significant disease free interval months Survival secondary cytoreduction optimized cytoreduction microscopic disease yet recognized risk surgical morbidity Therefore strong relationship gynecologic oncology surgeon patient key obtaining appropriate informed consent relaying appropriate outcome expectations', 'Visual loss intoxication The Foster Kennedy Syndrome uncommon The literature cited often confusing This paper presents lucid definition syndrome contemporary diagnostic value tenacious historical roots We attempt sort true Foster Kennedy syndrome congeners discuss misnomers involved An illustrative case presented', 'Mixed cryoglobulinemia chronic hepatitis infection clinicopathologic analysis cases review recent literature We present cases mixed cryoglobulinemia patients infected hepatitis including pertinent clinical serologic pathological data The findings attributable MC appear similar patients HCV infected unknown HCV status The prevalence MC HCV infected patients appears lower region southern Europe although difference due requirement patients included study cryocrit least In patients cryoglobulins shown deposited skin kidney liver The mechanisms HCV MC related remain uncertain Although others evidence enrichment HCV RNA cryoprecipitates patients always case yet clear finding fundamental pathogenic importance Finally appears patients HCV MC may beneficial clinical response vasculitic symptoms therapy alpha interferon well glucocorticoids immunosuppressants In group predictors apparent distinguish responders nonresponders treatment Similarly duration response remains determined', 'Severe venous congestive encephalopathy secondary dialysis arteriovenous graft The clinical presentation imaging venous congestive encephalopathy venous congestive encephalopathy mimic several neurological conditions making diagnosis challenging We report patient end stage renal disease dialysis presented right occipital infarction The patient developed progressive encephalopathy increased intracranial pressure Extensive imaging electroencephalography serum analysis explain cause infarction progressive neurological deterioration Finally cerebral angiography venography demonstrated severe generalized venous congestive encephalopathy due arterial shunting right upper extremity arteriovenous graft extremity arteriovenous graft occluded right innominate venous trunk The right arm shunt resulted severe cerebral venous hypertension due ipsilateral occlusion innominate venous trunk After extremity arteriovenous graft repaired cerebral venous hypertension resolved patient returned baseline', 'The effect pregnancy HIV infected women The diagnosis HIV pregnant women pregnancy HIV positive woman associated affective disorder caring present major challenge physicians health care workers The associated somatic symptoms cause serious morbidity This study aims explore mental health pregnant HIV infected women The study took place University Teaching Hospital Lusaka Zambia satellite clinics Study participants either pregnant women diagnosed HIV course pregnancy women knew HIV positive status prior becoming pregnant questionnaire used assess psychological state women The majority women showed major depressive episodes significant suicidal thoughts About women whose HIV diagnosed pregnancy showed signs somatic illness Those knew HIV status becoming pregnant show severe depressive episodes showed anxiety HIV status babies This suggests women discover HIV status course pregnancy liable develop major depressive illness somatic disorders Physicians dealing women need cognisant strain HIV add pregnancy psychological psychiatric support may require', 'Risk factors cancer oesophagus Kerala India case control study oesophageal cancer carried Trivandrum Kerala involving cases controls Risk factors studied males pan betel tobacco chewing bidi cigarette smoking drinking alcohol taking snuff Only pan tobacco chewing investigated females indulged habits Among males significant associations higher risk observed bidi smoking less bidi plus cigarette smoking greater drinking alcohol less While significant effect duration pan tobacco chewing less observed males significant trend risk first falling rising duration use increased This partly due confounding smoking No effect pan tobacco use observed females step wise model fitted retaining risk factors significant adjusted factors risk factors included duration pan tobacco chewing duration bidi smoking daily frequency bidi cigarette smoking alcohol use yes An adjusted relative risk observed pan tobacco habit years duration years bidi smoking bidis cigarettes per day regular alcohol use category relative baseline never indulging relevant habit']

stopwords.words('english')
 7/1:
# l = ['Buccal cellulitis reevaluated We studied children prospectively acute buccal cellulitis The median age months Fifty five percent patients bacteremic three children without meningeal signs symptoms concomitant meningitis Cellulitis aspirate cultures eight positive urine bacterial antigen tests positive useful making etiologic diagnosis Infections due bacteria clinically indistinguishable due Haemophilus influenzae type The right cheek affected often left patients otitis media ipsilateral involved cheek The pathogenesis buccal cellulitis likely involves direct mucous membrane invasion rather spread ipsilateral middle ear', 'Gas absorption middle ear The middle ear middle ear pressure alert anesthetized rhesus monkeys monitored serial tympanograms order determine gas absorption process middle ear cavity During four hour observation period middle ear pressure showed small random fluctuations around zero pressure alert animals whereas anesthetized animals following initial positive pressure phase middle ear pressure reached plateau average pressure mm HO The middle ear gas composition changed air oxygen gas politzerization anesthetized animals determine effect different initial gas compositions rate absorption gas magnitude middle ear pressure Politzerization air oxygen gas resulted average maximum middle ear pressures mm HO respectively The experimental evidence suggested physiological composition middle ear gases nearly surrounding environment rate gas absorption low When gas composition middle ear changed middle ear pressure decreases considerably Based findings possible mechanism development negative middle ear pressure proposed', 'Potential levocetirizine relief nasal congestion Nasal congestion common troublesome symptom allergic rhinitis Because impairs natural human drive nasal breathing addition leads lower self esteem impaired quality life It symptom difficult treat Traditionally intranasal steroids potent anti inflammatory properties vasoconstrictors utilized relieving nasal passages inflamed congested mucosal tissues Recent studies last generation antihistamines demonstrated decongestant properties antihistamines acute seasonal allergic rhinitis chronic lasting perennial allergic rhinitis This study aims review efficacy potent antihistamine levocetirizine relieving nasal congestion reported various studies settings Comparisons placebo antihistamines presented order help general medical practitioners differentiate properties various available antihistamines', 'Placental ischemia induced increases brain water content cerebrovascular permeability role tumor necrosis factor Cerebrovascular complications increased risk encephalopathies characteristic preeclampsia contribute preeclampsia eclampsia related deaths Circulating tumor necrosis factor tumor necrosis factor elevated preeclamptic women infusion tumor necrosis factor pregnant rats mimics characteristics preeclampsia While suggests tumor necrosis factor mechanistic role promote preeclampsia impact tumor necrosis factor cerebral vasculature pregnancy remains unclear We tested hypothesis tumor necrosis factor contributes cerebrovascular abnormalities placental ischemia first infusing tumor necrosis factor pregnant rats ng day ip gestational day levels mimic reported preeclamptic women tumor necrosis factor increased mean arterial pressure mean arterial pressure brain water content anterior cerebrum however tumor necrosis factor infusion effect blood brain barrier blood brain barrier permeability anterior cerebrum posterior cerebrum We assessed role endogenous tumor necrosis factor mediating abnormalities model placental ischemia induced reducing uterine perfusion pressure followed treatment soluble tumor necrosis factor receptor etanercept mg kg sc gestational day Etanercept reduced placental ischemia mediated increases mean arterial pressure anterior brain water content blood brain barrier permeability placental ischemic rats normal pregnant rats Our results indicate tumor necrosis factor mechanistically contributes cerebral edema increasing blood brain barrier permeability underlying factor development cerebrovascular abnormalities associated preeclampsia complicated placental ischemia', 'Recurrent ovarian carcinoma place surgeryThe role cytoreductive surgery well established patients primary ovarian carcinoma Minimal residual disease translates improved response adjuvant treatment prolonged survival For close clinical follow different approaches may helpful detecting recurrent disease including regular physical pelvic examination serial CA levels imaging studies using computerized tomography magnetic resonance imaging positron emission testing At recurrence patients good performance status good response primary therapy macronodular tumor distribution pattern may candidates secondary cytoreductive procedure Data suggests secondary cytoreduction superior chemotherapy alone patients significant disease free interval months Survival secondary cytoreduction optimized cytoreduction microscopic disease yet recognized risk surgical morbidity Therefore strong relationship gynecologic oncology surgeon patient key obtaining appropriate informed consent relaying appropriate outcome expectations', 'Visual loss intoxication The Foster Kennedy Syndrome uncommon The literature cited often confusing This paper presents lucid definition syndrome contemporary diagnostic value tenacious historical roots We attempt sort true Foster Kennedy syndrome congeners discuss misnomers involved An illustrative case presented', 'Mixed cryoglobulinemia chronic hepatitis infection clinicopathologic analysis cases review recent literature We present cases mixed cryoglobulinemia patients infected hepatitis including pertinent clinical serologic pathological data The findings attributable MC appear similar patients HCV infected unknown HCV status The prevalence MC HCV infected patients appears lower region southern Europe although difference due requirement patients included study cryocrit least In patients cryoglobulins shown deposited skin kidney liver The mechanisms HCV MC related remain uncertain Although others evidence enrichment HCV RNA cryoprecipitates patients always case yet clear finding fundamental pathogenic importance Finally appears patients HCV MC may beneficial clinical response vasculitic symptoms therapy alpha interferon well glucocorticoids immunosuppressants In group predictors apparent distinguish responders nonresponders treatment Similarly duration response remains determined', 'Severe venous congestive encephalopathy secondary dialysis arteriovenous graft The clinical presentation imaging venous congestive encephalopathy venous congestive encephalopathy mimic several neurological conditions making diagnosis challenging We report patient end stage renal disease dialysis presented right occipital infarction The patient developed progressive encephalopathy increased intracranial pressure Extensive imaging electroencephalography serum analysis explain cause infarction progressive neurological deterioration Finally cerebral angiography venography demonstrated severe generalized venous congestive encephalopathy due arterial shunting right upper extremity arteriovenous graft extremity arteriovenous graft occluded right innominate venous trunk The right arm shunt resulted severe cerebral venous hypertension due ipsilateral occlusion innominate venous trunk After extremity arteriovenous graft repaired cerebral venous hypertension resolved patient returned baseline', 'The effect pregnancy HIV infected women The diagnosis HIV pregnant women pregnancy HIV positive woman associated affective disorder caring present major challenge physicians health care workers The associated somatic symptoms cause serious morbidity This study aims explore mental health pregnant HIV infected women The study took place University Teaching Hospital Lusaka Zambia satellite clinics Study participants either pregnant women diagnosed HIV course pregnancy women knew HIV positive status prior becoming pregnant questionnaire used assess psychological state women The majority women showed major depressive episodes significant suicidal thoughts About women whose HIV diagnosed pregnancy showed signs somatic illness Those knew HIV status becoming pregnant show severe depressive episodes showed anxiety HIV status babies This suggests women discover HIV status course pregnancy liable develop major depressive illness somatic disorders Physicians dealing women need cognisant strain HIV add pregnancy psychological psychiatric support may require', 'Risk factors cancer oesophagus Kerala India case control study oesophageal cancer carried Trivandrum Kerala involving cases controls Risk factors studied males pan betel tobacco chewing bidi cigarette smoking drinking alcohol taking snuff Only pan tobacco chewing investigated females indulged habits Among males significant associations higher risk observed bidi smoking less bidi plus cigarette smoking greater drinking alcohol less While significant effect duration pan tobacco chewing less observed males significant trend risk first falling rising duration use increased This partly due confounding smoking No effect pan tobacco use observed females step wise model fitted retaining risk factors significant adjusted factors risk factors included duration pan tobacco chewing duration bidi smoking daily frequency bidi cigarette smoking alcohol use yes An adjusted relative risk observed pan tobacco habit years duration years bidi smoking bidis cigarettes per day regular alcohol use category relative baseline never indulging relevant habit']

stopwords.words('english')
 7/2:
from nltk import word_tokenize
from nltk.corpus import stopwords
import pandas as pd
import re
import numpy as np
from collections import defaultdict
 7/3:
# l = ['Buccal cellulitis reevaluated We studied children prospectively acute buccal cellulitis The median age months Fifty five percent patients bacteremic three children without meningeal signs symptoms concomitant meningitis Cellulitis aspirate cultures eight positive urine bacterial antigen tests positive useful making etiologic diagnosis Infections due bacteria clinically indistinguishable due Haemophilus influenzae type The right cheek affected often left patients otitis media ipsilateral involved cheek The pathogenesis buccal cellulitis likely involves direct mucous membrane invasion rather spread ipsilateral middle ear', 'Gas absorption middle ear The middle ear middle ear pressure alert anesthetized rhesus monkeys monitored serial tympanograms order determine gas absorption process middle ear cavity During four hour observation period middle ear pressure showed small random fluctuations around zero pressure alert animals whereas anesthetized animals following initial positive pressure phase middle ear pressure reached plateau average pressure mm HO The middle ear gas composition changed air oxygen gas politzerization anesthetized animals determine effect different initial gas compositions rate absorption gas magnitude middle ear pressure Politzerization air oxygen gas resulted average maximum middle ear pressures mm HO respectively The experimental evidence suggested physiological composition middle ear gases nearly surrounding environment rate gas absorption low When gas composition middle ear changed middle ear pressure decreases considerably Based findings possible mechanism development negative middle ear pressure proposed', 'Potential levocetirizine relief nasal congestion Nasal congestion common troublesome symptom allergic rhinitis Because impairs natural human drive nasal breathing addition leads lower self esteem impaired quality life It symptom difficult treat Traditionally intranasal steroids potent anti inflammatory properties vasoconstrictors utilized relieving nasal passages inflamed congested mucosal tissues Recent studies last generation antihistamines demonstrated decongestant properties antihistamines acute seasonal allergic rhinitis chronic lasting perennial allergic rhinitis This study aims review efficacy potent antihistamine levocetirizine relieving nasal congestion reported various studies settings Comparisons placebo antihistamines presented order help general medical practitioners differentiate properties various available antihistamines', 'Placental ischemia induced increases brain water content cerebrovascular permeability role tumor necrosis factor Cerebrovascular complications increased risk encephalopathies characteristic preeclampsia contribute preeclampsia eclampsia related deaths Circulating tumor necrosis factor tumor necrosis factor elevated preeclamptic women infusion tumor necrosis factor pregnant rats mimics characteristics preeclampsia While suggests tumor necrosis factor mechanistic role promote preeclampsia impact tumor necrosis factor cerebral vasculature pregnancy remains unclear We tested hypothesis tumor necrosis factor contributes cerebrovascular abnormalities placental ischemia first infusing tumor necrosis factor pregnant rats ng day ip gestational day levels mimic reported preeclamptic women tumor necrosis factor increased mean arterial pressure mean arterial pressure brain water content anterior cerebrum however tumor necrosis factor infusion effect blood brain barrier blood brain barrier permeability anterior cerebrum posterior cerebrum We assessed role endogenous tumor necrosis factor mediating abnormalities model placental ischemia induced reducing uterine perfusion pressure followed treatment soluble tumor necrosis factor receptor etanercept mg kg sc gestational day Etanercept reduced placental ischemia mediated increases mean arterial pressure anterior brain water content blood brain barrier permeability placental ischemic rats normal pregnant rats Our results indicate tumor necrosis factor mechanistically contributes cerebral edema increasing blood brain barrier permeability underlying factor development cerebrovascular abnormalities associated preeclampsia complicated placental ischemia', 'Recurrent ovarian carcinoma place surgeryThe role cytoreductive surgery well established patients primary ovarian carcinoma Minimal residual disease translates improved response adjuvant treatment prolonged survival For close clinical follow different approaches may helpful detecting recurrent disease including regular physical pelvic examination serial CA levels imaging studies using computerized tomography magnetic resonance imaging positron emission testing At recurrence patients good performance status good response primary therapy macronodular tumor distribution pattern may candidates secondary cytoreductive procedure Data suggests secondary cytoreduction superior chemotherapy alone patients significant disease free interval months Survival secondary cytoreduction optimized cytoreduction microscopic disease yet recognized risk surgical morbidity Therefore strong relationship gynecologic oncology surgeon patient key obtaining appropriate informed consent relaying appropriate outcome expectations', 'Visual loss intoxication The Foster Kennedy Syndrome uncommon The literature cited often confusing This paper presents lucid definition syndrome contemporary diagnostic value tenacious historical roots We attempt sort true Foster Kennedy syndrome congeners discuss misnomers involved An illustrative case presented', 'Mixed cryoglobulinemia chronic hepatitis infection clinicopathologic analysis cases review recent literature We present cases mixed cryoglobulinemia patients infected hepatitis including pertinent clinical serologic pathological data The findings attributable MC appear similar patients HCV infected unknown HCV status The prevalence MC HCV infected patients appears lower region southern Europe although difference due requirement patients included study cryocrit least In patients cryoglobulins shown deposited skin kidney liver The mechanisms HCV MC related remain uncertain Although others evidence enrichment HCV RNA cryoprecipitates patients always case yet clear finding fundamental pathogenic importance Finally appears patients HCV MC may beneficial clinical response vasculitic symptoms therapy alpha interferon well glucocorticoids immunosuppressants In group predictors apparent distinguish responders nonresponders treatment Similarly duration response remains determined', 'Severe venous congestive encephalopathy secondary dialysis arteriovenous graft The clinical presentation imaging venous congestive encephalopathy venous congestive encephalopathy mimic several neurological conditions making diagnosis challenging We report patient end stage renal disease dialysis presented right occipital infarction The patient developed progressive encephalopathy increased intracranial pressure Extensive imaging electroencephalography serum analysis explain cause infarction progressive neurological deterioration Finally cerebral angiography venography demonstrated severe generalized venous congestive encephalopathy due arterial shunting right upper extremity arteriovenous graft extremity arteriovenous graft occluded right innominate venous trunk The right arm shunt resulted severe cerebral venous hypertension due ipsilateral occlusion innominate venous trunk After extremity arteriovenous graft repaired cerebral venous hypertension resolved patient returned baseline', 'The effect pregnancy HIV infected women The diagnosis HIV pregnant women pregnancy HIV positive woman associated affective disorder caring present major challenge physicians health care workers The associated somatic symptoms cause serious morbidity This study aims explore mental health pregnant HIV infected women The study took place University Teaching Hospital Lusaka Zambia satellite clinics Study participants either pregnant women diagnosed HIV course pregnancy women knew HIV positive status prior becoming pregnant questionnaire used assess psychological state women The majority women showed major depressive episodes significant suicidal thoughts About women whose HIV diagnosed pregnancy showed signs somatic illness Those knew HIV status becoming pregnant show severe depressive episodes showed anxiety HIV status babies This suggests women discover HIV status course pregnancy liable develop major depressive illness somatic disorders Physicians dealing women need cognisant strain HIV add pregnancy psychological psychiatric support may require', 'Risk factors cancer oesophagus Kerala India case control study oesophageal cancer carried Trivandrum Kerala involving cases controls Risk factors studied males pan betel tobacco chewing bidi cigarette smoking drinking alcohol taking snuff Only pan tobacco chewing investigated females indulged habits Among males significant associations higher risk observed bidi smoking less bidi plus cigarette smoking greater drinking alcohol less While significant effect duration pan tobacco chewing less observed males significant trend risk first falling rising duration use increased This partly due confounding smoking No effect pan tobacco use observed females step wise model fitted retaining risk factors significant adjusted factors risk factors included duration pan tobacco chewing duration bidi smoking daily frequency bidi cigarette smoking alcohol use yes An adjusted relative risk observed pan tobacco habit years duration years bidi smoking bidis cigarettes per day regular alcohol use category relative baseline never indulging relevant habit']

stopwords.words('english')
 7/4: nltk.download('stopwords')
 7/5:
import nltk
nltk.download('stopwords')
 7/6:
# l = ['Buccal cellulitis reevaluated We studied children prospectively acute buccal cellulitis The median age months Fifty five percent patients bacteremic three children without meningeal signs symptoms concomitant meningitis Cellulitis aspirate cultures eight positive urine bacterial antigen tests positive useful making etiologic diagnosis Infections due bacteria clinically indistinguishable due Haemophilus influenzae type The right cheek affected often left patients otitis media ipsilateral involved cheek The pathogenesis buccal cellulitis likely involves direct mucous membrane invasion rather spread ipsilateral middle ear', 'Gas absorption middle ear The middle ear middle ear pressure alert anesthetized rhesus monkeys monitored serial tympanograms order determine gas absorption process middle ear cavity During four hour observation period middle ear pressure showed small random fluctuations around zero pressure alert animals whereas anesthetized animals following initial positive pressure phase middle ear pressure reached plateau average pressure mm HO The middle ear gas composition changed air oxygen gas politzerization anesthetized animals determine effect different initial gas compositions rate absorption gas magnitude middle ear pressure Politzerization air oxygen gas resulted average maximum middle ear pressures mm HO respectively The experimental evidence suggested physiological composition middle ear gases nearly surrounding environment rate gas absorption low When gas composition middle ear changed middle ear pressure decreases considerably Based findings possible mechanism development negative middle ear pressure proposed', 'Potential levocetirizine relief nasal congestion Nasal congestion common troublesome symptom allergic rhinitis Because impairs natural human drive nasal breathing addition leads lower self esteem impaired quality life It symptom difficult treat Traditionally intranasal steroids potent anti inflammatory properties vasoconstrictors utilized relieving nasal passages inflamed congested mucosal tissues Recent studies last generation antihistamines demonstrated decongestant properties antihistamines acute seasonal allergic rhinitis chronic lasting perennial allergic rhinitis This study aims review efficacy potent antihistamine levocetirizine relieving nasal congestion reported various studies settings Comparisons placebo antihistamines presented order help general medical practitioners differentiate properties various available antihistamines', 'Placental ischemia induced increases brain water content cerebrovascular permeability role tumor necrosis factor Cerebrovascular complications increased risk encephalopathies characteristic preeclampsia contribute preeclampsia eclampsia related deaths Circulating tumor necrosis factor tumor necrosis factor elevated preeclamptic women infusion tumor necrosis factor pregnant rats mimics characteristics preeclampsia While suggests tumor necrosis factor mechanistic role promote preeclampsia impact tumor necrosis factor cerebral vasculature pregnancy remains unclear We tested hypothesis tumor necrosis factor contributes cerebrovascular abnormalities placental ischemia first infusing tumor necrosis factor pregnant rats ng day ip gestational day levels mimic reported preeclamptic women tumor necrosis factor increased mean arterial pressure mean arterial pressure brain water content anterior cerebrum however tumor necrosis factor infusion effect blood brain barrier blood brain barrier permeability anterior cerebrum posterior cerebrum We assessed role endogenous tumor necrosis factor mediating abnormalities model placental ischemia induced reducing uterine perfusion pressure followed treatment soluble tumor necrosis factor receptor etanercept mg kg sc gestational day Etanercept reduced placental ischemia mediated increases mean arterial pressure anterior brain water content blood brain barrier permeability placental ischemic rats normal pregnant rats Our results indicate tumor necrosis factor mechanistically contributes cerebral edema increasing blood brain barrier permeability underlying factor development cerebrovascular abnormalities associated preeclampsia complicated placental ischemia', 'Recurrent ovarian carcinoma place surgeryThe role cytoreductive surgery well established patients primary ovarian carcinoma Minimal residual disease translates improved response adjuvant treatment prolonged survival For close clinical follow different approaches may helpful detecting recurrent disease including regular physical pelvic examination serial CA levels imaging studies using computerized tomography magnetic resonance imaging positron emission testing At recurrence patients good performance status good response primary therapy macronodular tumor distribution pattern may candidates secondary cytoreductive procedure Data suggests secondary cytoreduction superior chemotherapy alone patients significant disease free interval months Survival secondary cytoreduction optimized cytoreduction microscopic disease yet recognized risk surgical morbidity Therefore strong relationship gynecologic oncology surgeon patient key obtaining appropriate informed consent relaying appropriate outcome expectations', 'Visual loss intoxication The Foster Kennedy Syndrome uncommon The literature cited often confusing This paper presents lucid definition syndrome contemporary diagnostic value tenacious historical roots We attempt sort true Foster Kennedy syndrome congeners discuss misnomers involved An illustrative case presented', 'Mixed cryoglobulinemia chronic hepatitis infection clinicopathologic analysis cases review recent literature We present cases mixed cryoglobulinemia patients infected hepatitis including pertinent clinical serologic pathological data The findings attributable MC appear similar patients HCV infected unknown HCV status The prevalence MC HCV infected patients appears lower region southern Europe although difference due requirement patients included study cryocrit least In patients cryoglobulins shown deposited skin kidney liver The mechanisms HCV MC related remain uncertain Although others evidence enrichment HCV RNA cryoprecipitates patients always case yet clear finding fundamental pathogenic importance Finally appears patients HCV MC may beneficial clinical response vasculitic symptoms therapy alpha interferon well glucocorticoids immunosuppressants In group predictors apparent distinguish responders nonresponders treatment Similarly duration response remains determined', 'Severe venous congestive encephalopathy secondary dialysis arteriovenous graft The clinical presentation imaging venous congestive encephalopathy venous congestive encephalopathy mimic several neurological conditions making diagnosis challenging We report patient end stage renal disease dialysis presented right occipital infarction The patient developed progressive encephalopathy increased intracranial pressure Extensive imaging electroencephalography serum analysis explain cause infarction progressive neurological deterioration Finally cerebral angiography venography demonstrated severe generalized venous congestive encephalopathy due arterial shunting right upper extremity arteriovenous graft extremity arteriovenous graft occluded right innominate venous trunk The right arm shunt resulted severe cerebral venous hypertension due ipsilateral occlusion innominate venous trunk After extremity arteriovenous graft repaired cerebral venous hypertension resolved patient returned baseline', 'The effect pregnancy HIV infected women The diagnosis HIV pregnant women pregnancy HIV positive woman associated affective disorder caring present major challenge physicians health care workers The associated somatic symptoms cause serious morbidity This study aims explore mental health pregnant HIV infected women The study took place University Teaching Hospital Lusaka Zambia satellite clinics Study participants either pregnant women diagnosed HIV course pregnancy women knew HIV positive status prior becoming pregnant questionnaire used assess psychological state women The majority women showed major depressive episodes significant suicidal thoughts About women whose HIV diagnosed pregnancy showed signs somatic illness Those knew HIV status becoming pregnant show severe depressive episodes showed anxiety HIV status babies This suggests women discover HIV status course pregnancy liable develop major depressive illness somatic disorders Physicians dealing women need cognisant strain HIV add pregnancy psychological psychiatric support may require', 'Risk factors cancer oesophagus Kerala India case control study oesophageal cancer carried Trivandrum Kerala involving cases controls Risk factors studied males pan betel tobacco chewing bidi cigarette smoking drinking alcohol taking snuff Only pan tobacco chewing investigated females indulged habits Among males significant associations higher risk observed bidi smoking less bidi plus cigarette smoking greater drinking alcohol less While significant effect duration pan tobacco chewing less observed males significant trend risk first falling rising duration use increased This partly due confounding smoking No effect pan tobacco use observed females step wise model fitted retaining risk factors significant adjusted factors risk factors included duration pan tobacco chewing duration bidi smoking daily frequency bidi cigarette smoking alcohol use yes An adjusted relative risk observed pan tobacco habit years duration years bidi smoking bidis cigarettes per day regular alcohol use category relative baseline never indulging relevant habit']

stopwords.words('english')
 7/7:
# l = ['Buccal cellulitis reevaluated We studied children prospectively acute buccal cellulitis The median age months Fifty five percent patients bacteremic three children without meningeal signs symptoms concomitant meningitis Cellulitis aspirate cultures eight positive urine bacterial antigen tests positive useful making etiologic diagnosis Infections due bacteria clinically indistinguishable due Haemophilus influenzae type The right cheek affected often left patients otitis media ipsilateral involved cheek The pathogenesis buccal cellulitis likely involves direct mucous membrane invasion rather spread ipsilateral middle ear', 'Gas absorption middle ear The middle ear middle ear pressure alert anesthetized rhesus monkeys monitored serial tympanograms order determine gas absorption process middle ear cavity During four hour observation period middle ear pressure showed small random fluctuations around zero pressure alert animals whereas anesthetized animals following initial positive pressure phase middle ear pressure reached plateau average pressure mm HO The middle ear gas composition changed air oxygen gas politzerization anesthetized animals determine effect different initial gas compositions rate absorption gas magnitude middle ear pressure Politzerization air oxygen gas resulted average maximum middle ear pressures mm HO respectively The experimental evidence suggested physiological composition middle ear gases nearly surrounding environment rate gas absorption low When gas composition middle ear changed middle ear pressure decreases considerably Based findings possible mechanism development negative middle ear pressure proposed', 'Potential levocetirizine relief nasal congestion Nasal congestion common troublesome symptom allergic rhinitis Because impairs natural human drive nasal breathing addition leads lower self esteem impaired quality life It symptom difficult treat Traditionally intranasal steroids potent anti inflammatory properties vasoconstrictors utilized relieving nasal passages inflamed congested mucosal tissues Recent studies last generation antihistamines demonstrated decongestant properties antihistamines acute seasonal allergic rhinitis chronic lasting perennial allergic rhinitis This study aims review efficacy potent antihistamine levocetirizine relieving nasal congestion reported various studies settings Comparisons placebo antihistamines presented order help general medical practitioners differentiate properties various available antihistamines', 'Placental ischemia induced increases brain water content cerebrovascular permeability role tumor necrosis factor Cerebrovascular complications increased risk encephalopathies characteristic preeclampsia contribute preeclampsia eclampsia related deaths Circulating tumor necrosis factor tumor necrosis factor elevated preeclamptic women infusion tumor necrosis factor pregnant rats mimics characteristics preeclampsia While suggests tumor necrosis factor mechanistic role promote preeclampsia impact tumor necrosis factor cerebral vasculature pregnancy remains unclear We tested hypothesis tumor necrosis factor contributes cerebrovascular abnormalities placental ischemia first infusing tumor necrosis factor pregnant rats ng day ip gestational day levels mimic reported preeclamptic women tumor necrosis factor increased mean arterial pressure mean arterial pressure brain water content anterior cerebrum however tumor necrosis factor infusion effect blood brain barrier blood brain barrier permeability anterior cerebrum posterior cerebrum We assessed role endogenous tumor necrosis factor mediating abnormalities model placental ischemia induced reducing uterine perfusion pressure followed treatment soluble tumor necrosis factor receptor etanercept mg kg sc gestational day Etanercept reduced placental ischemia mediated increases mean arterial pressure anterior brain water content blood brain barrier permeability placental ischemic rats normal pregnant rats Our results indicate tumor necrosis factor mechanistically contributes cerebral edema increasing blood brain barrier permeability underlying factor development cerebrovascular abnormalities associated preeclampsia complicated placental ischemia', 'Recurrent ovarian carcinoma place surgeryThe role cytoreductive surgery well established patients primary ovarian carcinoma Minimal residual disease translates improved response adjuvant treatment prolonged survival For close clinical follow different approaches may helpful detecting recurrent disease including regular physical pelvic examination serial CA levels imaging studies using computerized tomography magnetic resonance imaging positron emission testing At recurrence patients good performance status good response primary therapy macronodular tumor distribution pattern may candidates secondary cytoreductive procedure Data suggests secondary cytoreduction superior chemotherapy alone patients significant disease free interval months Survival secondary cytoreduction optimized cytoreduction microscopic disease yet recognized risk surgical morbidity Therefore strong relationship gynecologic oncology surgeon patient key obtaining appropriate informed consent relaying appropriate outcome expectations', 'Visual loss intoxication The Foster Kennedy Syndrome uncommon The literature cited often confusing This paper presents lucid definition syndrome contemporary diagnostic value tenacious historical roots We attempt sort true Foster Kennedy syndrome congeners discuss misnomers involved An illustrative case presented', 'Mixed cryoglobulinemia chronic hepatitis infection clinicopathologic analysis cases review recent literature We present cases mixed cryoglobulinemia patients infected hepatitis including pertinent clinical serologic pathological data The findings attributable MC appear similar patients HCV infected unknown HCV status The prevalence MC HCV infected patients appears lower region southern Europe although difference due requirement patients included study cryocrit least In patients cryoglobulins shown deposited skin kidney liver The mechanisms HCV MC related remain uncertain Although others evidence enrichment HCV RNA cryoprecipitates patients always case yet clear finding fundamental pathogenic importance Finally appears patients HCV MC may beneficial clinical response vasculitic symptoms therapy alpha interferon well glucocorticoids immunosuppressants In group predictors apparent distinguish responders nonresponders treatment Similarly duration response remains determined', 'Severe venous congestive encephalopathy secondary dialysis arteriovenous graft The clinical presentation imaging venous congestive encephalopathy venous congestive encephalopathy mimic several neurological conditions making diagnosis challenging We report patient end stage renal disease dialysis presented right occipital infarction The patient developed progressive encephalopathy increased intracranial pressure Extensive imaging electroencephalography serum analysis explain cause infarction progressive neurological deterioration Finally cerebral angiography venography demonstrated severe generalized venous congestive encephalopathy due arterial shunting right upper extremity arteriovenous graft extremity arteriovenous graft occluded right innominate venous trunk The right arm shunt resulted severe cerebral venous hypertension due ipsilateral occlusion innominate venous trunk After extremity arteriovenous graft repaired cerebral venous hypertension resolved patient returned baseline', 'The effect pregnancy HIV infected women The diagnosis HIV pregnant women pregnancy HIV positive woman associated affective disorder caring present major challenge physicians health care workers The associated somatic symptoms cause serious morbidity This study aims explore mental health pregnant HIV infected women The study took place University Teaching Hospital Lusaka Zambia satellite clinics Study participants either pregnant women diagnosed HIV course pregnancy women knew HIV positive status prior becoming pregnant questionnaire used assess psychological state women The majority women showed major depressive episodes significant suicidal thoughts About women whose HIV diagnosed pregnancy showed signs somatic illness Those knew HIV status becoming pregnant show severe depressive episodes showed anxiety HIV status babies This suggests women discover HIV status course pregnancy liable develop major depressive illness somatic disorders Physicians dealing women need cognisant strain HIV add pregnancy psychological psychiatric support may require', 'Risk factors cancer oesophagus Kerala India case control study oesophageal cancer carried Trivandrum Kerala involving cases controls Risk factors studied males pan betel tobacco chewing bidi cigarette smoking drinking alcohol taking snuff Only pan tobacco chewing investigated females indulged habits Among males significant associations higher risk observed bidi smoking less bidi plus cigarette smoking greater drinking alcohol less While significant effect duration pan tobacco chewing less observed males significant trend risk first falling rising duration use increased This partly due confounding smoking No effect pan tobacco use observed females step wise model fitted retaining risk factors significant adjusted factors risk factors included duration pan tobacco chewing duration bidi smoking daily frequency bidi cigarette smoking alcohol use yes An adjusted relative risk observed pan tobacco habit years duration years bidi smoking bidis cigarettes per day regular alcohol use category relative baseline never indulging relevant habit']

map(capitalize, stopwords.words('english'))
 7/8:
# l = ['Buccal cellulitis reevaluated We studied children prospectively acute buccal cellulitis The median age months Fifty five percent patients bacteremic three children without meningeal signs symptoms concomitant meningitis Cellulitis aspirate cultures eight positive urine bacterial antigen tests positive useful making etiologic diagnosis Infections due bacteria clinically indistinguishable due Haemophilus influenzae type The right cheek affected often left patients otitis media ipsilateral involved cheek The pathogenesis buccal cellulitis likely involves direct mucous membrane invasion rather spread ipsilateral middle ear', 'Gas absorption middle ear The middle ear middle ear pressure alert anesthetized rhesus monkeys monitored serial tympanograms order determine gas absorption process middle ear cavity During four hour observation period middle ear pressure showed small random fluctuations around zero pressure alert animals whereas anesthetized animals following initial positive pressure phase middle ear pressure reached plateau average pressure mm HO The middle ear gas composition changed air oxygen gas politzerization anesthetized animals determine effect different initial gas compositions rate absorption gas magnitude middle ear pressure Politzerization air oxygen gas resulted average maximum middle ear pressures mm HO respectively The experimental evidence suggested physiological composition middle ear gases nearly surrounding environment rate gas absorption low When gas composition middle ear changed middle ear pressure decreases considerably Based findings possible mechanism development negative middle ear pressure proposed', 'Potential levocetirizine relief nasal congestion Nasal congestion common troublesome symptom allergic rhinitis Because impairs natural human drive nasal breathing addition leads lower self esteem impaired quality life It symptom difficult treat Traditionally intranasal steroids potent anti inflammatory properties vasoconstrictors utilized relieving nasal passages inflamed congested mucosal tissues Recent studies last generation antihistamines demonstrated decongestant properties antihistamines acute seasonal allergic rhinitis chronic lasting perennial allergic rhinitis This study aims review efficacy potent antihistamine levocetirizine relieving nasal congestion reported various studies settings Comparisons placebo antihistamines presented order help general medical practitioners differentiate properties various available antihistamines', 'Placental ischemia induced increases brain water content cerebrovascular permeability role tumor necrosis factor Cerebrovascular complications increased risk encephalopathies characteristic preeclampsia contribute preeclampsia eclampsia related deaths Circulating tumor necrosis factor tumor necrosis factor elevated preeclamptic women infusion tumor necrosis factor pregnant rats mimics characteristics preeclampsia While suggests tumor necrosis factor mechanistic role promote preeclampsia impact tumor necrosis factor cerebral vasculature pregnancy remains unclear We tested hypothesis tumor necrosis factor contributes cerebrovascular abnormalities placental ischemia first infusing tumor necrosis factor pregnant rats ng day ip gestational day levels mimic reported preeclamptic women tumor necrosis factor increased mean arterial pressure mean arterial pressure brain water content anterior cerebrum however tumor necrosis factor infusion effect blood brain barrier blood brain barrier permeability anterior cerebrum posterior cerebrum We assessed role endogenous tumor necrosis factor mediating abnormalities model placental ischemia induced reducing uterine perfusion pressure followed treatment soluble tumor necrosis factor receptor etanercept mg kg sc gestational day Etanercept reduced placental ischemia mediated increases mean arterial pressure anterior brain water content blood brain barrier permeability placental ischemic rats normal pregnant rats Our results indicate tumor necrosis factor mechanistically contributes cerebral edema increasing blood brain barrier permeability underlying factor development cerebrovascular abnormalities associated preeclampsia complicated placental ischemia', 'Recurrent ovarian carcinoma place surgeryThe role cytoreductive surgery well established patients primary ovarian carcinoma Minimal residual disease translates improved response adjuvant treatment prolonged survival For close clinical follow different approaches may helpful detecting recurrent disease including regular physical pelvic examination serial CA levels imaging studies using computerized tomography magnetic resonance imaging positron emission testing At recurrence patients good performance status good response primary therapy macronodular tumor distribution pattern may candidates secondary cytoreductive procedure Data suggests secondary cytoreduction superior chemotherapy alone patients significant disease free interval months Survival secondary cytoreduction optimized cytoreduction microscopic disease yet recognized risk surgical morbidity Therefore strong relationship gynecologic oncology surgeon patient key obtaining appropriate informed consent relaying appropriate outcome expectations', 'Visual loss intoxication The Foster Kennedy Syndrome uncommon The literature cited often confusing This paper presents lucid definition syndrome contemporary diagnostic value tenacious historical roots We attempt sort true Foster Kennedy syndrome congeners discuss misnomers involved An illustrative case presented', 'Mixed cryoglobulinemia chronic hepatitis infection clinicopathologic analysis cases review recent literature We present cases mixed cryoglobulinemia patients infected hepatitis including pertinent clinical serologic pathological data The findings attributable MC appear similar patients HCV infected unknown HCV status The prevalence MC HCV infected patients appears lower region southern Europe although difference due requirement patients included study cryocrit least In patients cryoglobulins shown deposited skin kidney liver The mechanisms HCV MC related remain uncertain Although others evidence enrichment HCV RNA cryoprecipitates patients always case yet clear finding fundamental pathogenic importance Finally appears patients HCV MC may beneficial clinical response vasculitic symptoms therapy alpha interferon well glucocorticoids immunosuppressants In group predictors apparent distinguish responders nonresponders treatment Similarly duration response remains determined', 'Severe venous congestive encephalopathy secondary dialysis arteriovenous graft The clinical presentation imaging venous congestive encephalopathy venous congestive encephalopathy mimic several neurological conditions making diagnosis challenging We report patient end stage renal disease dialysis presented right occipital infarction The patient developed progressive encephalopathy increased intracranial pressure Extensive imaging electroencephalography serum analysis explain cause infarction progressive neurological deterioration Finally cerebral angiography venography demonstrated severe generalized venous congestive encephalopathy due arterial shunting right upper extremity arteriovenous graft extremity arteriovenous graft occluded right innominate venous trunk The right arm shunt resulted severe cerebral venous hypertension due ipsilateral occlusion innominate venous trunk After extremity arteriovenous graft repaired cerebral venous hypertension resolved patient returned baseline', 'The effect pregnancy HIV infected women The diagnosis HIV pregnant women pregnancy HIV positive woman associated affective disorder caring present major challenge physicians health care workers The associated somatic symptoms cause serious morbidity This study aims explore mental health pregnant HIV infected women The study took place University Teaching Hospital Lusaka Zambia satellite clinics Study participants either pregnant women diagnosed HIV course pregnancy women knew HIV positive status prior becoming pregnant questionnaire used assess psychological state women The majority women showed major depressive episodes significant suicidal thoughts About women whose HIV diagnosed pregnancy showed signs somatic illness Those knew HIV status becoming pregnant show severe depressive episodes showed anxiety HIV status babies This suggests women discover HIV status course pregnancy liable develop major depressive illness somatic disorders Physicians dealing women need cognisant strain HIV add pregnancy psychological psychiatric support may require', 'Risk factors cancer oesophagus Kerala India case control study oesophageal cancer carried Trivandrum Kerala involving cases controls Risk factors studied males pan betel tobacco chewing bidi cigarette smoking drinking alcohol taking snuff Only pan tobacco chewing investigated females indulged habits Among males significant associations higher risk observed bidi smoking less bidi plus cigarette smoking greater drinking alcohol less While significant effect duration pan tobacco chewing less observed males significant trend risk first falling rising duration use increased This partly due confounding smoking No effect pan tobacco use observed females step wise model fitted retaining risk factors significant adjusted factors risk factors included duration pan tobacco chewing duration bidi smoking daily frequency bidi cigarette smoking alcohol use yes An adjusted relative risk observed pan tobacco habit years duration years bidi smoking bidis cigarettes per day regular alcohol use category relative baseline never indulging relevant habit']

[x.capitalize() for x in stopwords.words('english')]
 7/9:
def abbr_expander(s):
    ABBR_LIST = defaultdict(list)
    
    res = re.findall(r'\(\s?[A-Z]\s?[A-Za-z]+\s?\)', s) # Misses acronyms with hyphen
    outs = s
    retstr = ''
    for abbr in res:
        ix = s.index(abbr)
        abbr_clean = re.sub(r'[^A-Za-z]', '', abbr) # keep only letters, discard all other chars
        abbr_len = len(re.sub(r'[a-z]','',abbr_clean)) # Number of capital letters ~ number of tokens to look back for expansion
        if abbr_clean[-1]=='s': abbr_len-=1 # Singularing a plural acronym
        # When looking for expansion candidates, consider (-) and (/) as word-breaks
        expanded = ' '.join(s[ix-1:0:-1][::-1].rstrip().split(' ')[-abbr_len:]) # identify possible candidates for expansion from preivous abbr_len words
        ABBR_LIST[abbr_clean].append(expanded)
        outs = outs.replace(abbr_clean, expanded) # Remove first instance of abbreviation before expanding other instances
        if abbr_clean[-1]=='s':
            outs = outs.replace(abbr_clean[:-1], expanded)
    return outs
7/10:
s='alpha beta (AB) is AB'
abbr_expander(s)
7/11:
ABBR_LIST = defaultdict(list)
def abbr_expander(s):   
    res = re.findall(r'\(\s?[A-Z]\s?[A-Za-z]+\s?\)', s) # Misses acronyms with hyphen
    outs = s
    retstr = ''
    for abbr in res:
        ix = s.index(abbr)
        abbr_clean = re.sub(r'[^A-Za-z]', '', abbr) # keep only letters, discard all other chars
        abbr_len = len(re.sub(r'[a-z]','',abbr_clean)) # Number of capital letters ~ number of tokens to look back for expansion
        if abbr_clean[-1]=='s': abbr_len-=1 # Singularing a plural acronym
        # When looking for expansion candidates, consider (-) and (/) as word-breaks
        expanded = ' '.join(s[ix-1:0:-1][::-1].rstrip().split(' ')[-abbr_len:]) # identify possible candidates for expansion from preivous abbr_len words
        ABBR_LIST[abbr_clean].append(expanded)
        outs = outs.replace(abbr_clean, expanded) # Remove first instance of abbreviation before expanding other instances
        if abbr_clean[-1]=='s':
            outs = outs.replace(abbr_clean[:-1], expanded)
    return outs
7/12:
s='alpha beta (AB) is AB'
abbr_expander(s)
7/13:
# s='alpha beta (AB) is AB'
# abbr_expander(s)
ABBR_LIST
7/14:
s='alpha beta (AB) is AB'
re.findall(r'\(\s?[A-Z]\s?[A-Za-z]+\s?\)', s)
7/15:
s='alpha beta (AB) is AB'

s.index('(AB)')
7/16:
s='alpha beta (AB) is AB'

ix = s.index('(AB)')
s[ix-1:0:-1][::-1].rstrip().split(' ')[-abbr_len:]
7/17:
s='alpha beta (AB) is AB'

ix = s.index('(AB)')
abbr_clean = re.sub(r'[^A-Za-z]', '', abbr) # keep only letters, discard all other chars
abbr_len = len(re.sub(r'[a-z]','',abbr_clean)) # Number of capital letters ~ number of tokens to look back for expansion
if abbr_clean[-1]=='s': abbr_len-=1 # Singularing a plural acronym
s[ix-1:0:-1][::-1].rstrip().split(' ')[-abbr_len:]
7/18:
s='alpha beta (AB) is AB'
abbr='(AB)'
ix = s.index(abbr)
abbr_clean = re.sub(r'[^A-Za-z]', '', abbr) # keep only letters, discard all other chars
abbr_len = len(re.sub(r'[a-z]','',abbr_clean)) # Number of capital letters ~ number of tokens to look back for expansion
if abbr_clean[-1]=='s': abbr_len-=1 # Singularing a plural acronym
s[ix-1:0:-1][::-1].rstrip().split(' ')[-abbr_len:]
7/19:
s='alpha beta (AB) is AB'
abbr='(AB)'
ix = s.index(abbr)
abbr_clean = re.sub(r'[^A-Za-z]', '', abbr) # keep only letters, discard all other chars
abbr_len = len(re.sub(r'[a-z]','',abbr_clean)) # Number of capital letters ~ number of tokens to look back for expansion
if abbr_clean[-1]=='s': abbr_len-=1 # Singularing a plural acronym
s[ix:0:-1][::-1].rstrip().split(' ')[-abbr_len:]
7/20: s[ix]
7/21: s[ix-1:0]
7/22: s[ix-1:0:-1]
7/23: s[ix-1::-1]
7/24:
s='alpha beta (AB) is AB'
abbr='(AB)'
ix = s.index(abbr)
abbr_clean = re.sub(r'[^A-Za-z]', '', abbr) # keep only letters, discard all other chars
abbr_len = len(re.sub(r'[a-z]','',abbr_clean)) # Number of capital letters ~ number of tokens to look back for expansion
if abbr_clean[-1]=='s': abbr_len-=1 # Singularing a plural acronym
s[ix-1::-1][::-1].rstrip().split(' ')[-abbr_len:]
7/25:
ABBR_LIST = defaultdict(list)
def abbr_expander(s):   
    res = re.findall(r'\(\s?[A-Z]\s?[A-Za-z]+\s?\)', s) # Misses acronyms with hyphen
    outs = s
    retstr = ''
    for abbr in res:
        ix = s.index(abbr)
        abbr_clean = re.sub(r'[^A-Za-z]', '', abbr) # keep only letters, discard all other chars
        abbr_len = len(re.sub(r'[a-z]','',abbr_clean)) # Number of capital letters ~ number of tokens to look back for expansion
        if abbr_clean[-1]=='s': abbr_len-=1 # Singularing a plural acronym
        # When looking for expansion candidates, consider (-) and (/) as word-breaks
        expanded = ' '.join(s[ix-1::-1][::-1].rstrip().split(' ')[-abbr_len:]) # identify possible candidates for expansion from preivous abbr_len words
        ABBR_LIST[abbr_clean].append(expanded)
        outs = outs.replace(abbr_clean, expanded) # Remove first instance of abbreviation before expanding other instances
        if abbr_clean[-1]=='s':
            outs = outs.replace(abbr_clean[:-1], expanded)
    return outs
7/26: abbr_expander(s)
7/27:
ABBR_LIST = defaultdict(list)
def abbr_expander(s):   
    res = re.findall(r'\(\s?[A-Z]\s?[A-Za-z]+\s?\)', s) # Misses acronyms with hyphen
    outs = s
    retstr = ''
    for abbr in res:
        ix = s.index(abbr)
        abbr_clean = re.sub(r'[^A-Za-z]', '', abbr) # keep only letters, discard all other chars
        abbr_len = len(re.sub(r'[a-z]','',abbr_clean)) # Number of capital letters ~ number of tokens to look back for expansion
        if abbr_clean[-1]=='s': abbr_len-=1 # Singularing a plural acronym
        # When looking for expansion candidates, consider (-) and (/) as word-breaks
        expanded = ' '.join(s[ix-1::-1][::-1].rstrip().split(' ')[-abbr_len:]) # identify possible candidates for expansion from preivous abbr_len words
        ABBR_LIST[abbr_clean].append(expanded)
        outs = outs.replace(abbr, '') # Remove first instance of abbreviation before expanding other instances
        if abbr_clean[-1]=='s':
            outs = outs.replace(abbr_clean[:-1], expanded)
        else:
            outs = outs.replace(abbr_clean, expanded) 
        
    return outs
7/28: abbr_expander(s)
7/29:
ABBR_LIST = defaultdict(list)
def abbr_expander(s):   
    res = re.findall(r'\(\s?[A-Z]\s?[A-Za-z]+\s?\)', s) # Misses acronyms with hyphen
    outs = s
    retstr = ''
    for abbr in res:
        ix = s.index(abbr)
        abbr_clean = re.sub(r'[^A-Za-z]', '', abbr) # keep only letters, discard all other chars
        abbr_len = len(re.sub(r'[a-z]','',abbr_clean)) # Number of capital letters ~ number of tokens to look back for expansion
        if abbr_clean[-1]=='s': abbr_len-=1 # Singularing a plural acronym
        # When looking for expansion candidates, consider (-) and (/) as word-breaks
        expanded = ' '.join(s[ix-1::-1][::-1].rstrip().split(' ')[-abbr_len:]) # identify possible candidates for expansion from preivous abbr_len words
        ABBR_LIST[abbr_clean].append(expanded)
        outs = outs.replace(' '+abbr, '') # Remove first instance of abbreviation before expanding other instances
        if abbr_clean[-1]=='s':
            outs = outs.replace(abbr_clean[:-1], expanded)
        else:
            outs = outs.replace(abbr_clean, expanded) 
        
    return outs
7/30: abbr_expander(s)
7/31:
def clean_line_generator():
    try:
        c = data_df.content
    except NameError:
        data_df = pd.read_pickle('data/data_df.pkl')
    for abst in iter(data_df.content):
        abst1 = remove_stopwords(abst)
        outs = abbr_expander(abst1)
        f.write(outs+'\n')
    
with open('one-abstract-per-line.txt','w') as f:
    clean_line_generator()
7/32: s=abst.lower()
7/33: s.lower()
7/34:
s='sdf. Tgds SDFS'
re.findall(r"(?<=\. )[A-Z]",s)
7/35:
s='sdf. Tgds SDFS'
re.sub(r"(?<=\. )[A-Z]",r'sadf\1',s)
7/36:
s='sdf. Tgds SDFS'
re.sub(r"(?<=\. )[A-Z]",lambda t:t.lower(), s)
7/37:
s='sdf. Tgds SDFS'
# re.sub(r"(?<=\. )[A-Z]",lambda t:t.lower(), s)
re.match(r"(?<=\. )[A-Z]",s)
7/38:
s='sdf. Tgds SDFS'
# re.sub(r"(?<=\. )[A-Z]",lambda t:t.lower(), s)
m=re.match(r"(?<=\. )[A-Z]",s)
7/39:
s='sdf. Tgds SDFS'
# re.sub(r"(?<=\. )[A-Z]",lambda t:t.lower(), s)
m=re.match(r"(?<=\. )[A-Z]",s)
m.
7/40:
s='sdf. Tgds SDFS'
# re.sub(r"(?<=\. )[A-Z]",lambda t:t.lower(), s)
m=re.match(r"(?<=\. )[A-Z]",s)
m
7/41:
s='sdf. Tgds SDFS'
re.sub(r"(?<=\. )[A-Z]",lambda t:t.group().lower(), s)
7/42:
def clean_line_generator():
    data_df = pd.read_pickle('data/data_df.pkl')
    for abst in iter(data_df.content):
        abst = abbr_expander(abst)
        # Make post-period uppercase chars lower
        abst = re.sub(r"(?<=\. )[A-Z]",lambda t:t.group().lower(), abst)
        abst = remove_stopwords(abst)
#         f.write(abst+'\n')
        print(abst[:100])
    
# with open('one-abstract-per-line.txt','w') as f:
#     clean_line_generator()
7/43:
pprint off
nltk.stopwords('english')
7/44:

nltk.stopwords('english')
7/45:

stopwords.words('english')
7/46:
nltk_stopwords = ['i',  'me',  'my',  'myself',  'we',  'our',  'ours',  'ourselves',  'you',  "you're",  "you've",  "you'll",  "you'd",  'your',  'yours',  'yourself',  'yourselves',  'he',  'him',  'his',  'himself',  'she',  "she's",  'her',  'hers',  'herself',  'it',  "it's",  'its',  'itself',  'they',  'them',  'their',  'theirs',  'themselves',  'what',  'which',  'who',  'whom',  'this',  'that',  "that'll",  'these',  'those',  'am',  'is',  'are',  'was',  'were',  'be',  'been',  'being',  'have',  'has',  'had',  'having',  'do',  'does',  'did',  'doing',  'a',  'an',  'the',  'and',  'but',  'if',  'or',  'because',  'as',  'until',  'while',  'of',  'at',  'by',  'for',  'with',  'about',  'against',  'between',  'into',  'through',  'during',  'before',  'after',  'above',  'below',  'to',  'from',  'up',  'down',  'in',  'out',  'on',  'off',  'over',  'under',  'again',  'further',  'then',  'once',  'here',  'there',  'when',  'where',  'why',  'how',  'all',  'any',  'both',  'each',  'few',  'more',  'most',  'other',  'some',  'such',  'no',  'nor',  'not',  'only',  'own',  'same',  'so',  'than',  'too',  'very',  's',  't',  'can',  'will',  'just',  'don',  "don't",  'should',  "should've",  'now',  'd',  'll',  'm',  'o',  're',  've',  'y',  'ain',  'aren',  "aren't",  'couldn',  "couldn't",  'didn',  "didn't",  'doesn',  "doesn't",  'hadn',  "hadn't",  'hasn',  "hasn't",  'haven',  "haven't",  'isn',  "isn't",  'ma',  'mightn',  "mightn't",  'mustn',  "mustn't",  'needn',  "needn't",  'shan',  "shan't",  'shouldn',  "shouldn't",  'wasn',  "wasn't",  'weren',  "weren't",  'won',  "won't",  'wouldn',  "wouldn't"]
nltk_stopwords+["'s", "s"]+[x.capitalize() for x in nltk_stopwords]
7/47:
from nltk import word_tokenize
from nltk.corpus import stopwords
import pandas as pd
import re
import numpy as np
from collections import defaultdict

from gensim.test.utils import common_texts
from gensim.models import Word2Vec
7/48:
from nltk import word_tokenize
from nltk.corpus import stopwords
import pandas as pd
import re
import numpy as np
from collections import defaultdict

from gensim.test.utils import common_texts
from gensim.models import Word2Vec
7/49:
from nltk import word_tokenize
from nltk.corpus import stopwords
import pandas as pd
import re
import numpy as np
from collections import defaultdict

from gensim.test.utils import common_texts
from gensim.models import Word2Vec
7/50:


class sentGenerator:
    def __init__(self, f, limit=None):
        self.f=f
        self.li = limit
    def __iter__(self):
        c=0
        for l in open(self.f):
            if self.li and c>=self.li:
                break
            c+=1
            yield l.split()
7/51:
start = time.time()
fname = 'data/oapl_sample.txt'
sg=sentGen(fname)
model = Word2Vec(s, size=100, window=5, min_count=1, workers=4)
word_vectors = model.wv

duration = time.time()-start
print(duration)
7/52:
import time
start = time.time()
fname = 'data/oapl_sample.txt'
sg=sentGen(fname)
model = Word2Vec(s, size=100, window=5, min_count=1, workers=4)
word_vectors = model.wv

duration = time.time()-start
print(duration)
7/53:


class sentenceGenerator:
    def __init__(self, f, limit=None):
        self.f=f
        self.li = limit
    def __iter__(self):
        c=0
        for l in open(self.f):
            if self.li and c>=self.li:
                break
            c+=1
            yield l.split()
7/54:
import time
start = time.time()
fname = 'data/oapl_sample.txt'
sg=sentenceGenerator(fname)
model = Word2Vec(s, size=100, window=5, min_count=1, workers=4)
word_vectors = model.wv

duration = time.time()-start
print(duration)
7/55: word_vectors.vocab
7/56:
import time
start = time.time()
fname = 'data/oapl_sample.txt'
sg=sentenceGenerator(fname)
model = Word2Vec(sg, size=100, window=5, min_count=1, workers=4)
word_vectors = model.wv

duration = time.time()-start
print(duration)
7/57: word_vectors.vocab
7/58: word_vectors.most_similar('levocetirizine')
7/59: word_vectors.most_similar('cerebral')
7/60:
import time
start = time.time()
fname = 'data/oapl_sample.txt'
sg=sentenceGenerator(fname)
model = Word2Vec(sg, size=100, window=5, min_count=1, workers=4)
word_vectors = model.wv

duration = time.time()-start
print(duration)
7/61: word_vectors.vocab
7/62: word_vectors.most_similar('cerebral')
7/63:
import random
import time
import matplotlib.pyplot as plt

def quicksort(arr):
   def split(arr, left, right):
    i = left-1
    j = left
    while j<right:
      if arr[j]<=arr[right]:
        i+=1
        arr[i], arr[j] = arr[j], arr[i]
      j+=1
    arr[i+1], arr[right] = arr[right],arr[i+1]
    s = i+1
    return s

  left = 0
  right = len(arr) - 1
  ix_to_visit = []
  ix_to_visit.append(left)
  ix_to_visit.append(right)

  while len(ix_to_visit) > 0:
    right = ix_to_visit.pop()
    left = ix_to_visit.pop()
    s = split(arr, left, right)
    if s - 1 > left:
        ix_to_visit.append(left)
        ix_to_visit.append(s - 1)
    if s + 1 < right:
        ix_to_visit.append(s + 1)
        ix_to_visit.append(right)
  
  return arr



def timeit():
  times = []
  N = range(100, 1100, 100)
  for n in N:
    arr = [random.randint(1,100) for x in range(n)]
    start = time.time()
    quicksort(arr)
    duration = time.time()-start
    times.append(duration)
  plt.plot(N, times)
7/65:
import random
import time
import matplotlib.pyplot as plt

def quicksort(arr):
  def split(arr, left, right):
    i = left-1
    j = left
    while j<right:
      if arr[j]<=arr[right]:
        i+=1
        arr[i], arr[j] = arr[j], arr[i]
      j+=1
    arr[i+1], arr[right] = arr[right],arr[i+1]
    s = i+1
    return s

  left = 0
  right = len(arr) - 1
  ix_to_visit = []
  ix_to_visit.append(left)
  ix_to_visit.append(right)

  while len(ix_to_visit) > 0:
    right = ix_to_visit.pop()
    left = ix_to_visit.pop()
    s = split(arr, left, right)
    if s - 1 > left:
        ix_to_visit.append(left)
        ix_to_visit.append(s - 1)
    if s + 1 < right:
        ix_to_visit.append(s + 1)
        ix_to_visit.append(right)
  
  return arr



def timeit():
  times = []
  N = range(100, 1100, 100)
  for n in N:
    arr = [random.randint(1,100) for x in range(n)]
    start = time.time()
    quicksort(arr)
    duration = time.time()-start
    times.append(duration)
  plt.plot(N, times)
7/66: timeit()
7/67:
import random
import time
import matplotlib.pyplot as plt

def quicksort(arr):
  def split(arr, left, right):
    i = left-1
    j = left
    while j<right:
      if arr[j]<=arr[right]:
        i+=1
        arr[i], arr[j] = arr[j], arr[i]
      j+=1
    arr[i+1], arr[right] = arr[right],arr[i+1]
    s = i+1
    return s

  left = 0
  right = len(arr) - 1
  ix_to_visit = []
  ix_to_visit.append(left)
  ix_to_visit.append(right)

  while len(ix_to_visit) > 0:
    right = ix_to_visit.pop()
    left = ix_to_visit.pop()
    s = split(arr, left, right)
    if s - 1 > left:
        ix_to_visit.append(left)
        ix_to_visit.append(s - 1)
    if s + 1 < right:
        ix_to_visit.append(s + 1)
        ix_to_visit.append(right)
  
  return arr



def timeit():
  times = []
  N = range(1000, 11000, 1000)
  for n in N:
    arr = [random.randint(1,100) for x in range(n)]
    start = time.time()
    quicksort(arr)
    duration = time.time()-start
    times.append(duration)
  plt.plot(N, times)
7/68: timeit()
7/69:
import random
import time
import matplotlib.pyplot as plt

def quicksort(arr):
  def split(arr, left, right):
    i = left-1
    j = left
    while j<right:
      if arr[j]<=arr[right]:
        i+=1
        arr[i], arr[j] = arr[j], arr[i]
      j+=1
    arr[i+1], arr[right] = arr[right],arr[i+1]
    s = i+1
    return s

  left = 0
  right = len(arr) - 1
  ix_to_visit = []
  ix_to_visit.append(left)
  ix_to_visit.append(right)

  while len(ix_to_visit) > 0:
    right = ix_to_visit.pop()
    left = ix_to_visit.pop()
    s = split(arr, left, right)
    if s - 1 > left:
        ix_to_visit.append(left)
        ix_to_visit.append(s - 1)
    if s + 1 < right:
        ix_to_visit.append(s + 1)
        ix_to_visit.append(right)
  
  return arr



def timeit():
  times = []
  N = range(1000, 66000, 1000)
  for n in N:
    arr = [random.randint(1,100) for x in range(n)]
    start = time.time()
    quicksort(arr)
    duration = time.time()-start
    times.append(duration)
  plt.plot(N, times)
7/70: timeit()
7/71:
import random
import time
import matplotlib.pyplot as plt

def quicksort(arr):
  def split(arr, left, right):
    i = left-1
    j = left
    while j<right:
      if arr[j]<=arr[right]:
        i+=1
        arr[i], arr[j] = arr[j], arr[i]
      j+=1
    arr[i+1], arr[right] = arr[right],arr[i+1]
    s = i+1
    return s

  left = 0
  right = len(arr) - 1
  ix_to_visit = []
  ix_to_visit.append(left)
  ix_to_visit.append(right)

  while len(ix_to_visit) > 0:
    right = ix_to_visit.pop()
    left = ix_to_visit.pop()
    s = split(arr, left, right)
    if s - 1 > left:
        ix_to_visit.append(left)
        ix_to_visit.append(s - 1)
    if s + 1 < right:
        ix_to_visit.append(s + 1)
        ix_to_visit.append(right)
  
  return arr



def timeit():
  times = []
  N = range(100, 6600, 100)
  for n in N:
    arr = [random.randint(1,100) for x in range(n)]
    start = time.time()
    quicksort(arr)
    duration = time.time()-start
    times.append(duration)
  plt.plot(N, times)
7/72: timeit()
7/73:
import random
import time
import matplotlib.pyplot as plt

def quicksort(arr):
  def split(arr, left, right):
    i = left-1
    j = left
    while j<right:
      if arr[j]<=arr[right]:
        i+=1
        arr[i], arr[j] = arr[j], arr[i]
      j+=1
    arr[i+1], arr[right] = arr[right],arr[i+1]
    s = i+1
    return s

  left = 0
  right = len(arr) - 1
  ix_to_visit = []
  ix_to_visit.append(left)
  ix_to_visit.append(right)

  while len(ix_to_visit) > 0:
    right = ix_to_visit.pop()
    left = ix_to_visit.pop()
    s = split(arr, left, right)
    if s - 1 > left:
        ix_to_visit.append(left)
        ix_to_visit.append(s - 1)
    if s + 1 < right:
        ix_to_visit.append(s + 1)
        ix_to_visit.append(right)
  
  return arr



def timeit():
  times = []
  N = range(1000, 10000, 1000)
  for n in N:
    arr = [random.randint(1,100) for x in range(n)]
    start = time.time()
    quicksort(arr)
    duration = time.time()-start
    times.append(duration)
  plt.plot(N, times)
7/74: timeit()
7/75:
import random
import time
import matplotlib.pyplot as plt

def quicksort(arr):
  def split(arr, left, right):
    i = left-1
    j = left
    while j<right:
      if arr[j]<=arr[right]:
        i+=1
        arr[i], arr[j] = arr[j], arr[i]
      j+=1
    arr[i+1], arr[right] = arr[right],arr[i+1]
    s = i+1
    return s

  left = 0
  right = len(arr) - 1
  ix_to_visit = []
  ix_to_visit.append(left)
  ix_to_visit.append(right)

  while len(ix_to_visit) > 0:
    right = ix_to_visit.pop()
    left = ix_to_visit.pop()
    s = split(arr, left, right)
    if s - 1 > left:
        ix_to_visit.append(left)
        ix_to_visit.append(s - 1)
    if s + 1 < right:
        ix_to_visit.append(s + 1)
        ix_to_visit.append(right)
  
  return arr



def timeit():
  times = []
  N = range(1000, 10000, 1000)
  for n in N:
    arr = [random.randint(1,100) for x in range(n)]
    start = time.time()
    quicksort(arr)
    duration = time.time()-start
    times.append(duration)
  plt.plot(N, times)
  print(list(zip(N,times)))
7/76: timeit()
7/77: timeit()
7/78:
import random
import time
import matplotlib.pyplot as plt

def quicksort(arr):
  def split(arr, left, right):
    i = left-1
    j = left
    while j<right:
      if arr[j]<=arr[right]:
        i+=1
        arr[i], arr[j] = arr[j], arr[i]
      j+=1
    arr[i+1], arr[right] = arr[right],arr[i+1]
    s = i+1
    return s

  left = 0
  right = len(arr) - 1
  ix_to_visit = []
  ix_to_visit.append(left)
  ix_to_visit.append(right)

  while len(ix_to_visit) > 0:
    right = ix_to_visit.pop()
    left = ix_to_visit.pop()
    s = split(arr, left, right)
    if s - 1 > left:
        ix_to_visit.append(left)
        ix_to_visit.append(s - 1)
    if s + 1 < right:
        ix_to_visit.append(s + 1)
        ix_to_visit.append(right)
  
  return arr



def timeit():
  times = []
  N = range(1000, 10000, 1000)
  for n in N:
    arr = sorted([random.randint(1,100) for x in range(n)], reverse=True)
    start = time.time()
    quicksort(arr)
    duration = time.time()-start
    times.append(duration)
  plt.plot(N, times)
  print(list(zip(N,times)))
7/79: timeit()
7/80:
df = pd.read_csv('data/output.psv', sep='|', error_bad_lines=False) #bad lines: [4399, 170914, 247161, 264812, 384966, 400521, 522365, 527597, 651243, 747036]

# at least one row had the title in label col. 
df.art_arttitle = np.where(df.art_arttitle.isnull() & df.label.notnull(), df.label, df.art_arttitle)

sectioned_abstracts = df[df.label.notnull()]
unsectioned_abstracts = df[~sectioned_abstracts.index]

# remove spurious newlines in unsectioned
unsectioned_str = unsectioned_abstracts.to_csv(index=False, sep='|')
unsectioned_str2 = re.sub(r'(?<!\|)\n', ' ', unsectioned_str)
7/81:
# df = pd.read_csv('data/output.psv', sep='|', error_bad_lines=False) #bad lines: [4399, 170914, 247161, 264812, 384966, 400521, 522365, 527597, 651243, 747036]

# # at least one row had the title in label col. 
# df.art_arttitle = np.where(df.art_arttitle.isnull() & df.label.notnull(), df.label, df.art_arttitle)

# sectioned_abstracts = df[df.label.notnull()]
unsectioned_abstracts = df.ix[~sectioned_abstracts.index]

# remove spurious newlines in unsectioned
unsectioned_str = unsectioned_abstracts.to_csv(index=False, sep='|')
unsectioned_str2 = re.sub(r'(?<!\|)\n', ' ', unsectioned_str)
7/82: sectioned_abstracts.index
7/83: df.ix[sectioned_abstracts.index]
7/84:
# df = pd.read_csv('data/output.psv', sep='|', error_bad_lines=False) #bad lines: [4399, 170914, 247161, 264812, 384966, 400521, 522365, 527597, 651243, 747036]

# # at least one row had the title in label col. 
# df.art_arttitle = np.where(df.art_arttitle.isnull() & df.label.notnull(), df.label, df.art_arttitle)

# sectioned_abstracts = df[df.label.notnull()]
unsectioned_abstracts = df[~df.isin(sectioned_abstracts)].dropna()

# remove spurious newlines in unsectioned
unsectioned_str = unsectioned_abstracts.to_csv(index=False, sep='|')
unsectioned_str2 = re.sub(r'(?<!\|)\n', ' ', unsectioned_str)
7/85: unsectioned_str2
7/86: unsectioned_str
7/87: unsectioned_abstracts
7/88:
# df = pd.read_csv('data/output.psv', sep='|', error_bad_lines=False) #bad lines: [4399, 170914, 247161, 264812, 384966, 400521, 522365, 527597, 651243, 747036]

# # at least one row had the title in label col. 
# df.art_arttitle = np.where(df.art_arttitle.isnull() & df.label.notnull(), df.label, df.art_arttitle)

# sectioned_abstracts = df[df.label.notnull()]
unsectioned_abstracts = df[~df.isin(sectioned_abstracts)]

# remove spurious newlines in unsectioned
unsectioned_str = unsectioned_abstracts.to_csv(index=False, sep='|')
unsectioned_str2 = re.sub(r'(?<!\|)\n', ' ', unsectioned_str)
7/89: unsectioned_str2
7/90: unsectioned_str2
7/91: unsectioned_str2[:1000]
7/92: unsectioned_str[:1000]
7/93: unsectioned_str.index('29045102')
7/94:
# df = pd.read_csv('data/output.psv', sep='|', error_bad_lines=False) #bad lines: [4399, 170914, 247161, 264812, 384966, 400521, 522365, 527597, 651243, 747036]

# # at least one row had the title in label col. 
# df.art_arttitle = np.where(df.art_arttitle.isnull() & df.label.notnull(), df.label, df.art_arttitle)

# sectioned_abstracts = df[df.label.notnull()]
# unsectioned_abstracts = df[~df.isin(sectioned_abstracts)]

# # remove spurious newlines in unsectioned
# unsectioned_str = unsectioned_abstracts.to_csv(index=False, sep='|')
unsectioned_str2 = re.sub(r'(?<!\|\r)\n', ' ', unsectioned_str)
7/95:
# unsectioned_str.index('29045102')
unsectioned_str2[79601856:79602656]
7/96:
# unsectioned_str.index('29045102')
unsectioned_str2[79601856:79603656]
7/97:
# df = pd.read_csv('data/output.psv', sep='|', error_bad_lines=False) #bad lines: [4399, 170914, 247161, 264812, 384966, 400521, 522365, 527597, 651243, 747036]

# # at least one row had the title in label col. 
# df.art_arttitle = np.where(df.art_arttitle.isnull() & df.label.notnull(), df.label, df.art_arttitle)

# unsectioned_str = unsectioned_abstracts.to_csv(index=False, sep='|')
with open('data/output.psv') as f:
    strr = f.read()

# unsectioned_str2 = re.sub(r'\n(?![0-9])', ' ', strr)
7/98:
# df = pd.read_csv('data/output.psv', sep='|', error_bad_lines=False) #bad lines: [4399, 170914, 247161, 264812, 384966, 400521, 522365, 527597, 651243, 747036]

# # at least one row had the title in label col. 
# df.art_arttitle = np.where(df.art_arttitle.isnull() & df.label.notnull(), df.label, df.art_arttitle)

# unsectioned_str = unsectioned_abstracts.to_csv(index=False, sep='|')
with open('data/output.psv') as f:
    strr = f.read()

unsectioned_str2 = re.sub(r'\n(?![0-9])', ' ', strr)
7/99:
# df = pd.read_csv('data/output.psv', sep='|', error_bad_lines=False) #bad lines: [4399, 170914, 247161, 264812, 384966, 400521, 522365, 527597, 651243, 747036]

# # at least one row had the title in label col. 
# df.art_arttitle = np.where(df.art_arttitle.isnull() & df.label.notnull(), df.label, df.art_arttitle)

# unsectioned_str = unsectioned_abstracts.to_csv(index=False, sep='|')
with open('data/output.psv') as f:
    strr = f.read()

unsectioned_str2 = re.sub(r'\n(?![0-9])', ' ', strr)
7/100:
# df = pd.read_csv('data/output.psv', sep='|', error_bad_lines=False) #bad lines: [4399, 170914, 247161, 264812, 384966, 400521, 522365, 527597, 651243, 747036]

# # at least one row had the title in label col. 
# df.art_arttitle = np.where(df.art_arttitle.isnull() & df.label.notnull(), df.label, df.art_arttitle)

# unsectioned_str = unsectioned_abstracts.to_csv(index=False, sep='|')
with open('data/output.psv', encoding="utf-8") as f:
    strr = f.read()

unsectioned_str2 = re.sub(r'\n(?![0-9])', ' ', strr)
7/101:
strr.index('29045102')
# unsectioned_str2[79601856:79603656]
7/102:
strr.index('29045102')
strr[149724033:149725033]
# unsectioned_str2[79601856:79603656]
7/103:
# strr.index('29045102')
# strr[149724033:149725033]
unsectioned_str2[149724033:149725033]
7/104:
import io
# strr.index('29045102')
# strr[149724033:149725033]
# unsectioned_str2[149724033:149725033]
df = pd.read_csv(io.StringIO(unsectioned_str2), sep='|', error_bad_lines=False)
df.head()
7/105: df[df.pmid=='29045102']
7/106: df[df.pmid==29045102]
7/107: df[df.pmid==29045102].values
7/108: df.pmid.str.numeric().sum()
7/109: df.art_arttitle.isnull().sum()
7/110: df[df.art_arttitle.isnull()]
7/111: df.columns
7/112:
import datetime
str(datetime.datetime.now())
7/113:
import datetime
str(datetime.datetime.now()).replace(' ','_')
7/114:
import datetime
str(datetime.datetime.now()).replace(' ','_').replace(':','')
7/115:
def get_df_from_psv(fout=None):
    with open('data/output.psv', encoding="utf-8") as f:
        strr = f.read()
    df_str = re.sub(r'\n(?![0-9])', ' ', strr)

    df = pd.read_csv(io.StringIO(df_str), sep='|', error_bad_lines=False) #bad lines: [4399, 170914, 247161, 264812, 384966, 400521, 522365, 527597, 651243, 747036]

    # at least one row had the title in label col. 
    df.art_arttitle = np.where(df.art_arttitle.isnull() & df.label.notnull(), df.label, df.art_arttitle)

    df.columns = ['pmid', 'title', 'abstract', 'label']

    # combine sectioned abstracts
    s=df.pmid.value_counts()
    mult_pmid = df[df.pmid.isin(s.index[s>1])]
    single_pmid = df[~df.pmid.isin(s.index[s>1])]
    mult_pmid2 = mult_pmid.groupby(['pmid']).abstract.transform(lambda x: ' '.join(x))
    mult_pmid['abstract'] = mult_pmid2
    mult_pmid = mult_pmid.drop_duplicates(subset=['pmid','abstract'])
    data_df = single_pmid.append(mult_pmid)

    data_df['content'] = np.where(data_df.title!=data_df.abstract, data_df.title+' '+data_df.abstract, data_df.abstract)

    if not fout:
        import datetime
        fout = "data/datadf_"+str(datetime.datetime.now()).replace(' ','_').replace(':','')+".pkl"
    data_df.to_pickle(fout)

    return data_df
7/116: stopwords.words('english')
 9/1: import pandas as pd
 9/2: df = pd.read_csv('linear_regression.csv')
 9/3: params = [1,1,1,1]
 9/4: x1 = df.head(1).values[:-1]
 9/5: df.head(1)
 9/6: x1 = df.head(1).values
 9/7: x1 = df.head(1).values[0][-1]
 9/8: x1 = df.head(1).values[0][:-1]
 9/9: x2 = df.iloc[1].values[0][:-1]
9/10: df.index
9/11: x2 = df.loc[1].values[0][:-1]
9/12: x2 = df.loc[1].values
9/13: x2 = df.loc[1].values[:-1]
9/14: func1(params, x1)
9/15:
"""
Spyder Editor

This is a temporary script file.
"""

import pandas as pd


def func1(params, x):
    bias = params[0]
    coefs = params[1:]
    return bias + dotprod(coefs, x)



def dotprod(a,b):
    assert len(a)==len(b)
    p = []
    for c,aa in enumerate(a):
        p[c] = aa*b[c]
    return sum(p)
9/16: func1(params,x1)
9/17:
def dotprod(a,b):
    assert len(a)==len(b)
    p = []
    print(b)
    for c,aa in enumerate(a):
        p[c] = aa*b[c]
    return sum(p)
9/18: func1(params,x1)
9/19: x1=[0,-2,3]
9/20: func1(params,x1)
9/21: import numpy as np
9/22: np.multiply([1,1],[2,2])
9/23: np.multiply([1,1],[2,2]).sum()
9/24:
"""
Spyder Editor

This is a temporary script file.
"""

import pandas as pd
import numpy as np


def func1(params, x):
    bias = params[0]
    coefs = params[1:]
    dp = np.multiply(coefs, x).sum()
    return bias + dp



def dotprod(a,b):
    assert len(a)==len(b)
    p = []
    print(b)
    for c,aa in enumerate(a):
        p[c] = aa*b[c]
    return sum(p)
9/25: func1(params, x)
9/26: func1(params, x1)
9/27: x2 = [2,0,1]
9/28: func1(params, x2)
9/29: x1 = df.head(1).values[0][:-1]
9/30: x1 = df.head(1).values[0][:-1].values
9/31: x1 = df.head(1).values[0][:-1]
9/32: func1(params, x1)
9/33: x1 = df.ix[1].values[0][:-1]
9/34: df.ix[1].values
9/35: df.ix[1].values[:-1]
9/36: %clear
9/37: [1]*4
9/38:
import pandas as pd
import numpy as np


def func1(params, x):
    bias = params[0]
    coefs = params[1:]
    dp = np.multiply(coefs, x).sum()
    return bias + dp



def run():
    df = pd.read_csv('linear_regression.csv')
    x1=df.head(1).values[0][:-1]
    x2=df.ix[1].values[:-1]
    params = [1]*4
    assert func1(params,x1)==2
    assert func1(params,x2)==4
9/39: run()
9/40:
"""
Spyder Editor

This is a temporary script file.
"""

import pandas as pd
import numpy as np


def func1(params, x):
    bias = params[0]
    coefs = params[1:]
    dp = np.multiply(coefs, x).sum()
    return bias + dp



def run():
    df = pd.read_csv('linear_regression.csv')
    x1=df.head(1).values[0][:-1]
    x2=df.ix[1].values[:-1]
    params = [1]*4
    assert func1(params,x1)==2
    assert func1(params,x2)==4
    print(func1(params,x1))
    print(func1(params,x2))




def loss_calc(params, dfx):
9/41:
"""
Spyder Editor

This is a temporary script file.
"""

import pandas as pd
import numpy as np


def func1(params, x):
    bias = params[0]
    coefs = params[1:]
    dp = np.multiply(coefs, x).sum()
    return bias + dp



def run():
    df = pd.read_csv('linear_regression.csv')
    x1=df.head(1).values[0][:-1]
    x2=df.ix[1].values[:-1]
    params = [1]*4
    assert func1(params,x1)==2
    assert func1(params,x2)==4
    print(func1(params,x1))
    print(func1(params,x2))
9/42: run()
9/43: x = df[['x1','x2','x3']]
9/44: dfx=df.head(2)
9/45: loss_calc(params, dfx)
9/46:
def loss_calc(params, dfx):
    y = df['y']
    x = df[['x1','x2','x3']]
    df['yhat'] = x.apply(lambda row: func1(params, row), axis=1)
    df['loss'] = df['yhat']-y
    avg_loss = np.mean(df.loss*df.loss)
    return avg_loss
9/47: loss_calc(params, dfx)
9/48: dfx
9/49: dfx[['x1','x2','x3']].apply(lambda row: func1(params, row), axis=1)
9/50: y=dfx[['x1','x2','x3']].apply(lambda row: func1(params, row), axis=1)
9/51: y-dfx.y
9/52: (y-dfx.y)**2
9/53:
def loss_calc(params, dfx):
    y = df['y']
    x = df[['x1','x2','x3']]
    df['yhat'] = x.apply(lambda row: func1(params, row), axis=1)
    df['loss'] = (df['yhat']-y)**2
    avg_loss = df.loss.mean()
    return avg_loss
9/54: %clear
9/55: loss_calc(params, dfx)
9/56: dfx
9/57:
def loss_calc(params, dfx):
    y = dfx['y']
    x = dfx[['x1','x2','x3']]
    dfx['yhat'] = x.apply(lambda row: func1(params, row), axis=1)
    dfx['loss'] = (dfx['yhat']-y)**2
    avg_loss = dfx.loss.mean()
    return avg_loss
9/58: loss_calc(params, dfx)
9/59: dfx
9/60: x = dfx[['x1','x2','x3']]
9/61: x.values
9/62: 2*x.values
9/63: a=2*x.values
9/64: a.mean()
9/65: np.mean(a)
9/66: np.mean(a, 1)
9/67: np.mean(a, 0)
9/68: grad(params, dfx)
9/69:
def grad(params, dfx):
    y = dfx['y']
    x = dfx[['x1','x2','x3']]
    dfx['yhat'] = x.apply(lambda row: func1(params, row), axis=1)
    dfx['loss'] = (dfx['yhat']-y)
    g0 = 2*dfx.loss
    params = g0*(x.values)
    return np.mean(params, 0)
9/70: grad(params, dfx)
9/71:
y = dfx['y']
x = dfx[['x1','x2','x3']]
dfx['yhat'] = x.apply(lambda row: func1(params, row), axis=1)
dfx['loss'] = (dfx['yhat']-y)
9/72: g0 = 2*dfx.loss.value
9/73: g0 = 2*dfx.loss.values
10/1: import pandas as pd
10/2: import matplotlib.pyplot as plt
10/3: df = pd.read_csv('dataset.csv')
10/4: df.columns
10/5: df.head()
10/6: col='TV'
10/7:
def scat(col, df):
    plt.scatter(df[col], df['sales'])
10/8: scat(col, df)
10/9: scat('radio', df)
10/10: scat('newspaper', df)
10/11: scat('TV', df)
10/12: df.col.quantile(0.8)
10/13: df.TV.quantile(0.8)
10/14: df.TV.quantile(0.9)
10/15: df.TV.quantile(0.95)
10/16:
def scat(col, df):
    maxx = df.col.quantile(0.98)
    x = df[col]
    xh = x[x<=maxx & x.notnull()]
    plt.scatter(xh, df['sales'])
10/17: scat('TV',df)
10/18:
def scat(col, df):
    maxx = df[col]quantile(0.98)
    x = df[col]
    xh = x[x<=maxx & x.notnull()]
    plt.scatter(xh, df['sales'])
10/19:
def scat(col, df):
    maxx = df[col].quantile(0.98)
    x = df[col]
    xh = x[x<=maxx & x.notnull()]
    plt.scatter(xh, df['sales'])
10/20: scat('TV',df)
10/21:
def scat(col, df):
    maxx = df[col].quantile(0.98)
    x = df[col]
    xh = x[x<=maxx]
    xh = xh[xh.notnull()]
    plt.scatter(xh, df['sales'])
10/22: scat('TV',df)
10/23:
def scat(col, df):
    maxx = df[col].quantile(0.98)
    dfx = df[df[col]<=maxx]
    dfx = dfx[dfx[col].notnull()]

#    x = df[col]
#    xh = x[x<=maxx]
#    xh = xh[xh.notnull()]
    plt.scatter(dfx[col], dfx['sales'])
10/24: scat('TV',df)
10/25: scat('radio',df)
10/26:
def scat(col, df):
    maxx = df[col].quantile(0.98)
    dfx = df[df[col]<=maxx]
    dfx = dfx[dfx[col].notnull()]

#    x = df[col]
#    xh = x[x<=maxx]
#    xh = xh[xh.notnull()]
    plt.xlabel(col)
    plt.ylabel('sales')
    plt.scatter(dfx[col], dfx['sales'])
10/27: scat('radio',df)
10/28:
import numpy as np

def scat(col, df):
    maxx = df[col].quantile(0.98)
    dfx = df[df[col]<=maxx]
    dfx = dfx[dfx[col].notnull()]

#    x = df[col]
#    xh = x[x<=maxx]
#    xh = xh[xh.notnull()]
    plt.xlabel(col)
    plt.ylabel('sales')
    plt.scatter(dfx[col], dfx['sales'])
    print(np.corr(dfx[col], dfx['sales']))
10/29: scat('radio',df)
10/30:
import numpy as np

def scat(col, df):
    maxx = df[col].quantile(0.98)
    dfx = df[df[col]<=maxx]
    dfx = dfx[dfx[col].notnull()]

#    x = df[col]
#    xh = x[x<=maxx]
#    xh = xh[xh.notnull()]
    plt.xlabel(col)
    plt.ylabel('sales')
    plt.scatter(dfx[col], dfx['sales'])
    print(np.corrcoef(dfx[col], dfx['sales']))
10/31: scat('radio',df)
10/32:
"""
Created on Wed Nov 13 10:06:58 2019

@author: suraj
"""


import numpy as np

def scat(col, df):
    maxx = df[col].quantile(0.98)
    dfx = df[df[col]<=maxx]
    dfx = dfx[dfx[col].notnull()]

#    x = df[col]
#    xh = x[x<=maxx]
#    xh = xh[xh.notnull()]
    plt.xlabel(col)
    plt.ylabel('sales')
    plt.scatter(dfx[col], dfx['sales'])
    print(np.corrcoef(dfx[col], dfx['sales'])[0][1])
10/33: scat('radio',df)
10/34: scat('tv',df)
10/35: scat('TV',df)
10/36: scat('newspaper',df)
10/37: scat('radio',df)
10/38: df.head()
10/39: channels = ['TV','radio','newspaper']
10/40: df.head()[channels].sum(axis=1)
10/41:
def a2(df):
    channels = ['TV','radio','newspaper']
    df['avg_amt'] = df[channels].sum(axis=1)/df['sales']
    print(df)
10/42: a2(df.head())
10/43: 332/22
10/44:
def a2(df):
    channels = ['TV','radio','newspaper']
    df['avg_amt'] = df[channels].sum(axis=1)/df['sales']
    df['success'] = np.where(np.logical_and(df.avg_amt<=20,df[channels].sum(axis=1)>=15), 'S','F')    
    print(df)
10/45: a2(df.head())
10/46: df1=a2(df)
10/47:
def a2(df):
    channels = ['TV','radio','newspaper']
    df['avg_amt'] = df[channels].sum(axis=1)/df['sales']
    df['success'] = np.where(np.logical_and(df.avg_amt<=20,df[channels].sum(axis=1)>=15), 'S','F')    
    return df
10/48: df1=a2(df)
10/49: df1.success.value_counts()
10/50:
def a2(df):
    channels = ['TV','radio','newspaper']
    df['avg_amt'] = df[channels].sum(axis=1)/df['sales']
    df['success'] = np.where(np.logical_and(df.avg_amt<=20,df['sales']>=15), 'S','F')    
    return df
10/51: df1=a2(df)
10/52: df1.success.value_counts()
10/53: df.id
11/1: debugfile('C:/Users/suraj/Documents/BEcon Term Project/app.py', wdir='C:/Users/suraj/Documents/BEcon Term Project')
11/2: runfile('C:/Users/suraj/Documents/BEcon Term Project/app.py', wdir='C:/Users/suraj/Documents/BEcon Term Project')
11/3: runfile('C:/Users/suraj/Documents/BEcon Term Project/app.py', wdir='C:/Users/suraj/Documents/BEcon Term Project')
11/4: runfile('C:/Users/suraj/Documents/BEcon Term Project/app.py', wdir='C:/Users/suraj/Documents/BEcon Term Project')
11/5: runfile('C:/Users/suraj/Documents/BEcon Term Project/app.py', wdir='C:/Users/suraj/Documents/BEcon Term Project')
11/6: %tb
11/7: runfile('C:/Users/suraj/Documents/BEcon Term Project/app.py', wdir='C:/Users/suraj/Documents/BEcon Term Project')
12/1: runfile('C:/Users/suraj/Documents/BEcon Term Project/app.py', wdir='C:/Users/suraj/Documents/BEcon Term Project')
12/2: %tb
12/3: runfile('C:/Users/suraj/Documents/BEcon Term Project/app.py', wdir='C:/Users/suraj/Documents/BEcon Term Project')
12/4: runfile('C:/Users/suraj/Documents/BEcon Term Project/app.py', wdir='C:/Users/suraj/Documents/BEcon Term Project')
12/5: runfile('C:/Users/suraj/Documents/BEcon Term Project/app.py', wdir='C:/Users/suraj/Documents/BEcon Term Project')
12/6: runfile('C:/Users/suraj/Documents/BEcon Term Project/app.py', wdir='C:/Users/suraj/Documents/BEcon Term Project')
12/7: %tb
12/8: runfile('C:/Users/suraj/Documents/BEcon Term Project/app.py', wdir='C:/Users/suraj/Documents/BEcon Term Project')
12/9: runfile('C:/Users/suraj/Documents/BEcon Term Project/app.py', wdir='C:/Users/suraj/Documents/BEcon Term Project')
12/10: runfile('C:/Users/suraj/Documents/BEcon Term Project/app.py', wdir='C:/Users/suraj/Documents/BEcon Term Project')
12/11: runfile('C:/Users/suraj/Documents/BEcon Term Project/app.py', wdir='C:/Users/suraj/Documents/BEcon Term Project')
12/12: runfile('C:/Users/suraj/Documents/BEcon Term Project/app.py', wdir='C:/Users/suraj/Documents/BEcon Term Project')
12/13: runfile('C:/Users/suraj/Documents/BEcon Term Project/app.py', wdir='C:/Users/suraj/Documents/BEcon Term Project')
12/14: runfile('C:/Users/suraj/Documents/BEcon Term Project/app.py', wdir='C:/Users/suraj/Documents/BEcon Term Project')
12/15: import numpy as np
12/16: np.linspace(-1, 1, 100)
12/17: np.linspace(-1, 1, 3)
12/18: np.linspace(-1, 1, 3)*2
12/19: np.linspace(-1, 1, 3)**2
12/20: runfile('C:/Users/suraj/Documents/BEcon Term Project/test.py', wdir='C:/Users/suraj/Documents/BEcon Term Project')
12/21: runfile('C:/Users/suraj/Documents/BEcon Term Project/test.py', wdir='C:/Users/suraj/Documents/BEcon Term Project')
12/22: runfile('C:/Users/suraj/Documents/BEcon Term Project/test.py', wdir='C:/Users/suraj/Documents/BEcon Term Project')
12/23: runfile('C:/Users/suraj/Documents/BEcon Term Project/test.py', wdir='C:/Users/suraj/Documents/BEcon Term Project')
12/24:
"""
Created on Thu Nov 14 20:45:30 2019

@author: suraj
"""

from plotly import graph_objs as go

import numpy as np

# Generate curve data
t = np.linspace(-1, 1, 100)
x = t + t ** 2
y = t - t ** 2
xm = np.min(x) - 1.5
xM = np.max(x) + 1.5
ym = np.min(y) - 1.5
yM = np.max(y) + 1.5
N = 50
s = np.linspace(-1, 1, N)
xx = s + s ** 2
yy = s - s ** 2


# Create figure
fig = go.Figure(
    data=[go.Scatter(x=x, y=y,
                     mode="lines",
                     line=dict(width=2, color="blue")),
          go.Scatter(x=x, y=y,
                     mode="lines",
                     line=dict(width=2, color="blue"))],
    layout=go.Layout(
        xaxis=dict(range=[xm, xM], autorange=False, zeroline=False),
        yaxis=dict(range=[ym, yM], autorange=False, zeroline=False),
        title_text="Kinematic Generation of a Planar Curve", hovermode="closest",
        updatemenus=[dict(type="buttons",
                          buttons=[dict(label="Play",
                                        method="animate",
                                        args=[None])])]),
    frames=[go.Frame(
        data=[go.Scatter(
            x=[xx[k]],
            y=[yy[k]],
            mode="markers",
            marker=dict(color="red", size=10))])
        
        for k in range(N)]
)
12/25: fig.show()
12/26: %matplotlib inline
12/27: fig.show()
13/1: 49*5
13/2: 260
13/3: len(list(range(15,260,5)))
13/4: len(list(range(15,261,5)))
13/5: import pandas as pd
13/6:
x=list(range(50))
y=[random.random() for x in range(50)]
df = pd.DataFrame(index=list(range(15,261,5)))
df['x']=x
df['y']=y
13/7: import random
13/8:
=list(range(50))
y=[random.random() for x in range(50)]
df = pd.DataFrame(index=list(range(15,261,5)))
df['x']=x
df['y']=y
13/9:
x=list(range(50))
y=[random.random() for x in range(50)]
df = pd.DataFrame(index=list(range(15,261,5)))
df['x']=x
df['y']=y
13/10: df.ix[15]
13/11: type(df.ix[15])
13/12: df.ix[5]
13/13:
x=list(range(50))
y=[random.random() for x in range(50)]
df = pd.DataFrame(index=list(range(5,231,5)))
df['x']=x
df['y']=y
13/14: len(list(range(5,231,5)))
13/15: len(list(range(5,251,5)))
13/16: y.*100
13/17:
x=list(range(50))
y=[random.random() for x in range(50)]
df = pd.DataFrame(index=list(range(5,251,5)))
df['x']=x
df['y']=y
df['y'] = df['y']*100
13/18: df['y'] = df['y']*100
13/19:
x=list(range(50))
y=[random.random() for x in range(50)]
df = pd.DataFrame(index=list(range(5,251,5)))
df['x']=x
df['y']=y
df['y'] = df['y']*100
13/20:
x=list(range(50))
y=[random.random() for x in range(50)]
df = pd.DataFrame(index=list(range(50)))
df['x']=x
df['y']=y
df['y'] = df['y']*100
13/21: from plotly import graph_objs as go
13/22:
import os
import pathlib
import numpy as np
import datetime as dt
import dash
import dash_core_components as dcc
import dash_html_components as html

from dash.exceptions import PreventUpdate
from dash.dependencies import Input, Output, State
from scipy.stats import rayleigh
from db.api import get_wind_data, get_wind_data_by_id
13/23: os.environ.get("GRAPH_INTERVAL", 5000)
13/24:
import pathlib
import sqlite3
import pandas as pd


DB_FILE = pathlib.Path(__file__).resolve().parent.joinpath("wind-data.db").resolve()


def get_wind_data(start, end):
    """
    Query wind data rows between two ranges
    
    :params start: start row id
    :params end: end row id
    :returns: pandas dataframe object 
    """
    
    con = sqlite3.connect(str(DB_FILE))
    statement = f'SELECT Speed, SpeedError, Direction FROM Wind WHERE rowid > "{start}" AND rowid <= "{end}";'
    df = pd.read_sql_query(statement, con)
    return df



def get_wind_data_by_id(id):
    """
    Query a row from the Wind Table
    
    :params id: a row id
    :returns: pandas dataframe object 
    """
    
    con = sqlite3.connect(str(DB_FILE))
    statement = f'SELECT * FROM Wind WHERE rowid = "{id}";'
    df = pd.read_sql_query(statement, con)
    return df
13/25: dbf = 'wind-data.db'
13/26:
import sqlite3
import pandas as pd
13/27: con = sqlite3.connect(str(DB_FILE))
13/28: con = sqlite3.connect(dbf)
13/29: statement = 'SELECT Speed, SpeedError, Direction FROM Wind limit 10'
13/30: df = pd.read_sql_query(statement, con)
13/31: statement = 'SELECT Speed, SpeedError, Direction. rowid FROM Wind limit 10'
13/32: df = pd.read_sql_query(statement, con)
13/33: statement = 'SELECT Speed, SpeedError, Direction, rowid FROM Wind limit 10'
13/34: df = pd.read_sql_query(statement, con)
14/1:
import dash
import dash_core_components as dcc
import dash_html_components as html
from dash.dependencies import Input, Output, State
from dash.exceptions import PreventUpdate
import pandas as pd
import random
from plotly import graph_objs as go


price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates='Localtime')
14/2: price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
14/3: price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
14/4: price_df['Localtime']
14/5: 7200/5
14/6: 7200/5/60
14/7: a=price_df['Localtime']
14/8: a.dt.date
14/9: a.dt.time
14/10: a.dt.time.str.[:2]
14/11: a.dt.time.str.slice(0,2)
14/12: type(a.dt.time)
14/13: a.dt.time.dtype
14/14: a.dt.time.head(1)
14/15: a.dt.time.head(1).values
14/16: a.dt.time.loc[23].values
14/17: a.dt.time.loc[23]
14/18: x=a.dt.time.loc[23]
14/19: x.hour
14/20:
price_df['date'] = price_df.Localtime.dt.date
price_df['time'] = price_df.Localtime.dt.time
14/21: df2 = price_df[price_df.Volume>0]
14/22: df21 = df[df.date==df.date.head(1)]
14/23: df21 = df2[df2.date==df2.date.head(1)]
14/24: df21 = df2[df2.date==df2.date.values[0]]
14/25: 390/60
14/26: df21.High.plot.line()
14/27: df21.High.plot.line()
14/28: %matplotlib inline
14/29: df21.High.plot.line()
14/30: 390*5
14/31: df2.High.plot.line()
14/32: df2.index = range(len(df2))
14/33: df2.High.plot.line()
14/34: df2.High.plot.line(figsize=(10,10))
14/35: 1950/3
14/36: df2.High.plot.line(figsize=(10,10))
14/37:
df2.High.plot.line(figsize=(10,10))
plt.axvline(x=650)
plt.axvline(x=1300)
14/38: import matplotlib.pyplot as plt
14/39:
df2.High.plot.line(figsize=(10,10))
plt.axvline(x=650)
plt.axvline(x=1300)
14/40:
df2.High.plot.line(figsize=(15,10))
plt.axvline(x=700)
plt.axvline(x=1226)
14/41: %clear
14/42: %clear
14/43: df2[:700][-1]
14/44: df2[:700].tail(1)
14/45: 1226-700
14/46: 1920-1226
14/47: 700/5
14/48: 700/5
14/49: 700/180
14/50: 526/180
14/51: 694/180
14/52: 700/3
14/53:
price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
price_df['date'] = price_df.Localtime.dt.date
price_df['time'] = price_df.Localtime.dt.time
price_df = price_df[price_df.Volume>0]
14/54: price_df.rolling(3)
14/55: r=price_df.rolling(3)
14/56: r.High
14/57: price_df.index
14/58: price_df.index = range(len(price_df))
14/59: price_df.index
14/60: price_df.index//3
14/61: (price_df.index//3)+1
14/62:
price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
#price_df['date'] = price_df.Localtime.dt.date
#price_df['time'] = price_df.Localtime.dt.time
price_df = price_df[price_df.Volume>0]
price_df.index = range(len(price_df))
price_df['slice_index'] = (price_df.index//3)+DELAY
14/63:
DELAY = 1

price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
#price_df['date'] = price_df.Localtime.dt.date
#price_df['time'] = price_df.Localtime.dt.time
price_df = price_df[price_df.Volume>0]
price_df.index = range(len(price_df))
price_df['slice_index'] = (price_df.index//3)+DELAY
## conversion: 3m => 1 sec
14/64: price_df.to_csv('AAPL_DF.csv',index=False)
14/65: price_df.slice_index.max()
14/66: price_df.ix[0:3]
14/67: price_df.ix[0:2]
14/68: price_df.ix[1:3]
14/69: price_df.Localtime
14/70: price_df.Localtime.dt.tz_localize()
14/71: %clear
14/72: import timezone as tx
14/73: from datetime import datetime, timezone
14/74: price_df.Localtime.dt.tz_localize(timezone.EST)
14/75: price_df.Localtime.dt.tz_localize(timezone.utc)
14/76: price_df.Localtime.tz_convert('US/Eastern')
14/77: price_df.Localtime.tz_localize()
14/78: price_df.Localtime.tz_localize('US/Central')
14/79: price_df.Localtime.tz_localize(tz='US/Central')
14/80: price_df.Localtime.tz_localize(tz=''US/Eastern'')
14/81: price_df.Localtime.tz_localize(tz='US/Eastern')
14/82: price_df.Localtime
14/83: %clear
14/84: price_df.Localtime.tz_localize(pytz.timezone('US/Eastern'))
14/85: import pytz
14/86: price_df.Localtime.tz_localize(pytz.timezone('US/Eastern'))
14/87: price_df.Localtime.dt.tz_convert('Asia/Kolkata')
14/88: price_df.Localtime.dt.tz_convert('US/Eastern')
14/89: price_df.Localtime
14/90: price_df.Localtime.tz_localize('UTC', level=0)
14/91: price_df.Localtime.dt.tz_localize('UTC', level=0)
14/92: price_df.Localtime.dt.tz_localize('UTC')
14/93: price_df.Localtime.dt.tz_convert('US/Eastern')
14/94: price_df.Localtime.dt.tz_convert('UTC')
14/95: price_df.Localtime.dt.tz_convert('UTC').dt.tz_convert('US/Eastern')
14/96: price_df.Localtime.dt.tz_convert('US/Eastern')
14/97: price_df.Localtime
14/98:
DELAY = 1

price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
#price_df['date'] = price_df.Localtime.dt.date
#price_df['time'] = price_df.Localtime.dt.time
price_df = price_df[price_df.Volume>0]
price_df.index = range(len(price_df))
price_df['slice_index'] = (price_df.index//3)+DELAY ## conversion: 3m => 1 sec
14/99:
price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
#price_df['date'] = price_df.Localtime.dt.date
#price_df['time'] = price_df.Localtime.dt.time
price_df = price_df[price_df.Volume>0]
price_df.index = range(len(price_df))
price_df['slice_index'] = (price_df.index//3)+DELAY ## conversion: 3m => 1 sec
14/100: price_df.to_csv('AAPL_DF.csv')
14/101: price_df.ix
14/102: price_df.ixndex
14/103: price_df.index
14/104: list(price_df.index)
14/105: list(price_df.index)[:10]
15/1:
import dash
import dash_core_components as dcc
import dash_html_components as html
from dash.dependencies import Input, Output, State
from dash.exceptions import PreventUpdate
import pandas as pd
from plotly import graph_objs as go
from datetime import datetime, timezone
15/2: price_df=pd.read_csv('AAPL_DF.csv')
15/3: price_df['timestr'] = price_df.apply(lambda row: f"{row.Localtime.dt.date} {row.Localtime.dt.time}")
15/4: price_df['timestr'] = price_df.apply(lambda row: f"{row.Localtime.dt.date} {row.Localtime.dt.time}", axis=1)
15/5: price_df.apply(lambda row: row.Localtime, axis=1)
15/6: price_df.apply(lambda row: str(row.Localtime), axis=1)
15/7: import datetime
15/8: t = datetime.datetime.now()
15/9:
t.strftime(
    "2016-01-05 %H:%M:%S"
)
15/10: price_df['High'].resample(3)
15/11: price_df.Localtime.resample(3)
15/12: price_df.Localtime.resample('3T')
15/13: price_df.Localtime
15/14: price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
15/15: price_df.Localtime.resample('3')
15/16: price_df.Localtime.resample('3T')
15/17: price_df.resample('3T', on='Localtime')
15/18: price_df.resample('3T', on='Localtime').ohlc()
15/19: price_df.dtypes
15/20: %clear
15/21: price_df.index=price_df.Localtime
15/22: price_df.High.resample(3).ohlc()
15/23: price_df.High.resample('3T').ohlc()
15/24: price_df = price_df[price_df.Volume>0]
15/25: price_df.High.resample('3T').ohlc()
15/26: price_df.resample('3T')
15/27: price_df.loc
15/28: price_df.loc[0]
15/29: price_df.iloc[0]
15/30: price_df.iloc[0:3]
15/31: price_df.iloc[0:3].index
15/32: price_df.iloc[0:3].Localtime
15/33: price_df.iloc[0:3].Localtime.dt.strftime("%H:%M<br>%d %B")
15/34: price_df.iloc[0:3].Localtime.dt.strftime("%H:%M\n%d %B")
15/35: price_df.iloc[0:3].Localtime.dt.strftime("%d %b - %H:%M")
15/36: price_df.iloc[0:3].Localtime.dt.strftime("%d %b-%H:%M")
15/37: price_df['strtime'] = price_df.Localtime.dt.strftime("%d %b-%H:%M")
15/38: price_df.iloc[700]
15/39: price_df.iloc[1226]
15/40: price_df.iloc[0]
15/41: price_df.iloc[-1]
15/42: df=price_df.iloc[-1]
15/43: df['High']
15/44: round(df['High'], 2)
15/45: df['High']
15/46: type(df['High'])
15/47: float('2.5')
15/48: %clear
15/49: price_df.head()
15/50: price_df.index = range(1,len(price_df)+1)
15/51: price_df.head()
15/52: price_df['index2'] = price_df.index//3
15/53: price_df.head()
15/54: price_df.index = range(len(price_df))
15/55: price_df['index2'] = price_df.index//3
15/56: price_df.head()
15/57: price_df[price_df.strtime==end_P1]
15/58:
end_P1 = "16 Sep-14:38" # TODO_ CHANGE TP REAL
end_P2 = "17 Sep-10:56"
15/59: price_df[price_df.strtime==end_P1]
15/60: price_df[price_df.strtime=="19 Sep-10:55"]
16/1:
import dash
import dash_core_components as dcc
import dash_html_components as html
17/1: import pandas as pd
17/2: s = pd.Series([8666943.12, 1412617.919, 1274873.007, 1113475.942, 1032599.857, 979283.9087, 953272.6192, 890173.7287, 854120.4993, 844260.835, 802865.5399, 792240.4812, 783341.1823, 739942.9713, 710625.0192, 691987.51, 689326.7864, 650459.7511, 585164.8319, 579756.3098, 556411.1334, 554053.6549, 534391.3635, 531987.831, 511318.7968, 494389.8621, 489798.629, 481546.465, 470291.7124, 462640.3217, 452186.4135, 448383.1801, 438600.3515, 429867.7799, 422847.6907, 412009.2217, 399214.9824, 395098.9986, 394876.4643, 367136.9143, 363750.8067, 354941.1465, 335325.7424, 335314.7063, 334566.5862, 326738.7486, 322714.2289, 316547.8745, 303787.4154, 303360.9617, 302313.6394, 302110.1836, 294866.0081, 294635.3679, 288166.7284, 284413.7293, 280989.1275, 280144.6583, 274656.6264, 274083.4428, 273651.5507, 269800.0982, 264790.9526, 258325.3162, 258309.3846, 257247.1899, 251215.9084, 250698.3663, 248768.9204, 246988.2348, 244818.5668, 243217.1171, 242357.3626, 242150.1986, 240748.7634, 235970.701, 235958.0325, 234754.0771, 234711.5714, 233489.8923, 233247.8453, 232488.5911, 231612.5887, 228902.9392, 228143.2911, 227891.8268, 221752.6763, 218314.248, 217795.0503, 216965.7489, 216802.4786, 216186.2682, 215582.1024, 214603.2729, 209964.4144, 208156.8664, 205824.2697, 203926.2783, 203014.6797, 201262.5152, 197278.292, 195357.785, 194030.72, 193387.7444, 186457.8729, 185857.6296, 185838.7478, 185626.5668, 184646.1367, 184560.8098, 183710.2603, 181111.7057, 178828.4751, 176761.8666, 176735.6471, 176387.7913, 174333.5591, 174301.9007, 173273.6302, 173096.3683, 170713.9624, 169630.6075, 169159.4284, 165629.7895, 165444.3433, 165077.7218, 162146.9946, 159714.9487, 158030.4835, 154100.4472, 153428.0266, 150619.0284, 150615.5356, 149908.221, 148770.0087, 148163.3774, 147961.3872, 147929.8516, 147610.0878, 146824.869, 143354.0891, 141708.4975, 141391.9004, 140959.0294, 140801.2551, 140070.6191, 139670.4422, 138531.2817, 137845.6166, 137790.2623, 137448.7839, 135611.3947, 135504.4053, 134487.5517, 134006.5844, 133944.2644, 131545.5932, 130326.2451, 129488.7319, 129207.5695, 129017.2662, 128748.3435, 128427.5659, 127996.0272, 127901.0207, 127329.2468, 126889.1911, 126812.2944, 126517.409, 125456.3313, 125389.8853, 124985.036, 124265.3636, 123175.3346, 123137.7819, 122161.7012, 121061.147, 119555.4561, 119495.8662, 118206.0912, 118010.3615, 115819.0402, 114206.893, 114163.3603, 114025.1999, 114004.7742, 113932.2095, 113644.7142, 113528.3157, 112839.0246, 112724.3371, 112566.7901, 112272.0594, 111095.2651, 109908.5413, 109721.484, 109018.7131, 107463.4882, 107421.7283, 107258.2374, 106977.454, 106914.1237, 106501.7099, 105357.3657, 103213.666, 102517.2827, 102103.8636, 101697.6963, 101508.6663, 101189.19, 100682.9195, 99861.40749, 99343.53563, 99201.6484, 99074.86616, 98979.41138, 98976.27094, 98601.31811, 98213.02706, 96679.40028, 95986.97033, 95540.72662, 95429.24256, 95318.28729, 94098.31973, 93561.89277, 93387.16465, 93232.5647, 93008.21998, 92903.42717, 92037.48914, 92009.97162, 91282.56379, 90522.46117, 90261.25573, 90226.79611, 89944.70418, 89557.21727, 89301.45948, 89174.3582, 88996.06785, 88870.88635, 88782.81351, 88075.80136, 88037.15901, 87960.89821, 87612.83608, 87606.58369, 87590.56942, 87129.97234, 87001.87247, 86939.13846, 86491.97572, 86414.8993, 86397.57564, 86255.07453, 85950.77975, 85836.86007, 85681.94444, 85445.99589, 85073.25253, 84828.42437, 83375.8935, 83322.30687, 83273.4925, 83136.10149, 83059.3662, 83051.05291, 83024.41726, 82811.42173, 82630.77408, 82096.86485, 81498.49117, 80870.08624, 80403.43898, 80214.04179, 80126.01888, 79885.24388, 79849.71385, 79682.16504, 79324.99277, 79304.22545, 78517.22167, 78315.68325, 78058.74777, 77506.33005, 77480.68437, 77449.51626, 77384.94386, 76378.23854, 75867.71992, 75854.31264, 75812.58865, 75387.88634, 74644.25228, 74432.14657, 74316.8566, 74184.14612, 74162.2524, 73718.2785, 73675.70307, 73545.59394, 72866.32809, 72685.46113, 72238.70035, 72179.99728, 71603.27108, 71562.76833, 71522.02137, 71507.4136, 71444.30189, 71167.86148, 71019.5604, 70378.91872, 69699.85519, 69589.45761, 69580.56823, 69552.31515, 69523.16, 68973.82249, 68903.01678, 68555.73049, 68547.20342, 68452.834, 68264.54664, 68217.87313, 68067.18861, 67612.87997, 67346.78224, 67039.02972, 66917.5814, 66482.60841, 66415.78501, 66295.07287, 66040.88723, 65886.30308, 65649.44146, 65649.31735, 65433.63692, 65306.76103, 65277.31844, 65084.84104, 65059.76423, 64926.56981, 64903.57308, 64591.26344, 64239.62111, 64215.3877, 64000.33279, 63992.6629, 63987.25285, 63775.24244, 63769.75277, 63665.59699, 63612.54299, 63558.64793, 63546.18916, 63383.46667, 63356.07037, 63330.57651, 63241.8543, 62974.27645, 62261.9734, 62062.86986, 62040.68156, 62004.4874, 61944.66033, 61860.705, 61722.54349, 61692.44361, 61691.14737, 61525.93937, 61219.66505, 61108.93817, 61090.58468, 60551.21801, 60401.13116, 60337.42092, 60079.66032, 59808.9851, 58665.43993, 58555.06266, 58425.79964, 58309.5869, 58157.06311, 57990.21739, 57450.55103, 57136.02736, 57039.73052, 56765.22722, 56719.28005, 56654.25605, 56513.40407, 56439.21296, 56353.90228, 56018.54001, 55889.54761, 55810.28091, 55541.70148, 55325.89545, 55207.50506, 54762.23584, 54715.90472, 54396.44538, 54352.64724, 54264.6021, 54248.48882, 54088.41848, 54080.83517, 53348.47864, 53247.0722, 52916.73184, 52785.01077, 52724.9814, 52411.49419, 52397.21186, 52251.33919, 52040.83736, 51784.14108, 51636.96158, 51621.0419, 51533.98058, 51276.34305, 51166.12905, 51114.28136, 51061.99051, 50889.20631, 50881.93887, 50863.81091, 50761.02821, 50587.33904, 50207.24766, 50152.88187, 49817.86318, 49748.83642, 49146.14339, 49031.32663, 48988.23757, 48962.84924, 48742.61102, 48706.92509, 48574.68727, 48342.88913, 48299.49863, 48283.32999, 48120.01656, 48051.65573, 47972.35153, 47958.72241, 47924.49094, 47916.10402, 47766.74903, 47638.55124, 47545.62412, 47288.96824, 47052.24108, 46989.74441, 46972.13741, 46967.74319, 46921.05972, 46870.38781, 46669.86414, 46632.63869, 46538.27353, 46530.78926, 46454.66591, 46429.43585, 46422.98645, 46287.36964, 46207.53128, 46121.38442, 46020.17054, 45883.98665, 45874.9624, 45874.74085, 45762.32814, 45678.42516, 45644.12993, 45513.28342, 45486.52156, 45406.67279, 45259.42474, 45196.67059, 45023.7981, 44945.20041, 44914.79858, 44815.42746, 44427.9108, 44408.3319, 44393.47179, 44276.06683, 44156.22368, 44140.6234, 43979.89222, 43968.5791, 43849.07755, 43627.98308, 43582.10938, 43485.91752, 43396.0615, 43318.87282, 43262.54789, 43241.81935, 43224.36311, 43064.31287, 42857.9597, 42807.94681, 42722.67622, 42689.88892, 42653.91927, 42504.92771, 42468.01826, 42335.84779, 42314.21575, 42223.66173, 42052.80276, 41998.04643, 41945.16014, 41828.91076, 41684.26079, 41617.67921, 41612.70645, 41575.72571, 41415.5179, 41379.01768, 41042.43137, 40915.58085, 40849.68462, 40843.60179, 40791.01148, 40764.70573, 40645.92961, 40547.53498, 40456.50979, 40440.96695, 40381.01785, 40314.20247, 40174.51624, 40132.14991, 39781.86036, 39776.42946, 39706.86737, 39652.98685, 39652.61762, 39595.70419, 39590.49075, 39590.04352, 39341.75084, 39192.16841, 39063.27589, 39061.10976, 39008.90487, 38942.13079, 38928.10823, 38858.51628, 38838.2956, 38747.90719, 38666.11037, 38622.96929, 38568.83772, 38553.141, 38339.12039, 38321.02083, 38216.03726, 38157.38336, 38111.95929, 38096.64151, 37853.91435, 37654.5619, 37635.23978, 37628.87753, 37624.03531, 37351.50336, 37315.40814, 37222.86612, 37186.00102, 37068.78912, 37015.067, 36963.48816, 36904.73082, 36904.46527, 36843.82041, 36802.10084, 36680.73068, 36641.63727, 36624.31275, 36591.25704, 36547.72994, 36495.60402, 36462.61715, 36420.85384, 36391.12363, 36361.29455, 36277.06743, 36143.16766, 36113.8673, 36039.81508, 36039.58823, 35932.91871, 35928.66196, 35684.31413, 35494.45862, 35431.32785, 35308.01553, 35238.79565, 35080.31345, 35058.87238, 35048.29294, 34940.14298, 34860.38007, 34718.48374, 34662.83331, 34601.0728, 34589.63725, 34425.53852, 34127.63777, 34101.68399, 34091.52701, 34074.54472, 34043.0611, 33844.7707, 33797.89816, 33766.43267, 33724.08642, 33710.00513, 33708.79759, 33562.09108, 33542.65163, 33224.50707, 33120.66802, 33049.79106, 33040.89664, 32784.77051, 32780.02593, 32729.29388, 32727.23733, 32708.56679, 32696.79343, 32664.80464, 32600.55167, 32595.9436, 32548.87949, 32541.56109, 32534.98516, 32525.59845, 32524.54826, 32520.52827, 32509.42325, 32405.78306, 32360.18136, 32359.67413, 32347.0157, 32345.26044, 32337.44222, 32293.37145, 32230.11786, 31950.66216, 31816.00892, 31649.88725, 31477.4344, 31452.39568, 31415.39917, 31384.74582, 31372.37887, 31332.12969, 31141.19218, 31137.92339, 31119.2954, 30959.86933, 30890.1249, 30889.1123, 30887.89296, 30860.98212, 30751.94145, 30710.23477, 30622.14687, 30440.32849, 30428.92603, 30402.80787, 30354.89383, 30340.68628, 30277.99197, 30237.03986, 30197.41912, 30061.13861, 30015.64514, 29955.01869, 29915.84077, 29906.85979, 29879.52384, 29808.61372, 29732.54054, 29680.04673, 29446.08116, 29430.58048, 29354.4765, 29291.35327, 29259.12052, 29168.88753, 29140.18787, 29138.27269, 29044.1581, 29028.5716, 28926.10881, 28874.18893, 28848.69526, 28815.83897, 28730.86158, 28623.59482, 28592.58889, 28577.91129, 28568.45383, 28561.52665, 28529.39482, 28391.01959, 28374.25965, 28298.0744, 28099.11837, 28045.5675, 27921.50081, 27850.55527, 27747.22513, 27730.50186, 27549.75761, 27539.46654, 27469.37956, 27419.72413, 27418.55062, 27396.44905, 27387.1254, 27368.22535, 27332.98536, 27318.17996, 27303.23903, 27276.98823, 27249.51927, 27226.97552, 27172.79791, 27151.07076, 27134.52025, 27097.62045, 26988.22692, 26962.08769, 26960.39142, 26867.36086, 26785.59158, 26782.07949, 26657.64453, 26655.13246, 26652.64319, 26488.36885, 26484.08694, 26483.40376, 26293.23238, 26245.49987, 26221.55518, 26103.74231, 26080.98558, 26037.81007, 26033.59467, 25987.53332, 25935.56638, 25844.2402, 25807.7684, 25752.55848, 25698.07852, 25686.20466, 25658.15855, 25625.163, 25609.32004, 25602.88992, 25554.18153, 25479.98893, 25409.50304, 25387.79165, 25364.84152, 25307.63661, 25285.79261, 25258.99434, 25237.96845, 25204.53766, 25189.84644, 25114.01775, 25101.97825, 25038.41153, 25016.13756, 24981.89584, 24898.61186, 24803.00963, 24784.30585, 24765.09045, 24641.23888, 24635.0726, 24493.96426, 24420.31789, 24401.48044, 24397.89521, 24387.91342, 24358.10905, 24355.37344, 24321.29672, 24315.68477, 24273.2406, 24243.96117, 24190.99877, 24169.00929, 24152.09967, 24104.87283, 24087.93854, 24072.14079, 24024.93372, 23994.50145, 23901.6389, 23896.48323, 23745.07933, 23720.89155, 23610.20411, 23592.81767, 23585.85098, 23583.61373, 23510.65687, 23432.90027, 23361.65914, 23261.19547, 23243.65731, 23229.08906, 23146.83414, 23114.70493, 23096.65741, 23094.95483, 23037.64583, 22955.72037, 22901.97799, 22900.28155, 22899.85714, 22898.31018, 22843.86746, 22808.21595, 22807.69393, 22801.31677, 22730.67266, 22714.11859, 22691.16935, 22666.6959, 22617.46645, 22553.15187, 22544.23302, 22539.8986, 22534.2255, 22420.51444, 22402.91895, 22350.2473, 22321.48003, 22279.61433, 22275.87982, 22273.09506, 22267.25327, 22242.65442, 22231.71041, 22184.52451, 22023.0332, 22019.73548, 21996.10838, 21945.2945, 21945.08905, 21942.83256, 21906.51319, 21880.03892, 21857.81428, 21794.14288, 21788.68797, 21764.43499, 21660.96586, 21659.89569, 21645.07526, 21343.25696, 21257.52084, 21190.81479, 21152.79806, 21149.00314, 21115.74707, 21086.84725, 21079.15418, 21042.60898, 20983.07149, 20954.24348, 20927.63621, 20921.96215, 20893.74144, 20819.9268, 20809.62698, 20784.80027, 20738.68301, 20727.77854, 20712.86287, 20689.88158, 20680.29647, 20672.16682, 20633.8272, 20582.04941, 20525.14044, 20508.07879, 20465.76035, 20408.55453, 20401.12766, 20360.49101, 20305.42242, 20279.10225, 20269.86009, 20257.09084, 20250.16408, 20211.12678, 20152.84333, 20131.00113, 20049.40873, 20043.18718, 20030.74168, 19977.16668, 19901.27999, 19899.21383, 19881.22554, 19878.41991, 19814.55765, 19775.20507, 19730.75437, 19723.1682, 19700.34484, 19696.33562, 19694.15032, 19689.04691, 19679.48803, 19620.15289, 19605.69962, 19583.82577, 19569.17695, 19560.66126, 19543.90266, 19532.10844, 19521.82387, 19450.2857, 19399.31886, 19370.45469, 19351.22654, 19236.88052, 19207.94743, 19179.63404, 19009.27176, 18975.14217, 18900.08759, 18893.01448, 18877.11036, 18842.56162, 18820.09784, 18757.41632, 18672.46471, 18660.93367, 18622.24094, 18608.07308, 18575.14535, 18518.86607, 18513.20172, 18465.86268, 18455.21332, 18427.74197, 18426.90612, 18414.48505, 18374.88439, 18304.69672, 18293.58242, 18270.62345, 18247.47969, 18239.69332, 18234.81332, 18187.36462, 18168.64004, 18100.67522, 18099.51218, 18077.5094, 18042.85285, 17979.84195, 17976.6999, 17955.7137, 17953.55815, 17913.08175, 17911.41301, 17856.85524, 17846.1107, 17841.18411, 17808.3087, 17794.00546, 17766.37985, 17752.28452, 17743.60945, 17734.21171, 17729.49367, 17697.67986, 17646.19274, 17633.13147, 17616.2597, 17599.82784, 17595.96641, 17584.80629, 17560.87267, 17539.27923, 17527.84449, 17527.33483, 17521.71093, 17520.90248, 17497.50209, 17494.29651, 17445.177, 17424.00438, 17391.02548, 17352.0792, 17313.6215, 17307.93182, 17282.48842, 17266.3669, 17266.03031, 17246.29376, 17204.35649, 17196.57326, 17111.06306, 17103.81771, 17100.6897, 17095.75119, 17089.23088, 17089.20709, 17041.26603, 17031.99536, 17023.22274, 17019.06586, 16995.20144, 16958.9326, 16934.32355, 16929.66035, 16926.97138, 16900.49332, 16852.89575, 16807.74718, 16807.60134, 16759.7709, 16677.84422, 16656.42712, 16640.94997, 16622.70416, 16619.32859, 16610.18775, 16601.23824, 16536.84324, 16460.37312, 16447.31409, 16442.8598, 16430.9015, 16350.86752, 16318.51705, 16312.17769, 16309.64473, 16276.73034, 16271.36879, 16241.12654, 16239.79137, 16226.25288, 16207.95924, 16203.22714, 16180.96004, 16165.04464, 16121.7491, 16106.56031, 16074.31848, 16050.0481, 16049.41845, 16043.53942, 16026.6087, 16022.56888, 15969.63172, 15962.39654, 15933.66028, 15932.52722, 15915.62219, 15873.04264, 15872.40058, 15828.16984, 15749.77939, 15747.1186, 15729.96766, 15666.8734, 15658.2329, 15637.94486, 15606.2758, 15598.26882, 15588.00204, 15574.30593, 15573.39144, 15525.35486, 15512.51241, 15495.31643, 15488.21439, 15411.48316, 15386.09444, 15373.60584, 15367.95707, 15327.46474, 15313.8388, 15288.61887, 15280.40554, 15276.05495, 15273.15367, 15270.32024, 15263.3839, 15203.51827, 15156.17665, 15145.95473, 15143.67508, 15132.45409, 15131.54163, 15020.69675, 14983.34146, 14960.63069, 14912.61478, 14863.32746, 14823.06839, 14817.46492, 14789.07024, 14757.73243, 14735.59927, 14704.89215, 14696.64412, 14691.03481, 14675.30345, 14670.91731, 14664.06791, 14608.33449, 14604.23502, 14557.02648, 14542.56563, 14538.5181, 14529.78788, 14491.76757, 14372.54694, 14320.83546, 14319.59903, 14305.14091, 14297.44806, 14250.91765, 14235.07481, 14229.70971, 14181.77546, 14180.19783, 14179.30454, 14173.25148, 14166.94036, 14131.85279, 14100.8579, 14041.51806, 14021.23935, 13946.52116, 13945.08705, 13844.19114, 13799.13895, 13796.86986, 13771.24069, 13741.4576, 13730.69637, 13729.68298, 13712.23828, 13673.25868, 13670.78196, 13657.02635, 13630.48068, 13590.91533, 13561.8925, 13533.19802, 13466.57094, 13464.45332, 13455.08945, 13423.95046, 13420.11048, 13409.80871, 13406.65542, 13381.27482, 13357.86088, 13236.96585, 13139.26885, 13097.75105, 13079.24447, 12982.03774, 12976.26401, 12956.02434, 12802.41047, 12773.77774, 12758.44806, 12741.90202, 12705.06594, 12701.54352, 12672.40151, 12659.72026, 12658.52843, 12548.59792, 12521.18344, 12503.93605, 12500.42609, 12498.40441, 12451.08424, 12447.82764, 12425.22603, 12389.3259, 12359.42861, 12354.37083, 12348.43667, 12344.86725, 12321.46143, 12303.2559, 12299.04817, 12256.15021, 12245.76949, 12238.91039, 12198.32572, 12182.8058, 12126.6241, 12052.22081, 12036.51386, 12025.14807, 11936.74614, 11923.98459, 11910.97679, 11910.93143, 11858.26088, 11856.68573, 11852.12973, 11800.61686, 11767.93589, 11734.92484, 11636.8991, 11621.36161, 11580.36636, 11574.31448, 11567.41735, 11559.3697, 11556.22085, 11543.23689, 11500.6268, 11480.35281, 11461.42016, 11408.73279, 11383.8128, 11379.40016, 11371.99276, 11368.35362, 11354.52769, 11340.53586, 11322.71474, 11307.9244, 11297.36745, 11292.66584, 11269.43705, 11210.81401, 11139.61231, 11134.75167, 11130.051, 11069.50538, 11029.30591, 10980.03665, 10971.2637, 10963.64578, 10948.37232, 10940.97158, 10924.14509, 10868.20289, 10829.88334, 10784.74148, 10780.56923, 10689.84248, 10672.13938, 10664.03572, 10580.70491, 10552.80873, 10549.32115, 10541.42947, 10517.29511, 10470.05143, 10450.92512, 10450.15887, 10439.54381, 10410.17526, 10400.24634, 10384.06607, 10349.48792, 10270.03888, 10208.81411, 10194.17109, 10177.23419, 10166.99161, 10150.14833, 10140.14539, 10122.71267, 10118.99239, 10103.22641, 10073.23181, 10072.85324, 10059.0806, 10046.58856, 10009.72454, 10000.85945, 9992.776415, 9986.391234, 9977.533724, 9974.539043, 9962.634913, 9946.493444, 9938.768786, 9880.279929, 9879.821189, 9871.788102, 9830.651095, 9826.297325, 9813.539371, 9773.83266, 9765.812219, 9764.411688, 9756.497028, 9742.687861, 9736.993365, 9734.612232, 9728.644961, 9728.46505, 9681.22887, 9663.802623, 9662.669274, 9645.356936, 9640.294675, 9634.552327, 9632.838486, 9613.052004, 9598.138209, 9570.974674, 9560.324357, 9560.224019, 9509.495885, 9479.05416, 9475.181449, 9464.971733, 9452.573393, 9431.918692, 9407.503397, 9392.44596, 9392.300876, 9386.860106, 9378.343586, 9365.863464, 9314.39255, 9279.44762, 9259.978876, 9248.979284, 9228.641099, 9211.7063, 9201.552221, 9200.715445, 9195.405182, 9179.856783, 9157.780953, 9113.05344, 9082.495788, 9031.689575, 9023.20061, 8973.283411, 8961.550123, 8911.291759, 8874.00082, 8868.303303, 8859.649638, 8857.327087, 8842.456338, 8800.614028, 8779.792435, 8765.3214, 8759.465677, 8752.074343, 8728.182996, 8718.022906, 8676.270743, 8654.997215, 8642.217816, 8625.231678, 8624.697499, 8620.855604, 8601.682148, 8586.891128, 8558.495107, 8555.089895, 8518.427558, 8504.787146, 8503.215491, 8457.884842, 8453.234915, 8420.296703, 8365.466122, 8349.631588, 8344.472046, 8296.448066, 8292.679027, 8286.946199, 8282.015272, 8210.845944, 8202.538765, 8182.41571, 8166.548656, 8135.822454, 8131.031766, 8114.476508, 8093.443966, 8061.016561, 8058.06051, 8029.721219, 8021.08707, 8016.588489, 7993.262892, 7981.049448, 7977.128993, 7973.273538, 7966.343117, 7943.940391, 7943.425423, 7942.20505, 7903.079593, 7854.402796, 7795.963155, 7793.406152, 7787.518021, 7779.115126, 7758.086308, 7751.708735, 7717.248444, 7680.472997, 7669.301933, 7663.580507, 7628.84798, 7613.670627, 7598.89695, 7597.519317, 7585.282213, 7580.043983, 7550.123827, 7544.700054, 7520.605149, 7513.746206, 7484.576109, 7482.110517, 7442.279223, 7425.490104, 7413.766485, 7406.562092, 7398.733672, 7395.861989, 7392.215066, 7383.276169, 7371.541578, 7367.310592, 7283.605659, 7278.566193, 7251.638397, 7234.155394, 7229.454858, 7213.0803, 7196.428271, 7190.819909, 7189.508624, 7172.497717, 7149.46204, 7097.447631, 7096.458559, 7083.70261, 7067.695117, 7065.146241, 6982.746493, 6953.686889, 6941.254259, 6934.483449, 6919.289901, 6916.141489, 6892.656619, 6874.410017, 6848.573714, 6845.743091, 6793.40445, 6783.434634, 6766.586, 6737.621814, 6736.563492, 6731.560038, 6715.838489, 6686.317696, 6668.919075, 6661.641808, 6659.436213, 6659.239119, 6634.023161, 6529.990423, 6523.748792, 6510.356032, 6508.272452, 6497.626889, 6490.512819, 6460.327349, 6420.221998, 6399.6647, 6387.2525, 6367.472466, 6365.002147, 6345.779307, 6342.780249, 6325.386472, 6314.460722, 6277.805336, 6233.216043, 6217.659481, 6206.64469, 6194.187624, 6173.9904, 6143.24019, 6141.029092, 6117.689858, 6083.026811, 6074.517397, 6054.449812, 6025.554622, 6005.202315, 5996.553193, 5983.395888, 5979.024909, 5972.728904, 5966.757529, 5951.237509, 5898.351954, 5889.906435, 5873.181551, 5863.603984, 5857.411768, 5831.219762, 5819.931758, 5817.63035, 5805.698985, 5803.485165, 5797.426503, 5791.102053, 5788.836822, 5783.558498, 5778.761568, 5778.39683, 5772.407803, 5770.178674, 5763.30246, 5736.912903, 5730.289286, 5698.826702, 5698.412093, 5685.492556, 5679.113629, 5655.690509, 5643.9081, 5636.713051, 5630.731143, 5604.767443, 5596.710746, 5593.830647, 5547.059347, 5542.727872, 5524.908583, 5513.765092, 5507.172292, 5507.14076, 5502.037568, 5496.994886, 5479.662256, 5474.430114, 5467.622274, 5465.969544, 5453.896773, 5447.814953, 5436.109165, 5430.312954, 5421.72366, 5411.220375, 5364.244701, 5359.264486, 5350.873308, 5331.760612, 5330.103491, 5315.082986, 5302.160373, 5273.297623, 5266.413804, 5248.866687, 5226.275734, 5207.947418, 5206.527682, 5193.105009, 5193.103077, 5181.010824, 5172.067604, 5165.065041, 5142.989303, 5140.933643, 5127.816377, 5122.267825, 5119.798474, 5108.612722, 5103.950958, 5090.366032, 5085.556032, 5055.669694, 5024.06182, 5015.880961, 5014.062684, 5013.439155, 5006.465748, 4993.959595, 4991.285371, 4977.713532, 4957.784321, 4957.414025, 4948.170557, 4939.259301, 4937.272644, 4925.530852, 4919.399344, 4916.246335, 4914.285045, 4913.479007, 4878.577685, 4877.712365, 4864.509631, 4836.953246, 4830.433741, 4825.374761, 4780.583316, 4771.413506, 4742.239574, 4734.7635, 4727.738333, 4702.451523, 4697.206727, 4694.126788, 4678.002514, 4674.010468, 4669.622547, 4666.72301, 4650.479258, 4639.798913, 4636.695895, 4607.509773, 4591.140094, 4589.539844, 4589.007788, 4583.481687, 4580.264763, 4573.862108, 4564.555353, 4562.745324, 4552.947047, 4541.947457, 4521.958003, 4518.755386, 4496.945905, 4487.132548, 4486.177652, 4483.782885, 4480.065268, 4474.394525, 4450.979896, 4442.90056, 4441.335972, 4439.505865, 4427.051137, 4420.990688, 4409.883207, 4401.501946, 4394.82961, 4343.467883, 4338.990653, 4322.071244, 4314.382364, 4286.913809, 4286.786041, 4282.208297, 4280.379109, 4274.649987, 4272.731899, 4272.528556, 4265.089801, 4260.665717, 4255.890004, 4255.202842, 4253.140664, 4250.805236, 4246.631347, 4244.771538, 4229.123279, 4227.597506, 4225.707177, 4224.502745, 4221.141853, 4214.385798, 4209.994943, 4198.717775, 4185.803181, 4184.71341, 4181.512389, 4179.154723, 4171.137553, 4169.163709, 4162.147693, 4154.142078, 4144.249947, 4138.984318, 4138.039737, 4130.411181, 4129.048665, 4116.568751, 4094.580753, 4076.426061, 4075.189188, 4072.803252, 4069.234641, 4045.686688, 4043.946911, 4042.400454, 4019.360563, 4015.004944, 4002.741003, 3996.267891, 3986.263341, 3986.00123, 3981.176633, 3976.784744, 3959.293769, 3942.68159, 3929.264143, 3904.028492, 3903.651223, 3898.011215, 3897.840907, 3897.033136, 3889.264483, 3886.834552, 3880.630437, 3859.24903, 3858.143812, 3846.456852, 3827.680513, 3819.766668, 3819.084796, 3807.2421, 3802.793428, 3801.59368, 3800.542857, 3796.675323, 3782.476771, 3758.700339, 3732.499467, 3717.579294, 3716.393981, 3701.158361, 3700.126077, 3657.292048, 3652.12114, 3641.256729, 3633.545965, 3629.488437, 3612.554201, 3608.502115, 3606.562518, 3605.312196, 3583.667948, 3579.667379, 3579.315632, 3576.424519, 3571.370219, 3567.688044, 3564.222721, 3560.657705, 3560.365584, 3551.879164, 3548.811258, 3545.154736, 3540.08886, 3535.555902, 3526.18777, 3524.050956, 3519.996893, 3519.020574, 3510.799133, 3508.468185, 3502.555942, 3499.147075, 3492.718435, 3491.550489, 3470.794749, 3470.669897, 3469.950202, 3467.84439, 3435.932232, 3431.34444, 3430.205026, 3425.69997, 3419.577486, 3415.465864, 3412.067425, 3396.282846, 3379.558718, 3377.767906, 3370.079379, 3364.157839, 3357.527966, 3350.446432, 3346.304925, 3342.886084, 3342.459033, 3338.718939, 3335.471874, 3333.842167, 3312.695108, 3310.974665, 3306.533255, 3300.502777, 3292.393425, 3289.481124, 3279.333671, 3272.726632, 3272.592843, 3268.239975, 3246.723901, 3241.335106, 3225.697714, 3225.287739, 3222.744099, 3220.814707, 3213.835902, 3198.374004, 3192.321554, 3189.019862, 3184.957736, 3178.086774, 3176.848723, 3168.909191, 3167.041212, 3165.862397, 3163.593481, 3158.357157, 3150.453958, 3148.353021, 3141.460309, 3138.337877, 3136.406767, 3134.16541, 3128.277334, 3127.691697, 3120.975325, 3120.924299, 3118.843119, 3110.479736, 3108.244885, 3107.176439, 3106.033737, 3105.746201, 3096.417006, 3094.422841, 3084.778292, 3078.494206, 3068.444694, 3066.365605, 3047.580462, 3047.210585, 3043.162994, 3043.059768, 3034.398334, 3027.86101, 3025.369413, 3024.321841, 3022.297061, 2999.938284, 2990.652911, 2989.746588, 2989.659649, 2984.96801, 2984.138227, 2968.63497, 2964.582616, 2963.67943, 2955.243004, 2937.669219, 2937.377773, 2935.399931, 2934.325297, 2915.540424, 2907.901249, 2904.770367, 2904.631899, 2901.778313, 2900.329678, 2898.512684, 2894.000185, 2877.913766, 2876.541807, 2874.743138, 2874.211488, 2863.95234, 2851.8991, 2848.022362, 2841.573963, 2836.820033, 2836.696321, 2836.16974, 2826.409711, 2817.2562, 2814.435234, 2812.734052, 2793.909844, 2793.020941, 2787.866807, 2785.25467, 2782.904744, 2770.452477, 2766.307309, 2760.771057, 2757.282413, 2746.633089, 2739.332297, 2735.856065, 2733.914951, 2729.358947, 2719.715734, 2708.48103, 2696.349973, 2693.397831, 2693.182931, 2690.105651, 2686.880824, 2686.58832, 2682.285301, 2681.325139, 2676.237147, 2662.780066, 2655.645664, 2651.107878, 2647.579065, 2645.879751, 2641.371542, 2641.095455, 2640.754788, 2640.565987, 2630.677729, 2620.903004, 2615.962976, 2615.20401, 2611.790166, 2611.097751, 2610.535246, 2604.453102, 2594.758412, 2590.58907, 2580.602514, 2579.60744, 2579.050674, 2573.590539, 2570.983557, 2568.628228, 2554.379469, 2553.962715, 2551.797854, 2541.857448, 2540.659844, 2540.310121, 2535.204287, 2535.123955, 2532.20654, 2531.929177, 2530.548567, 2525.179807, 2525.067787, 2525.005351, 2522.855222, 2508.942441, 2508.120945, 2504.1526, 2500.844029, 2498.905989, 2496.936923, 2496.901044, 2495.688693, 2490.02671, 2489.855041, 2482.592667, 2480.309271, 2476.359706, 2475.907148, 2472.693602, 2468.150933, 2457.908158, 2444.795865, 2428.654478, 2427.966835, 2419.932231, 2412.944486, 2412.498689, 2410.673084, 2406.536933, 2404.398838, 2399.32608, 2395.302329, 2393.528352, 2385.936893, 2385.159713, 2384.966425, 2381.357568, 2375.260084, 2371.997444, 2368.541438, 2367.371261, 2354.619443, 2348.694541, 2346.440522, 2346.259565, 2337.6528, 2331.549196, 2327.731111, 2325.563205, 2325.511723, 2324.858534, 2317.828262, 2311.858155, 2296.166446, 2296.150644, 2294.865686, 2289.674707, 2286.847742, 2283.040258, 2280.545777, 2279.711971, 2273.541107, 2265.356391, 2262.811678, 2262.75243, 2255.11577, 2254.177097, 2250.784343, 2249.853938, 2249.798798, 2241.78576, 2241.123439, 2237.890107, 2233.703684, 2233.053009, 2231.928046, 2231.293615, 2229.217629, 2224.789162, 2222.733831, 2218.44947, 2215.946292, 2213.136564, 2212.978326, 2212.41779, 2211.855498, 2207.273416, 2203.962381, 2202.385716, 2197.001103, 2195.246688, 2190.812178, 2188.157791, 2184.329785, 2184.057599, 2182.954478, 2182.193407, 2173.544951, 2173.211224, 2172.3365, 2171.096412, 2167.693543, 2166.637566, 2166.328648, 2163.076212, 2160.536335, 2159.723797, 2158.612, 2158.414114, 2149.357115, 2148.913738, 2147.100234, 2144.008806, 2143.883226, 2141.172572, 2138.535919, 2137.488377, 2136.022453, 2129.317743, 2127.744818, 2123.971901, 2122.978823, 2117.043372, 2116.681117, 2115.246587, 2111.423233, 2110.120171, 2107.451395, 2107.36288, 2102.751611, 2099.723538, 2094.663712, 2091.62328, 2085.933662, 2084.248092, 2081.894191, 2081.661751, 2079.552937, 2076.050255, 2071.61728, 2069.625916, 2068.616785, 2068.285798, 2058.537959, 2057.993773, 2048.601754, 2046.737196, 2044.592114, 2042.106839, 2040.589214, 2038.982369, 2037.329032, 2036.644519, 2033.158279, 2027.779717, 2022.489981, 2021.360418, 2018.147367, 2010.219264, 2010.206236, 2010.097383, 2008.402117, 2005.607455, 2000.766255, 1997.648097, 1997.491886, 1996.573973, 1993.557934, 1985.490153, 1984.317721, 1979.969228, 1977.878866, 1976.823801, 1976.610724, 1970.870281, 1967.483727, 1963.003837, 1961.074877, 1960.381106, 1956.470793, 1953.989521, 1950.90704, 1945.074379, 1944.111895, 1938.721551, 1931.650218, 1929.230638, 1922.88278, 1922.760113, 1921.785083, 1920.783523, 1919.512997, 1919.125809, 1917.669985, 1917.295271, 1916.325431, 1913.165426, 1911.294906, 1905.571604, 1904.955422, 1901.234177, 1900.153499, 1896.198588, 1893.794643, 1889.374952, 1888.685242, 1886.326346, 1886.196691, 1883.931807, 1882.709226, 1882.069153, 1875.471493, 1871.925748, 1869.648488, 1863.282294, 1861.947764, 1859.746949, 1855.130784, 1848.569005, 1848.030692, 1848.026369, 1842.735332, 1842.42917, 1840.705107, 1840.005765, 1838.427787, 1826.339548, 1824.72625, 1824.372535, 1816.090446, 1815.884116, 1815.043816, 1809.673336, 1808.740134, 1804.254609, 1803.62002, 1803.262673, 1796.870546, 1796.746923, 1791.271807, 1786.419746, 1786.341772, 1784.542393, 1782.097144, 1780.730765, 1778.384831, 1776.512454, 1768.132716, 1763.592499, 1761.178793, 1758.860273, 1746.348915, 1746.148808, 1746.028601, 1743.750192, 1742.316807, 1734.551286, 1729.250159, 1727.422944, 1727.18852, 1724.117326, 1723.497394, 1722.126342, 1718.157474, 1716.739888, 1715.595768, 1709.890939, 1704.541513, 1702.05372, 1699.987647, 1699.023669, 1696.329308, 1692.379946, 1690.750902, 1684.135233, 1678.688407, 1675.060971, 1666.645163, 1662.322091, 1660.684903, 1659.596194, 1656.579237, 1655.004851, 1654.039772, 1646.773565, 1642.485381, 1641.314679, 1641.100399, 1640.722408, 1639.272202, 1637.181133, 1631.066992, 1628.661624, 1628.501807, 1626.378232, 1625.070149, 1625.039166, 1624.946346, 1619.766554, 1613.052286, 1604.806341, 1603.759145, 1599.994318, 1597.272271, 1594.590478, 1591.926702, 1590.392068, 1586.826494, 1586.031577, 1578.54561, 1574.034649, 1571.67719, 1560.551261, 1558.847552, 1553.87693, 1552.458901, 1552.070044, 1551.681071, 1550.883114, 1549.865688, 1549.674102, 1544.661819, 1543.909223, 1539.846315, 1529.75701, 1528.834856, 1524.336019, 1523.901667, 1519.328563, 1519.14605, 1515.020438, 1510.5275, 1510.189226, 1509.937937, 1509.372399, 1505.484741, 1502.892797, 1501.716506, 1498.205728, 1497.220488, 1495.938727, 1495.240302, 1492.254133, 1489.873295, 1489.039824, 1488.197014, 1482.384612, 1481.5291, 1477.307264, 1477.056926, 1476.928645, 1473.674839, 1472.845719, 1472.263741, 1456.287077, 1455.305871, 1454.746703, 1453.77485, 1449.604902, 1444.018302, 1443.289835, 1442.892873, 1439.284586, 1431.201888, 1429.576277, 1423.969712, 1415.018696, 1414.482614, 1414.03137, 1412.899143, 1411.988448, 1407.011865, 1400.976808, 1399.913812, 1397.853159, 1390.122811, 1387.861224, 1386.472204, 1386.011421, 1385.670198, 1384.774077, 1384.731332, 1381.371795, 1375.698459, 1372.456942, 1369.203655, 1367.880951, 1366.725926, 1365.705839, 1365.165656, 1361.594696, 1359.611963, 1358.656569, 1358.250133, 1358.177256, 1354.875144, 1345.314455, 1343.836249, 1340.155929, 1339.038067, 1337.338029, 1334.010891, 1332.237088, 1332.06597, 1331.832001, 1330.448597, 1330.424121, 1327.946466, 1325.895686, 1324.655577, 1322.806343, 1321.317367, 1321.123722, 1320.541157, 1317.895239, 1315.905272, 1313.660261, 1313.102349, 1310.358343, 1310.304252, 1308.140814, 1307.601857, 1304.258939, 1302.58292, 1302.3057, 1302.135822, 1301.375871, 1301.281588, 1298.764532, 1298.172199, 1297.743616, 1297.440339, 1297.318167, 1297.026315, 1292.197653, 1290.384935, 1285.254844, 1285.03022, 1283.991099, 1280.257115, 1278.353016, 1278.24226, 1277.79071, 1277.674908, 1277.205494, 1270.571868, 1266.210882, 1265.455482, 1265.230944, 1263.134051, 1258.980003, 1258.921617, 1258.663516, 1257.828882, 1257.782766, 1254.878768, 1253.003859, 1252.588458, 1248.452407, 1247.355838, 1246.458002, 1244.537618, 1241.863404, 1238.439641, 1238.00849, 1234.506405, 1229.454268, 1228.672002, 1227.174129, 1225.4624, 1223.541347, 1222.893429, 1221.798565, 1219.833103, 1217.768243, 1217.04509, 1216.378843, 1213.429547, 1210.013349, 1206.286974, 1205.200517, 1204.894161, 1202.736431, 1199.822427, 1198.817705, 1197.692994, 1196.614932, 1193.694498, 1189.522941, 1189.063301, 1187.069946, 1186.700893, 1185.76912, 1185.090471, 1183.663003, 1181.348698, 1181.12443, 1175.008484, 1167.35581, 1165.01654, 1163.426534, 1162.118491, 1160.093799, 1156.605133, 1153.547129, 1149.971461, 1146.892674, 1146.232281, 1142.603803, 1141.758993, 1141.266827, 1138.556178, 1137.32489, 1135.152944, 1128.593222, 1126.846338, 1124.367824, 1120.588277, 1120.087856, 1118.011422, 1115.208706, 1112.871991, 1109.416288, 1105.567951, 1104.148744, 1101.919546, 1101.790234, 1099.552397, 1096.330423, 1095.331264, 1092.786203, 1092.200989, 1087.919883, 1081.727391, 1079.016261, 1077.629402, 1077.337904, 1075.163403, 1074.38301, 1068.35739, 1067.703136, 1061.867968, 1059.795267, 1056.403644, 1056.044431, 1054.111103, 1052.682596, 1051.921115, 1048.968314, 1044.110108, 1043.687343, 1043.199388, 1038.856078, 1037.281691, 1036.599627, 1034.075416, 1032.787145, 1030.838362, 1028.42951, 1027.812613, 1027.591789, 1026.260065, 1025.879691, 1025.75754, 1023.460704, 1022.863728, 1019.757197, 1017.758314, 1016.780901, 1014.628674, 1013.73617, 1013.571363, 1010.782543, 1008.002858, 999.1295579, 995.9273681, 991.183645, 989.3236493, 987.7500268, 983.6201851, 982.9974888, 980.8408934, 979.1990608, 978.8809599, 976.0016728, 974.9642359, 971.2768189, 970.0478772, 966.2127785, 965.9969779, 965.3784171, 964.8874629, 964.3332358, 963.7801881, 961.7790379, 960.3579337, 959.2394311, 957.7809415, 955.7453887, 954.2210196, 951.8517159, 946.535368, 945.336902, 944.1094468, 943.7373967, 941.6220537, 937.0831836, 935.8070177, 931.2122754, 930.6109547, 928.0853433, 925.3397715, 922.3438811, 920.4115184, 912.3886736, 910.9261675, 907.2110536, 906.6352857, 905.8184391, 904.4796787, 902.9721771, 902.6241265, 901.9696065, 897.7124732, 897.1383293, 895.8622194, 895.4508619, 893.8418179, 893.3889239, 892.7747139, 888.2900491, 887.4427224, 885.1091102, 885.0932412, 884.7679141, 882.508377, 882.2446372, 881.1754247, 877.5187442, 877.2593994, 876.0743598, 871.5644348, 871.4076601, 871.2209637, 870.3421614, 863.7657341, 863.5264852, 861.8701546, 858.6815321, 856.4581569, 855.0391591, 854.49846, 854.3054712, 853.7921748, 853.0318913, 852.4065466, 851.6714207, 846.3565313, 845.2228731, 844.3701974, 843.3983658, 842.7717954, 841.4215785, 839.9374039, 838.825599, 837.4895447, 834.1876936, 829.6385765, 827.8416141, 826.9371588, 826.0664343, 825.5125662, 816.5339323, 812.1963748, 811.0966651, 808.242991, 806.9416975, 806.8619116, 797.566492, 796.5167433, 793.6278253, 793.3642457, 792.5243127, 792.1382432, 791.9169485, 791.8131184, 789.7831803, 788.1740508, 787.1351167, 786.8782125, 784.9867586, 784.8882175, 783.99879, 782.8663198, 781.194765, 779.3572327, 776.745938, 775.9758159, 770.0498065, 769.3365962, 767.3833588, 764.8125372, 764.2587013, 761.496592, 753.4879043, 753.4396713, 752.651651, 751.1689672, 750.9291798, 749.9579882, 748.8358646, 748.5594224, 745.4921107, 745.2207257, 742.8873921, 742.6788197, 738.6775629, 738.5538945, 736.9886416, 736.4084405, 732.6672067, 732.010734, 729.1289623, 727.9928802, 726.4722843, 720.225938, 715.4438851, 715.1373219, 713.1165075, 712.1278465, 710.6048683, 707.7094165, 706.5709888, 705.8874023, 705.167483, 702.2412876, 701.7661214, 697.3678422, 694.9209607, 694.709037, 694.3889136, 688.9089624, 684.1451165, 683.418807, 683.0899077, 678.3101892, 675.6672961, 675.2123986, 672.4930593, 669.1535667, 660.9548301, 660.8600083, 658.3616346, 652.4744751, 652.4610268, 651.1408316, 649.3858614, 646.1171222, 644.4245654, 644.2383479, 643.2469813, 642.6131328, 638.9827071, 637.634711, 637.2864392, 637.1182495, 635.3725568, 634.9386303, 634.8946421, 629.334521, 628.8774872, 627.9954648, 625.9946813, 625.911998, 625.3008377, 624.8996751, 624.0852773, 624.0660064, 615.344153, 613.2017369, 607.6403875, 607.1923657, 606.1986159, 604.5820313, 603.52593, 602.6713422, 600.8101596, 599.8733035, 597.789212, 589.2054877, 586.9803775, 586.1527025, 584.7219409, 583.3484541, 581.7812837, 579.773307, 578.5019814, 570.3919119, 568.6506694, 563.1666411, 562.2517999, 552.1449616, 550.4393414, 547.300693, 546.199954, 543.0632524, 536.9720894, 531.24197, 527.6668709, 526.6427123, 525.3108337, 524.7920941, 522.5171913, 520.3634474, 518.9653116, 518.4083525, 518.2941518, 515.1354377, 511.0017664, 510.5201361, 510.2869385, 507.9307213, 499.431885, 496.1968125, 495.70367, 494.895735, 492.3695445, 482.4942435, 476.8777563, 471.2113451, 470.0378379, 469.7546877, 467.6250301, 455.3656781, 454.4813888, 453.1099747, 451.4517154, 448.7331175, 448.6784533, 448.5829792, 442.5506385, 439.0521476, 428.2520539, 427.3923817, 424.9362254, 422.4766047, 419.2830827, 416.8038118, 413.03827, 411.574815, 410.0609457, 406.9578849, 406.319656, 405.3870964, 403.2090975, 402.8716719, 398.9192623, 398.1385597, 397.8572882, 394.8155985, 394.65625, 394.2608063, 391.585713, 390.2813817, 389.450683, 387.9230856, 380.9508317, 379.3785577, 379.0625644, 378.6560079, 375.9492167, 374.4047483, 373.2471322, 369.7069281, 368.7658805, 367.4239889, 366.651874, 364.6774251, 364.0237849, 358.4132221, 358.1093351, 354.5073579, 352.170439, 351.8251809, 350.3130112, 346.1239591, 342.7323196, 341.7343064, 341.4242564, 340.0187058, 334.9679536, 333.4959415, 333.0417597, 332.3546815, 331.361254, 329.7876285, 326.0159617, 325.2043804, 317.5399889, 314.7799543, 313.4700666, 312.5299039, 311.7391861, 311.096812, 310.2794472, 308.6645816, 305.0758202, 302.1172086, 297.1469824, 296.4859168, 295.762957, 288.0399864, 287.1573414, 277.3186857, 275.5900518, 274.9778621, 274.4869129, 266.0997099, 264.6975498, 263.4358851, 261.9270252, 259.3890358, 256.041567, 255.961886, 253.9610731, 249.7660092, 249.2191663, 240.9764332, 240.531582, 238.6676034, 238.3738099, 238.0981782, 237.7533817, 237.1176187, 235.4332357, 225.5610561, 223.3514699, 222.9435143, 222.0260022, 221.3971456, 215.0581897, 211.0630019, 209.5825405, 209.1806303, 202.3178019, 200.9174509, 193.971056, 191.0415744, 180.2851978, 176.2487497, 169.7335347, 167.7747727, 167.5659703, 165.9889034, 164.9295344, 163.5380752, 158.1349273, 155.466609, 154.892205, 152.0283988, 150.8599908, 145.314505, 144.8697298, 143.5452798, 140.6873862, 139.5603473, 137.004739, 135.6383229, 134.5633322, 133.8079022, 133.1436126, 133.081783, 131.7795327, 130.6306546, 126.9572748, 126.7740367, 126.2487134, 126.1497647, 123.9997075, 123.8521277, 122.9318168, 120.8381798, 119.0574193, 118.0278118, 112.7328413, 110.8521303, 108.0587739, 106.8793041, 103.2836099, 98.86715022, 97.95189541, 95.78056389, 95.53893423, 89.22290653, 88.20915368, 84.87361316, 79.00572759, 78.11489486, 77.02644374, 76.54465528, 74.55876029, 73.14647847, 71.67859344, 70.48401211, 69.32345053, 68.14130712, 67.3939601, 63.75767917, 63.50609491, 63.05148636, 60.34090639, 60.21976552, 60.10883406, 59.25293442, 58.55301575, 57.23145638, 53.38775428, 52.54602644, 51.27034256, 48.28938364, 47.80301575, 46.4300768, 45.80231686, 44.38462525, 43.1317386, 42.3162932, 38.84064875, 38.7525052, 34.38919372, 31.04706856, 27.58017777, 22.42884977, 20.77124446, 19.13843769, 10.79672252, 10.52793788])
17/3: l
17/4: s
17/5: s.describe()
17/6: s.quantile(0.85)
16/2: help(html.Br)
16/3: help(html.H1)
16/4: %clear
16/5: help(html.Table)
16/6: help(html.Th)
16/7: help(html.Table)
16/8: %clear
16/9: import server
18/1:

import esrver
18/2: import server
18/3: from server import server
18/4: from app import app
18/5: import app2
18/6:
price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
price_df = price_df[price_df.Volume>0]
price_df['strtime'] = price_df.Localtime.dt.strftime("%m/%d %H:%M")
price_df.index = range(len(price_df))
price_df['index2'] = price_df.index//3
18/7: import pandas as pd
18/8:
price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
price_df = price_df[price_df.Volume>0]
price_df['strtime'] = price_df.Localtime.dt.strftime("%m/%d %H:%M")
price_df.index = range(len(price_df))
price_df['index2'] = price_df.index//3
18/9:
price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
price_df = price_df[price_df.Volume>0]
price_df['strtime'] = price_df.Localtime.dt.strftime("%m/%d %H:%M")
price_df.index = range(len(price_df))
price_df['index2'] = price_df.index//3
18/10: price_df.tail(1).index2
18/11: price_df[price_df.index2==408]
18/12: d1 = {'display':'none', 'float':'center'}
18/13:
def start_exp(nclick):
    if nclick:
        d1 = {'display':'none', 'float':'center'}
        d2 = {'display':'block'}
        return [d1,d2]
18/14: start_exp(1)
18/15: type(start_exp(1))
18/16: isinstance(start_exp(1), (list, tuple))
18/17: floor(23.5)
18/18: round(23.5)
18/19: import math
18/20: math.floor(23.5)
18/21: a = 2 if 1>0 else -1
18/22: a
18/23:
conn = sqlite3.connect('database.db')
conn.execute('CREATE TABLE results (exp TEXT, q1 TEXT, p1 TEXT, q2 TEXT, p2 TEXT)')
conn.close()
18/24:
import sqlite3
conn = sqlite3.connect('database.db')
conn.execute('CREATE TABLE results (exp TEXT, q1 TEXT, p1 TEXT, q2 TEXT, p2 TEXT)')
18/25: conn.execute('INSERT INTO results VALUES ("app1", 23,210,-13,280')
18/26: conn.execute('INSERT INTO results VALUES ("app1", 23,210,-13,280)')
18/27: conn.execute('select * from results')
18/28: pd.read_sql_table('results', conn)
18/29: conn.commit()
18/30: cur=conn.execute('select * from results')
18/31:
for r in cur:
    print(r)
18/32: price_df.iloc[1919]
18/33: price_df.iloc[1920]
18/34: conn.close()
18/35:
def create_db():
    import sqlite3
    conn = sqlite3.connect('database.db')
    conn.execute('CREATE TABLE results (exp TEXT, q1 TEXT, p1 TEXT, q2 TEXT, p2 TEXT)')
    conn.commit()
    conn.close()
18/36: create_sb()
18/37: create_db()
18/38: conn = sqlite3.connect('database.db')
18/39: cur = conn.execute('select * from results')
18/40: conn.close()
18/41: create_db()
18/42: conn = sqlite3.connect('database.db')
18/43: cur = conn.execute('select * from results')
18/44:
for r in cur:
    print(r)
18/45: conn.close()
18/46:
def create_db():
    conn = sqlite3.connect('database.db')
    conn.execute('CREATE TABLE results (exp_id TEXT, mturk_id TEXT, q1 TEXT, p1 TEXT, q2 TEXT, p2 TEXT)')
    conn.commit()
    conn.close()
18/47: create_db()
18/48: conn = sqlite3.connect('database.db')
18/49: cur = conn.execute('select * from results')
18/50:
for r in cur:
    print(r)
18/51: conn.close()
18/52: create_db()
18/53: conn = sqlite3.connect('database.db')
18/54: cur = conn.execute('select * from results')
18/55:
for r in cur:
    print(r)
18/56: price_df[price_df.index2==end_P1]
18/57: price_df[price_df.index2==20]
18/58: price_df[price_df.index2==40]
18/59: conn
18/60: cur = conn.execute('select * from results')
18/61: for r in cur: print(r)
18/62: price_df[price_df.index2==20].index
18/63: price_df[0:60]
18/64: price_df[-1:59]
18/65: a=3
18/66:
if 1<a<4:
    print(2)
18/67:
if 1<a<2:
    print(2)
20/1: import nltk
20/2: l = open('test-corpus-for-homework.txt').read()
20/3: l
20/4: len(l)
20/5: tokens = nltk.word_tokenize(l)
20/6: nltk.download('punkt')
20/7: tokens = nltk.word_tokenize(l)
20/8: tokens
20/9: freq = nltk.FreqDist(tokens)
20/10: freq
20/11: freq['Association']
20/12: freq['Breast']
20/13: freq['Cancer']
20/14: freq['Hormone']
20/15: freq['replacement']
20/16: freq['Therapy']
20/17: l
20/18: import re
20/19: l1 = re.sub(r'\-+\nDOCUMENT .{1,3}\n\-+', 'enddoc', l)
20/20: l1
20/21: l1 = re.sub(r'\-+\nDOCUMENT .{1,3}\n\-+\n', 'enddoc', l)
20/22: doclist = l1.split('enddoc')
20/23: doclist
20/24: len(doclist)
20/25: doclist = l1.split('enddoc')[1:]
20/26: len(doclist)
20/27: doclist[0]
20/28:
def idf(w):
    N = 3
    dcount=1
    for doc in doclist:
        if w in doc.replace(r'\n',' '):
            dcount+=1
    return N/dcount
20/29: idf('Breast')
20/30: idf('Association')
20/31: wordlist = ['Association', 'Breast', 'Cancer', 'Hormone', 'replacement', 'Therapy']
20/32: [idf(w) for w in wordlist]
20/33: log(2)
20/34: import math
20/35: math.log(2)
20/36:
def idf(w):
    N = 3
    dcount=1
    for doc in doclist:
        if w in doc.replace(r'\n',' '):
            dcount+=1
    return math.log(N/dcount)
20/37: [idf(w) for w in wordlist]
20/38: [round(idf(w),2) for w in wordlist]
20/39: freqlist = [nltk.FreqDist(l) for l in doclist]
20/40:
def tfidf(w, d):
    d=d-1
    tf = freqlist[d][w]
    idf = idf(w)
    return tf*idf
20/41: [tfidf(w,1) for w in wordlist]
20/42:
def tfidf(w, d):
    d=d-1
    tf = freqlist[d][w]
    idf0 = idf(w)
    return tf*idf0
20/43: [tfidf(w,1) for w in wordlist]
20/44: [tfidf(w,2) for w in wordlist]
20/45: [tfidf(w,3) for w in wordlist]
20/46: freqlist[0]
20/47: doclist
20/48: doclist[0]
20/49: freqlist = [nltk.FreqDist(l) for l in nltk.word_tokenize(doclist)]
20/50: freqlist = [nltk.FreqDist(nltk.word_tokenize(l)) for l in doclist]
20/51: freqlist[0]
20/52: [tfidf(w,1) for w in wordlist]
20/53: [round(tfidf(w,1),2) for w in wordlist]
20/54: [round(tfidf(w,2),2) for w in wordlist]
20/55: [round(tfidf(w,3),2) for w in wordlist]
20/56: idf('Cancer')
20/57: from sklearn.feature_extraction.text import TfidfVectorizer
20/58: sktf = TfidfVectorizer().fit_transform(doclist)
20/59: sktf
20/60: sktf.toarray()
20/61: import pandas as pd
20/62: tf = TfidfVectorizer()
20/63: sktf = tf.fit_transform(doclist)
20/64: tfdf = pd.DataFrame(sktf.toarray(), columns=tf.get_feature_names())
20/65: tfdf
20/66: tfdfw = tfdf[wordlist]
20/67: tfdfw = tfdf[[wordlist]]
20/68: tfdfw = tfdf['Cancer']
20/69: tfdf
20/70: tfdf.columns
20/71: tfdfw = tfdf['cancer']
20/72: tfdfw
20/73: wordlist = [w.lower() for w in wordlist]
20/74: wordlist
20/75: doclist = [d.lower() for d in doclist]
20/76: doclist[0]
20/77: doclist = [d.lower().replace(r'\n', ' ') for d in doclist]
20/78: doclist[0]
20/79: doclist = [re.sub(r'\n', ' ', d.lower()) for d in doclist]
20/80: doclist[0]
20/81: idf('cancer')
20/82: idf
20/83:
def idf(w):
    den = sum([1 for x in doclist if w in x])
    return math.log(3/(1+den))
20/84: idf('cancer')
20/85:
def idf(w):
    den = sum([1 for x in doclist if w in set(nltk.word_tokenize(x))])
    return math.log(3/(1+den))
20/86: idf('cancer')
20/87: w='cancer'
20/88: sum([1 for x in doclist if w in set(nltk.word_tokenize(x))])
20/89:
def idf(w):
    den = sum([1 for x in doclist if w in set(nltk.word_tokenize(x))])
    return math.log(3/(max(1,den)))
20/90: idf('cancer')
20/91: wordlist
20/92: [round(idf(w), 2) for w in wordlist]
20/93: freqlist
20/94: freqlist = [nltk.FreqDist(d) for d in doclist]
20/95: freqlist
20/96: freqlist = [nltk.FreqDist(nltk.word_tokenize(d)) for d in doclist]
20/97: freqlist
20/98: freqs = nltk.FreqDist(nltk.word_tokenize(nltk.word_tokenize(' '.join(doclist))))
20/99: freqs = nltk.FreqDist(nltk.word_tokenize(' '.join(doclist)))
20/100: [freqs[w] for w in wordlist]
20/101: [tfidf(w,1) for w in wordlist]
20/102: sktf
20/103: tfdf
20/104: tfdf[wordlist]
20/105: tfdf[wordlist[1:]]
20/106: freqlist[0]['breast']
20/107: freqlist[1]['breast']
20/108: freqlist[2]['breast']
20/109: idf('breast')
20/110: idf('breast')*9
20/111: math.log(100,10)
20/112: idf
20/113:
def idf(w):
    den = sum([1 for x in doclist if w in set(nltk.word_tokenize(x))])
    return math.log(3/(max(1,den)),10)
20/114: idf('breast')
20/115: idf('breast')*9
20/116:  sum([1 for x in doclist if 'breast' in set(nltk.word_tokenize(x))])
20/117:  sum([1 for x in doclist if 'hormone' in set(nltk.word_tokenize(x))])
20/118:
def tfidf(w, d):
    d=d-1
    tf = 1+log(freqlist[d][w],10)
    idf0 = idf(w)
    return tf*idf0
20/119: tfidf('breast',1)
20/120:
def tfidf(w, d):
    d=d-1
    tf = 1+math.log(freqlist[d][w],10)
    idf0 = idf(w)
    return tf*idf0
20/121: tfidf('breast',1)
20/122: tfidf('breast',3)
20/123:
def tfidf(w, d):
    d=d-1
    tf = 1+math.log(max(1,freqlist[d][w],10))
    idf0 = idf(w)
    return tf*idf0
20/124: tfidf('breast',3)
20/125: idf('breast')
20/126: freqlist[2]['breast']
20/127:
def tfidf(w, d):
    d=d-1
    tf = 1+math.log(max(1,freqlist[d][w]),10)
    idf0 = idf(w)
    return tf*idf0
20/128: tfidf('breast',3)
20/129: [idf(w) for w in wordlist]
20/130: [tfidf(w,1) for w in wordlist]
20/131: [tfidf(w,2) for w in wordlist]
20/132: [tfidf(w,3) for w in wordlist]
20/133: sktf
20/134: from scipy import spatial
20/135: 1-spatial.distance.cosine(sktf.toarray()[0], sktf.toarray()[1])
20/136: 1-spatial.distance.cosine(sktf.toarray()[0], sktf.toarray()[2])
20/137: 1-spatial.distance.cosine(sktf.toarray()[1], sktf.toarray()[2])
20/138: 0.344+0.378
20/139: 0.409+0.176
20/140: 176+229
21/1:
import pandas as pd
import prince
import numpy as np
import math
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RandomizedSearchCV
from sklearn.svm import SVC
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, roc_curve
import xgboost as xgb
21/2:
import pandas as pd
# import prince
import numpy as np
import math
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RandomizedSearchCV
from sklearn.svm import SVC
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, roc_curve
import xgboost as xgb
21/3:
import pandas as pd
# import prince
import numpy as np
import math
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RandomizedSearchCV
from sklearn.svm import SVC
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, roc_curve
# import xgboost as xgb
21/4: X_cont = pd.read_pickle('ContinuousTrainData.pkl')
22/1:
import pandas as pd
import numpy as np
import math
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RandomizedSearchCV
from sklearn.svm import SVC
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, roc_curve
# import xgboost as xgb
22/2: X_cont = pd.read_pickle('preproc_data/ContinuousTrainData.pkl')
22/3:
X = X_cont.copy()

# X = X[X.DISEASE_TYPE=='UC']
22/4:
# Drop rows with missing charge data
X.dropna(subset=['CHARGE_FLAG_85pctile'], inplace=True)
Y = X.CHARGE_FLAG_85pctile

# continuous:
X.drop(['ANNUAL_AVG_CHARGE', 'CHARGE_FLAG_85pctile', 'AUTO_ID', 'PAT_ID', 'PAT_MRN_ID', 'DISEASE_TYPE'], axis=1, inplace=True)
22/5: xtr, xte, ytr, yte = train_test_split(X, Y, test_size=0.33, random_state=3)
22/6:
X_SVM = X.copy()
X_SVM.drop(['AVG_SIBDQ_SCORE','PSYCH_DX'], axis=1, inplace=True)
X_SVM.dropna(subset=['PSYCH_DURATION_YEARS'], inplace=True)
X_SVM.fillna(0, inplace=True)
22/7:
Y_SVM = Y[X_SVM.index]
xtr, xte, ytr, yte = train_test_split(X_SVM, Y_SVM, test_size=0.33, random_state=3)
22/8:
scaler = StandardScaler().fit(xtr)

kfold=5
results = []
for c in [5, 8, 12]:
    for gamma in [0.01, 0.05, 0.1]:
        scores = cross_val_score(SVC(C=c, gamma=gamma), scaler.transform(xtr), ytr, cv=kfold)
        mean_score = np.mean(scores)
        results.append((c, gamma, mean_score))

best_c, best_gamma, best_score = max(results, key=lambda tup:tup[2])
winner = SVC(C=best_c, gamma=best_gamma).fit(scaler.transform(xtr), ytr)
22/9:
print(winner)
print("Train Score: ", best_score)
print("Test Score: ", winner.score(scaler.transform(xte), yte))
print("AUC: ", roc_auc_score(yte, winner.predict(scaler.transform(xte))))
print("ConfMat: \n",confusion_matrix(yte, winner.predict(scaler.transform(xte))))
print("Train Label_Count: ", np.bincount(ytr))
print("Test Label_Count: ", np.bincount(yte))
22/10:
winner = SVC(C=best_c, gamma=best_gamma, class_weight='balanced').fit(scaler.transform(xtr), ytr)
print(winner)
print("Train Score: ", best_score)
print("Test Score: ", winner.score(scaler.transform(xte), yte))
print("AUC: ", roc_auc_score(yte, winner.predict(scaler.transform(xte))))
print("ConfMat: \n",confusion_matrix(yte, winner.predict(scaler.transform(xte))))
print("Train Label_Count: ", np.bincount(ytr))
print("Test Label_Count: ", np.bincount(yte))
22/11:
winner = SVC(C=best_c, gamma=best_gamma).fit(scaler.transform(xtr), ytr)
print(winner)
print("Train Score: ", best_score)
print("Test Score: ", winner.score(scaler.transform(xte), yte))
print("AUC: ", roc_auc_score(yte, winner.predict(scaler.transform(xte))))
print("ConfMat: \n",confusion_matrix(yte, winner.predict(scaler.transform(xte))))
print("Train Label_Count: ", np.bincount(ytr))
print("Test Label_Count: ", np.bincount(yte))
22/12:
winner = SVC(C=best_c, gamma=best_gamma, coef0=0.5).fit(scaler.transform(xtr), ytr)
print(winner)
print("Train Score: ", best_score)
print("Test Score: ", winner.score(scaler.transform(xte), yte))
print("AUC: ", roc_auc_score(yte, winner.predict(scaler.transform(xte))))
print("ConfMat: \n",confusion_matrix(yte, winner.predict(scaler.transform(xte))))
print("Train Label_Count: ", np.bincount(ytr))
print("Test Label_Count: ", np.bincount(yte))
22/13:
winner = SVC(C=best_c, gamma=best_gamma, coef0=5).fit(scaler.transform(xtr), ytr)
print(winner)
print("Train Score: ", best_score)
print("Test Score: ", winner.score(scaler.transform(xte), yte))
print("AUC: ", roc_auc_score(yte, winner.predict(scaler.transform(xte))))
print("ConfMat: \n",confusion_matrix(yte, winner.predict(scaler.transform(xte))))
print("Train Label_Count: ", np.bincount(ytr))
print("Test Label_Count: ", np.bincount(yte))
22/14:
winner = SVC(C=best_c, gamma=best_gamma, degree=2).fit(scaler.transform(xtr), ytr)
print(winner)
print("Train Score: ", best_score)
print("Test Score: ", winner.score(scaler.transform(xte), yte))
print("AUC: ", roc_auc_score(yte, winner.predict(scaler.transform(xte))))
print("ConfMat: \n",confusion_matrix(yte, winner.predict(scaler.transform(xte))))
print("Train Label_Count: ", np.bincount(ytr))
print("Test Label_Count: ", np.bincount(yte))
22/15:
winner = SVC(C=best_c, gamma=best_gamma, degree=4).fit(scaler.transform(xtr), ytr)
print(winner)
print("Train Score: ", best_score)
print("Test Score: ", winner.score(scaler.transform(xte), yte))
print("AUC: ", roc_auc_score(yte, winner.predict(scaler.transform(xte))))
print("ConfMat: \n",confusion_matrix(yte, winner.predict(scaler.transform(xte))))
print("Train Label_Count: ", np.bincount(ytr))
print("Test Label_Count: ", np.bincount(yte))
22/16:
winner = SVC(C=best_c, gamma=best_gamma, degree=11).fit(scaler.transform(xtr), ytr)
print(winner)
print("Train Score: ", best_score)
print("Test Score: ", winner.score(scaler.transform(xte), yte))
print("AUC: ", roc_auc_score(yte, winner.predict(scaler.transform(xte))))
print("ConfMat: \n",confusion_matrix(yte, winner.predict(scaler.transform(xte))))
print("Train Label_Count: ", np.bincount(ytr))
print("Test Label_Count: ", np.bincount(yte))
22/17:
winner = SVC(C=best_c, gamma=best_gamma, probability=True).fit(scaler.transform(xtr), ytr)
print(winner)
print("Train Score: ", best_score)
print("Test Score: ", winner.score(scaler.transform(xte), yte))
print("AUC: ", roc_auc_score(yte, winner.predict(scaler.transform(xte))))
print("ConfMat: \n",confusion_matrix(yte, winner.predict(scaler.transform(xte))))
print("Train Label_Count: ", np.bincount(ytr))
print("Test Label_Count: ", np.bincount(yte))
22/18:
winner = SVC(C=1, gamma=best_gamma).fit(scaler.transform(xtr), ytr)
print(winner)
print("Train Score: ", best_score)
print("Test Score: ", winner.score(scaler.transform(xte), yte))
print("AUC: ", roc_auc_score(yte, winner.predict(scaler.transform(xte))))
print("ConfMat: \n",confusion_matrix(yte, winner.predict(scaler.transform(xte))))
print("Train Label_Count: ", np.bincount(ytr))
print("Test Label_Count: ", np.bincount(yte))
22/19:
winner = SVC(C=best_c).fit(scaler.transform(xtr), ytr)
print(winner)
print("Train Score: ", best_score)
print("Test Score: ", winner.score(scaler.transform(xte), yte))
print("AUC: ", roc_auc_score(yte, winner.predict(scaler.transform(xte))))
print("ConfMat: \n",confusion_matrix(yte, winner.predict(scaler.transform(xte))))
print("Train Label_Count: ", np.bincount(ytr))
print("Test Label_Count: ", np.bincount(yte))
22/20:
winner = rf_gridsearch.best_estimator_
print("Best estimator: \n",winner)
print("Train Score: ", winner.score(xtr,ytr))
print("Test Score: ", winner.score(xte, yte))
print("AUC: ", roc_auc_score(yte, winner.predict(xte)))
print("ConfMat: \n",confusion_matrix(yte, winner.predict(xte)))
print("Train Label_Count: ", np.bincount(ytr))
print("Test Label_Count: ", np.bincount(yte))
print("\n\n",sorted(list(zip(X.columns, winner.feature_importances_)), key=lambda x:x[1], reverse=True))
22/21:
# Number of features to consider at every split
max_features = [10, 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(60, 100, num = 5)]
# Minimum number of samples required to split a node
min_samples_split = [5]
# Minimum number of samples required at each leaf node
min_samples_leaf = [2]
# Method of selecting samples for training each tree
bootstrap = [True]
# Create the random grid
random_grid = {'n_estimators': [int(x) for x in np.linspace(90, 150, num = 3)],
               'max_features': max_features,
               'max_depth': [5,10],
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap,
               'class_weight':['balanced', 'balanced_subsample']}
rf_gridsearch = RandomizedSearchCV(estimator = RandomForestClassifier(), param_distributions = random_grid, n_iter = 30, cv = 3, verbose=2, random_state=42, n_jobs = -1)
rf_gridsearch.fit(xtr, ytr)
22/22:
# Number of features to consider at every split
max_features = [10, 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(60, 100, num = 5)]
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 7 ]
# Minimum number of samples required at each leaf node
min_samples_leaf = [2]
# Method of selecting samples for training each tree
bootstrap = [True]
# Create the random grid
random_grid = {'n_estimators': [int(x) for x in np.linspace(90, 150, num = 3)],
               'max_features': max_features,
               'max_depth': [5,10],
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap,
               'class_weight':['balanced', 'balanced_subsample']}
print(random_grid)
rf_gridsearch = RandomizedSearchCV(estimator = RandomForestClassifier(), param_distributions = random_grid, n_iter = 30, cv = 3, verbose=2, random_state=42, n_jobs = -1)
rf_gridsearch.fit(xtr, ytr)
22/23:
winner = rf_gridsearch.best_estimator_
print("Best estimator: \n",winner)
print("Train Score: ", winner.score(xtr,ytr))
print("Test Score: ", winner.score(xte, yte))
print("AUC: ", roc_auc_score(yte, winner.predict(xte)))
print("ConfMat: \n",confusion_matrix(yte, winner.predict(xte)))
print("Train Label_Count: ", np.bincount(ytr))
print("Test Label_Count: ", np.bincount(yte))
print("\n\n",sorted(list(zip(X.columns, winner.feature_importances_)), key=lambda x:x[1], reverse=True))
23/1: from nltk.tag.stanford import POSTagger
23/2: from nltk import pos_tag
23/3: pos_tag(['hello','world'])
23/4: nltk.download('averaged_perceptron_tagger')
23/5: import nltk
23/6: nltk.download('averaged_perceptron_tagger')
23/7: pos_tag(['hello','world'])
23/8: %clear
23/9: from nltk import word_tokenize
23/10: %clear
23/11:
def remove_stopwords(s):
    # nltk_stopwords = ['i',  'me',  'my',  'myself',  'we',  'our',  'ours',  'ourselves',  'you',  "you're",  "you've",  "you'll",  "you'd",  'your',  'yours',  'yourself',  'yourselves',  'he',  'him',  'his',  'himself',  'she',  "she's",  'her',  'hers',  'herself',  'it',  "it's",  'its',  'itself',  'they',  'them',  'their',  'theirs',  'themselves',  'what',  'which',  'who',  'whom',  'this',  'that',  "that'll",  'these',  'those',  'am',  'is',  'are',  'was',  'were',  'be',  'been',  'being',  'have',  'has',  'had',  'having',  'do',  'does',  'did',  'doing',  'a',  'an',  'the',  'and',  'but',  'if',  'or',  'because',  'as',  'until',  'while',  'of',  'at',  'by',  'for',  'with',  'about',  'against',  'between',  'into',  'through',  'during',  'before',  'after',  'above',  'below',  'to',  'from',  'up',  'down',  'in',  'out',  'on',  'off',  'over',  'under',  'again',  'further',  'then',  'once',  'here',  'there',  'when',  'where',  'why',  'how',  'all',  'any',  'both',  'each',  'few',  'more',  'most',  'other',  'some',  'such',  'no',  'nor',  'not',  'only',  'own',  'same',  'so',  'than',  'too',  'very',  's',  't',  'can',  'will',  'just',  'don',  "don't",  'should',  "should've",  'now',  'd',  'll',  'm',  'o',  're',  've',  'y',  'ain',  'aren',  "aren't",  'couldn',  "couldn't",  'didn',  "didn't",  'doesn',  "doesn't",  'hadn',  "hadn't",  'hasn',  "hasn't",  'haven',  "haven't",  'isn',  "isn't",  'ma',  'mightn',  "mightn't",  'mustn',  "mustn't",  'needn',  "needn't",  'shan',  "shan't",  'shouldn',  "shouldn't",  'wasn',  "wasn't",  'weren',  "weren't",  'won',  "won't",  'wouldn',  "wouldn't"]
    nltk_stopwords = stopwords.words('english')
    STOPWORDS = nltk_stopwords+["'s", "s"]+[x.capitalize() for x in nltk_stopwords]
    
    s = re.sub(r'([a-zA-Z])[\-\/]([a-zA-Z])', r'\1 \2', s) # split hyphenated and slashed words
    s = re.sub(r'[^A-Za-z\s]+', '', s)
    tokens = word_tokenize(s)
    okstr = ' '.join([x for x in tokens if x in ['(',')'] or x not in STOPWORDS and len(x)>1])
    return okstr



def filter_pos_tags(s):
    tagger = POSTagger('tagger/english-left3words-distsim.tagger', 'tagger/stanford-postagger.jar')
    tagged_tokens = tagger.tag(tokens)



def abbr_expander(s):   
    res = re.findall(r'\(\s?[A-Z]\s?[A-Za-z]+\s?\)', s) # Misses acronyms with hyphen
    outs = s
    for abbr in res:
        ix = s.index(abbr)
        abbr_clean = re.sub(r'[^A-Za-z]', '', abbr) # keep only letters, discard all other chars
        abbr_len = len(re.sub(r'[a-z]','',abbr_clean)) # Number of capital letters ~ number of tokens to look back for expansion
        if abbr_clean[-1]=='s': abbr_len-=1 # Singularing a plural acronym
        # When looking for expansion candidates, consider (-) and (/) as word-breaks
        expanded = ' '.join(s[ix-1::-1][::-1].rstrip().split(' ')[-abbr_len:]) # identify possible candidates for expansion from preivous abbr_len words
        # ABBR_LIST[abbr_clean].append(expanded)
        outs = outs.replace(' '+abbr, '') # Remove first instance of abbreviation before expanding other instances
        if abbr_clean[-1]=='s':
            outs = outs.replace(abbr_clean[:-1], expanded)
        else:
            outs = outs.replace(abbr_clean, expanded) 
    
    return outs
23/12:
def remove_stopwords(s):
    # nltk_stopwords = ['i',  'me',  'my',  'myself',  'we',  'our',  'ours',  'ourselves',  'you',  "you're",  "you've",  "you'll",  "you'd",  'your',  'yours',  'yourself',  'yourselves',  'he',  'him',  'his',  'himself',  'she',  "she's",  'her',  'hers',  'herself',  'it',  "it's",  'its',  'itself',  'they',  'them',  'their',  'theirs',  'themselves',  'what',  'which',  'who',  'whom',  'this',  'that',  "that'll",  'these',  'those',  'am',  'is',  'are',  'was',  'were',  'be',  'been',  'being',  'have',  'has',  'had',  'having',  'do',  'does',  'did',  'doing',  'a',  'an',  'the',  'and',  'but',  'if',  'or',  'because',  'as',  'until',  'while',  'of',  'at',  'by',  'for',  'with',  'about',  'against',  'between',  'into',  'through',  'during',  'before',  'after',  'above',  'below',  'to',  'from',  'up',  'down',  'in',  'out',  'on',  'off',  'over',  'under',  'again',  'further',  'then',  'once',  'here',  'there',  'when',  'where',  'why',  'how',  'all',  'any',  'both',  'each',  'few',  'more',  'most',  'other',  'some',  'such',  'no',  'nor',  'not',  'only',  'own',  'same',  'so',  'than',  'too',  'very',  's',  't',  'can',  'will',  'just',  'don',  "don't",  'should',  "should've",  'now',  'd',  'll',  'm',  'o',  're',  've',  'y',  'ain',  'aren',  "aren't",  'couldn',  "couldn't",  'didn',  "didn't",  'doesn',  "doesn't",  'hadn',  "hadn't",  'hasn',  "hasn't",  'haven',  "haven't",  'isn',  "isn't",  'ma',  'mightn',  "mightn't",  'mustn',  "mustn't",  'needn',  "needn't",  'shan',  "shan't",  'shouldn',  "shouldn't",  'wasn',  "wasn't",  'weren',  "weren't",  'won',  "won't",  'wouldn',  "wouldn't"]
    nltk_stopwords = stopwords.words('english')
    STOPWORDS = nltk_stopwords+["'s", "s"]+[x.capitalize() for x in nltk_stopwords]
    
    s = re.sub(r'([a-zA-Z])[\-\/]([a-zA-Z])', r'\1 \2', s) # split hyphenated and slashed words
    s = re.sub(r'[^A-Za-z\s]+', '', s)
    tokens = word_tokenize(s)
    okstr = ' '.join([x for x in tokens if x in ['(',')'] or x not in STOPWORDS and len(x)>1])
    return okstr



def filter_pos_tags(s):
    tagger = POSTagger('tagger/english-left3words-distsim.tagger', 'tagger/stanford-postagger.jar')
    tagged_tokens = tagger.tag(tokens)



def abbr_expander(s):   
    res = re.findall(r'\(\s?[A-Z]\s?[A-Za-z]+\s?\)', s) # Misses acronyms with hyphen
    outs = s
    for abbr in res:
        ix = s.index(abbr)
        abbr_clean = re.sub(r'[^A-Za-z]', '', abbr) # keep only letters, discard all other chars
        abbr_len = len(re.sub(r'[a-z]','',abbr_clean)) # Number of capital letters ~ number of tokens to look back for expansion
        if abbr_clean[-1]=='s': abbr_len-=1 # Singularing a plural acronym
        # When looking for expansion candidates, consider (-) and (/) as word-breaks
        expanded = ' '.join(s[ix-1::-1][::-1].rstrip().split(' ')[-abbr_len:]) # identify possible candidates for expansion from preivous abbr_len words
        # ABBR_LIST[abbr_clean].append(expanded)
        outs = outs.replace(' '+abbr, '') # Remove first instance of abbreviation before expanding other instances
        if abbr_clean[-1]=='s':
            outs = outs.replace(abbr_clean[:-1], expanded)
        else:
            outs = outs.replace(abbr_clean, expanded) 
    
    return outs


def clean_sent(abst):
    abst = abbr_expander(abst)
    # Make post-period uppercase chars lower
    abst = re.sub(r"(?<=\. )[A-Z]",lambda t:t.group().lower(), abst)
    abst = remove_stopwords(abst)
    return abst
23/13: s="Serotonin is strongly implicated in the mammalian stress response but surprisingly little is known about its mode of action. Recent data suggest that serotonin can inhibit aversive responding in humans but this remains underspecified. In particular data in rodents suggest that global serotonin depletion may specifically increase long-duration bed nucleus of the stria terminalis (BNST)-mediated aversive responses (ie anxiety) but not short-duration BNST-independent responses (ie fear). Here we extend these findings to humans. In a balanced placebo-controlled crossover design healthy volunteers (n=20) received a controlled diet with and without the serotonin precursor tryptophan (acute tryptophan depletion; ATD). Aversive states were indexed by translational acoustic startle measures. Fear and anxiety were operationally defined as the increase in startle reactivity during short- and long-duration threat periods evoked by predictable shock (fear-potentiated startle) and by the context in which the shocks were administered (anxiety-potentiated startle) respectively. ATD significantly increased long-duration anxiety-potentiated startle but had no effect on short-duration fear-potentiated startle. These results suggest that serotonin depletion in humans selectively increases anxiety but not fear. Current translational frameworks support the proposition that ATD thus disinhibits dorsal raphe-originating serotonergic control of corticotropin-releasing hormone-mediated excitation of the BNST. This generates a candidate neuropharmacological mechanism by which depleted serotonin may increase response to sustained threats alongside clear implications for our understanding of the manifestation and treatment of mood and anxiety disorders."
23/14: a = clean_sent(s)
23/15: import re
23/16: a = clean_sent(s)
23/17:
from nltk import word_tokenize
from nltk.corpus import stopwords
import io
import pandas as pd
import re
import numpy as np
from nltk import pos_tag
23/18: a = clean_sent(s)
23/19: A
23/20: a
23/21: pos_tag(word_tokenize(a))
23/22: a
23/23: map(upper, ['vbd', 'in', 'vbp', 'rb', 'md', 'vb', 'vbz', 'vbg'])
23/24: map(str.upper, ['vbd', 'in', 'vbp', 'rb', 'md', 'vb', 'vbz', 'vbg'])
23/25: list(map(str.upper, ['vbd', 'in', 'vbp', 'rb', 'md', 'vb', 'vbz', 'vbg']))
23/26:
def filter_pos_tags(s):
    OUT = ['VBD', 'IN', 'VBP', 'RB', 'MD', 'VB', 'VBZ', 'VBG']
    return ' '.join([w for w,t in pos_tag(word_tokenize(s)) if t not in OUT])
23/27: clear
23/28: filter_pos_tags(a)
23/29:
def filter_pos_tags(s):
    OUT = ['VBD', 'IN', 'VBP', 'RB', 'MD', 'VB', 'VBZ', 'VBG']
    return ' '.join([w for w,t in pos_tag(word_tokenize(s)) if t in OUT])
23/30: %clear
23/31: filter_pos_tags(a)
24/1: import pandas as pd
24/2: from sklearn.metrics import average_precision_score
24/3: dfuw = pd.read_csv('test_results_unweighted.csv')
24/4: dftf = pd.read_csv('test_results_tfidf.csv')
24/5:
def ap(df, rdoc='fear'):
    df['rdoc'] = np.where(df['rdoc']==rdoc, 1, 0)
    return average_precision_score(df['rdoc'], df[rdoc+'_sim'])
24/6: ap(dftf)
24/7: import numpy as np
24/8: ap(dftf)
24/9: ap(dfuw)
24/10: ap(dftf, 'loss')
24/11: ap(dftf, 'sleep_wf')
24/12: dftf
24/13: dftf.rdoc
24/14:
def ap(df, rdoc='fear'):
    df = df.copy()
    df['rdoc'] = np.where(df['rdoc']==rdoc, 1, 0)
    return average_precision_score(df['rdoc'], df[rdoc+'_sim'])
24/15: dftf = pd.read_csv('test_results_tfidf.csv')
24/16: dfuw = pd.read_csv('test_results_unweighted.csv')
24/17: ap(dftf, 'loss')
24/18: dftf.rdoc
24/19: ap(dfuw, 'loss')
24/20: ap(dftf, 'sleep_wf')
24/21: clear
24/22: scores = pd.DataFrame(index=['fear','loss','sleep_wf','circadian','arousal','frusnrew','anxiety','susthreat'])
24/23: scores = pd.DataFrame(index=['fear','loss','sleep_wf','circadian','arousal','frusnrew','anxiety','susthreat'], columns=['tfidf','unweighted'])
24/24:
for ix in scores.index:
    scores.iloc[ix]['tfidf'] = ap(dftf, ix)
    scores.iloc[ix]['unweighted'] = ap(dfuw, ix)
24/25:
for ix in scores.index:
    scores.iloc[ix] = [ap(dftf, ix), ap(dfuw, ix)]
24/26:
for ix in scores.index:
    scores.ix[ix] = [ap(dftf, ix), ap(dfuw, ix)]
24/27: scores
24/28: scores.mean()
25/1: import pandas as pd
25/2: n = pd.DataFrame(columns=['strtime','High','Low'])
25/3: n.strtime = list(range(1,10))
25/4: n.High = list(range(1,4))
25/5: n.High = list(range(1,4))+[np.nan]*6
25/6: import numpy as np
25/7: n.High = list(range(1,4))+[np.nan]*6
25/8: n
25/9: n.plot.line()
25/10: n.plot.line(x='strtime', y='High')
25/11: pd.concat([n[:4], n[4:]])
25/12:
price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
price_df = price_df[price_df.Volume>0]
25/13:
price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])

price_df = price_df[price_df.Volume>0]
25/14: price_df.loc[750:850]
25/15: price_df.loc[750:850].plot.line()
25/16: price_df.loc[750:850].High.plot.line
25/17: price_df.loc[750:850].plot.line()
25/18: price_df.loc[750:850].High.plot.line()
25/19: price_df.loc[750:950].High.plot.line()
25/20: price_df.High.plot.line()
25/21: price_df.index = range(len(price_df))
25/22: price_df.High.plot.line()
25/23: price_df.loc[750:850].High.plot.line()
25/24: price_df.loc[700:800].High.plot.line()
25/25: price_df.loc[760:800].High.plot.line()
25/26: 795//3
25/27: price_df.High.plot.line().get_figure().savefig('AAPL')
25/28: import matplotlib.pyplot as plt
25/29: plt.figure(figsize=(20,10))
25/30: price_df.High.plot.line()
25/31: plt.figure(figsize=(20,10))
25/32: price_df.High.plot.line()
25/33: %clear
25/34: plt.figure(figsize=(20,10))
25/35: plt.plot(price_df.High)
25/36: %matplotlib inline
25/37: plt.figure(figsize=(20,10))
25/38: plt.plot(price_df.High)
25/39: price_df.High.plot.line(figsize=(40,40))
25/40: price_df.High.plot.line(figsize=(40,30)).get_figure().savefig('AAPL')
25/41: %clear
25/42: 1000//3
25/43: 1320//3
25/44: price_df[1300:1575].High.plot.line(figsize=(20,15))
25/45: price_df[1300:1575].High.plot.line(xticklabels=range(1300,1576), figsize=(20,15))
25/46: price_df[1300:1575].High.plot.line(xlabels=range(1300,1576), figsize=(20,15))
25/47: %clear
25/48: plt.xticks(range(1300, 1576))
25/49: price_df[1300:1575].High.plot.line(xlabels=range(1300,1576), figsize=(20,15))
25/50: price_df[1300:1575].High.plot.line(xticks=range(1300,1576), figsize=(20,15))
25/51: price_df[1300:1575].High.plot.line(xticks=range(1300,1576, 100), figsize=(20,15))
25/52: price_df[1300:1575].High.plot.line(xticks=range(1300,1576, 10), figsize=(20,15))
25/53:
price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
price_df = price_df[price_df.Volume>0]
price_df=price_df.loc[1000:1650]
price_df.index = range(len(price_df))
25/54:
price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
price_df = price_df[price_df.Volume>0]
price_df=price_df.loc[1000:1650]
price_df.index = range(len(price_df))
25/55:
price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
price_df = price_df[price_df.Volume>0]
25/56:
price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
price_df = price_df[price_df.Volume>0]
price_df.index = range(len(price_df))
price_df=price_df.loc[1000:1650]
25/57: %clear
25/58: price_df.High.plot.line(xticks=range(1,652, 10), figsize=(20,15))
25/59:
price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
price_df = price_df[price_df.Volume>0]
25/60: price_df.index
25/61: price_df.loc[0:5]
25/62: price_df.iloc[0:5]
25/63:
price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
price_df = price_df[price_df.Volume>0]
price_df=price_df.iloc[1000:1650]
price_df.index = range(len(price_df))
price_df['strtime'] = price_df.Localtime.dt.strftime("%m/%d %H:%M")
price_df['index2'] = price_df.index//minutes_per_second
25/64:
minutes_per_second = 3
end_P1 = 320//minutes_per_second
end_P2 = 520//minutes_per_second
25/65:
price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
price_df = price_df[price_df.Volume>0]
price_df=price_df.iloc[1000:1650]
price_df.index = range(len(price_df))
price_df['strtime'] = price_df.Localtime.dt.strftime("%m/%d %H:%M")
price_df['index2'] = price_df.index//minutes_per_second
MAX_LEN = len(price_df)-1
25/66: price_df.High.plot.line(xticks=range(1,652, 10), figsize=(20,15)).get_figure().savefig('trend1.png')
25/67: price_df.High.plot.line(xticks=df.index, figsize=(20,15)).get_figure().savefig('trend1.png')
25/68: price_df.High.plot.line(xticks=price_df.index, figsize=(20,15)).get_figure().savefig('trend1.png')
25/69: price_Df.index
25/70: price_df.index
25/71: price_df.High.plot.line(xticks=range(0,650,10), figsize=(20,15)).get_figure().savefig('trend1.png')
25/72:
end_P1 = 330//minutes_per_second
end_P2 = 510//minutes_per_second
25/73: price_df.iloc[509]
25/74: price_df.iloc[330]
25/75: price_df.iloc[510]
25/76: 10000-15000
25/77: 100000-15000
25/78:
WINDOW_SIZE = 500
minutes_per_second = 6
price_multiplier = 10

end_P1 = 330//minutes_per_second
end_P2 = 510//minutes_per_second
27/1: import config
27/2: import config
27/3: config.screen1
27/4: import utils
27/5: html.Div(utils.get_screens())
27/6: import dash_html_components as html
27/7: html.Div(utils.get_screens())
27/8: app = dash.Dash('app1', external_stylesheets=external_stylesheets)
27/9: import dash
27/10: app = dash.Dash('app1', external_stylesheets=external_stylesheets)
27/11:
external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']
app = dash.Dash('app1', external_stylesheets=external_stylesheets)
27/12: app.callback
27/13: app.callback()
27/14: app.layout()
27/15: app.layout
27/16: app.layout = html.Div(utils.get_screens())
27/17: app.layout
27/18: app.callback_map
27/19: app.config
27/20: app = dash.Dash(__name__, external_stylesheets=external_stylesheets)
27/21: app = dash.Dash(__name__, external_stylesheets=external_stylesheets)
27/22: app.config
28/1: import utils
28/2: utils.watercooler_break(340//6, "app2")
28/3: utils.watercooler_break(329//6, "app2")
28/4: 510//3
28/5: 330//3
28/6: 510-330
28/7:
def get_df():
    price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
    price_df = price_df[price_df.Volume>0]
    price_df=price_df.iloc[cfg.df_start:cfg.df_end]
    price_df.index = range(len(price_df))
    price_df['strtime'] = price_df.Localtime.dt.strftime("%m/%d %H:%M")
    price_df['index2'] = price_df.index//cfg.minutes_per_second
    price_df['High']*=cfg.price_multiplier
    return price_df
28/8: df = get_df()
28/9: import pandas as pd
28/10: df = get_df()
28/11: import config as cfg
28/12: df = get_df()
28/13: df.iloc[330:510]
28/14: df.iloc[33:213]
28/15:
null_df = pd.DataFrame(columns=['strtime','High','Low'])
null_df['strtime'] = ['NA']*180
28/16: df = pd.concat([df,null_df])
28/17: df = get_df()
28/18: PRICE_DF = get_df()
28/19:
df = PRICE_DF[['strtime', 'High', 'Low']].iloc[pre:ix]
# pad empty data points 
null_df = pd.DataFrame(columns=['strtime','High','Low'])
null_df['strtime'] = ['NA']*180

df = pd.concat([df,null_df])
28/20: pre = 0
28/21: ix=50
28/22:
df = PRICE_DF[['strtime', 'High', 'Low']].iloc[pre:ix]
# pad empty data points 
null_df = pd.DataFrame(columns=['strtime','High','Low'])
null_df['strtime'] = ['NA']*180

df = pd.concat([df,null_df])
28/23: import matplotlib.pyplot as plt
28/24: plt.line(x=df['strtime'], y=df['High'])
28/25: plt.plot(x=df['strtime'], y=df['High'])
28/26: plt(df['strtime'], df['High'])
28/27: plt.plot(df['strtime'], df['High'])
28/28:
df = PRICE_DF[['strtime', 'High', 'Low']].iloc[pre:ix]
# pad empty data points 
null_df = pd.DataFrame(columns=['strtime','High','Low'])
null_df['strtime'] = PRICE_DF['strtime'].iloc[ix:ix+3*(cfg.end_P2-cfg.end_P1)]
28/29: df = pd.concat([df,null_df])
28/30: plt.plot(df['strtime'], df['High'])
28/31: PRICE_DF.iloc[cfg.end_P1*3]['strtime']
28/32: lx=PRICE_DF.iloc[cfg.end_P1*3]['strtime']
28/33: PRICE_DF.iloc[cfg.end_P1*3]['strtime']
28/34: PRICE_DF.iloc[cfg.end_P1*3]
28/35: ix=384
28/36: PRICE_DF.iloc[384]
28/37: PRICE_DF.iloc[cfg.end_P1*6]
28/38: PRICE_DF.iloc[cfg.end_P1*6]['strtime']
28/39: cfg.end_P1*3
28/40: PRICE_DF.iloc[cfg.end_P1*3]
28/41: 330/6
28/42: cfg.end_P1
28/43: PRICE_DF.iloc[cfg.end_P1*6]
28/44: a=None
28/45: a==False
28/46: not a
30/1: import dash
30/2: dash.__version__
30/3: import dash_daq
30/4: import dash-daq
31/1: import pandas as pd
31/2:
def get_df():
    price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
    price_df = price_df[price_df.Volume>0]
    price_df=price_df.iloc[df_start:df_end]
    price_df.index = range(len(price_df))
    price_df['strtime'] = price_df.Localtime.dt.strftime("%m/%d %H:%M")
    price_df['index2'] = price_df.index//minutes_per_interval
    
    ix_p1 = end_P1*minutes_per_interval
    ix_p2 = end_P2*minutes_per_interval
    price_df.loc[ix_p1:ix_p2]['High'] = dist_stretcher(price_df.loc[ix_p1:ix_p2]['High'].copy(), volatile_price_multiplier)
    
    price_df['High']= dist_stretcher(price_df['High'].copy(), overall_price_multiplier)
    return price_df
31/3: df = get_df()
31/4:
WINDOW_SIZE = 500
minutes_per_interval = 15  # CHANGE!
overall_price_multiplier = 6
volatile_price_multiplier = 2

end_P1 = 330//minutes_per_interval
end_P2 = 510//minutes_per_interval

df_start = 1000
df_end = 1650

MAX_LEN = df_end-df_start
31/5: df = get_df()
31/6:

def dist_stretcher(series, multiplier):
    meann = series.mean()
    return multiplier*(series-meann) + meann # X <- vpm*(x-mean) + mean
31/7: df = get_df()
31/8: df.to_csv('AAPL_Final_Trend.csv')
31/9:
def get_df():
    price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
    price_df = price_df[price_df.Volume>0]
    price_df=price_df.iloc[df_start:df_end]
    price_df.index = range(len(price_df))
    price_df['strtime'] = price_df.Localtime.dt.strftime("%m/%d %H:%M")
    price_df['index2'] = price_df.index//minutes_per_interval
    
    ix_p1 = end_P1*minutes_per_interval
    ix_p2 = end_P2*minutes_per_interval
    #price_df.loc[ix_p1:ix_p2]['High'] = dist_stretcher(price_df.loc[ix_p1:ix_p2]['High'].copy(), volatile_price_multiplier)
    
    price_df['High']= dist_stretcher(price_df['High'].copy(), overall_price_multiplier)
    return price_df
31/10: df = get_df()
31/11: df.to_csv('AAPL_Final_Trend.csv')
31/12:
def get_df():
    price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
    price_df = price_df[price_df.Volume>0]
    price_df=price_df.iloc[df_start:df_end]
    price_df.index = range(len(price_df))
    price_df['strtime'] = price_df.Localtime.dt.strftime("%m/%d %H:%M")
    price_df['index2'] = price_df.index//minutes_per_interval
    
    ix_p1 = end_P1*minutes_per_interval
    ix_p2 = end_P2*minutes_per_interval
    price_df.loc[ix_p1:ix_p2]['High'] = dist_stretcher(price_df.loc[ix_p1:ix_p2]['High'].copy(), volatile_price_multiplier)
    
    price_df['High']= dist_stretcher(price_df['High'].copy(), overall_price_multiplier)
    return price_df
31/13: df = get_df()
31/14: df.to_csv('AAPL_Final_Trend.csv')
31/15:
def get_df():
    price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
    price_df = price_df[price_df.Volume>0]
    price_df=price_df.iloc[df_start:df_end]
    price_df.index = range(len(price_df))
    price_df['strtime'] = price_df.Localtime.dt.strftime("%m/%d %H:%M")
    price_df['index2'] = price_df.index//minutes_per_interval
    
    ix_p1 = end_P1*minutes_per_interval
    ix_p2_old = 510
    price_df.loc[ix_p1:ix_p2_old]['High'] = dist_stretcher(price_df.loc[ix_p1:ix_p2_old]['High'].copy(), volatile_price_multiplier)

    # additional volatility to get equal divergence from first buy price
    price_df.loc[479:ix_p2_old]['High'] = dist_stretcher(price_df.loc[479:ix_p2_old]['High'].copy(), volatile_price_multiplier, price_df.loc[501]['High'])
    
    price_df['High']= dist_stretcher(price_df['High'].copy(), overall_price_multiplier)
    return price_df
31/16: df = get_df()
31/17:
def dist_stretcher(series, multiplier, anchor=None):
    if not anchor:
        anchor = series.mean()

    return multiplier*(series-anchor) + anchor # X <- vpm*(x-mean) + mean
31/18: df = get_df()
31/19: cp = df.loc[ix_p1]['High']
31/20: cp = df.loc[330]['High']
31/21: cp
31/22: sp1 = df.loc[491]['High']
31/23: df.loc[491]
31/24: sp1
31/25: sp2 = df.loc[502]['High']
31/26: sp2
31/27: sp1-cp
31/28: cp-sp2
31/29: %matplotlib inline
32/1:
def get_df():
    overall_price_multiplier = 6
    volatile_price_multiplier = 2
    
    price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
    price_df = price_df[price_df.Volume>0]
    price_df=price_df.iloc[df_start:df_end]
    price_df.index = range(len(price_df))
    price_df['strtime'] = price_df.Localtime.dt.strftime("%m/%d %H:%M")
    price_df['index2'] = price_df.index//minutes_per_interval
    
    ix_p1 = end_P1*minutes_per_interval
    ix_p2_old = 510
    price_df.loc[ix_p1:ix_p2_old]['High'] = dist_stretcher(price_df.loc[ix_p1:ix_p2_old]['High'].copy(), volatile_price_multiplier)

    # additional volatility to get equal divergence from first buy price
    price_df.loc[479:ix_p2_old]['High'] = dist_stretcher(price_df.loc[479:ix_p2_old]['High'].copy(), volatile_price_multiplier, price_df.loc[501]['High'])
    
    price_df['High']= dist_stretcher(price_df['High'].copy(), overall_price_multiplier)
    return price_df
32/2:
WINDOW_SIZE = 500
minutes_per_interval = 2
end_P1 = 330//minutes_per_interval
end_P2 = 491//minutes_per_interval
32/3: get_df()
32/4: import pandas as pd
32/5: get_df()
32/6:
def get_df():
    overall_price_multiplier = 6
    volatile_price_multiplier = 2
    
    price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
    price_df = price_df[price_df.Volume>0]
    price_df=price_df.iloc[1000:1650]
    price_df.index = range(len(price_df))
    price_df['strtime'] = price_df.Localtime.dt.strftime("%m/%d %H:%M")
    price_df['index2'] = price_df.index//minutes_per_interval
    
    ix_p1 = end_P1*minutes_per_interval
    ix_p2_old = 510
    price_df.loc[ix_p1:ix_p2_old]['High'] = dist_stretcher(price_df.loc[ix_p1:ix_p2_old]['High'].copy(), volatile_price_multiplier)

    # additional volatility to get equal divergence from first buy price
    price_df.loc[479:ix_p2_old]['High'] = dist_stretcher(price_df.loc[479:ix_p2_old]['High'].copy(), volatile_price_multiplier, price_df.loc[501]['High'])
    
    price_df['High']= dist_stretcher(price_df['High'].copy(), overall_price_multiplier)
    return price_df
32/7: get_df()
32/8:
def get_df():
    def dist_stretcher(series, multiplier, anchor=None):
        if not anchor:
            anchor = series.mean()
        return multiplier*(series-anchor) + anchor # X <- vpm*(x-mean) + mean
        
    overall_price_multiplier = 6
    volatile_price_multiplier = 2
    
    price_df = pd.read_csv('AAPL_1M_16.09.2019-20.09.2019.csv', parse_dates=['Localtime'])
    price_df = price_df[price_df.Volume>0]
    price_df=price_df.iloc[1000:1650]
    price_df.index = range(len(price_df))
    price_df['strtime'] = price_df.Localtime.dt.strftime("%m/%d %H:%M")
    price_df['index2'] = price_df.index//minutes_per_interval
    
    ix_p1 = end_P1*minutes_per_interval
    ix_p2_old = 510
    price_df.loc[ix_p1:ix_p2_old]['High'] = dist_stretcher(price_df.loc[ix_p1:ix_p2_old]['High'].copy(), volatile_price_multiplier)

    # additional volatility to get equal divergence from first buy price
    price_df.loc[479:ix_p2_old]['High'] = dist_stretcher(price_df.loc[479:ix_p2_old]['High'].copy(), volatile_price_multiplier, price_df.loc[501]['High'])
    
    price_df['High']= dist_stretcher(price_df['High'].copy(), overall_price_multiplier)
    return price_df
32/9: get_df()
32/10: get_df().to_csv('AAPL_Final_Trend.csv')
33/1: import pandas as pd
33/2: df = pd.read_csv('datadump.csv')
33/3: df
33/4: df.head()
33/5: df.head(10)
33/6: df = df.loc[8:]
33/7: df
33/8: duq = df[df.experiment=='rc_profit']
33/9: duq.shape
33/10: df.experiment.head(1)
33/11: df.experiment=='rc_profit'
33/12: df.experiment=="'rc_profit'"
33/13: duq = df[df.experiment=="'rc_profit'"]
33/14: duq
33/15: df['price1']=217.76
33/16: df['price2']=219.65
33/17: df = pd.read_csv('datadump.csv')
33/18: df['price1']=217.76
33/19: df[df.experiment.str.contains("profit")]['price2']=219.65
33/20: df['price0']=217.76
33/21: src = pd.read_csv('AAPL_Final_Trend.csv)
33/22: src = pd.read_csv('AAPL_Final_Trend.csv')
33/23: df[df.experiment.str.contains("profit")]['price1']=src.loc[330]['High']
33/24: df.price1
33/25: df[df.experiment.str.contains("profit")]
33/26: src.loc[330]['High']
33/27: df[df.experiment.str.contains("profit")]['price1']
33/28: df[df.experiment.str.contains("profit")]["price1"]=src.loc[330]['High']
33/29: df[df.experiment.str.contains("profit")]['price1']
33/30: df.loc[df.experiment.str.contains("profit")]["price1"]=src.loc[330]['High']
33/31: df[df.experiment.str.contains("profit")]['price1']
33/32: df.columns
33/33: df['price1'][df.experiment.str.contains("profit")] = 219.65
33/34: df.price1
33/35: df
33/36: df['price1'] = 219.65
33/37: df['price2'][df.experiment.str.contains("profit")] = src.loc[491]['High']
33/38: df.price2
33/39: df['price2'][df.experiment.str.contains("loss")] = src.loc[502]['High']
33/40: df = df.loc[8:]
33/41: df
33/42: rcp = df[df.experiment="'rc_profit'"]
33/43: rcp = df[df.experiment=="'rc_profit'"]
33/44: rcp
33/45: df=df.fillna(0)
33/46: rcp = df[df.experiment=="'rc_profit'"]
33/47: rcp
33/48: rcp.qty1.value_counts()
33/49: rcp.rng.unique()
33/50: %history
34/1: import pandas as pd
34/2:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price1'] = 219.65
df['price2'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price2'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
rcp = df[df.experiment=="'rc_profit'"]
34/3:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price1'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price2'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price2'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
rcp = df[df.experiment=="'rc_profit'"]
34/4: rcp
34/5: rcp['qty1'].describe()
34/6:
import pandas as pd
import seaborn as sns
34/7: sns.distplot(rcp.qty1)
34/8: sns.hist(rcp.qty1)
34/9: sns.histogram(rcp.qty1)
34/10: sns.plot(rcp.qty1)
34/11:

rcp.price.plot.hist()
34/12:

rcp.price1.plot.hist()
34/13:

rcp.price1.value_counts().plot.hist()
34/14:

rcp.price1.value_counts()#.plot.hist()
34/15:

rcp.qty1.value_counts()#.plot.hist()
34/16:

rcp.qty1.value_counts().plot.hist()
34/17:

rcp.qty1.value_counts()#.plot.hist()
34/18:

rcp.qty1.value_counts()#.plot.hist()
34/19:

rcp.qty1.value_counts().plot.hist()
34/20:
rcp.qty1.value_counts().plot.hist()
rcp[rcp.qty1<0].qty1.value_counts().pl0t.hist()
34/21:
rcp.qty1.value_counts().plot.hist()
rcp[rcp.qty1<0].qty1.value_counts().plot.hist()
34/22:
# rcp.qty1.value_counts().plot.hist()
rcp[rcp.qty1<0].qty1.value_counts()#.plot.hist()
34/23:
rcp.qty1.value_counts()#.plot.hist()
# rcp[rcp.qty1<0].qty1.value_counts().plot.hist()
34/24:
rcp.qty1.plot.hist()
# rcp[rcp.qty1<0].qty1.value_counts().plot.hist()
34/25:
rcp.qty1.plot.hist()
rcp[rcp.qty1<0].qty1plot.hist()
34/26:
rcp.qty1.plot.hist()
rcp[rcp.qty1<0].qty1.plot.hist()
34/27: rcp.qty1.plot.hist(dticks=range(-10,11))
34/28: rcp.qty1.plot.hist(ticks=range(-10,11))
34/29: rcp.qty1.plot.hist(xticks=range(-10,11))
34/30: rcp.qty1.plot.hist(xticks=rcp.qty1)
34/31: print(rcp.qty1.value_counts())rcp.qty1.plot.hist(xticks=rcp.qty1)
34/32:
print(rcp.qty1.value_counts())
rcp.qty1.plot.hist(xticks=rcp.qty1)
34/33: sns.distplot(rcp.qty1, kde=False)
34/34: sns.distplot(rcp.qty1, bins=20, kde=False)
34/35: sns.distplot(rcp.qty1, bins=20, kde=False, xticks=rcp.qty1)
34/36: sns.distplot(rcp.qty1, bins=20, kde=False, ticks=rcp.qty1)
34/37: sns.distplot(rcp.qty1, bins=20, kde=False)
34/38:
sns.distplot(rcp.qty1, bins=20, kde=False)
print(rcp.qty1.value_counts())
34/39:
sns.distplot(rcp.qty1, bins=20, kde=False, title="Price: $219.65")
print(rcp.qty1.value_counts())
34/40:
print("Price: $219.65")
sns.distplot(rcp.qty1, bins=20, kde=False)
34/41:
sns.distplot(rcp.qty1, bins=20, kde=False)
print("Price: $219.65")
34/42:
sns.distplot(rcp.qty1, bins=20, kde=False)
print("Price: $222.65")
34/43:
sns.distplot(rcp.qty2, bins=20, kde=False)
print("Price: $222.65")
34/44:
print(rcp.qty2.value_counts())
sns.distplot(rcp.qty2, bins=20, kde=False)
print("Price: $222.65")
34/45:
print(rcp.qty2.value_counts().sort_index())
sns.distplot(rcp.qty2, bins=20, kde=False)
print("Price: $222.65")
34/46:
df['activity1'] = df['qty1']
df['qty1'] = df['activity1']+10
34/47:
df['activity1'] = df['qty1']
df['qty1'] = df['activity1']+10
df
34/48:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price1'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price2'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price2'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
rcp = df[df.experiment=="'rc_profit'"]
34/49:
df['activity1'] = df['qty1']
df['qty1'] = df['activity1']+10
df
34/50:
df['activity1'] = df['qty1']
df['qty1'] = df['activity1']+10
df['activity2'] = df['qty2']
df['qty2'] = df['activity2']+df['activity1']
df.columns
34/51:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price1'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price2'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price2'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
rcp = df[df.experiment=="'rc_profit'"]
34/52:
df1=[['experiment', 'activity1', 'qty1', 'price1', 'activity2', 'qty2', 'price2', 'winnings', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
rcp = df1[df1.experiment=="'rc_profit'"]
34/53:
df1=df[['experiment', 'activity1', 'qty1', 'price1', 'activity2', 'qty2', 'price2', 'winnings', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
rcp = df1[df1.experiment=="'rc_profit'"]
34/54:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price1'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price2'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price2'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
df['activity1'] = df['qty1']
df['qty1'] = df['activity1']+10
df['activity2'] = df['qty2']
df['qty2'] = df['activity2']+df['activity1']
34/55:
df1=df[['experiment', 'activity1', 'qty1', 'price1', 'activity2', 'qty2', 'price2', 'winnings', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
rcp = df1[df1.experiment=="'rc_profit'"]
34/56: rcp
34/57:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price1'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price2'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price2'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
df['activity1'] = df['qty1']
df['qty1'] = df['activity1']+10
df['activity2'] = df['qty2']
df['qty2'] = df['activity2']+df['qty1']
34/58:
df1=df[['experiment', 'activity1', 'qty1', 'price1', 'activity2', 'qty2', 'price2', 'winnings', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
rcp = df1[df1.experiment=="'rc_profit'"]
34/59: rcp
34/60:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price1'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price2'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price2'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
df['activity1'] = df['qty1']
df['qty1'] = df['activity1']+10
df['activity2'] = df['qty2']
df['qty2'] = df['activity2']+df['qty1']
df['qty0'] = 10
34/61:
df1=df[['experiment', 'qty0', 'activity1', 'qty1', 'price1', 'activity2', 'qty2', 'price2', 'winnings', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
rcp = df1[df1.experiment=="'rc_profit'"]
34/62: dftmp = rcp[['qty0','qty1','qty2']].T
34/63:
dftmp = rcp[['qty0','qty1','qty2']].T
dft,p
34/64:
dftmp = rcp[['qty0','qty1','qty2']].T
dftmp
34/65:
dftmp = rcp[['qty0','qty1','qty2']].T
dftmp.plot.line()
34/66:
dftmp = rcp[['qty0','qty1','qty2']].T
dftmp.plot.line(figsize=(10,7))
34/67:
dftmp = rcp[['qty0','qty1','qty2']].T
# dftmp.plot.line(figsize=(10,7))
34/68:
dftmp = rcp[['qty0','qty1','qty2']].T
# dftmp.plot.line(figsize=(10,7))
dftmp
34/69:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA.fit_transform(dftmp)
sns.scatterplot(pca)
34/70:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA().fit_transform(dftmp)
sns.scatterplot(pca)
34/71:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
sns.scatterplot(pca)
34/72:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
# sns.scatterplot(pca)
pca
34/73:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
sns.scatterplot(pca)
34/74:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
sns.scatterplot(data=pca)
34/75:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
pca
sns.scatterplot(data=pca)
34/76:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
print(pca)
sns.scatterplot(data=pca)
34/77:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
print(pca)
sns.scatterplot(x=pca[:,0], y=pca[:,1])
34/78: dftmp
34/79: df[['activity1','activity2']]
34/80: df1[['activity1','activity2']]
34/81: rcp[['activity1','activity2']]
34/82:
rcp[['activity1','activity2']]
inv_inc_t1 = rcp[rcp.activity1>0]
inv_dec_t1 = rcp[rcp.activity1<0]
inactive_t1 = rcp[rcp.activity1==0]
34/83:
rcp[['activity1','activity2']]
inv_inc_t1 = rcp[rcp.activity1>0]
inv_dec_t1 = rcp[rcp.activity1<0]
inactive_t1 = rcp[rcp.activity1==0]
rcp.loc[inactive_t1.index]['inv_bhvr'] = 'inactive'
rcp.loc[inv_dec_t1.index]['inv_bhvr'] = 'decrease'
rcp.loc[inv_inc_t1.index]['inv_bhvr'] = 'increase'
34/84:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
print(pca)
sns.scatterplot(x=pca[:,0], y=pca[:,1], hue=rcp.inv_bhvr)
34/85:
rcp[['activity1','activity2']]
inv_inc_t1 = rcp[rcp.activity1>0]
inv_dec_t1 = rcp[rcp.activity1<0]
inactive_t1 = rcp[rcp.activity1==0]
rcp.loc[inactive_t1.index]['inv_bhvr'] = 'inactive'
rcp.loc[inv_dec_t1.index]['inv_bhvr'] = 'decrease'
rcp.loc[inv_inc_t1.index]['inv_bhvr'] = 'increase'
rcp
34/86:
rcp[['activity1','activity2']]
inv_inc_t1 = rcp[rcp.activity1>0]
inv_dec_t1 = rcp[rcp.activity1<0]
inactive_t1 = rcp[rcp.activity1==0]
rcp['inv_bhvr'].loc[inactive_t1.index] = 'inactive'
rcp.loc[inv_dec_t1.index]['inv_bhvr'] = 'decrease'
rcp.loc[inv_inc_t1.index]['inv_bhvr'] = 'increase'
rcp
34/87:
rcp[['activity1','activity2']]
inv_inc_t1 = rcp[rcp.activity1>0]
inv_dec_t1 = rcp[rcp.activity1<0]
inactive_t1 = rcp[rcp.activity1==0]
rcp['inv_bhvr']=None
rcp['inv_bhvr'].loc[inactive_t1.index] = 'inactive'
rcp.loc[inv_dec_t1.index]['inv_bhvr'] = 'decrease'
rcp.loc[inv_inc_t1.index]['inv_bhvr'] = 'increase'
rcp
34/88:
rcp[['activity1','activity2']]
inv_inc_t1 = rcp[rcp.activity1>0]
inv_dec_t1 = rcp[rcp.activity1<0]
inactive_t1 = rcp[rcp.activity1==0]
rcp['inv_bhvr']=None
rcp['inv_bhvr'].loc[inactive_t1.index] = 'inactive'
rcp['inv_bhvr'].loc[inv_dec_t1.index] = 'decrease'
rcp['inv_bhvr'].loc[inv_inc_t1.index] = 'increase'
rcp
34/89:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
print(pca)
sns.scatterplot(x=pca[:,0], y=pca[:,1], hue=rcp.inv_bhvr)
34/90:
inv_inc_t2 = rcp[rcp.activity2>0]
inv_dec_t2 = rcp[rcp.activity2<0]
inactive_t2 = rcp[rcp.activity2==0]
rcp['inv_bhvr2']=None
rcp['inv_bhvr2'].loc[inactive_t2.index] = 'inactive'
rcp['inv_bhvr2'].loc[inv_dec_t2.index] = 'decrease'
rcp['inv_bhvr2'].loc[inv_inc_t2.index] = 'increase'
34/91:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
print(pca)
sns.scatterplot(x=pca[:,0], y=pca[:,1], hue=rcp.inv_bhvr1)
34/92:
df1=df[['experiment', 'qty0', 'activity1', 'qty1', 'price1', 'activity2', 'qty2', 'price2', 'winnings', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
rcp = df1[df1.experiment=="'rc_profit'"]
34/93:
nv_inc_t1 = rcp[rcp.activity1>0]
inv_dec_t1 = rcp[rcp.activity1<0]
inactive_t1 = rcp[rcp.activity1==0]
rcp['inv_bhvr1']=None
rcp['inv_bhvr1'].loc[inactive_t1.index] = 'inactive'
rcp['inv_bhvr1'].loc[inv_dec_t1.index] = 'decrease'
rcp['inv_bhvr1'].loc[inv_inc_t1.index] = 'increase'
34/94:
inv_inc_t2 = rcp[rcp.activity2>0]
inv_dec_t2 = rcp[rcp.activity2<0]
inactive_t2 = rcp[rcp.activity2==0]
rcp['inv_bhvr2']=None
rcp['inv_bhvr2'].loc[inactive_t2.index] = 'inactive'
rcp['inv_bhvr2'].loc[inv_dec_t2.index] = 'decrease'
rcp['inv_bhvr2'].loc[inv_inc_t2.index] = 'increase'
34/95:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
print(pca)
sns.scatterplot(x=pca[:,0], y=pca[:,1], hue=rcp.inv_bhvr1)
34/96:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
print(pca)
sns.scatterplot(x=pca[:,0], y=pca[:,1], hue=rcp.inv_bhvr2)
34/97:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
sns.scatterplot(x=pca[:,0], y=pca[:,1], hue=rcp.inv_bhvr1)
34/98: sns.scatterplot(x=pca[:,0], y=pca[:,1], hue=rcp.inv_bhvr2)
34/99:
nv_inc_t1 = rcp[rcp.activity1>0]
inv_dec_t1 = rcp[rcp.activity1<0]
inactive_t1 = rcp[rcp.activity1==0]
rcp['inv_bhvr1']=None
rcp['inv_bhvr1'].loc[inactive_t1.index] = 'inactive'
rcp['inv_bhvr1'].loc[inv_dec_t1.index] = 'decrease'
rcp['inv_bhvr1'].loc[inv_inc_t1.index] = 'increase'
inv_inc_t2 = rcp[rcp.activity2>0]
inv_dec_t2 = rcp[rcp.activity2<0]
inactive_t2 = rcp[rcp.activity2==0]
rcp['inv_bhvr2']=None
rcp['inv_bhvr2'].loc[inactive_t2.index] = 'inactive'
rcp['inv_bhvr2'].loc[inv_dec_t2.index] = 'decrease'
rcp['inv_bhvr2'].loc[inv_inc_t2.index] = 'increase'
rcp['net_bhvr'] = rcp['inv_bhvr1']+rcp['inv_bhvr2']
34/100: sns.scatterplot(x=pca[:,0], y=pca[:,1], hue=rcp.net_bhvr, figsize=(10,7))
34/101: sns.scatterplot(x=pca[:,0], y=pca[:,1], hue=rcp.net_bhvr, size=(10,7))
34/102:
fig, ax = pyplot.subplots(figsize=a4_dims)
sns.scatterplot(x=pca[:,0], y=pca[:,1], hue=rcp.net_bhvr,ax=ax)
34/103:
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
34/104:
fig, ax =plt.subplots(figsize=a4_dims)
sns.scatterplot(x=pca[:,0], y=pca[:,1], hue=rcp.net_bhvr,ax=ax)
34/105:
fig, ax =plt.subplots(figsize=(10,7))
sns.scatterplot(x=pca[:,0], y=pca[:,1], hue=rcp.net_bhvr,ax=ax)
34/106:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
print(PCA)
34/107:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
print(pca)
34/108:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
rcp[['pca1','pca2']]=pca
rcp
34/109:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
rcp[['pca1','pca2']]=pca[:,0],pca[:,1]
rcp
34/110:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
rcp[['pca1','pca2']]=[pca[:,0],pca[:,1]]
rcp
34/111:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
rcp[['pca1','pca2']]=[None, None]
rcp[['pca1','pca2']]=pca
rcp
34/112:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
rcp['pca0']=pca[:,0]
rcp['pca1']=pca[:,1]
    
rcp
34/113:
df1=df[['experiment', 'qty0', 'activity1', 'qty1', 'price1', 'activity2', 'qty2', 'price2', 'winnings', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
rcp = df1[df1.experiment=="'rc_profit'"]
34/114:
nv_inc_t1 = rcp[rcp.activity1>0]
inv_dec_t1 = rcp[rcp.activity1<0]
inactive_t1 = rcp[rcp.activity1==0]
rcp['inv_bhvr1']=None
rcp['inv_bhvr1'].loc[inactive_t1.index] = 'inactive'
rcp['inv_bhvr1'].loc[inv_dec_t1.index] = 'decrease'
rcp['inv_bhvr1'].loc[inv_inc_t1.index] = 'increase'
inv_inc_t2 = rcp[rcp.activity2>0]
inv_dec_t2 = rcp[rcp.activity2<0]
inactive_t2 = rcp[rcp.activity2==0]
rcp['inv_bhvr2']=None
rcp['inv_bhvr2'].loc[inactive_t2.index] = 'inactive'
rcp['inv_bhvr2'].loc[inv_dec_t2.index] = 'decrease'
rcp['inv_bhvr2'].loc[inv_inc_t2.index] = 'increase'
rcp['net_bhvr'] = rcp['inv_bhvr1']+rcp['inv_bhvr2']
34/115:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
rcp['pca0']=pca[:,0]
rcp['pca1']=pca[:,1]
    
rcp
34/116:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
rcp['pca0']=pca[:,0]
rcp['pca1']=pca[:,1]
    
rcp[10<rcp.pca0<15]
34/117:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
rcp['pca0']=pca[:,0]
rcp['pca1']=pca[:,1]
    
10<rcp.pca0<15
34/118:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
rcp['pca0']=pca[:,0]
rcp['pca1']=pca[:,1]
    
rcp[rcp.pca0.between(10,15)]
34/119:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
rcp['pca0']=pca[:,0]
rcp['pca1']=pca[:,1]
    
rcp[rcp.pca0.between(5,10)]
34/120:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca=PCA(2).fit_transform(dftmp)
rcp['pca0']=pca[:,0]
rcp['pca1']=pca[:,1]
    
rcp[rcp.pca0.between(0,5)]
34/121: PCA.get_params
34/122: PCA.get_params()
34/123: pca.get_params()
34/124:
pca2 = PCA(2).fit(dftmp)
tmp = pca2.transform(dftmp)
sns.scatterplot(tmp[:,0], tmp[:1])
34/125:
pca2 = PCA(2).fit(dftmp)
tmp = pca2.transform(dftmp)
tmp
# sns.scatterplot(tmp[:,0], tmp[:1])
34/126:
pca2 = PCA(2).fit(dftmp)
tmp = pca2.transform(dftmp)
pca2.get_params()
# sns.scatterplot(tmp[:,0], tmp[:1])
34/127:
pca2 = PCA(2).fit(dftmp)
tmp = pca2.transform(dftmp)
sns.scatterplot(tmp[:,0], tmp[:,1])
34/128:
fig, ax =plt.subplots(figsize=(10,7))
sns.scatterplot(x=pca[:,0], y=pca[:,1], hue=rcp.net_bhvr,ax=ax)
34/129:
fig, ax =plt.subplots(figsize=(10,7))
sns.scatterplot(x=pca2[:,0], y=pca2[:,1], hue=rcp.net_bhvr,ax=ax)
34/130:
fig, ax =plt.subplots(figsize=(10,7))
sns.scatterplot(x=pca[:,0], y=pca[:,1], hue=rcp.net_bhvr,ax=ax)
34/131:
fig, ax =plt.subplots(figsize=(10,7))
sns.scatterplot(x=tmp[:,0], y=tmp[:,1], hue=rcp.net_bhvr,ax=ax)
34/132:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca2 = PCA(2).fit(dftmp)
tmp = pca2.transform(dftmp)
rcp['pca0']=tmp[:,0]
rcp['pca1']=tmp[:,1]
    
rcp[rcp.pca0.between(-1,5)]
34/133:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca2 = PCA(2).fit(dftmp)
tmp = pca2.transform(dftmp)
rcp['pca0']=tmp[:,0]
rcp['pca1']=tmp[:,1]
    
rcp[rcp.pca0.between(-20,-5)]
34/134:
dftmp = rcp[['qty0','qty1','qty2']]
from sklearn.decomposition import PCA
# dftmp.plot.line(figsize=(10,7))
pca2 = PCA(2).fit(dftmp)
tmp = pca2.transform(dftmp)
rcp['pca0']=tmp[:,0]
rcp['pca1']=tmp[:,1]
    
rcp[rcp.pca0.between(10,15)]
34/135:
def get_bhvr(exp):
    df1=df[['experiment', 'qty0', 'activity1', 'qty1', 'price1', 'activity2', 'qty2', 'price2']]
    rcp = df1[df1.experiment==exp]
    nv_inc_t1 = rcp[rcp.activity1>0]
    inv_dec_t1 = rcp[rcp.activity1<0]
    inactive_t1 = rcp[rcp.activity1==0]
    rcp['inv_bhvr1']=None
    rcp['inv_bhvr1'].loc[inactive_t1.index] = 'inactive'
    rcp['inv_bhvr1'].loc[inv_dec_t1.index] = 'decrease'
    rcp['inv_bhvr1'].loc[inv_inc_t1.index] = 'increase'
    inv_inc_t2 = rcp[rcp.activity2>0]
    inv_dec_t2 = rcp[rcp.activity2<0]
    inactive_t2 = rcp[rcp.activity2==0]
    rcp['inv_bhvr2']=None
    rcp['inv_bhvr2'].loc[inactive_t2.index] = 'inactive'
    rcp['inv_bhvr2'].loc[inv_dec_t2.index] = 'decrease'
    rcp['inv_bhvr2'].loc[inv_inc_t2.index] = 'increase'
    rcp['net_bhvr'] = rcp['inv_bhvr1']+rcp['inv_bhvr2']
    return rcp

def plot_qty_pca(rcp):
    dftmp = rcp[['qty0','qty1','qty2']]
    from sklearn.decomposition import PCA
    pca2 = PCA(2).fit(dftmp)
    tmp = pca2.transform(dftmp)
    rcp['pca0']=tmp[:,0]
    rcp['pca1']=tmp[:,1]
    fig, ax =plt.subplots(figsize=(15,10))
    sns.scatterplot(x=tmp[:,0], y=tmp[:,1], hue=rcp.net_bhvr,ax=ax)
    return rcp
34/136:
wcp = get_bhvr("'wc_profit'")
plot_qty_pca(wcp)
# rcp[rcp.pca0.between(10,15)]
34/137: df1[df1.experiment=="'wc_profit'"]
34/138:
rcp = df1[df1.experiment=="'wc_profit'"]
rcp[rcp.activity1>0]
34/139:
rcp = df1[df1.experiment=="'wc_profit'"]
rcp['inv_bhvr1'].loc[rcp[rcp.activity1>0]]
34/140:
rcp = df1[df1.experiment=="'wc_profit'"]
rcp['inv_bhvr1']=None
rcp['inv_bhvr1'].loc[rcp[rcp.activity1>0]]
34/141:
rcp = df1[df1.experiment=="'wc_profit'"]
rcp['inv_bhvr1']=None
rcp['inv_bhvr1'].loc[rcp.activity1>0]
34/142:
def get_bhvr(exp):
    df1=df[['experiment', 'qty0', 'activity1', 'qty1', 'price1', 'activity2', 'qty2', 'price2']]
    rcp = df1[df1.experiment==exp]
    
    rcp['inv_bhvr1']=None
    rcp['inv_bhvr1'].loc[rcp.activity1==0] = 'inactive'
    rcp['inv_bhvr1'].loc[rcp.activity1<0] = 'decrease'
    rcp['inv_bhvr1'].loc[rcp.activity1>0] = 'increase'
    
    rcp['inv_bhvr2']=None
    rcp['inv_bhvr2'].loc[rcp.activity2==0] = 'inactive'
    rcp['inv_bhvr2'].loc[rcp.activity2<0] = 'decrease'
    rcp['inv_bhvr2'].loc[rcp.activity2>0] = 'increase'
    rcp['net_bhvr'] = rcp['inv_bhvr1']+rcp['inv_bhvr2']
    return rcp

def plot_qty_pca(rcp):
    dftmp = rcp[['qty0','qty1','qty2']]
    from sklearn.decomposition import PCA
    pca2 = PCA(2).fit(dftmp)
    tmp = pca2.transform(dftmp)
    rcp['pca0']=tmp[:,0]
    rcp['pca1']=tmp[:,1]
    fig, ax =plt.subplots(figsize=(15,10))
    sns.scatterplot(x=tmp[:,0], y=tmp[:,1], hue=rcp.net_bhvr,ax=ax)
    return rcp
34/143: rcp = get_bhvr("'wc_profit'")
34/144:
rcp = get_bhvr("'wc_profit'")
rcpca = plot_qty_pca(rcp)
34/145: rcpca[rcpca.pca0.between(-20,-10)]
34/146: rcpca[rcpca.pca0.between(-10,-5)]
34/147: rcpca[rcpca.pca0.between(-10,-5)].sort_values(by='pca0')
34/148:
def get_bhvr(exp):
    df1=df[['experiment', 'qty0', 'activity1', 'qty1', 'price1', 'activity2', 'qty2', 'price2']]
    rcp = df1[df1.experiment==exp]
    
    rcp['inv_bhvr1']=None
    rcp['inv_bhvr1'].loc[rcp.activity1==0] = 'inactive'
    rcp['inv_bhvr1'].loc[rcp.activity1<0] = 'decrease'
    rcp['inv_bhvr1'].loc[rcp.activity1>0] = 'increase'
    
    rcp['inv_bhvr2']=None
    rcp['inv_bhvr2'].loc[rcp.activity2==0] = 'inactive'
    rcp['inv_bhvr2'].loc[rcp.activity2<0] = 'decrease'
    rcp['inv_bhvr2'].loc[rcp.activity2>0] = 'increase'
    rcp['net_bhvr'] = rcp['inv_bhvr1']+rcp['inv_bhvr2']
    return rcp

def plot_qty_pca(rcp):
    dftmp = rcp[['qty0','qty1','qty2']]
    from sklearn.decomposition import PCA
    pca2 = PCA(2).fit(dftmp)
    tmp = pca2.transform(dftmp)
    rcp['pca0']=tmp[:,0]
    rcp['pca1']=tmp[:,1]
    fig, ax =plt.subplots(figsize=(15,10))
    sns.scatterplot(x=tmp[:,0], y=tmp[:,1], hue=rcp.net_bhvr,ax=ax)
    return rcp[['activity1','activity2','netbhvr', 'pca0','pca1']]
34/149:
rcp = get_bhvr("'wc_profit'")
rcpca = plot_qty_pca(rcp)
34/150: rcpca[rcpca.pca0.between(-10,-5)].sort_values(by='pca0')
34/151:
def get_bhvr(exp):
    df1=df[['experiment', 'qty0', 'activity1', 'qty1', 'price1', 'activity2', 'qty2', 'price2']]
    rcp = df1[df1.experiment==exp]
    
    rcp['inv_bhvr1']=None
    rcp['inv_bhvr1'].loc[rcp.activity1==0] = 'inactive'
    rcp['inv_bhvr1'].loc[rcp.activity1<0] = 'decrease'
    rcp['inv_bhvr1'].loc[rcp.activity1>0] = 'increase'
    
    rcp['inv_bhvr2']=None
    rcp['inv_bhvr2'].loc[rcp.activity2==0] = 'inactive'
    rcp['inv_bhvr2'].loc[rcp.activity2<0] = 'decrease'
    rcp['inv_bhvr2'].loc[rcp.activity2>0] = 'increase'
    rcp['net_bhvr'] = rcp['inv_bhvr1']+rcp['inv_bhvr2']
    return rcp

def plot_qty_pca(rcp):
    dftmp = rcp[['qty0','qty1','qty2']]
    from sklearn.decomposition import PCA
    pca2 = PCA(2).fit(dftmp)
    tmp = pca2.transform(dftmp)
    rcp['pca0']=tmp[:,0]
    rcp['pca1']=tmp[:,1]
    fig, ax =plt.subplots(figsize=(15,10))
    sns.scatterplot(x=tmp[:,0], y=tmp[:,1], hue=rcp.net_bhvr,ax=ax)
    return rcp[['activity1','activity2','netbhvr', 'pca0','pca1']]
34/152:
rcp = get_bhvr("'wc_profit'")
rcpca = plot_qty_pca(rcp)
34/153: rcpca[rcpca.pca0.between(-10,-5)].sort_values(by='pca0')
34/154: rcpca[['activity1','activity2','net_bhvr', 'pca0','pca1']][rcpca.pca0.between(-10,-5)].sort_values(by='pca0')
34/155: rcpca[['activity1','activity2','net_bhvr', 'pca0','pca1']][rcpca.pca0.between(-5,0)].sort_values(by='pca0')
34/156: rcpca[['activity1','activity2','net_bhvr', 'pca0','pca1']][rcpca.pca0.between(0,5)].sort_values(by='pca0')
34/157: rcpca[['activity1','activity2','net_bhvr', 'pca0','pca1']][rcpca.pca0.between(-1,1)].sort_values(by='pca0')
34/158: rcpca[['activity1','activity2','net_bhvr', 'pca0','pca1']][rcpca.pca0.between(5,10)].sort_values(by='pca0')
34/159: rcpca[['activity1','activity2','net_bhvr', 'pca0','pca1']][rcpca.pca0.between(9,20)].sort_values(by='pca0')
34/160:
dftmp = rcp[['qty0','qty1','qty2']]
# pca2 = PCA(2).fit(dftmp)
tmp = pca2.transform(dftmp)
rcp['pca0']=tmp[:,0]
rcp['pca1']=tmp[:,1]
fig, ax =plt.subplots(figsize=(15,10))
sns.scatterplot(x=tmp[:,0], y=tmp[:,1], hue=rcp.net_bhvr,ax=ax)
34/161:
def get_bhvr(exp):
    df1=df[['experiment', 'qty0', 'activity1', 'qty1', 'price1', 'activity2', 'qty2', 'price2']]
    rcp = df1[df1.experiment==exp]
    
    rcp['inv_bhvr1']=None
    rcp['inv_bhvr1'].loc[rcp.activity1==0] = 'inactive'
    rcp['inv_bhvr1'].loc[rcp.activity1<0] = 'decrease'
    rcp['inv_bhvr1'].loc[rcp.activity1>0] = 'increase'
    
    rcp['inv_bhvr2']=None
    rcp['inv_bhvr2'].loc[rcp.activity2==0] = 'inactive'
    rcp['inv_bhvr2'].loc[rcp.activity2<0] = 'decrease'
    rcp['inv_bhvr2'].loc[rcp.activity2>0] = 'increase'
    rcp['net_bhvr'] = rcp['inv_bhvr1']+rcp['inv_bhvr2']
    return rcp

def plot_qty_pca(rcp):
    dftmp = rcp[['qty0','qty1','qty2']]
    from sklearn.decomposition import PCA
    pca2 = PCA(2).fit(dftmp)
    tmp = pca2.transform(dftmp)
    rcp['pca0']=tmp[:,0]
    rcp['pca1']=tmp[:,1]
    fig, ax =plt.subplots(figsize=(15,10))
    sns.scatterplot(x=tmp[:,0], y=tmp[:,1], hue=rcp.net_bhvr,ax=ax)
    return rcp[['activity1','activity2','net_bhvr', 'pca0','pca1']], pca2
34/162:
wcp, wcp_mdl = get_bhvr("'wc_profit'")
wcpca = plot_qty_pca(wcp)
34/163:
wcp = get_bhvr("'wc_profit'")
wcpca, wcp_mdl = plot_qty_pca(wcp)
34/164:
rcp = get_bhvr("'rc_profit'")
rcpca, rcp_mdl = plot_qty_pca(rcp)
34/165:
def get_bhvr(exp):
    df1=df[['experiment', 'qty0', 'activity1', 'qty1', 'price1', 'activity2', 'qty2', 'price2']]
    rcp = df1[df1.experiment==exp]
    
    rcp['inv_bhvr1']=None
    rcp['inv_bhvr1'].loc[rcp.activity1==0] = 'inactive'
    rcp['inv_bhvr1'].loc[rcp.activity1<0] = 'decrease'
    rcp['inv_bhvr1'].loc[rcp.activity1>0] = 'increase'
    
    rcp['inv_bhvr2']=None
    rcp['inv_bhvr2'].loc[rcp.activity2==0] = 'inactive'
    rcp['inv_bhvr2'].loc[rcp.activity2<0] = 'decrease'
    rcp['inv_bhvr2'].loc[rcp.activity2>0] = 'increase'
    rcp['net_bhvr'] = rcp['inv_bhvr1']+rcp['inv_bhvr2']
    return rcp

def plot_qty_pca(rcp, mdl=None):
    dftmp = rcp[['qty0','qty1','qty2']]
    from sklearn.decomposition import PCA
    pca2 = PCA(2).fit(dftmp)
    if mdl:
        pca2=mdl    
    tmp = pca2.transform(dftmp)
    rcp['pca0']=tmp[:,0]
    rcp['pca1']=tmp[:,1]
    fig, ax =plt.subplots(figsize=(15,10))
    sns.scatterplot(x=tmp[:,0], y=tmp[:,1], hue=rcp.net_bhvr,ax=ax)
    return rcp[['activity1','activity2','net_bhvr', 'pca0','pca1']], pca2
34/166: _,_= plot_qty_pca(rcp, wcp_mdl)
34/167: _,_= plot_qty_pca(wcp, rcp_mdl)
34/168:
plt.plot([1,2,3,4,5],[1,2,3,4,5])
plt.title("hello")
34/169:
rcl = get_bhvr("'rc_loss'")
rcpca, rcl_mdl = plot_qty_pca(rcl)
34/170:
wcl = get_bhvr("'wc_loss'")
# rcpca, rcl_mdl = plot_qty_pca(rcl)
plot_qty_pca(wcl, rcl_mdl)
36/1:
def remove_stopwords(s):
    # nltk_stopwords = ['i',  'me',  'my',  'myself',  'we',  'our',  'ours',  'ourselves',  'you',  "you're",  "you've",  "you'll",  "you'd",  'your',  'yours',  'yourself',  'yourselves',  'he',  'him',  'his',  'himself',  'she',  "she's",  'her',  'hers',  'herself',  'it',  "it's",  'its',  'itself',  'they',  'them',  'their',  'theirs',  'themselves',  'what',  'which',  'who',  'whom',  'this',  'that',  "that'll",  'these',  'those',  'am',  'is',  'are',  'was',  'were',  'be',  'been',  'being',  'have',  'has',  'had',  'having',  'do',  'does',  'did',  'doing',  'a',  'an',  'the',  'and',  'but',  'if',  'or',  'because',  'as',  'until',  'while',  'of',  'at',  'by',  'for',  'with',  'about',  'against',  'between',  'into',  'through',  'during',  'before',  'after',  'above',  'below',  'to',  'from',  'up',  'down',  'in',  'out',  'on',  'off',  'over',  'under',  'again',  'further',  'then',  'once',  'here',  'there',  'when',  'where',  'why',  'how',  'all',  'any',  'both',  'each',  'few',  'more',  'most',  'other',  'some',  'such',  'no',  'nor',  'not',  'only',  'own',  'same',  'so',  'than',  'too',  'very',  's',  't',  'can',  'will',  'just',  'don',  "don't",  'should',  "should've",  'now',  'd',  'll',  'm',  'o',  're',  've',  'y',  'ain',  'aren',  "aren't",  'couldn',  "couldn't",  'didn',  "didn't",  'doesn',  "doesn't",  'hadn',  "hadn't",  'hasn',  "hasn't",  'haven',  "haven't",  'isn',  "isn't",  'ma',  'mightn',  "mightn't",  'mustn',  "mustn't",  'needn',  "needn't",  'shan',  "shan't",  'shouldn',  "shouldn't",  'wasn',  "wasn't",  'weren',  "weren't",  'won',  "won't",  'wouldn',  "wouldn't"]
    nltk_stopwords = stopwords.words('english')
    STOPWORDS = nltk_stopwords+["'s", "s"]+[x.capitalize() for x in nltk_stopwords]
    
    s = re.sub(r'([a-zA-Z])[\/]([a-zA-Z])', r'\1 \2', s) # split slashed words
    s = re.sub(r'(?<!non)\-', ' ', s) # remove all hyphens except "non-"
    s = re.sub(r'[^A-Za-z\s\-]+', '', s)
    tokens = word_tokenize(s)
    okstr = ' '.join([x for x in tokens if x in ['(',')'] or x not in STOPWORDS and len(x)>1])
    return okstr


def filter_pos_tags(s):
    OUT = ['VBD', 'IN', 'VBP', 'RB', 'MD', 'VB', 'VBZ', 'VBG']
    return ' '.join([w for w,t in pos_tag(word_tokenize(s)) if t not in OUT])


def abbr_expander(s):   
    res = re.findall(r'\(\s?[A-Z]\s?[A-Za-z]+\s?\)', s) # Misses acronyms with hyphen
    outs = s
    for abbr in res:
        ix = s.index(abbr)
        abbr_clean = re.sub(r'[^A-Za-z]', '', abbr) # keep only letters, discard all other chars
        abbr_len = len(re.sub(r'[a-z]','',abbr_clean)) # Number of capital letters ~ number of tokens to look back for expansion
        if abbr_clean[-1]=='s': abbr_len-=1 # Singularing a plural acronym
        # When looking for expansion candidates, consider (-) and (/) as word-breaks
        expanded = ' '.join(s[ix-1::-1][::-1].rstrip().split(' ')[-abbr_len:]) # identify possible candidates for expansion from preivous abbr_len words
        # ABBR_LIST[abbr_clean].append(expanded)
        outs = outs.replace(' '+abbr, '') # Remove first instance of abbreviation before expanding other instances
        if abbr_clean[-1]=='s':
            outs = outs.replace(abbr_clean[:-1], expanded)
        else:
            outs = outs.replace(abbr_clean, expanded) 
    return outs



def clean_sent(abst):
    abst = remove_stopwords(abst)
    abst = abbr_expander(abst)
    # Make post-period uppercase chars lower
    abst = re.sub(r"(?<=\. )[A-Z]",lambda t:t.group().lower(), abst)
    abst = filter_pos_tags(abst)
    return abst



def get_sumvec(clean_strr, w2v_model, idf=None):
    tokens = nltk.FreqDist(word_tokenize(clean_strr))
    
    vecs = []
    for t,cnt in tokens.items():
        mul = 1
        if t.startswith('non-'):
            t = t.replace('non-','')
            mul = -1
        try:
            if not idf: vecs.append(w2v_model[t]*mul*cnt)
            else: vecs.append(w2v_model[t]*mul*cnt*idf[t])
            
        except KeyError:
            continue
    return np.array(vecs).sum(0)




def sim(v1, v2):
    return 1-cosine(v1,v2)




def idf(doclist):
    N = len(doclist)
    vocab = list(nltk.FreqDist(word_tokenize(' '.join(dfn_list))).keys())
    idfd={}
    for w in vocab:
        idfd[w] = math.log(N/max(1,sum([1 for d in doclist if w in d])), 10)
    return idfd
        

#Vectorize constructs definition
def gen_rdoc_dict(w2v_model):
    rdoc={}
    rdoc['fear'] = {'dfn':"Activation of the brains defensive motivational system to promote behaviors that protect the organism from perceived danger. Normal fear involves a pattern of adaptive responses to conditioned or unconditioned threat stimuli (exteroceptive or interoceptive). Fear can involve internal representations and cognitive processing and can be modulated by a variety of factors."}
    rdoc['loss'] = {'dfn':"A state of deprivation of a motivationally non-significant con-specific, object, or situation. Loss may be social or non-social and may include permanent or sustained loss of shelter, behavioral control, status, loved ones, or relationships. The response to loss may be episodic (e.g., grief) or sustained."}
    rdoc['sleep_wf'] = {'dfn': "Sleep and wakefulness endogenous, recurring, behavioral states that reflect coordinated changes in the dynamic functional organization of the brain and that optimize physiology, behavior, and health. Homeostatic and circadian processes regulate the propensity for wakefulness and sleep."}
    rdoc['circadian'] = {'dfn': " Circadian rhythms are endogenous self-sustaining oscillations that organize the timing of biological systems to optimize physiology and behavior, and health. They are synchronized by recurring environmental cues; anticipate the external environment; allow effective response to challenges and opportunities in the physical and social environment; modulate homeostasis within the brain and other (central/peripheral) systems, tissues and organs; are evident across levels of organization including molecules, cells, circuits, systems, organisms, and social systems."}
    rdoc['arousal'] = {'dfn':" Arousal is a continuum of sensitivity of the organism to stimuli, both external and internal. Arousal facilitates interaction with the environment in a context-specific manner (e.g., under conditions of threat, some stimuli must be ignored while sensitivity to and responses to others is enhanced, as exemplified in the startle reflex). It can be evoked by either external/environmental stimuli or internal stimuli (e.g., emotions and cognition). I can be modulated by the physical characteristics and motivational significance of stimuli. It varies along a continuum that can be quantified in any behavioral state, including wakefulness and low-arousal states including sleep, anesthesia, and coma. Arousal is distinct from motivation and valence but can covary with intensity of motivation and valence. It may be associated with increased or decreased locomotor activity, and can be regulated by homeostatic drives (e.g., hunger, sleep, thirst, sex)."}
    rdoc['frusnrew'] = {'dfn': "Frustrate Non-reward: Reactions elicited in response to withdrawal/prevention of reward, i.e., by the inability to obtain positive rewards following repeated or sustained efforts."}
    rdoc['anxiety'] = {'dfn': "Activation of a brain system in which harm may potentially occur but is distant, ambiguous, or low/uncertain in probability, characterized by a pattern of responses such as enhanced risk assessment (vigilance). These responses to low imminence threats are qualitatively different than the high imminence threat behaviors that characterize fear."}
    rdoc['susthreat'] = {'dfn':"An aversive emotional state caused by prolonged (i.e., weeks to months) exposure to internal and/or external condition(s), state(s), or stimuli that are adaptive to escape or avoid. The exposure may be actual or anticipated; the changes in affect, cognition, physiology, and behavior caused by sustained threat persist in the absence of the threat and can be differentiated from those changes evoked by acute threat."}
    # get cleaned defn
    for key in rdoc:
        rdoc[key]['clean'] = clean_sent(rdoc[key]['dfn'])
    dfn_list = [rdoc[key]['clean'] for key in rdoc]
    construct_idf = idf(dfn_list)
    # get_sumvec for definitons
    for key in rdoc:
        rdoc[key]['vec'] = get_sumvec(rdoc[key]['clean'], w2v_model, construct_idf)
    return rdoc


#Vectorize test data + get Sim
def compute_sim(w2v_model, out_fname='test_results.csv', test_fname='data/training_data.csv'):
    rdoc = gen_rdoc_dict(w2v_model)
    test_data = pd.read_csv(test_fname)
    test_data['content'] = test_data['title']+test_data['abstract'] 
    test_idf = idf(test_data['content'].values)
    test_data['vec'] = test_data['content'].apply(get_sumvec, args=(w2v_model, test_idf, ))
    for key in rdoc:
        keyvec = rdoc[key]['vec']
        test_data[key+'_sim'] = test_data['vec'].apply(sim, args=(keyvec,))
    test_data.to_csv(out_fname)
    return test_data

# Get avg precision for each construct from results df
def avg_precision(results_df):
    precision = pd.Series(index=results_df.rdoc.unique())
    for rdoc in precision.index:
        df1 = results_df.copy()
        df1['rdoc'] = np.where(df1['rdoc']==rdoc, 1, 0) #tag relevant/irrelevant groudn truth
        precision[rdoc] = average_precision_score(df1['rdoc'], df1[rdoc+'_sim']) # compute based on relevance score
    return precision

def get_model(f):
    return gensim.models.KeyedVectors.load(f)

def get_heatmaps(results_df, rdoc, prec=''):
    df=results_df[results_df.rdoc==rdoc].copy()
    relevance_c = [c for c in df.columns if '_sim' in c]
    scores = MinMaxScaler().fit_transform(df[relevance_c].T).T
    xlabels = [c.replace('_sim','') for c in relevance_c]
    fig, ax = plt.subplots()
    fig.set_size_inches(7, 5)
    plt.title(rdoc+' '+str(prec))
    sns.heatmap(scores, center=0.5, xticklabels=xlabels)
    

def get_ap(adf):
    qap={}
    df = adf.copy()
    for const in df.rdoc.unique():
        df.sort_values(const+'_sim', ascending=False, inplace=True)
        df['docid'] = range(1,len(df)+1)
        df['groundtruth'] = np.where(df.rdoc==const, 1, 0)
        tp = 0
        ap=0
        for _, row in df.iterrows():
            tp+=row['groundtruth']
            ap += tp*row['groundtruth']/row['docid']
        qap[const] = ap/df['groundtruth'].sum()
    return qap
36/2:
import seaborn as sns
import gensim
from scipy.spatial.distance import cosine
import numpy as np
import matplotlib.pyplot as plt
import re
from nltk.corpus import stopwords
from nltk import word_tokenize, pos_tag
import pandas as pd
import nltk
import math
from sklearn.metrics import average_precision_score
from sklearn.preprocessing import MinMaxScaler
36/3: clean_sent("Loneliness, anxiety, and emotional and psychological loss of support resulting from desertion or neglect")
36/4: clean_sent("Amount of time or space that an individual can dedicate to particular task or content without becoming distracted.")
36/5: clean_sent("""Bipolar disorder or bipolar affective disorder (historically known as manic-depressive disorder) is a psychiatric diagnosis for a mood disorder in which people experience disruptive mood swings that encompass a frenzied state known as mania (or hypomania) and, usually, symptoms of depression. Bipolar disorder is defined by the presence of one or more episodes of abnormally elevated energy levels, cognition, and mood with or without one or more depressive episodes. At the lower levels of mania, such as hypomania, individuals may appear energetic and excitable. At a higher level, individuals may behave erratically and impulsively, often making poor decisions due to unrealistic ideas about the future, and may have great difficulty with sleep. At the highest level, individuals can show psychotic behavior, including violence. Individuals who experience manic episodes also commonly experience depressive episodes, or symptoms, or a mixed state in which features of both mania and depression are present at the same time. These events are usually separated by periods of "normal" mood; but, in some individuals, depression and mania may rapidly alternate, which is known as rapid cycling. Severe manic episodes can sometimes lead to such psychotic symptoms as delusions and hallucinations. The lifetime prevalence of all types of bipolar disorder is thought to be about 4% (meaning that about 4% of people experience some of the characteristic symptoms at some point in their life). Prevalence is similar in men and women and, broadly, across different cultures and ethnic groups. Genetic factors contribute substantially to the likelihood of developing bipolar disorder, and environmental factors are also implicated. Bipolar disorder is often treated with mood stabilizing medications and psychotherapy. In serious cases, in which there is a risk of harm to oneself or others, involuntary commitment may be used. These cases generally involve severe manic episodes with dangerous behavior or depressive episodes with suicidal ideation. There are widespread problems with social stigma, stereotypes, and prejudice against individuals with a diagnosis of bipolar disorder. People with bipolar disorder exhibiting psychotic symptoms can sometimes be misdiagnosed as having schizophrenia.""")
37/1: n=500
37/2: arr2 = sorted(list( set([random.randint(1,100) for x in range(int(n))]) ), reverse=True)
37/3: import random
37/4: import time
37/5: arr2 = sorted(list( set([random.randint(1,100) for x in range(int(n))]) ), reverse=True)
37/6: ix_a = random.randint(0, len(arr2)-1)
37/7:
def partition(arr, lo, hi, probab=False):
  if probab:
    pivot = random.randint(lo,hi)
    arr[lo], arr[pivot] = arr[pivot], arr[lo]
  pivot = lo
  j = lo
  for i in range(lo+1, hi+1): #<=, ++
    if arr[i]<arr[pivot]:
      j+=1 # shifts to undisturbed biggie. waits till i finds a smallie to swap with
      arr[j], arr[i] = arr[i], arr[j] # swap mismatched biggie and smallie
  arr[lo], arr[j] = arr[j], arr[lo]
  return j


def k_selection(arr, lo, hi, k):
  if lo==hi:
    return arr[lo]
  else:
    sorted_ix = partition(arr, lo, hi)
    if sorted_ix==k:
      return arr[sorted_ix]
    elif sorted_ix<k:
      return k_selection(arr, sorted_ix+1, hi, k)
    else:
      return k_selection(arr, lo, sorted_ix-1, k)
37/8: ans = k_selection(arr2, 0, len(arr2)-1, ix_a)
37/9: ans
37/10: ix_a
37/11: sorted(arr2)[78]
37/12: flag = True
37/13:
while flag:
    arr2 = sorted(list(set([random.randint(1,100) for x in range(int(n))])))
    ix_a = random.randint(0, len(arr2)-1)
    ans = k_selection(arr2, 0, len(arr2)-1, ix_a)
    flag = ans==sorted(arr2)[ix_a]
38/1:
import time
from matplotlib import pyplot as plt
import random
import math

def exchange_sort(arr):
  N = len(arr)
  for i in range(N-1):
    for j in range(i+1, N-1):
      if arr[i]>arr[j]:
        arr[i], arr[j] = arr[j], arr[i]
  return arr


def bin_search(arr, i):
  # arr = exchange_sort(arr)
  hi = len(arr)-1
  low = 0
  mid = 1
  while mid>0:
    mid = (hi+low)//2
    if arr[mid]==i:
      return mid
    elif arr[mid]>i:
      hi = mid-1
    else:
      low = mid+1
  return -1


def interp_search(arr, i): # linear interpolation (in first else block)
  lo=0
  hi=len(arr)-1
  mid=None
  if arr[lo]<=i<=arr[hi]:
    while lo<=hi:
      denom = arr[hi] - arr[lo]
      if denom==0:
        mid = lo
      else: # linear incrememnt of mid
        mid = lo+math.floor((i-arr[lo])*(hi-lo)/denom)
      if arr[mid] == i:
        return mid
      if arr[mid]<i:
        lo = mid+1
      else:
        hi = mid-1
  return -1


def robust_interp(arr, i):
  mid=lo=0
  hi=len(arr)-1
  if arr[lo] <= i <= arr[hi]:
    while lo<=hi:
      gap = math.floor((hi-lo)**0.5)
      mid = min(hi-gap, max(lo+gap, mid))
      if arr[mid]==i:
        return mid
      if arr[mid]>i:
        hi = mid-1
      else:
        lo = mid+1
  return -1


def partition(arr, lo, hi, probab=False):
  if probab:
    pivot = random.randint(lo,hi)
    arr[lo], arr[pivot] = arr[pivot], arr[lo]
  pivot = lo
  j = lo
  for i in range(lo+1, hi+1): #<=, ++
    if arr[i]<arr[pivot]:
      j+=1 # shifts to undisturbed biggie. waits till i finds a smallie to swap with
      arr[j], arr[i] = arr[i], arr[j] # swap mismatched biggie and smallie
  arr[lo], arr[j] = arr[j], arr[lo]
  return j


def partition(arr, lo, hi, probab=False):
  if probab:
    pivot = random.randint(lo,hi)
    arr[lo], arr[pivot] = arr[pivot], arr[lo]
  pivot = lo
  j = lo
  for i in range(lo+1, hi+1): #<=, ++
    if arr[i]<arr[pivot]:
      j+=1 # shifts to undisturbed biggie. waits till i finds a smallie to swap with
      arr[j], arr[i] = arr[i], arr[j] # swap mismatched biggie and smallie
  arr[lo], arr[j] = arr[j], arr[lo]
  return j


def k_selection(arr, lo, hi, k):
  if lo==hi:
    return arr[lo]
  else:
    sorted_ix = partition(arr, lo, hi)
    if sorted_ix==k:
      return arr[sorted_ix]
    elif sorted_ix<k:
      return k_selection(arr, sorted_ix+1, hi, k)
    else:
      return k_selection(arr, lo, sorted_ix-1, k)



def partition2(arr, lo, hi, k):
  
  borders = list(range(lo, hi+1, 5))
  subind = zip(borders, borders[1:]+[hi+1])
  medians=[]
  for s,i in subind:
    medians.append(sorted(arr[s:i])[(i-s)//2]) 

  # this is the value at median
  # print(f"Looking for median. Median array={medians}", end=' ')
  medofmed = k_selection_median(medians, 0, len(medians)-1, len(medians)//2)
  
  # partition around median value
  j = lo
  pivot_loc=None
  # # print(f"i:-> {list(range(lo,hi+1))}")
  for i in range(lo, hi+1): 
    # print(f"i,j={i},{j}  medofmed={medofmed}  ",end='') 
    if arr[i]<medofmed: # normal swapping of mismatched shorties
      arr[i], arr[j] = arr[j], arr[i]
      j+=1
      # print(f"swap1. {arr[i]}<=>{arr[j-1]}. {arr}")
    elif arr[i]==medofmed: # reached the pivot. 
      arr[i], arr[j] = arr[j], arr[i]
      pivot_loc = j
      j+=1
      # print(f"swap2. {arr[i]}<=>{arr[j-1]}. {arr}")
    else:
      pass
    if pivot_loc: # pivot always needs to be after the shorties. sending to right place
      arr[j-1], arr[pivot_loc] = arr[pivot_loc], arr[j-1]
      # print(f"pivot shift")
  # # print(f"arr:{arr}, returning j:{j-1}")
  return j-1


    
def k_selection_median(arr, lo, hi, k):
  if lo>=hi:
    return arr[lo]
  else:
    # print(f"Calling partition on {arr}, {lo}, {hi}, {k}")
    sorted_ix = partition2(arr, lo, hi, k)
    # print(f"sortedIX:{sorted_ix},  k:{k}")
    if sorted_ix==k:
      return arr[sorted_ix]
    if sorted_ix>k:
      return k_selection_median(arr, lo, sorted_ix-1, k)
    else:
      return k_selection_median(arr, sorted_ix+1, hi, k)


def k_selection_probab(arr, lo, hi, k):
  if lo==hi:
    return arr[lo]
  else:
    sorted_ix = partition(arr, lo, hi, True)
    if sorted_ix==k:
      return arr[sorted_ix]
    elif sorted_ix<k:
      return k_selection_probab(arr, sorted_ix+1, hi, k)
    else:
      return k_selection_probab(arr, lo, sorted_ix-1, k)


def best_case_search(func, N):
  times_search=[] 
  for n in N:
    
    arr2 = sorted(list(set([random.randint(1,100) for x in range(int(n))])))
    ix_a = 0
    
    start = time.time()
    ans = func(arr2, arr2[ix_a])
    duration = time.time()-start
    if ans==ix_a:
      times_search.append(duration)
  return times_search


def best_case_kth(func, N):
  # sorted arrays
  times_kth=[]
  for n in N:
    
    arr2 = sorted(list(set([random.randint(1,100) for x in range(int(n))])))
    ix_a = random.randint(0, len(arr2)-1)
    
    start = time.time()
    ans = func(arr2, 0, len(arr2)-1, ix_a)
    duration=time.time()-start
    if ans==sorted(arr2)[ix_a]:
      times_kth.append(duration)    
    else:
      print("ERROR")
  return times_kth


def avg_case_search(func, N):
  times_search=[]
  for n in N:
    
    arr2 = sorted((list(set([random.randint(1,100) for x in range(int(n))]))))
    ix_a = random.randint(0, len(arr2)-1)
    start = time.time()
    ans = bin_search(arr2, arr2[ix_a])
    duration = time.time()-start
    # if ans==ix_a:
    times_search.append(duration)
  return times_search


def avg_case_kth(func, N):
  # sorted arrays

  times_kth=[]
  for n in N:
    
    for i in range(3):
      times_=[]
      arr2 = (list(set([random.randint(1,100) for x in range(int(n))])))
      ix_a = random.randint(0, len(arr2)-1)
      
      start = time.time()
      ans = func(arr2, 0, len(arr2)-1, ix_a)
      duration=time.time()-start
      if ans==sorted(arr2)[ix_a]:
        times_.append(duration)
      else:
        print("ERROR")    
    times_kth.append(sum(times_)/3)  
  return times_kth


def worst_case_search(func,N):
  times_search=[]
  for n in N:
    
    arr2 = sorted(list(set([random.randint(1,100) for x in range(int(n))])))
    start = time.time()
    ans = bin_search(arr2, -1)
    duration = time.time()-start
    # if ans==-1:
    times_search.append(duration)
  return times_search


def worst_case_kth(func, N):
  times_kth=[]
  for n in N:
    
    arr2 = sorted(list( set([random.randint(1,100) for x in range(int(n))]) ), reverse=True)
    ix_a = random.randint(0, len(arr2)-1)

    start = time.time()
    ans = func(arr2, 0, len(arr2)-1, ix_a)
    duration=time.time()-start
    if ans!=sorted(arr2)[ix_a]:
      print(f"ERROR: {n}")
    times_kth.append(duration)    
  
  return times_kth



  

def main_kth():
  import matplotlib as mpl
  mpl.use('Agg')
  import matplotlib.pyplot as plt

  N=[500*math.pow(2,x) for x in range(10)]
  # print(N)
  for func in [k_selection, k_selection_probab, k_selection_median]:
    fig1, ax1 = plt.subplots(1,3, figsize=(18, 6))
    for c,test in enumerate([best_case_kth, avg_case_kth, worst_case_kth]):
      print(f"{func.__name__} - {test.__name__}")
      times = test(func,N)
      print(times)
      ax1[c].plot(N, times)
      ax1[c].set_title(f"{test.__name__}")
      fig1.savefig(f"{func.__name__}.png")

   
def test(n):
  arr = sorted(list( set([random.randint(1,100) for x in range(int(n))]) ), reverse=True)
  ix_a = random.randint(0, len(arr)-1)
  ans = k_selection_median(arr, 0, len(arr)-1, ix_a)
  if ans!=sorted(arr)[ix_a]:
      print(f"ERROR: {n}")
      return False
  return True

  # print()
38/2: test(10000)
38/3: arr = sorted(list( set([random.randint(1,100) for x in range(int(n))]) ), reverse=True)ix_a = random.randint(0, len(arr)-1)
38/4: arr = sorted(list( set([random.randint(1,100) for x in range(int(10000))]) ), reverse=True)ix_a = random.randint(0, len(arr)-1)
38/5: len(arr)
38/6: arr = sorted(list( set([random.randint(1,100) for x in range(int(10000))]) ), reverse=True)ix_a = random.randint(0, len(arr)-1)
38/7: len(arr)
38/8: arr = sorted(list( set([random.randint(1,10000) for x in range(int(10000))]) ), reverse=True)ix_a = random.randint(0, len(arr)-1)
38/9: len(arr)
38/10: ix_a=3
38/11: ans = k_selection_median(arr, 0, len(arr)-1, 3)
38/12: sorted(arr)[3]
38/13: ans
38/14: arr = sorted(list( set([random.randint(1,10000) for x in range(5)]) ), reverse=True)ix_a = random.randint(0, len(arr)-1)
38/15: ans = k_selection_median(arr, 0, len(arr)-1, 3)
38/16: sorted(arr)[3]
38/17: ans
38/18: arr = sorted(list( set([random.randint(1,10000) for x in range(50)]) ), reverse=True)ix_a = random.randint(0, len(arr)-1)
38/19: ans = k_selection_median(arr, 0, len(arr)-1, 3)
38/20: sorted(arr)[3]
38/21: ans
38/22: arr
38/23: pprint off
38/24: arr
38/25: sorted(arr)[3]
39/1: arr = [348, 106, 656, 1142, 1058, 1961, 1842, 1767, 1526, 1339, 1201, 1174, 2208, 2127, 2006, 2691, 2525, 29, 5671, 5401, 5268, 5243, 4767, 4716, 4471, 4195, 3850, 3431, 3027, 2910, 2889, 2837, 9919, 9735, 9284, 9172, 9040, 7978, 7928, 7819, 7751, 7548, 7330, 7327, 7151, 6657, 6441, 6217, 6094, 6007]
39/2: len(arr)
39/3: sorted(arr)[27]
39/4: l=[348, 106, 656, 1142, 1058, 1961, 1842, 1767, 1526, 1339, 1201, 1174, 2208, 2127, 2006, 2691, 2525, 29, 2837, 4767, 4716, 4471, 4195, 3850, 3431, 3027, 2910, 2889, 5268, 5243, 5671, 5401, 9919, 9735, 9284, 9172, 9040, 7978, 7928, 7819, 7751, 7548, 7330, 7327, 7151, 6657, 6441, 6217, 6094, 6007]
39/5: l[27]
39/6:
for i in range(20):
    if i%20==0:
        pass
    print(i)
39/7:
for i in range(20):
    if i%2==0:
        pass
    print(i)
39/8: arr
39/9: pprint off
39/10: len(arr)
39/11: arr
39/12: sorted(arr)[3]
39/13: sorted(arr)[3]
39/14: [500*math.pow(2,x) for x in range(10)]
39/15: import math
39/16: [500*math.pow(2,x) for x in range(10)]
39/17: [100*math.pow(2,x) for x in range(10)]
40/1: clean_sent("""Short-term detachment from one's immediate surroundings, during which a person's contact with reality is blurred and partially substituted by a visionary fantasy, especially one of happy, pleasant thoughts, hopes or ambitions, imagined as coming to pass, and experienced while awake.""")
40/2:
import seaborn as sns
import gensim
from scipy.spatial.distance import cosine
import numpy as np
import matplotlib.pyplot as plt
import re
from nltk.corpus import stopwords
from nltk import word_tokenize, pos_tag
import pandas as pd
import nltk
import math
from sklearn.metrics import average_precision_score
from sklearn.preprocessing import MinMaxScaler
40/3:
def remove_stopwords(s):
    # nltk_stopwords = ['i',  'me',  'my',  'myself',  'we',  'our',  'ours',  'ourselves',  'you',  "you're",  "you've",  "you'll",  "you'd",  'your',  'yours',  'yourself',  'yourselves',  'he',  'him',  'his',  'himself',  'she',  "she's",  'her',  'hers',  'herself',  'it',  "it's",  'its',  'itself',  'they',  'them',  'their',  'theirs',  'themselves',  'what',  'which',  'who',  'whom',  'this',  'that',  "that'll",  'these',  'those',  'am',  'is',  'are',  'was',  'were',  'be',  'been',  'being',  'have',  'has',  'had',  'having',  'do',  'does',  'did',  'doing',  'a',  'an',  'the',  'and',  'but',  'if',  'or',  'because',  'as',  'until',  'while',  'of',  'at',  'by',  'for',  'with',  'about',  'against',  'between',  'into',  'through',  'during',  'before',  'after',  'above',  'below',  'to',  'from',  'up',  'down',  'in',  'out',  'on',  'off',  'over',  'under',  'again',  'further',  'then',  'once',  'here',  'there',  'when',  'where',  'why',  'how',  'all',  'any',  'both',  'each',  'few',  'more',  'most',  'other',  'some',  'such',  'no',  'nor',  'not',  'only',  'own',  'same',  'so',  'than',  'too',  'very',  's',  't',  'can',  'will',  'just',  'don',  "don't",  'should',  "should've",  'now',  'd',  'll',  'm',  'o',  're',  've',  'y',  'ain',  'aren',  "aren't",  'couldn',  "couldn't",  'didn',  "didn't",  'doesn',  "doesn't",  'hadn',  "hadn't",  'hasn',  "hasn't",  'haven',  "haven't",  'isn',  "isn't",  'ma',  'mightn',  "mightn't",  'mustn',  "mustn't",  'needn',  "needn't",  'shan',  "shan't",  'shouldn',  "shouldn't",  'wasn',  "wasn't",  'weren',  "weren't",  'won',  "won't",  'wouldn',  "wouldn't"]
    nltk_stopwords = stopwords.words('english')
    STOPWORDS = nltk_stopwords+["'s", "s"]+[x.capitalize() for x in nltk_stopwords]
    
    s = re.sub(r'([a-zA-Z])[\/]([a-zA-Z])', r'\1 \2', s) # split slashed words
    s = re.sub(r'(?<!non)\-', ' ', s) # remove all hyphens except "non-"
    s = re.sub(r'[^A-Za-z\s\-]+', '', s)
    tokens = word_tokenize(s)
    okstr = ' '.join([x for x in tokens if x in ['(',')'] or x not in STOPWORDS and len(x)>1])
    return okstr


def filter_pos_tags(s):
    OUT = ['VBD', 'IN', 'VBP', 'RB', 'MD', 'VB', 'VBZ', 'VBG']
    return ' '.join([w for w,t in pos_tag(word_tokenize(s)) if t not in OUT])


def abbr_expander(s):   
    res = re.findall(r'\(\s?[A-Z]\s?[A-Za-z]+\s?\)', s) # Misses acronyms with hyphen
    outs = s
    for abbr in res:
        ix = s.index(abbr)
        abbr_clean = re.sub(r'[^A-Za-z]', '', abbr) # keep only letters, discard all other chars
        abbr_len = len(re.sub(r'[a-z]','',abbr_clean)) # Number of capital letters ~ number of tokens to look back for expansion
        if abbr_clean[-1]=='s': abbr_len-=1 # Singularing a plural acronym
        # When looking for expansion candidates, consider (-) and (/) as word-breaks
        expanded = ' '.join(s[ix-1::-1][::-1].rstrip().split(' ')[-abbr_len:]) # identify possible candidates for expansion from preivous abbr_len words
        # ABBR_LIST[abbr_clean].append(expanded)
        outs = outs.replace(' '+abbr, '') # Remove first instance of abbreviation before expanding other instances
        if abbr_clean[-1]=='s':
            outs = outs.replace(abbr_clean[:-1], expanded)
        else:
            outs = outs.replace(abbr_clean, expanded) 
    return outs



def clean_sent(abst):
    abst = remove_stopwords(abst)
    abst = abbr_expander(abst)
    # Make post-period uppercase chars lower
    abst = re.sub(r"(?<=\. )[A-Z]",lambda t:t.group().lower(), abst)
    abst = filter_pos_tags(abst)
    return abst



def get_sumvec(clean_strr, w2v_model, idf=None):
    tokens = nltk.FreqDist(word_tokenize(clean_strr))
    
    vecs = []
    for t,cnt in tokens.items():
        mul = 1
        if t.startswith('non-'):
            t = t.replace('non-','')
            mul = -1
        try:
            if not idf: vecs.append(w2v_model[t]*mul*cnt)
            else: vecs.append(w2v_model[t]*mul*cnt*idf[t])
            
        except KeyError:
            continue
    return np.array(vecs).sum(0)




def sim(v1, v2):
    return 1-cosine(v1,v2)




def idf(doclist):
    N = len(doclist)
    vocab = list(nltk.FreqDist(word_tokenize(' '.join(dfn_list))).keys())
    idfd={}
    for w in vocab:
        idfd[w] = math.log(N/max(1,sum([1 for d in doclist if w in d])), 10)
    return idfd
        

#Vectorize constructs definition
def gen_rdoc_dict(w2v_model):
    rdoc={}
    rdoc['fear'] = {'dfn':"Activation of the brains defensive motivational system to promote behaviors that protect the organism from perceived danger. Normal fear involves a pattern of adaptive responses to conditioned or unconditioned threat stimuli (exteroceptive or interoceptive). Fear can involve internal representations and cognitive processing and can be modulated by a variety of factors."}
    rdoc['loss'] = {'dfn':"A state of deprivation of a motivationally non-significant con-specific, object, or situation. Loss may be social or non-social and may include permanent or sustained loss of shelter, behavioral control, status, loved ones, or relationships. The response to loss may be episodic (e.g., grief) or sustained."}
    rdoc['sleep_wf'] = {'dfn': "Sleep and wakefulness endogenous, recurring, behavioral states that reflect coordinated changes in the dynamic functional organization of the brain and that optimize physiology, behavior, and health. Homeostatic and circadian processes regulate the propensity for wakefulness and sleep."}
    rdoc['circadian'] = {'dfn': " Circadian rhythms are endogenous self-sustaining oscillations that organize the timing of biological systems to optimize physiology and behavior, and health. They are synchronized by recurring environmental cues; anticipate the external environment; allow effective response to challenges and opportunities in the physical and social environment; modulate homeostasis within the brain and other (central/peripheral) systems, tissues and organs; are evident across levels of organization including molecules, cells, circuits, systems, organisms, and social systems."}
    rdoc['arousal'] = {'dfn':" Arousal is a continuum of sensitivity of the organism to stimuli, both external and internal. Arousal facilitates interaction with the environment in a context-specific manner (e.g., under conditions of threat, some stimuli must be ignored while sensitivity to and responses to others is enhanced, as exemplified in the startle reflex). It can be evoked by either external/environmental stimuli or internal stimuli (e.g., emotions and cognition). I can be modulated by the physical characteristics and motivational significance of stimuli. It varies along a continuum that can be quantified in any behavioral state, including wakefulness and low-arousal states including sleep, anesthesia, and coma. Arousal is distinct from motivation and valence but can covary with intensity of motivation and valence. It may be associated with increased or decreased locomotor activity, and can be regulated by homeostatic drives (e.g., hunger, sleep, thirst, sex)."}
    rdoc['frusnrew'] = {'dfn': "Frustrate Non-reward: Reactions elicited in response to withdrawal/prevention of reward, i.e., by the inability to obtain positive rewards following repeated or sustained efforts."}
    rdoc['anxiety'] = {'dfn': "Activation of a brain system in which harm may potentially occur but is distant, ambiguous, or low/uncertain in probability, characterized by a pattern of responses such as enhanced risk assessment (vigilance). These responses to low imminence threats are qualitatively different than the high imminence threat behaviors that characterize fear."}
    rdoc['susthreat'] = {'dfn':"An aversive emotional state caused by prolonged (i.e., weeks to months) exposure to internal and/or external condition(s), state(s), or stimuli that are adaptive to escape or avoid. The exposure may be actual or anticipated; the changes in affect, cognition, physiology, and behavior caused by sustained threat persist in the absence of the threat and can be differentiated from those changes evoked by acute threat."}
    # get cleaned defn
    for key in rdoc:
        rdoc[key]['clean'] = clean_sent(rdoc[key]['dfn'])
    dfn_list = [rdoc[key]['clean'] for key in rdoc]
    construct_idf = idf(dfn_list)
    # get_sumvec for definitons
    for key in rdoc:
        rdoc[key]['vec'] = get_sumvec(rdoc[key]['clean'], w2v_model, construct_idf)
    return rdoc


#Vectorize test data + get Sim
def compute_sim(w2v_model, out_fname='test_results.csv', test_fname='data/training_data.csv'):
    rdoc = gen_rdoc_dict(w2v_model)
    test_data = pd.read_csv(test_fname)
    test_data['content'] = test_data['title']+test_data['abstract'] 
    test_idf = idf(test_data['content'].values)
    test_data['vec'] = test_data['content'].apply(get_sumvec, args=(w2v_model, test_idf, ))
    for key in rdoc:
        keyvec = rdoc[key]['vec']
        test_data[key+'_sim'] = test_data['vec'].apply(sim, args=(keyvec,))
    test_data.to_csv(out_fname)
    return test_data

# Get avg precision for each construct from results df
def avg_precision(results_df):
    precision = pd.Series(index=results_df.rdoc.unique())
    for rdoc in precision.index:
        df1 = results_df.copy()
        df1['rdoc'] = np.where(df1['rdoc']==rdoc, 1, 0) #tag relevant/irrelevant groudn truth
        precision[rdoc] = average_precision_score(df1['rdoc'], df1[rdoc+'_sim']) # compute based on relevance score
    return precision

def get_model(f):
    return gensim.models.KeyedVectors.load(f)

def get_heatmaps(results_df, rdoc, prec=''):
    df=results_df[results_df.rdoc==rdoc].copy()
    relevance_c = [c for c in df.columns if '_sim' in c]
    scores = MinMaxScaler().fit_transform(df[relevance_c].T).T
    xlabels = [c.replace('_sim','') for c in relevance_c]
    fig, ax = plt.subplots()
    fig.set_size_inches(7, 5)
    plt.title(rdoc+' '+str(prec))
    sns.heatmap(scores, center=0.5, xticklabels=xlabels)
    

def get_ap(adf):
    qap={}
    df = adf.copy()
    for const in df.rdoc.unique():
        df.sort_values(const+'_sim', ascending=False, inplace=True)
        df['docid'] = range(1,len(df)+1)
        df['groundtruth'] = np.where(df.rdoc==const, 1, 0)
        tp = 0
        ap=0
        for _, row in df.iterrows():
            tp+=row['groundtruth']
            ap += tp*row['groundtruth']/row['docid']
        qap[const] = ap/df['groundtruth'].sum()
    return qap
40/4: clean_sent("""Short-term detachment from one's immediate surroundings, during which a person's contact with reality is blurred and partially substituted by a visionary fantasy, especially one of happy, pleasant thoughts, hopes or ambitions, imagined as coming to pass, and experienced while awake.""")
40/5:
def remove_stopwords(s):
    # nltk_stopwords = ['i',  'me',  'my',  'myself',  'we',  'our',  'ours',  'ourselves',  'you',  "you're",  "you've",  "you'll",  "you'd",  'your',  'yours',  'yourself',  'yourselves',  'he',  'him',  'his',  'himself',  'she',  "she's",  'her',  'hers',  'herself',  'it',  "it's",  'its',  'itself',  'they',  'them',  'their',  'theirs',  'themselves',  'what',  'which',  'who',  'whom',  'this',  'that',  "that'll",  'these',  'those',  'am',  'is',  'are',  'was',  'were',  'be',  'been',  'being',  'have',  'has',  'had',  'having',  'do',  'does',  'did',  'doing',  'a',  'an',  'the',  'and',  'but',  'if',  'or',  'because',  'as',  'until',  'while',  'of',  'at',  'by',  'for',  'with',  'about',  'against',  'between',  'into',  'through',  'during',  'before',  'after',  'above',  'below',  'to',  'from',  'up',  'down',  'in',  'out',  'on',  'off',  'over',  'under',  'again',  'further',  'then',  'once',  'here',  'there',  'when',  'where',  'why',  'how',  'all',  'any',  'both',  'each',  'few',  'more',  'most',  'other',  'some',  'such',  'no',  'nor',  'not',  'only',  'own',  'same',  'so',  'than',  'too',  'very',  's',  't',  'can',  'will',  'just',  'don',  "don't",  'should',  "should've",  'now',  'd',  'll',  'm',  'o',  're',  've',  'y',  'ain',  'aren',  "aren't",  'couldn',  "couldn't",  'didn',  "didn't",  'doesn',  "doesn't",  'hadn',  "hadn't",  'hasn',  "hasn't",  'haven',  "haven't",  'isn',  "isn't",  'ma',  'mightn',  "mightn't",  'mustn',  "mustn't",  'needn',  "needn't",  'shan',  "shan't",  'shouldn',  "shouldn't",  'wasn',  "wasn't",  'weren',  "weren't",  'won',  "won't",  'wouldn',  "wouldn't"]
    nltk_stopwords = stopwords.words('english')
    STOPWORDS = nltk_stopwords+["'s", "s"]+[x.capitalize() for x in nltk_stopwords]
    
    s = re.sub(r'([a-zA-Z])[\/]([a-zA-Z])', r'\1 \2', s) # split slashed words
    s = re.sub(r'(?<!non)\-', ' ', s) # remove all hyphens except "non-"
    s = re.sub(r'[^A-Za-z\s\-]+', '', s)
    tokens = word_tokenize(s)
    okstr = ' '.join([x for x in tokens if x in ['(',')'] or x not in STOPWORDS and len(x)>1])
    return okstr


def filter_pos_tags(s):
    OUT = ['VBD', 'IN', 'VBP', 'RB', 'MD', 'VB', 'VBZ', 'VBG']
    return ' '.join([w for w,t in pos_tag(word_tokenize(s)) if t not in OUT])


def abbr_expander(s):   
    res = re.findall(r'\(\s?[A-Z]\s?[A-Za-z]+\s?\)', s) # Misses acronyms with hyphen
    outs = s
    for abbr in res:
        ix = s.index(abbr)
        abbr_clean = re.sub(r'[^A-Za-z]', '', abbr) # keep only letters, discard all other chars
        abbr_len = len(re.sub(r'[a-z]','',abbr_clean)) # Number of capital letters ~ number of tokens to look back for expansion
        if abbr_clean[-1]=='s': abbr_len-=1 # Singularing a plural acronym
        # When looking for expansion candidates, consider (-) and (/) as word-breaks
        expanded = ' '.join(s[ix-1::-1][::-1].rstrip().split(' ')[-abbr_len:]) # identify possible candidates for expansion from preivous abbr_len words
        # ABBR_LIST[abbr_clean].append(expanded)
        outs = outs.replace(' '+abbr, '') # Remove first instance of abbreviation before expanding other instances
        if abbr_clean[-1]=='s':
            outs = outs.replace(abbr_clean[:-1], expanded)
        else:
            outs = outs.replace(abbr_clean, expanded) 
    return outs



def clean_sent(abst):
    abst = remove_stopwords(abst)
    abst = abbr_expander(abst)
    # Make post-period uppercase chars lower
    abst = re.sub(r"(?<=\. )[A-Z]",lambda t:t.group().lower(), abst)
#     abst = filter_pos_tags(abst)
    return abst



def get_sumvec(clean_strr, w2v_model, idf=None):
    tokens = nltk.FreqDist(word_tokenize(clean_strr))
    
    vecs = []
    for t,cnt in tokens.items():
        mul = 1
        if t.startswith('non-'):
            t = t.replace('non-','')
            mul = -1
        try:
            if not idf: vecs.append(w2v_model[t]*mul*cnt)
            else: vecs.append(w2v_model[t]*mul*cnt*idf[t])
            
        except KeyError:
            continue
    return np.array(vecs).sum(0)




def sim(v1, v2):
    return 1-cosine(v1,v2)




def idf(doclist):
    N = len(doclist)
    vocab = list(nltk.FreqDist(word_tokenize(' '.join(dfn_list))).keys())
    idfd={}
    for w in vocab:
        idfd[w] = math.log(N/max(1,sum([1 for d in doclist if w in d])), 10)
    return idfd
        

#Vectorize constructs definition
def gen_rdoc_dict(w2v_model):
    rdoc={}
    rdoc['fear'] = {'dfn':"Activation of the brains defensive motivational system to promote behaviors that protect the organism from perceived danger. Normal fear involves a pattern of adaptive responses to conditioned or unconditioned threat stimuli (exteroceptive or interoceptive). Fear can involve internal representations and cognitive processing and can be modulated by a variety of factors."}
    rdoc['loss'] = {'dfn':"A state of deprivation of a motivationally non-significant con-specific, object, or situation. Loss may be social or non-social and may include permanent or sustained loss of shelter, behavioral control, status, loved ones, or relationships. The response to loss may be episodic (e.g., grief) or sustained."}
    rdoc['sleep_wf'] = {'dfn': "Sleep and wakefulness endogenous, recurring, behavioral states that reflect coordinated changes in the dynamic functional organization of the brain and that optimize physiology, behavior, and health. Homeostatic and circadian processes regulate the propensity for wakefulness and sleep."}
    rdoc['circadian'] = {'dfn': " Circadian rhythms are endogenous self-sustaining oscillations that organize the timing of biological systems to optimize physiology and behavior, and health. They are synchronized by recurring environmental cues; anticipate the external environment; allow effective response to challenges and opportunities in the physical and social environment; modulate homeostasis within the brain and other (central/peripheral) systems, tissues and organs; are evident across levels of organization including molecules, cells, circuits, systems, organisms, and social systems."}
    rdoc['arousal'] = {'dfn':" Arousal is a continuum of sensitivity of the organism to stimuli, both external and internal. Arousal facilitates interaction with the environment in a context-specific manner (e.g., under conditions of threat, some stimuli must be ignored while sensitivity to and responses to others is enhanced, as exemplified in the startle reflex). It can be evoked by either external/environmental stimuli or internal stimuli (e.g., emotions and cognition). I can be modulated by the physical characteristics and motivational significance of stimuli. It varies along a continuum that can be quantified in any behavioral state, including wakefulness and low-arousal states including sleep, anesthesia, and coma. Arousal is distinct from motivation and valence but can covary with intensity of motivation and valence. It may be associated with increased or decreased locomotor activity, and can be regulated by homeostatic drives (e.g., hunger, sleep, thirst, sex)."}
    rdoc['frusnrew'] = {'dfn': "Frustrate Non-reward: Reactions elicited in response to withdrawal/prevention of reward, i.e., by the inability to obtain positive rewards following repeated or sustained efforts."}
    rdoc['anxiety'] = {'dfn': "Activation of a brain system in which harm may potentially occur but is distant, ambiguous, or low/uncertain in probability, characterized by a pattern of responses such as enhanced risk assessment (vigilance). These responses to low imminence threats are qualitatively different than the high imminence threat behaviors that characterize fear."}
    rdoc['susthreat'] = {'dfn':"An aversive emotional state caused by prolonged (i.e., weeks to months) exposure to internal and/or external condition(s), state(s), or stimuli that are adaptive to escape or avoid. The exposure may be actual or anticipated; the changes in affect, cognition, physiology, and behavior caused by sustained threat persist in the absence of the threat and can be differentiated from those changes evoked by acute threat."}
    # get cleaned defn
    for key in rdoc:
        rdoc[key]['clean'] = clean_sent(rdoc[key]['dfn'])
    dfn_list = [rdoc[key]['clean'] for key in rdoc]
    construct_idf = idf(dfn_list)
    # get_sumvec for definitons
    for key in rdoc:
        rdoc[key]['vec'] = get_sumvec(rdoc[key]['clean'], w2v_model, construct_idf)
    return rdoc


#Vectorize test data + get Sim
def compute_sim(w2v_model, out_fname='test_results.csv', test_fname='data/training_data.csv'):
    rdoc = gen_rdoc_dict(w2v_model)
    test_data = pd.read_csv(test_fname)
    test_data['content'] = test_data['title']+test_data['abstract'] 
    test_idf = idf(test_data['content'].values)
    test_data['vec'] = test_data['content'].apply(get_sumvec, args=(w2v_model, test_idf, ))
    for key in rdoc:
        keyvec = rdoc[key]['vec']
        test_data[key+'_sim'] = test_data['vec'].apply(sim, args=(keyvec,))
    test_data.to_csv(out_fname)
    return test_data

# Get avg precision for each construct from results df
def avg_precision(results_df):
    precision = pd.Series(index=results_df.rdoc.unique())
    for rdoc in precision.index:
        df1 = results_df.copy()
        df1['rdoc'] = np.where(df1['rdoc']==rdoc, 1, 0) #tag relevant/irrelevant groudn truth
        precision[rdoc] = average_precision_score(df1['rdoc'], df1[rdoc+'_sim']) # compute based on relevance score
    return precision

def get_model(f):
    return gensim.models.KeyedVectors.load(f)

def get_heatmaps(results_df, rdoc, prec=''):
    df=results_df[results_df.rdoc==rdoc].copy()
    relevance_c = [c for c in df.columns if '_sim' in c]
    scores = MinMaxScaler().fit_transform(df[relevance_c].T).T
    xlabels = [c.replace('_sim','') for c in relevance_c]
    fig, ax = plt.subplots()
    fig.set_size_inches(7, 5)
    plt.title(rdoc+' '+str(prec))
    sns.heatmap(scores, center=0.5, xticklabels=xlabels)
    

def get_ap(adf):
    qap={}
    df = adf.copy()
    for const in df.rdoc.unique():
        df.sort_values(const+'_sim', ascending=False, inplace=True)
        df['docid'] = range(1,len(df)+1)
        df['groundtruth'] = np.where(df.rdoc==const, 1, 0)
        tp = 0
        ap=0
        for _, row in df.iterrows():
            tp+=row['groundtruth']
            ap += tp*row['groundtruth']/row['docid']
        qap[const] = ap/df['groundtruth'].sum()
    return qap
40/6: clean_sent("""Short-term detachment from one's immediate surroundings, during which a person's contact with reality is blurred and partially substituted by a visionary fantasy, especially one of happy, pleasant thoughts, hopes or ambitions, imagined as coming to pass, and experienced while awake.""")
40/7:
def remove_stopwords(s):
    # nltk_stopwords = ['i',  'me',  'my',  'myself',  'we',  'our',  'ours',  'ourselves',  'you',  "you're",  "you've",  "you'll",  "you'd",  'your',  'yours',  'yourself',  'yourselves',  'he',  'him',  'his',  'himself',  'she',  "she's",  'her',  'hers',  'herself',  'it',  "it's",  'its',  'itself',  'they',  'them',  'their',  'theirs',  'themselves',  'what',  'which',  'who',  'whom',  'this',  'that',  "that'll",  'these',  'those',  'am',  'is',  'are',  'was',  'were',  'be',  'been',  'being',  'have',  'has',  'had',  'having',  'do',  'does',  'did',  'doing',  'a',  'an',  'the',  'and',  'but',  'if',  'or',  'because',  'as',  'until',  'while',  'of',  'at',  'by',  'for',  'with',  'about',  'against',  'between',  'into',  'through',  'during',  'before',  'after',  'above',  'below',  'to',  'from',  'up',  'down',  'in',  'out',  'on',  'off',  'over',  'under',  'again',  'further',  'then',  'once',  'here',  'there',  'when',  'where',  'why',  'how',  'all',  'any',  'both',  'each',  'few',  'more',  'most',  'other',  'some',  'such',  'no',  'nor',  'not',  'only',  'own',  'same',  'so',  'than',  'too',  'very',  's',  't',  'can',  'will',  'just',  'don',  "don't",  'should',  "should've",  'now',  'd',  'll',  'm',  'o',  're',  've',  'y',  'ain',  'aren',  "aren't",  'couldn',  "couldn't",  'didn',  "didn't",  'doesn',  "doesn't",  'hadn',  "hadn't",  'hasn',  "hasn't",  'haven',  "haven't",  'isn',  "isn't",  'ma',  'mightn',  "mightn't",  'mustn',  "mustn't",  'needn',  "needn't",  'shan',  "shan't",  'shouldn',  "shouldn't",  'wasn',  "wasn't",  'weren',  "weren't",  'won',  "won't",  'wouldn',  "wouldn't"]
    nltk_stopwords = stopwords.words('english')
    STOPWORDS = nltk_stopwords+["'s", "s"]+[x.capitalize() for x in nltk_stopwords]
    
    s = re.sub(r'([a-zA-Z])[\/]([a-zA-Z])', r'\1 \2', s) # split slashed words
    s = re.sub(r'(?<!non)\-', ' ', s) # remove all hyphens except "non-"
    s = re.sub(r'[^A-Za-z\s\-]+', '', s)
    tokens = word_tokenize(s)
    okstr = ' '.join([x for x in tokens if x in ['(',')'] or x not in STOPWORDS and len(x)>1])
    return okstr


def filter_pos_tags(s):
#     OUT = ['VBD', 'IN', 'VBP', 'RB', 'MD', 'VB', 'VBZ', 'VBG']
    OUT = ['IN', 'RB', 'MD']
    return ' '.join([w for w,t in pos_tag(word_tokenize(s)) if t not in OUT])


def abbr_expander(s):   
    res = re.findall(r'\(\s?[A-Z]\s?[A-Za-z]+\s?\)', s) # Misses acronyms with hyphen
    outs = s
    for abbr in res:
        ix = s.index(abbr)
        abbr_clean = re.sub(r'[^A-Za-z]', '', abbr) # keep only letters, discard all other chars
        abbr_len = len(re.sub(r'[a-z]','',abbr_clean)) # Number of capital letters ~ number of tokens to look back for expansion
        if abbr_clean[-1]=='s': abbr_len-=1 # Singularing a plural acronym
        # When looking for expansion candidates, consider (-) and (/) as word-breaks
        expanded = ' '.join(s[ix-1::-1][::-1].rstrip().split(' ')[-abbr_len:]) # identify possible candidates for expansion from preivous abbr_len words
        # ABBR_LIST[abbr_clean].append(expanded)
        outs = outs.replace(' '+abbr, '') # Remove first instance of abbreviation before expanding other instances
        if abbr_clean[-1]=='s':
            outs = outs.replace(abbr_clean[:-1], expanded)
        else:
            outs = outs.replace(abbr_clean, expanded) 
    return outs



def clean_sent(abst):
    abst = remove_stopwords(abst)
    abst = abbr_expander(abst)
    # Make post-period uppercase chars lower
    abst = re.sub(r"(?<=\. )[A-Z]",lambda t:t.group().lower(), abst)
#     abst = filter_pos_tags(abst)
    return abst



def get_sumvec(clean_strr, w2v_model, idf=None):
    tokens = nltk.FreqDist(word_tokenize(clean_strr))
    
    vecs = []
    for t,cnt in tokens.items():
        mul = 1
        if t.startswith('non-'):
            t = t.replace('non-','')
            mul = -1
        try:
            if not idf: vecs.append(w2v_model[t]*mul*cnt)
            else: vecs.append(w2v_model[t]*mul*cnt*idf[t])
            
        except KeyError:
            continue
    return np.array(vecs).sum(0)




def sim(v1, v2):
    return 1-cosine(v1,v2)




def idf(doclist):
    N = len(doclist)
    vocab = list(nltk.FreqDist(word_tokenize(' '.join(dfn_list))).keys())
    idfd={}
    for w in vocab:
        idfd[w] = math.log(N/max(1,sum([1 for d in doclist if w in d])), 10)
    return idfd
        

#Vectorize constructs definition
def gen_rdoc_dict(w2v_model):
    rdoc={}
    rdoc['fear'] = {'dfn':"Activation of the brains defensive motivational system to promote behaviors that protect the organism from perceived danger. Normal fear involves a pattern of adaptive responses to conditioned or unconditioned threat stimuli (exteroceptive or interoceptive). Fear can involve internal representations and cognitive processing and can be modulated by a variety of factors."}
    rdoc['loss'] = {'dfn':"A state of deprivation of a motivationally non-significant con-specific, object, or situation. Loss may be social or non-social and may include permanent or sustained loss of shelter, behavioral control, status, loved ones, or relationships. The response to loss may be episodic (e.g., grief) or sustained."}
    rdoc['sleep_wf'] = {'dfn': "Sleep and wakefulness endogenous, recurring, behavioral states that reflect coordinated changes in the dynamic functional organization of the brain and that optimize physiology, behavior, and health. Homeostatic and circadian processes regulate the propensity for wakefulness and sleep."}
    rdoc['circadian'] = {'dfn': " Circadian rhythms are endogenous self-sustaining oscillations that organize the timing of biological systems to optimize physiology and behavior, and health. They are synchronized by recurring environmental cues; anticipate the external environment; allow effective response to challenges and opportunities in the physical and social environment; modulate homeostasis within the brain and other (central/peripheral) systems, tissues and organs; are evident across levels of organization including molecules, cells, circuits, systems, organisms, and social systems."}
    rdoc['arousal'] = {'dfn':" Arousal is a continuum of sensitivity of the organism to stimuli, both external and internal. Arousal facilitates interaction with the environment in a context-specific manner (e.g., under conditions of threat, some stimuli must be ignored while sensitivity to and responses to others is enhanced, as exemplified in the startle reflex). It can be evoked by either external/environmental stimuli or internal stimuli (e.g., emotions and cognition). I can be modulated by the physical characteristics and motivational significance of stimuli. It varies along a continuum that can be quantified in any behavioral state, including wakefulness and low-arousal states including sleep, anesthesia, and coma. Arousal is distinct from motivation and valence but can covary with intensity of motivation and valence. It may be associated with increased or decreased locomotor activity, and can be regulated by homeostatic drives (e.g., hunger, sleep, thirst, sex)."}
    rdoc['frusnrew'] = {'dfn': "Frustrate Non-reward: Reactions elicited in response to withdrawal/prevention of reward, i.e., by the inability to obtain positive rewards following repeated or sustained efforts."}
    rdoc['anxiety'] = {'dfn': "Activation of a brain system in which harm may potentially occur but is distant, ambiguous, or low/uncertain in probability, characterized by a pattern of responses such as enhanced risk assessment (vigilance). These responses to low imminence threats are qualitatively different than the high imminence threat behaviors that characterize fear."}
    rdoc['susthreat'] = {'dfn':"An aversive emotional state caused by prolonged (i.e., weeks to months) exposure to internal and/or external condition(s), state(s), or stimuli that are adaptive to escape or avoid. The exposure may be actual or anticipated; the changes in affect, cognition, physiology, and behavior caused by sustained threat persist in the absence of the threat and can be differentiated from those changes evoked by acute threat."}
    # get cleaned defn
    for key in rdoc:
        rdoc[key]['clean'] = clean_sent(rdoc[key]['dfn'])
    dfn_list = [rdoc[key]['clean'] for key in rdoc]
    construct_idf = idf(dfn_list)
    # get_sumvec for definitons
    for key in rdoc:
        rdoc[key]['vec'] = get_sumvec(rdoc[key]['clean'], w2v_model, construct_idf)
    return rdoc


#Vectorize test data + get Sim
def compute_sim(w2v_model, out_fname='test_results.csv', test_fname='data/training_data.csv'):
    rdoc = gen_rdoc_dict(w2v_model)
    test_data = pd.read_csv(test_fname)
    test_data['content'] = test_data['title']+test_data['abstract'] 
    test_idf = idf(test_data['content'].values)
    test_data['vec'] = test_data['content'].apply(get_sumvec, args=(w2v_model, test_idf, ))
    for key in rdoc:
        keyvec = rdoc[key]['vec']
        test_data[key+'_sim'] = test_data['vec'].apply(sim, args=(keyvec,))
    test_data.to_csv(out_fname)
    return test_data

# Get avg precision for each construct from results df
def avg_precision(results_df):
    precision = pd.Series(index=results_df.rdoc.unique())
    for rdoc in precision.index:
        df1 = results_df.copy()
        df1['rdoc'] = np.where(df1['rdoc']==rdoc, 1, 0) #tag relevant/irrelevant groudn truth
        precision[rdoc] = average_precision_score(df1['rdoc'], df1[rdoc+'_sim']) # compute based on relevance score
    return precision

def get_model(f):
    return gensim.models.KeyedVectors.load(f)

def get_heatmaps(results_df, rdoc, prec=''):
    df=results_df[results_df.rdoc==rdoc].copy()
    relevance_c = [c for c in df.columns if '_sim' in c]
    scores = MinMaxScaler().fit_transform(df[relevance_c].T).T
    xlabels = [c.replace('_sim','') for c in relevance_c]
    fig, ax = plt.subplots()
    fig.set_size_inches(7, 5)
    plt.title(rdoc+' '+str(prec))
    sns.heatmap(scores, center=0.5, xticklabels=xlabels)
    

def get_ap(adf):
    qap={}
    df = adf.copy()
    for const in df.rdoc.unique():
        df.sort_values(const+'_sim', ascending=False, inplace=True)
        df['docid'] = range(1,len(df)+1)
        df['groundtruth'] = np.where(df.rdoc==const, 1, 0)
        tp = 0
        ap=0
        for _, row in df.iterrows():
            tp+=row['groundtruth']
            ap += tp*row['groundtruth']/row['docid']
        qap[const] = ap/df['groundtruth'].sum()
    return qap
40/8: clean_sent("""Short-term detachment from one's immediate surroundings, during which a person's contact with reality is blurred and partially substituted by a visionary fantasy, especially one of happy, pleasant thoughts, hopes or ambitions, imagined as coming to pass, and experienced while awake.""")
40/9: clean_sent("""Short-term detachment from one's immediate surroundings, during which a person's contact with reality is blurred and partially substituted by a visionary fantasy, especially one of happy, pleasant thoughts, hopes or ambitions, imagined as coming to pass, and experienced while awake.""")
41/1: import owlready2
41/2: import owlready
42/1: import owlready
42/2: import owlready2
42/3: onto = owlready2.get_ontology('ontologies\MFOMD.owl').load()
42/4:
onto = owlready2.get_ontology('ontologies\MFOMD.owl')
onto.load()
42/5: onto.classes
42/6: onto.classes()
42/7:
for c in onto.classes():
    print(c)
42/8: onto.different_individuals()
42/9:
for i in onto.different_individuals():
    print(i)
42/10:
for i in onto.different_individuals():
    print(i)
42/11:
for i in onto:
    print(i)
42/12:
for i in onto.get_triples()
    print(i)
42/13:
for i in onto.get_triples():
    print(i)
42/14:
onto = owlready2.get_ontology('ontologies/apa-31-10-2014_13-30.owl')
onto.load()
42/15:
onto = owlready2.get_ontology('ontologies/apa-31-10-2014_13-30.owl')
onto.load()
43/1:
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
43/2:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price1'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price2'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price2'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
df['activity1'] = df['qty1']
df['qty1'] = df['activity1']+10
df['activity2'] = df['qty2']
df['qty2'] = df['activity2']+df['qty1']
df['qty0'] = 10
43/3:
def get_bhvr(exp):
    df1=df[['experiment', 'qty0', 'activity1', 'qty1', 'price1', 'activity2', 'qty2', 'price2']]
    rcp = df1[df1.experiment==exp]
    
    rcp['inv_bhvr1']=None
    rcp['inv_bhvr1'].loc[rcp.activity1==0] = 'inactive'
    rcp['inv_bhvr1'].loc[rcp.activity1<0] = 'decrease'
    rcp['inv_bhvr1'].loc[rcp.activity1>0] = 'increase'
    
    rcp['inv_bhvr2']=None
    rcp['inv_bhvr2'].loc[rcp.activity2==0] = 'inactive'
    rcp['inv_bhvr2'].loc[rcp.activity2<0] = 'decrease'
    rcp['inv_bhvr2'].loc[rcp.activity2>0] = 'increase'
    rcp['net_bhvr'] = rcp['inv_bhvr1']+rcp['inv_bhvr2']
    return rcp

def plot_qty_pca(rcp, mdl=None):
    dftmp = rcp[['qty0','qty1','qty2']]
    from sklearn.decomposition import PCA
    pca2 = PCA(2).fit(dftmp)
    if mdl:
        pca2=mdl    
    tmp = pca2.transform(dftmp)
    rcp['pca0']=tmp[:,0]
    rcp['pca1']=tmp[:,1]
    fig, ax =plt.subplots(figsize=(15,10))
    sns.scatterplot(x=tmp[:,0], y=tmp[:,1], hue=rcp.net_bhvr,ax=ax)
    return rcp[['activity1','activity2','net_bhvr', 'pca0','pca1']], pca2
43/4: _,_= plot_qty_pca(rcp, wcp_mdl)
43/5: rcpca[rcpca.pca0.between(-20,-5)]
43/6:
rcl = get_bhvr("'rc_loss'")
rcpca, rcl_mdl = plot_qty_pca(rcl)
43/7: rcpca[rcpca.pca0.between(-20,-5)]
43/8: rcpca
43/9: rcpca.sort_values(['pca0, pca1'])
43/10: rcpca.sort_values(['pca0', 'pca1'])
43/11:
def get_bhvr():
    rcp = df.copy()
    
    rcp['inv_bhvr1']=None
    rcp['inv_bhvr1'].loc[rcp.activity1==0] = 'inactive'
    rcp['inv_bhvr1'].loc[rcp.activity1<0] = 'decrease'
    rcp['inv_bhvr1'].loc[rcp.activity1>0] = 'increase'
    
    rcp['inv_bhvr2']=None
    rcp['inv_bhvr2'].loc[rcp.activity2==0] = 'inactive'
    rcp['inv_bhvr2'].loc[rcp.activity2<0] = 'decrease'
    rcp['inv_bhvr2'].loc[rcp.activity2>0] = 'increase'
    rcp['net_bhvr'] = rcp['inv_bhvr1']+rcp['inv_bhvr2']
    return rcp

def plot_qty_pca(rcp, mdl=None):
    dftmp = rcp[['qty0','qty1','qty2']]
    from sklearn.decomposition import PCA
    pca2 = PCA(2).fit(dftmp)
    if mdl:
        pca2=mdl    
    tmp = pca2.transform(dftmp)
    rcp['pca0']=tmp[:,0]
    rcp['pca1']=tmp[:,1]
    fig, ax =plt.subplots(figsize=(15,10))
    sns.scatterplot(x=tmp[:,0], y=tmp[:,1], hue=rcp.experiment,ax=ax)
    return rcp[['activity1','activity2','net_bhvr', 'pca0','pca1']], pca2
43/12:
bhvrs = get_bhvr()
bhvr_pca, bhvr_mdl = plot_qty_pca(bhvrs)
43/13: bhvr_pca.groupby(['pca0','pca1']).transform.count()
43/14: bhvr_pca.groupby(['pca0','pca1']).transform('count')
43/15: bhvr_pca.groupby(['pca0','pca1']).pca0.transform('count')
43/16:
def get_bhvr():
    rcp = df.copy()
    
    rcp['inv_bhvr1']=None
    rcp['inv_bhvr1'].loc[rcp.activity1==0] = 'inactive'
    rcp['inv_bhvr1'].loc[rcp.activity1<0] = 'decrease'
    rcp['inv_bhvr1'].loc[rcp.activity1>0] = 'increase'
    
    rcp['inv_bhvr2']=None
    rcp['inv_bhvr2'].loc[rcp.activity2==0] = 'inactive'
    rcp['inv_bhvr2'].loc[rcp.activity2<0] = 'decrease'
    rcp['inv_bhvr2'].loc[rcp.activity2>0] = 'increase'
    rcp['net_bhvr'] = rcp['inv_bhvr1']+rcp['inv_bhvr2']
    return rcp

def plot_qty_pca(rcp, mdl=None):
    dftmp = rcp[['qty0','qty1','qty2']]
    from sklearn.decomposition import PCA
    pca2 = PCA(2).fit(dftmp)
    if mdl:
        pca2=mdl    
    tmp = pca2.transform(dftmp)
    rcp['pca0']=tmp[:,0]
    rcp['pca1']=tmp[:,1]
    rcp['counts'] = rcp.groupby(['experiment', 'pca0', 'pca1']).pca0.transform('count')
    fig, ax =plt.subplots(figsize=(15,10))
    sns.scatterplot(x=tmp[:,0], y=tmp[:,1], hue=rcp.experiment,ax=ax, size=rcp.counts)
    return rcp[['activity1','activity2','net_bhvr', 'pca0','pca1']], pca2
43/17:
bhvrs = get_bhvr()
bhvr_pca, bhvr_mdl = plot_qty_pca(bhvrs)
43/18: bhvr_pca[bhvr_pca.pca0.between(0,2)]
43/19:
def get_bhvr():
    rcp = df.copy()
    
    rcp['inv_bhvr1']=None
    rcp['inv_bhvr1'].loc[rcp.activity1==0] = 'inactive'
    rcp['inv_bhvr1'].loc[rcp.activity1<0] = 'decrease'
    rcp['inv_bhvr1'].loc[rcp.activity1>0] = 'increase'
    
    rcp['inv_bhvr2']=None
    rcp['inv_bhvr2'].loc[rcp.activity2==0] = 'inactive'
    rcp['inv_bhvr2'].loc[rcp.activity2<0] = 'decrease'
    rcp['inv_bhvr2'].loc[rcp.activity2>0] = 'increase'
    rcp['net_bhvr'] = rcp['inv_bhvr1']+rcp['inv_bhvr2']
    return rcp

def plot_qty_pca(rcp, mdl=None):
    dftmp = rcp[['qty0','qty1','qty2']]
    from sklearn.decomposition import PCA
    pca2 = PCA(2).fit(dftmp)
    if mdl:
        pca2=mdl    
    tmp = pca2.transform(dftmp)
    rcp['pca0']=tmp[:,0]
    rcp['pca1']=tmp[:,1]
    rcp['counts'] = rcp.groupby(['experiment', 'pca0', 'pca1']).pca0.transform('count')
    fig, ax =plt.subplots(figsize=(15,10))
    sns.scatterplot(x=tmp[:,0], y=tmp[:,1], hue=rcp.experiment,ax=ax, size=rcp.counts)
    return rcp[['experiment', 'activity1','activity2','net_bhvr', 'pca0','pca1']], pca2
43/20:
bhvrs = get_bhvr()
bhvr_pca, bhvr_mdl = plot_qty_pca(bhvrs)
43/21: bhvr_pca[bhvr_pca.pca0.between(0,2)]
43/22:
def get_bhvr():
    rcp = df.copy()
    
    rcp['inv_bhvr1']=None
    rcp['inv_bhvr1'].loc[rcp.activity1==0] = 'inactive'
    rcp['inv_bhvr1'].loc[rcp.activity1<0] = 'decrease'
    rcp['inv_bhvr1'].loc[rcp.activity1>0] = 'increase'
    
    rcp['inv_bhvr2']=None
    rcp['inv_bhvr2'].loc[rcp.activity2==0] = 'inactive'
    rcp['inv_bhvr2'].loc[rcp.activity2<0] = 'decrease'
    rcp['inv_bhvr2'].loc[rcp.activity2>0] = 'increase'
    rcp['net_bhvr'] = rcp['inv_bhvr1']+rcp['inv_bhvr2']
    return rcp

def plot_qty_pca(rcp, mdl=None):
    dftmp = rcp[['qty0','qty1','qty2']]
    from sklearn.decomposition import PCA
    pca2 = PCA(2).fit(dftmp)
    if mdl:
        pca2=mdl    
    tmp = pca2.transform(dftmp)
    rcp['pca0']=tmp[:,0]
    rcp['pca1']=tmp[:,1]
    rcp['counts'] = rcp.groupby(['experiment', 'pca0', 'pca1']).pca0.transform('count')
    fig, ax =plt.subplots(figsize=(15,10))
    sns.scatterplot(x=tmp[:,0], y=tmp[:,1], style=rcp.experiment,ax=ax, size=rcp.counts)
    return rcp[['experiment', 'activity1','activity2','net_bhvr', 'pca0','pca1']], pca2
43/23:
bhvrs = get_bhvr()
bhvr_pca, bhvr_mdl = plot_qty_pca(bhvrs)
43/24:
def get_bhvr():
    rcp = df.copy()
    
    rcp['inv_bhvr1']=None
    rcp['inv_bhvr1'].loc[rcp.activity1==0] = 'inactive'
    rcp['inv_bhvr1'].loc[rcp.activity1<0] = 'decrease'
    rcp['inv_bhvr1'].loc[rcp.activity1>0] = 'increase'
    
    rcp['inv_bhvr2']=None
    rcp['inv_bhvr2'].loc[rcp.activity2==0] = 'inactive'
    rcp['inv_bhvr2'].loc[rcp.activity2<0] = 'decrease'
    rcp['inv_bhvr2'].loc[rcp.activity2>0] = 'increase'
    rcp['net_bhvr'] = rcp['inv_bhvr1']+rcp['inv_bhvr2']
    return rcp

def plot_qty_pca(rcp, mdl=None):
    dftmp = rcp[['qty0','qty1','qty2']]
    from sklearn.decomposition import PCA
    pca2 = PCA(2).fit(dftmp)
    if mdl:
        pca2=mdl    
    tmp = pca2.transform(dftmp)
    rcp['pca0']=tmp[:,0]
    rcp['pca1']=tmp[:,1]
    rcp['counts'] = rcp.groupby(['experiment', 'pca0', 'pca1']).pca0.transform('count')
    fig, ax =plt.subplots(figsize=(15,10))
    sns.scatterplot(x=tmp[:,0], y=tmp[:,1], hue=rcp.experiment, style=rcp.experiment,ax=ax, size=rcp.counts)
    return rcp[['experiment', 'activity1','activity2','net_bhvr', 'pca0','pca1']], pca2
43/25:
bhvrs = get_bhvr()
bhvr_pca, bhvr_mdl = plot_qty_pca(bhvrs)
43/26:
def get_bhvr():
    rcp = df.copy()
    
    rcp['inv_bhvr1']=None
    rcp['inv_bhvr1'].loc[rcp.activity1==0] = 'inactive'
    rcp['inv_bhvr1'].loc[rcp.activity1<0] = 'decrease'
    rcp['inv_bhvr1'].loc[rcp.activity1>0] = 'increase'
    
    rcp['inv_bhvr2']=None
    rcp['inv_bhvr2'].loc[rcp.activity2==0] = 'inactive'
    rcp['inv_bhvr2'].loc[rcp.activity2<0] = 'decrease'
    rcp['inv_bhvr2'].loc[rcp.activity2>0] = 'increase'
    rcp['net_bhvr'] = rcp['inv_bhvr1']+rcp['inv_bhvr2']
    return rcp

def plot_qty_pca(rcp, mdl=None):
    dftmp = rcp[['qty0','qty1','qty2']]
    from sklearn.decomposition import PCA
    pca2 = PCA(2).fit(dftmp)
    if mdl:
        pca2=mdl    
    tmp = pca2.transform(dftmp)
    rcp['pca0']=tmp[:,0]
    rcp['pca1']=tmp[:,1]
    rcp['counts'] = rcp.groupby(['experiment', 'pca0', 'pca1']).pca0.transform('count')
    fig, ax =plt.subplots(figsize=(15,10))
    sns.scatterplot(x=tmp[:,0], y=tmp[:,1], hue=rcp.experiment, style=rcp.experiment,ax=ax, size=rcp.counts, sizes=(5,10))
    return rcp[['experiment', 'activity1','activity2','net_bhvr', 'pca0','pca1']], pca2
43/27:
bhvrs = get_bhvr()
bhvr_pca, bhvr_mdl = plot_qty_pca(bhvrs)
43/28:
def get_bhvr():
    rcp = df.copy()
    
    rcp['inv_bhvr1']=None
    rcp['inv_bhvr1'].loc[rcp.activity1==0] = 'inactive'
    rcp['inv_bhvr1'].loc[rcp.activity1<0] = 'decrease'
    rcp['inv_bhvr1'].loc[rcp.activity1>0] = 'increase'
    
    rcp['inv_bhvr2']=None
    rcp['inv_bhvr2'].loc[rcp.activity2==0] = 'inactive'
    rcp['inv_bhvr2'].loc[rcp.activity2<0] = 'decrease'
    rcp['inv_bhvr2'].loc[rcp.activity2>0] = 'increase'
    rcp['net_bhvr'] = rcp['inv_bhvr1']+rcp['inv_bhvr2']
    return rcp

def plot_qty_pca(rcp, mdl=None):
    dftmp = rcp[['qty0','qty1','qty2']]
    from sklearn.decomposition import PCA
    pca2 = PCA(2).fit(dftmp)
    if mdl:
        pca2=mdl    
    tmp = pca2.transform(dftmp)
    rcp['pca0']=tmp[:,0]
    rcp['pca1']=tmp[:,1]
    rcp['counts'] = rcp.groupby(['experiment', 'pca0', 'pca1']).pca0.transform('count')
    fig, ax =plt.subplots(figsize=(15,10))
    sns.scatterplot(x=tmp[:,0], y=tmp[:,1], hue=rcp.experiment, style=rcp.experiment,ax=ax, size=rcp.counts, sizes=(60,100))
    return rcp[['experiment', 'activity1','activity2','net_bhvr', 'pca0','pca1']], pca2
43/29:
bhvrs = get_bhvr()
bhvr_pca, bhvr_mdl = plot_qty_pca(bhvrs)
43/30:
def get_bhvr():
    rcp = df.copy()
    
    rcp['inv_bhvr1']=None
    rcp['inv_bhvr1'].loc[rcp.activity1==0] = 'inactive'
    rcp['inv_bhvr1'].loc[rcp.activity1<0] = 'decrease'
    rcp['inv_bhvr1'].loc[rcp.activity1>0] = 'increase'
    
    rcp['inv_bhvr2']=None
    rcp['inv_bhvr2'].loc[rcp.activity2==0] = 'inactive'
    rcp['inv_bhvr2'].loc[rcp.activity2<0] = 'decrease'
    rcp['inv_bhvr2'].loc[rcp.activity2>0] = 'increase'
    rcp['net_bhvr'] = rcp['inv_bhvr1']+rcp['inv_bhvr2']
    return rcp

def plot_qty_pca(rcp, mdl=None):
    dftmp = rcp[['qty0','qty1','qty2']]
    from sklearn.decomposition import PCA
    pca2 = PCA(2).fit(dftmp)
    if mdl:
        pca2=mdl    
    tmp = pca2.transform(dftmp)
    rcp['pca0']=tmp[:,0]
    rcp['pca1']=tmp[:,1]
    rcp['counts'] = rcp.groupby(['experiment', 'pca0', 'pca1']).pca0.transform('count')
    fig, ax =plt.subplots(figsize=(15,10))
    sns.scatterplot(x=tmp[:,0], y=tmp[:,1], hue=rcp.experiment, style=rcp.experiment,ax=ax, size=rcp.counts, sizes=(60,200))
    return rcp[['experiment', 'activity1','activity2','net_bhvr', 'pca0','pca1']], pca2
43/31:
bhvrs = get_bhvr()
bhvr_pca, bhvr_mdl = plot_qty_pca(bhvrs)
43/32: bhvr_pca[bhvr_pca.pca0.between(8,10)]
43/33: bhvr_pca[bhvr_pca.pca0.between(8,10)].sort_values('pca1')
43/34: bhvr_pca[bhvr_pca.pca0.between(10,12)].sort_values('pca1')
43/35:
from sklearn.linear_model import LogisticRegression

X = bhvr_pca[['pca0','pca1']]
Y1 = np.where(bhvr_pca.experiment.str.contains('rc'),0,1)
Y2 = np.where(bhvr_pca.experiment.str.contains('profit'),0,1)

lr1 = LogisticRegression().fit(X,Y1)
lr2 = LogisticRegression().fit(X,Y2)
43/36: lr1.coef_
43/37:
from sklearn.linear_model import LogisticRegression

X = bhvr_pca[['qty0','qty1', 'qty2']]
Y1 = np.where(bhvr_pca.experiment.str.contains('rc'),0,1)
Y2 = np.where(bhvr_pca.experiment.str.contains('profit'),0,1)

lr1 = LogisticRegression().fit(X,Y1)
lr2 = LogisticRegression().fit(X,Y2)
43/38:
from sklearn.linear_model import LogisticRegression

X = bhvrs[['qty0','qty1', 'qty2']]
Y1 = np.where(bhvrs.experiment.str.contains('rc'),0,1)
Y2 = np.where(bhvrs.experiment.str.contains('profit'),0,1)

lr1 = LogisticRegression().fit(X,Y1)
lr2 = LogisticRegression().fit(X,Y2)
43/39: lr1.coef_
43/40: bhvrs.qty0.value_counts()
43/41:
from sklearn.linear_model import LogisticRegression

X = bhvrs[['qty1', 'qty2']]
Y1 = np.where(bhvrs.experiment.str.contains('rc'),0,1)
Y2 = np.where(bhvrs.experiment.str.contains('profit'),0,1)

lr1 = LogisticRegression().fit(X,Y1)
lr2 = LogisticRegression().fit(X,Y2)
43/42: lr1.coef_
43/43: lr2.coef_
43/44: lr1.coef_
43/45: lr2.coef_
43/46:
from sklearn.linear_model import LogisticRegression

X = bhvrs[['qty1', 'qty2']]
X['t2_profit'] = np.where(bhvrs.experiment.str.contains('profit'),1,0)

Y1 = np.where(bhvrs.experiment.str.contains('rc'),1,0)


lr1 = LogisticRegression().fit(X,Y1)
43/47: lr1.coef_
43/48:
def get_bhvr():
    rcp = df.copy()
    
    rcp['inv_bhvr1']=None
    rcp['inv_bhvr1'].loc[rcp.activity1==0] = 'inactive'
    rcp['inv_bhvr1'].loc[rcp.activity1<0] = 'decrease'
    rcp['inv_bhvr1'].loc[rcp.activity1>0] = 'increase'
    
    rcp['inv_bhvr2']=None
    rcp['inv_bhvr2'].loc[rcp.activity2==0] = 'inactive'
    rcp['inv_bhvr2'].loc[rcp.activity2<0] = 'decrease'
    rcp['inv_bhvr2'].loc[rcp.activity2>0] = 'increase'
    rcp['net_bhvr'] = rcp['inv_bhvr1']+rcp['inv_bhvr2']
    
    bhvrs['t2_profit'] = np.where(bhvrs.experiment.str.contains('profit'),1,0)
    
    return rcp

def plot_qty_pca(rcp, mdl=None):
    dftmp = rcp[['qty0','qty1','qty2']]
    from sklearn.decomposition import PCA
    pca2 = PCA(2).fit(dftmp)
    if mdl:
        pca2=mdl    
    tmp = pca2.transform(dftmp)
    rcp['pca0']=tmp[:,0]
    rcp['pca1']=tmp[:,1]
    rcp['counts'] = rcp.groupby(['experiment', 'pca0', 'pca1']).pca0.transform('count')
    fig, ax =plt.subplots(figsize=(15,10))
    sns.scatterplot(x=tmp[:,0], y=tmp[:,1], hue=rcp.experiment, style=rcp.experiment,ax=ax, size=rcp.counts, sizes=(60,200))
    return rcp[['experiment', 'activity1','activity2','net_bhvr', 'pca0','pca1']], pca2
43/49:
bhvrs = get_bhvr()
bhvr_pca, bhvr_mdl = plot_qty_pca(bhvrs)
43/50:
def get_bhvr():
    rcp = df.copy()
    
    rcp['inv_bhvr1']=None
    rcp['inv_bhvr1'].loc[rcp.activity1==0] = 'inactive'
    rcp['inv_bhvr1'].loc[rcp.activity1<0] = 'decrease'
    rcp['inv_bhvr1'].loc[rcp.activity1>0] = 'increase'
    
    rcp['inv_bhvr2']=None
    rcp['inv_bhvr2'].loc[rcp.activity2==0] = 'inactive'
    rcp['inv_bhvr2'].loc[rcp.activity2<0] = 'decrease'
    rcp['inv_bhvr2'].loc[rcp.activity2>0] = 'increase'
    rcp['net_bhvr'] = rcp['inv_bhvr1']+rcp['inv_bhvr2']
    
    bhvrs['t2_profit'] = np.where(bhvrs.experiment.str.contains('profit'),1,0)
    bhvrs['live_info'] = np.where(bhvrs.experiment.str.contains('rc'),1,0)
    
    return rcp

def plot_qty_pca(rcp, mdl=None):
    dftmp = rcp[['qty0','qty1','qty2']]
    from sklearn.decomposition import PCA
    pca2 = PCA(2).fit(dftmp)
    if mdl:
        pca2=mdl    
    tmp = pca2.transform(dftmp)
    rcp['pca0']=tmp[:,0]
    rcp['pca1']=tmp[:,1]
    rcp['counts'] = rcp.groupby(['experiment', 'pca0', 'pca1']).pca0.transform('count')
    fig, ax =plt.subplots(figsize=(15,10))
    sns.scatterplot(x=tmp[:,0], y=tmp[:,1], hue=rcp.experiment, style=rcp.experiment,ax=ax, size=rcp.counts, sizes=(60,200))
    return rcp[['experiment', 'activity1','activity2','net_bhvr', 'pca0','pca1']], pca2
43/51:
bhvrs = get_bhvr()
bhvr_pca, bhvr_mdl = plot_qty_pca(bhvrs)
43/52:
from sklearn.linear_model import LogisticRegression

bhvrs
X = bhvrs[['qty1', 'qty2', 't2_profit']]
Y1 = bhvrs['live_info']


#LOOCV
y_true=[]
y_pred=[]
for row in bhvrs:
    xx = X[~row.index]
    yy = Y1[~row.index]
    lr1 = LogisticRegression().fit(xx,yy)
    y_true.append(row['live_info'])
    y_pred.append(lr1.predict(row[['qty1', 'qty2', 't2_profit']]))
43/53:
def get_bhvr():
    rcp = df.copy()
    
    rcp['inv_bhvr1']=None
    rcp['inv_bhvr1'].loc[rcp.activity1==0] = 'inactive'
    rcp['inv_bhvr1'].loc[rcp.activity1<0] = 'decrease'
    rcp['inv_bhvr1'].loc[rcp.activity1>0] = 'increase'
    
    rcp['inv_bhvr2']=None
    rcp['inv_bhvr2'].loc[rcp.activity2==0] = 'inactive'
    rcp['inv_bhvr2'].loc[rcp.activity2<0] = 'decrease'
    rcp['inv_bhvr2'].loc[rcp.activity2>0] = 'increase'
    rcp['net_bhvr'] = rcp['inv_bhvr1']+rcp['inv_bhvr2']
    
    bhvrs['t2_profit'] = np.where(bhvrs.experiment.str.contains('profit'),1,0)
    bhvrs['live_info'] = np.where(bhvrs.experiment.str.contains('rc'),1,0)
    
    return rcp

def plot_qty_pca(rcp, mdl=None):
    dftmp = rcp[['qty0','qty1','qty2']]
    from sklearn.decomposition import PCA
    pca2 = PCA(2).fit(dftmp)
    if mdl:
        pca2=mdl    
    tmp = pca2.transform(dftmp)
    rcp['pca0']=tmp[:,0]
    rcp['pca1']=tmp[:,1]
    rcp['counts'] = rcp.groupby(['experiment', 'pca0', 'pca1']).pca0.transform('count')
    fig, ax =plt.subplots(figsize=(15,10))
    sns.scatterplot(x=tmp[:,0], y=tmp[:,1], hue=rcp.experiment, style=rcp.experiment,ax=ax, size=rcp.counts, sizes=(60,200))
    return rcp[['experiment', 'activity1','activity2','net_bhvr', 'pca0','pca1']], pca2
43/54:
bhvrs = get_bhvr()
# bhvr_pca, bhvr_mdl = plot_qty_pca(bhvrs)
43/55:
from sklearn.linear_model import LogisticRegression

bhvrs
X = bhvrs[['qty1', 'qty2', 't2_profit']]
Y1 = bhvrs['live_info']


#LOOCV
y_true=[]
y_pred=[]
for row in bhvrs:
    xx = X[~row.index]
    yy = Y1[~row.index]
    lr1 = LogisticRegression().fit(xx,yy)
    y_true.append(row['live_info'])
    y_pred.append(lr1.predict(row[['qty1', 'qty2', 't2_profit']]))
43/56:
bhvrs = get_bhvr()
# bhvr_pca, bhvr_mdl = plot_qty_pca(bhvrs)
bhvrs
43/57:
def get_bhvr():
    rcp = df.copy()
    
    rcp['inv_bhvr1']=None
    rcp['inv_bhvr1'].loc[rcp.activity1==0] = 'inactive'
    rcp['inv_bhvr1'].loc[rcp.activity1<0] = 'decrease'
    rcp['inv_bhvr1'].loc[rcp.activity1>0] = 'increase'
    
    rcp['inv_bhvr2']=None
    rcp['inv_bhvr2'].loc[rcp.activity2==0] = 'inactive'
    rcp['inv_bhvr2'].loc[rcp.activity2<0] = 'decrease'
    rcp['inv_bhvr2'].loc[rcp.activity2>0] = 'increase'
    rcp['net_bhvr'] = rcp['inv_bhvr1']+rcp['inv_bhvr2']
    
    rcp['t2_profit'] = np.where(rcp.experiment.str.contains('profit'),1,0)
    rcp['live_info'] = np.where(rcp.experiment.str.contains('rc'),1,0)
    
    return rcp

def plot_qty_pca(rcp, mdl=None):
    dftmp = rcp[['qty0','qty1','qty2']]
    from sklearn.decomposition import PCA
    pca2 = PCA(2).fit(dftmp)
    if mdl:
        pca2=mdl    
    tmp = pca2.transform(dftmp)
    rcp['pca0']=tmp[:,0]
    rcp['pca1']=tmp[:,1]
    rcp['counts'] = rcp.groupby(['experiment', 'pca0', 'pca1']).pca0.transform('count')
    fig, ax =plt.subplots(figsize=(15,10))
    sns.scatterplot(x=tmp[:,0], y=tmp[:,1], hue=rcp.experiment, style=rcp.experiment,ax=ax, size=rcp.counts, sizes=(60,200))
    return rcp[['experiment', 'activity1','activity2','net_bhvr', 'pca0','pca1']], pca2
43/58:
bhvrs = get_bhvr()
# bhvr_pca, bhvr_mdl = plot_qty_pca(bhvrs)
bhvrs
43/59:
from sklearn.linear_model import LogisticRegression

bhvrs
X = bhvrs[['qty1', 'qty2', 't2_profit']]
Y1 = bhvrs['live_info']


#LOOCV
y_true=[]
y_pred=[]
for row in bhvrs:
    xx = X[~row.index]
    yy = Y1[~row.index]
    lr1 = LogisticRegression().fit(xx,yy)
    y_true.append(row['live_info'])
    y_pred.append(lr1.predict(row[['qty1', 'qty2', 't2_profit']]))
43/60:
from sklearn.linear_model import LogisticRegression

bhvrs
X = bhvrs[['qty1', 'qty2', 't2_profit']]
Y1 = bhvrs['live_info']


#LOOCV
y_true=[]
y_pred=[]
for row in bhvrs:
    xx = X[X.index!=row.index]
    yy = Y1[Y1.index!=row.index]
    lr1 = LogisticRegression().fit(xx,yy)
    y_true.append(row['live_info'])
    y_pred.append(lr1.predict(row[['qty1', 'qty2', 't2_profit']]))
43/61:
for row in bhvrs:
    print(row['live_info'])
43/62:
for row in bhvrs:
    print(row.live_info)
43/63:
for row in bhvrs:
    print(row)
43/64:
for row in bhvrs.iterrows():
    print(row)
43/65:
for row in bhvrs.iterrows():
    print(row.mturkid)
43/66:
for row in bhvrs.iterrows():
    print(row)
43/67:
for ix, row in bhvrs.iterrows():
    print(row.mturkid)
43/68: bhvrs.shape
43/69:
from sklearn.linear_model import LogisticRegression

bhvrs
X = bhvrs[['qty1', 'qty2', 't2_profit']]
Y1 = bhvrs['live_info']


#LOOCV
y_true=[]
y_pred=[]
for ix, row in bhvrs.iterrows():
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    print(xx.shape)
    lr1 = LogisticRegression().fit(xx,yy)
    y_true.append(row['live_info'])
    y_pred.append(lr1.predict(row[['qty1', 'qty2', 't2_profit']]))
43/70:
from sklearn.linear_model import LogisticRegression

bhvrs
X = bhvrs[['qty1', 'qty2', 't2_profit']]
Y1 = bhvrs['live_info']


#LOOCV
y_true=[]
y_pred=[]
for ix, row in bhvrs.iterrows():
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    print(xx.shape)
    lr1 = LogisticRegression().fit(xx,yy)
    y_true.append(row['live_info'])
    y_pred.append(lr1.predict(row[['qty1', 'qty2', 't2_profit']].reshape(1,-1)))
43/71:
from sklearn.linear_model import LogisticRegression

bhvrs
X = bhvrs[['qty1', 'qty2', 't2_profit']]
Y1 = bhvrs['live_info']


#LOOCV
y_true=[]
y_pred=[]
for ix, row in bhvrs.iterrows():
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    print(xx.shape)
    lr1 = LogisticRegression().fit(xx,yy)
    y_true.append(row['live_info'])
    y_pred.append(lr1.predict(row[['qty1', 'qty2', 't2_profit']].values.reshape(1,-1)))
43/72: y_true
43/73:
from sklearn.metrics import confusion_matrix
confusion_matrix(y_true, y_pred)
43/74: bhvrs.live_info.value_counts()
43/75: bhvrs.experiment.value_counts()
43/76: bhvrs
43/77: bhvrs.mturkid.drop_duplicates().shape
43/78: bhvrs.mturkid.drop_duplicates().shape
43/79: bhvrs.mturkid.value_counts()
43/80: df.shape
43/81:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price1'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price2'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price2'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
df['activity1'] = df['qty1']
df['qty1'] = df['activity1']+10
df['activity2'] = df['qty2']
df['qty2'] = df['activity2']+df['qty1']
df['qty0'] = 10

df=df.sort_values['mturkid', 'created_at']
df = df.drop_duplicates(['mturkid'])
43/82:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price1'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price2'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price2'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
df['activity1'] = df['qty1']
df['qty1'] = df['activity1']+10
df['activity2'] = df['qty2']
df['qty2'] = df['activity2']+df['qty1']
df['qty0'] = 10

df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])
43/83: df.shape
43/84:
bhvrs = get_bhvr()
# bhvr_pca, bhvr_mdl = plot_qty_pca(bhvrs)
bhvrs
43/85: bhvrs.experiment.value_counts()
43/86:
from sklearn.linear_model import LogisticRegression

bhvrs
X = bhvrs[['qty1', 'qty2', 't2_profit']]
Y1 = bhvrs['live_info']


#LOOCV to predict live/watercooler
y_true=[]
y_pred=[]
for ix, row in bhvrs.iterrows():
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LogisticRegression().fit(xx,yy)
    y_true.append(row['live_info'])
    y_pred.append(lr1.predict(row[['qty1', 'qty2', 't2_profit']].values.reshape(1,-1)))
43/87:
from sklearn.metrics import confusion_matrix
confusion_matrix(y_true, y_pred)
43/88: Y1.value_counts()
43/89:
bhvrs.experiment.value_counts()
# Y1.value_counts()
43/90:
from sklearn.linear_model import LogisticRegression

bhvrs
X = bhvrs[['qty1', 'qty2', 't2_profit']].copy()
Y1 = bhvrs['live_info'].copy()


#LOOCV to predict live/watercooler
bhvrs['pred_live_info'] = None
for ix, row in bhvrs.iterrows():
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LogisticRegression().fit(xx,yy)
    bhvrs.loc[ix]['pred_live_info']= lr1.predict(row[['qty1', 'qty2', 't2_profit']].values.reshape(1,-1))
43/91: bhvrs
43/92:
from sklearn.linear_model import LogisticRegression

bhvrs
X = bhvrs[['qty1', 'qty2', 't2_profit']].copy()
Y1 = bhvrs['live_info'].copy()


#LOOCV to predict live/watercooler
bhvrs['pred_live_info'] = None
for ix, row in bhvrs.iterrows():
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LogisticRegression().fit(xx,yy)
    bhvrs.iloc[ix]['pred_live_info']= lr1.predict(row[['qty1', 'qty2', 't2_profit']].values.reshape(1,-1))
43/93:
from sklearn.linear_model import LogisticRegression

bhvrs
X = bhvrs[['qty1', 'qty2', 't2_profit']].copy()
Y1 = bhvrs['live_info'].copy()


#LOOCV to predict live/watercooler
bhvrs['pred_live_info'] = None
for ix in bhvrs.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LogisticRegression().fit(xx,yy)
    bhvrs.loc[ix]['pred_live_info']= lr1.predict(bhvrs.loc[ix][['qty1', 'qty2', 't2_profit']].values.reshape(1,-1))
43/94: bhvrs
43/95:
from sklearn.linear_model import LogisticRegression

bhvrs
X = bhvrs[['qty1', 'qty2', 't2_profit']].copy()
Y1 = bhvrs['live_info'].copy()


#LOOCV to predict live/watercooler
bhvrs['pred_live_info'] = None
for ix in bhvrs.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LogisticRegression().fit(xx,yy)
#     bhvrs.loc[ix]['pred_live_info']= lr1.predict(bhvrs.loc[ix][['qty1', 'qty2', 't2_profit']].values.reshape(1,-1))
    print(lr1.predict(bhvrs.loc[ix][['qty1', 'qty2', 't2_profit']].values.reshape(1,-1)))
43/96:
from sklearn.linear_model import LogisticRegression

bhvrs
X = bhvrs[['qty1', 'qty2', 't2_profit']].copy()
Y1 = bhvrs['live_info'].copy()


#LOOCV to predict live/watercooler
bhvrs['pred_live_info'] = None
for ix in bhvrs.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LogisticRegression().fit(xx,yy)
    bhvrs['pred_live_info'].loc[ix]= lr1.predict(bhvrs.loc[ix][['qty1', 'qty2', 't2_profit']].values.reshape(1,-1))[0]
43/97: bhvrs
43/98:
correct=bhvrs[bhvrs.live_info==bhvrs.pred_live_info]
wrong=bhvrs[bhvrs.live_info!=bhvrs.pred_live_info]
correct.experiment.value_counts()
43/99:
correct=bhvrs[bhvrs.live_info==bhvrs.pred_live_info]
wrong=bhvrs[bhvrs.live_info!=bhvrs.pred_live_info]
wrong.experiment.value_counts()
43/100:
from sklearn.metrics import confusion_matrix
confusion_matrix(bhvrs.live_info, bhvrs.pred_live_info)
43/101: lr1.coef_
43/102:
from sklearn.linear_model import LogisticRegression

bhvrs
X = bhvrs[['qty1', 'qty2']].copy()
Y1 = bhvrs['live_info'].copy()


#LOOCV to predict live/watercooler
bhvrs['pred_live_info'] = None
for ix in bhvrs.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LogisticRegression().fit(xx,yy)
    bhvrs['pred_live_info'].loc[ix]= lr1.predict(bhvrs.loc[ix][['qty1', 'qty2', 't2_profit']].values.reshape(1,-1))[0]
43/103:
from sklearn.linear_model import LogisticRegression

bhvrs
X = bhvrs[['qty1', 'qty2']].copy()
Y1 = bhvrs['live_info'].copy()


#LOOCV to predict live/watercooler
bhvrs['pred_live_info'] = None
for ix in bhvrs.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LogisticRegression().fit(xx,yy)
    bhvrs['pred_live_info'].loc[ix]= lr1.predict(bhvrs.loc[ix][['qty1', 'qty2']].values.reshape(1,-1))[0]
43/104: lr1.coef_
43/105:
from sklearn.metrics import confusion_matrix
confusion_matrix(bhvrs.live_info, bhvrs.pred_live_info)
43/106:
correct=bhvrs[bhvrs.live_info==bhvrs.pred_live_info]
wrong=bhvrs[bhvrs.live_info!=bhvrs.pred_live_info]
wrong.experiment.value_counts()
43/107:
correct=bhvrs[bhvrs.live_info==bhvrs.pred_live_info]
wrong=bhvrs[bhvrs.live_info!=bhvrs.pred_live_info]
correct.experiment.value_counts()
43/108: bhvrs.experiment.value_counts()
43/109: lr1.coef_
43/110: lr1.intercept_
43/111:
from sklearn.linear_model import LogisticRegression

bhvrs
X = bhvrs[['qty1', 'qty2']].copy()
Y1 = bhvrs['live_info'].copy()


#LOOCV to predict live/watercooler
bhvrs['pred_live_info'] = None
coef1 = coef2 = intercept = 0
for ix in bhvrs.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LogisticRegression().fit(xx,yy)
    coef1+=lr1.coef_[0]
    coef2+=lr1.coef_[1]
    intercept+=lr1.intercept_
    bhvrs['pred_live_info'].loc[ix]= lr1.predict(bhvrs.loc[ix][['qty1', 'qty2']].values.reshape(1,-1))[0]

n=len(intercept)
print(sum(intercept)/n, sum(coef1)/n, sum(coef2)/n)
43/112: lr1.coef_
43/113: lr1.intercept_
43/114:
from sklearn.linear_model import LogisticRegression

bhvrs
X = bhvrs[['qty1', 'qty2']].copy()
Y1 = bhvrs['live_info'].copy()


#LOOCV to predict live/watercooler
bhvrs['pred_live_info'] = None
coef1 = coef2 = intercept = 0
for ix in bhvrs.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LogisticRegression().fit(xx,yy)
    coef1+=lr1.coef_[0][0]
    coef2+=lr1.coef_[0][1]
    intercept+=lr1.intercept_[0]
    bhvrs['pred_live_info'].loc[ix]= lr1.predict(bhvrs.loc[ix][['qty1', 'qty2']].values.reshape(1,-1))[0]

n=len(intercept)
print(sum(intercept)/n, sum(coef1)/n, sum(coef2)/n)
43/115:
# from sklearn.linear_model import LogisticRegression

# bhvrs
# X = bhvrs[['qty1', 'qty2']].copy()
# Y1 = bhvrs['live_info'].copy()


# #LOOCV to predict live/watercooler
# bhvrs['pred_live_info'] = None
# coef1 = coef2 = intercept = 0
# for ix in bhvrs.index:
#     xx = X[X.index!=ix]
#     yy = Y1[Y1.index!=ix]
#     lr1 = LogisticRegression().fit(xx,yy)
#     coef1+=lr1.coef_[0][0]
#     coef2+=lr1.coef_[0][1]
#     intercept+=lr1.intercept_[0]
#     bhvrs['pred_live_info'].loc[ix]= lr1.predict(bhvrs.loc[ix][['qty1', 'qty2']].values.reshape(1,-1))[0]

n=len(bhvrs)-1
print(sum(intercept)/n, sum(coef1)/n, sum(coef2)/n)
43/116:
# from sklearn.linear_model import LogisticRegression

# bhvrs
# X = bhvrs[['qty1', 'qty2']].copy()
# Y1 = bhvrs['live_info'].copy()


# #LOOCV to predict live/watercooler
# bhvrs['pred_live_info'] = None
# coef1 = coef2 = intercept = 0
# for ix in bhvrs.index:
#     xx = X[X.index!=ix]
#     yy = Y1[Y1.index!=ix]
#     lr1 = LogisticRegression().fit(xx,yy)
#     coef1+=lr1.coef_[0][0]
#     coef2+=lr1.coef_[0][1]
#     intercept+=lr1.intercept_[0]
#     bhvrs['pred_live_info'].loc[ix]= lr1.predict(bhvrs.loc[ix][['qty1', 'qty2']].values.reshape(1,-1))[0]

n=len(bhvrs)-1
print(intercept/n, coef1/n, coef2/n)
43/117:
from sklearn.linear_model import LogisticRegression

bhvrs = get_bhvr()
X = bhvrs[['qty1', 'qty2']].copy()
Y1 = bhvrs['live_info'].copy()


#LOOCV to predict live/watercooler
bhvrs['pred_live_info'] = None
coef1 = coef2 = intercept = 0
for ix in bhvrs.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LogisticRegression(C=1e9).fit(xx,yy)
    coef1+=lr1.coef_[0][0]
    coef2+=lr1.coef_[0][1]
    intercept+=lr1.intercept_[0]
    bhvrs['pred_live_info'].loc[ix]= lr1.predict(bhvrs.loc[ix][['qty1', 'qty2']].values.reshape(1,-1))[0]

n=len(bhvrs)-1
print(intercept/n, coef1/n, coef2/n)

from sklearn.metrics import confusion_matrix
print(confusion_matrix(bhvrs.live_info, bhvrs.pred_live_info))
43/118:
from sklearn.linear_model import LogisticRegression

bhvrs = get_bhvr()
X = bhvrs[['qty1', 'qty2']].copy()
Y1 = bhvrs['live_info'].copy()


#LOOCV to predict live/watercooler
bhvrs['pred_live_info'] = None
coef1 = coef2 = intercept = 0
for ix in bhvrs.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LogisticRegression(C=1e9, solver='lbfgs').fit(xx,yy)
    coef1+=lr1.coef_[0][0]
    coef2+=lr1.coef_[0][1]
    intercept+=lr1.intercept_[0]
    bhvrs['pred_live_info'].loc[ix]= lr1.predict(bhvrs.loc[ix][['qty1', 'qty2']].values.reshape(1,-1))[0]

n=len(bhvrs)-1
print(intercept/n, coef1/n, coef2/n)

from sklearn.metrics import confusion_matrix
print(confusion_matrix(bhvrs.live_info, bhvrs.pred_live_info))
43/119:
from sklearn.linear_model import LogisticRegression
pd.options.mode.chained_assignment = None

bhvrs = get_bhvr()
X = bhvrs[['qty1', 'qty2']].copy()
Y1 = bhvrs['live_info'].copy()


#LOOCV to predict live/watercooler
bhvrs['pred_live_info'] = None
coef1 = coef2 = intercept = 0
for ix in bhvrs.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LogisticRegression(C=1e9, solver='lbfgs').fit(xx,yy)
    coef1+=lr1.coef_[0][0]
    coef2+=lr1.coef_[0][1]
    intercept+=lr1.intercept_[0]
    bhvrs['pred_live_info'].loc[ix]= lr1.predict(bhvrs.loc[ix][['qty1', 'qty2']].values.reshape(1,-1))[0]

n=len(bhvrs)-1
print(intercept/n, coef1/n, coef2/n)

from sklearn.metrics import confusion_matrix
print(confusion_matrix(bhvrs.live_info, bhvrs.pred_live_info))
43/120:
import math
1+(1/math.exp(-1.1900))
43/121:
import math
1+(1/math.exp(0.0375))
43/122:
import math
1+(1/math.exp(0.0574))
43/123:
from sklearn.linear_model import LogisticRegression
pd.options.mode.chained_assignment = None

bhvrs = get_bhvr()
X = bhvrs[['qty1', 'qty2']].copy()
Y1 = bhvrs['live_info'].copy()


#LOOCV to predict live/watercooler
bhvrs['pred_live_info'] = None
coef1 = coef2 = intercept = 0
for ix in bhvrs.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LogisticRegression(C=150, solver='lbfgs').fit(xx,yy)
    coef1+=lr1.coef_[0][0]
    coef2+=lr1.coef_[0][1]
    intercept+=lr1.intercept_[0]
    bhvrs['pred_live_info'].loc[ix]= lr1.predict(bhvrs.loc[ix][['qty1', 'qty2']].values.reshape(1,-1))[0]

n=len(bhvrs)-1
print(intercept/n, coef1/n, coef2/n)

from sklearn.metrics import confusion_matrix
print(confusion_matrix(bhvrs.live_info, bhvrs.pred_live_info))
43/124:
from sklearn.linear_model import LogisticRegression
pd.options.mode.chained_assignment = None

bhvrs = get_bhvr()
X = bhvrs[['qty1', 'qty2']].copy()
Y1 = bhvrs['live_info'].copy()


#LOOCV to predict live/watercooler
bhvrs['pred_live_info'] = None
coef1 = coef2 = intercept = 0
for ix in bhvrs.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LogisticRegression(C=10, solver='lbfgs').fit(xx,yy)
    coef1+=lr1.coef_[0][0]
    coef2+=lr1.coef_[0][1]
    intercept+=lr1.intercept_[0]
    bhvrs['pred_live_info'].loc[ix]= lr1.predict(bhvrs.loc[ix][['qty1', 'qty2']].values.reshape(1,-1))[0]

n=len(bhvrs)-1
print(intercept/n, coef1/n, coef2/n)

from sklearn.metrics import confusion_matrix
print(confusion_matrix(bhvrs.live_info, bhvrs.pred_live_info))
43/125:
from sklearn.linear_model import LogisticRegression
pd.options.mode.chained_assignment = None

bhvrs = get_bhvr()
X = bhvrs[['qty1', 'qty2']].copy()
Y1 = bhvrs['live_info'].copy()


#LOOCV to predict live/watercooler
bhvrs['pred_live_info'] = None
coef1 = coef2 = intercept = 0
for ix in bhvrs.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LogisticRegression(C=1, solver='lbfgs').fit(xx,yy)
    coef1+=lr1.coef_[0][0]
    coef2+=lr1.coef_[0][1]
    intercept+=lr1.intercept_[0]
    bhvrs['pred_live_info'].loc[ix]= lr1.predict(bhvrs.loc[ix][['qty1', 'qty2']].values.reshape(1,-1))[0]

n=len(bhvrs)-1
print(intercept/n, coef1/n, coef2/n)

from sklearn.metrics import confusion_matrix
print(confusion_matrix(bhvrs.live_info, bhvrs.pred_live_info))
43/126:
import math
1+(1/math.exp(0.0574))
43/127:
from sklearn.linear_model import LogisticRegression
pd.options.mode.chained_assignment = None

bhvrs = get_bhvr()
X = bhvrs[['qty1', 'qty2']].copy()
Y1 = bhvrs['live_info'].copy()


#LOOCV to predict live/watercooler
bhvrs['pred_live_info'] = None
coef1 = coef2 = intercept = 0
for ix in bhvrs.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LogisticRegression(C=1, solver='newton-cg').fit(xx,yy)
    coef1+=lr1.coef_[0][0]
    coef2+=lr1.coef_[0][1]
    intercept+=lr1.intercept_[0]
    bhvrs['pred_live_info'].loc[ix]= lr1.predict(bhvrs.loc[ix][['qty1', 'qty2']].values.reshape(1,-1))[0]

n=len(bhvrs)-1
print(intercept/n, coef1/n, coef2/n)

from sklearn.metrics import confusion_matrix
print(confusion_matrix(bhvrs.live_info, bhvrs.pred_live_info))
43/128:
from sklearn.linear_model import LinearRegression
pd.options.mode.chained_assignment = None

bhvrs = get_bhvr()
X = bhvrs[['qty1', 'live_info']].copy()
Y1 = bhvrs['qty2'].copy()


#LOOCV to predict live/watercooler
bhvrs['pred_qty2'] = None
coef1 = coef2 = intercept = 0
for ix in bhvrs.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LinearRegression().fit(xx,yy)
    coef1+=lr1.coef_[0][0]
    coef2+=lr1.coef_[0][1]
    intercept+=lr1.intercept_[0]
    bhvrs['pred_live_info'].loc[ix]= lr1.predict(bhvrs.loc[ix][['qty1', 'qty2']].values.reshape(1,-1))[0]

n=len(bhvrs)-1
print(intercept/n, coef1/n, coef2/n)

from sklearn.metrics import confusion_matrix
print(confusion_matrix(bhvrs.live_info, bhvrs.pred_live_info))
43/129:
from sklearn.linear_model import LinearRegression
pd.options.mode.chained_assignment = None

bhvrs = get_bhvr()
X = bhvrs[['qty1', 'live_info']].copy()
Y1 = bhvrs['qty2'].copy()


#LOOCV to predict live/watercooler
bhvrs['pred_qty2'] = None
coef1 = coef2 = intercept = 0
for ix in bhvrs.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LinearRegression().fit(xx,yy)
    coef1+=lr1.coef_[0]
    coef2+=lr1.coef_[1]
    intercept+=lr1.intercept_
    bhvrs['pred_live_info'].loc[ix]= lr1.predict(bhvrs.loc[ix][['qty1', 'qty2']].values.reshape(1,-1))[0]

n=len(bhvrs)-1
print(intercept/n, coef1/n, coef2/n)
43/130:
from sklearn.linear_model import LinearRegression
pd.options.mode.chained_assignment = None

bhvrs = get_bhvr()
X = bhvrs[['qty1', 'live_info']].copy()
Y1 = bhvrs['qty2'].copy()


#LOOCV to predict live/watercooler
bhvrs['pred_qty2'] = None
coef1 = coef2 = intercept = 0
for ix in bhvrs.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LinearRegression().fit(xx,yy)
    coef1+=lr1.coef_[0]
    coef2+=lr1.coef_[1]
    intercept+=lr1.intercept_
    bhvrs['pred_qty2'].loc[ix]= lr1.predict(bhvrs.loc[ix][['qty1', 'live_info']].values.reshape(1,-1))[0]

n=len(bhvrs)-1
print(intercept/n, coef1/n, coef2/n)
43/131: bhvrs
43/132: sns.distplot(bhvrs.qty2)
43/133: bhvrs
43/134:
from sklearn.linear_model import LinearRegression
pd.options.mode.chained_assignment = None

bhvrs = get_bhvr()
X = bhvrs[['activity1', 'live_info']].copy()
Y1 = bhvrs['activity2'].copy()


#LOOCV to predict live/watercooler
bhvrs['pred_act2'] = None
coef1 = coef2 = intercept = 0
for ix in bhvrs.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LinearRegression().fit(xx,yy)
    coef1+=lr1.coef_[0]
    coef2+=lr1.coef_[1]
    intercept+=lr1.intercept_
    bhvrs['pred_act2'].loc[ix]= lr1.predict(bhvrs.loc[ix][['activity1', 'live_info']].values.reshape(1,-1))[0]

n=len(bhvrs)-1
print(intercept/n, coef1/n, coef2/n)
43/135:
from sklearn.linear_model import LinearRegression
pd.options.mode.chained_assignment = None

bhvrs = get_bhvr()
bhvrs = bhvrs[bhvrs.experiment.str.contains('profit')]
X = bhvrs[['activity1', 'live_info']].copy()
Y1 = bhvrs['activity2'].copy()


#LOOCV to predict live/watercooler
bhvrs['pred_act2'] = None
coef1 = coef2 = intercept = 0
for ix in bhvrs.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LinearRegression().fit(xx,yy)
    coef1+=lr1.coef_[0]
    coef2+=lr1.coef_[1]
    intercept+=lr1.intercept_
    bhvrs['pred_act2'].loc[ix]= lr1.predict(bhvrs.loc[ix][['activity1', 'live_info']].values.reshape(1,-1))[0]

n=len(bhvrs)-1
print(intercept/n, coef1/n, coef2/n)
43/136:
from sklearn.linear_model import LinearRegression
pd.options.mode.chained_assignment = None

bhvrs = get_bhvr()
X = bhvrs[['activity1', 'live_info']].copy()
Y1 = bhvrs['activity2'].copy()


#LOOCV to predict live/watercooler
bhvrs['pred_act2'] = None
coef1 = coef2 = intercept = 0
for ix in bhvrs.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LinearRegression().fit(xx,yy)
    coef1+=lr1.coef_[0]
    coef2+=lr1.coef_[1]
    intercept+=lr1.intercept_
    bhvrs['pred_act2'].loc[ix]= lr1.predict(bhvrs.loc[ix][['activity1', 'live_info']].values.reshape(1,-1))[0]

n=len(bhvrs)-1
print(intercept/n, coef1/n, coef2/n)

# residual plot
bhvrs['residual'] = bhvrs['activity2'] - bhvrs['pred_act2']
sns.scatterplot(bhvrs.residual)
43/137:
from sklearn.linear_model import LinearRegression
pd.options.mode.chained_assignment = None

bhvrs = get_bhvr()
X = bhvrs[['activity1', 'live_info']].copy()
Y1 = bhvrs['activity2'].copy()


#LOOCV to predict live/watercooler
bhvrs['pred_act2'] = None
coef1 = coef2 = intercept = 0
for ix in bhvrs.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LinearRegression().fit(xx,yy)
    coef1+=lr1.coef_[0]
    coef2+=lr1.coef_[1]
    intercept+=lr1.intercept_
    bhvrs['pred_act2'].loc[ix]= lr1.predict(bhvrs.loc[ix][['activity1', 'live_info']].values.reshape(1,-1))[0]

n=len(bhvrs)-1
print(intercept/n, coef1/n, coef2/n)

# residual plot
bhvrs['residual'] = bhvrs['activity2'] - bhvrs['pred_act2']
sns.scatterplot(x=bhvrs.pred_act2, y=bhvrs.residual)
43/138:
from sklearn.linear_model import LinearRegression
pd.options.mode.chained_assignment = None

bhvrs = get_bhvr()
bhvrs = bhvrs[bhvrs.experiment.str.contains('profit')]
X = bhvrs[['activity1', 'live_info']].copy()
Y1 = bhvrs['activity2'].copy()


#LOOCV to predict live/watercooler
bhvrs['pred_act2'] = None
coef1 = coef2 = intercept = 0
for ix in bhvrs.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LinearRegression().fit(xx,yy)
    coef1+=lr1.coef_[0]
    coef2+=lr1.coef_[1]
    intercept+=lr1.intercept_
    bhvrs['pred_act2'].loc[ix]= lr1.predict(bhvrs.loc[ix][['activity1', 'live_info']].values.reshape(1,-1))[0]

n=len(bhvrs)-1
print(intercept/n, coef1/n, coef2/n)

bhvrs['residual'] = bhvrs['activity2'] - bhvrs['pred_act2']
sns.scatterplot(x=bhvrs.pred_act2, y=bhvrs.residual)
43/139: df.price2.value_counts()
43/140:
maxbuy1 = math.floor(2500/219.65)
maxsell1 = 10
df['invratio_1'] = np.where(df.activity1>0, df.activity1/maxbuy1, df.activity1/maxsell1)

avbl_cash = 2500-df.activity1*219.65
avbl_stock = 10+df.activity1
maxbuy2 = avbl_cash/df.price2
df['invratio_2'] = np.where(df.activity2>0, df.activity2/maxbuy2, df.activity2/avbl_stock)

df
43/141:
# ## INV RATIO: traded_volume/permissible_volume
# maxbuy1 = math.floor(2500/219.65)
# maxsell1 = 10
# df['invratio_1'] = np.where(df.activity1>0, df.activity1/maxbuy1, df.activity1/maxsell1)

# avbl_cash = 2500-df.activity1*219.65
# avbl_stock = 10+df.activity1
# maxbuy2 = avbl_cash/df.price2
# df['invratio_2'] = np.where(df.activity2>0, df.activity2/maxbuy2, df.activity2/avbl_stock)


## DELTA IN INV_RATIO: measures change in investment outlook
df['invratio_delta'] = df['invratio_1']-df['invratio_2']
df
43/142:
# ## INV RATIO: traded_volume/permissible_volume
# maxbuy1 = math.floor(2500/219.65)
# maxsell1 = 10
# df['invratio_1'] = np.where(df.activity1>0, df.activity1/maxbuy1, df.activity1/maxsell1)

# avbl_cash = 2500-df.activity1*219.65
# avbl_stock = 10+df.activity1
# maxbuy2 = avbl_cash/df.price2
# df['invratio_2'] = np.where(df.activity2>0, df.activity2/maxbuy2, df.activity2/avbl_stock)


## DELTA IN INV_RATIO: measures change in investment outlook
df['invratio_delta'] = math.abs(df['invratio_2'])-math.abs(df['invratio_1'])
df
43/143:
# ## INV RATIO: traded_volume/permissible_volume
# maxbuy1 = math.floor(2500/219.65)
# maxsell1 = 10
# df['invratio_1'] = np.where(df.activity1>0, df.activity1/maxbuy1, df.activity1/maxsell1)

# avbl_cash = 2500-df.activity1*219.65
# avbl_stock = 10+df.activity1
# maxbuy2 = avbl_cash/df.price2
# df['invratio_2'] = np.where(df.activity2>0, df.activity2/maxbuy2, df.activity2/avbl_stock)


## DELTA IN INV_RATIO: measures change in investment outlook
df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])
df
43/144: df.columns
43/145: sns.distplot(df.tradingfreq)
43/146: sns.distplot(df.confidence)
43/147: sns.distplot(df.focus)
43/148: sns.distplot(df.stateofmind)
43/149: sns.distplot(df.regret)
43/150: sns.distplot(df.regret, bars=4)
43/151:
# sns.distplot(df.regret)
help(sns.distplot)
43/152:
sns.distplot(df.regret, bins=4)
# help(sns.distplot)
43/153: df.regret.value_counts().sort_index()
43/154: src.head(1)
43/155:
# ## INV RATIO: traded_volume/permissible_volume
# maxbuy1 = math.floor(2500/219.65)
# maxsell1 = 10
# df['invratio_1'] = np.where(df.activity1>0, df.activity1/maxbuy1, df.activity1/maxsell1)

# avbl_cash = 2500-df.activity1*219.65
# avbl_stock = 10+df.activity1
# maxbuy2 = avbl_cash/df.price2
# df['invratio_2'] = np.where(df.activity2>0, df.activity2/maxbuy2, df.activity2/avbl_stock)


## DELTA IN INV_RATIO: measures change in investment behavior
# df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])


## STRENGTH OF DISPOSITION = -(inv value)/unrealizedP&L
df['disposition1'] = -1*df['activity1']*df['price1']/18.9  #18.9 => pnl at T1
t2_pnl = np.where(df.activity1>0, 18.9+df.activity1*(df.price2-df.price1), avbl_stock*(df.price2-217.76))
df['disposition2'] = -1*df['activity2']*df['price2']/t2_pnl


df
43/156:
# ## INV RATIO: traded_volume/permissible_volume
# maxbuy1 = math.floor(2500/219.65)
# maxsell1 = 10
# df['invratio_1'] = np.where(df.activity1>0, df.activity1/maxbuy1, df.activity1/maxsell1)

# avbl_cash = 2500-df.activity1*219.65
# avbl_stock = 10+df.activity1
# maxbuy2 = avbl_cash/df.price2
# df['invratio_2'] = np.where(df.activity2>0, df.activity2/maxbuy2, df.activity2/avbl_stock)


## DELTA IN INV_RATIO: measures change in investment behavior
# df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])


## STRENGTH OF DISPOSITION = -(inv value)/unrealizedP&L
df['disposition1'] = -1*df['activity1']*df['price1']/18.9  #18.9 => pnl at T1
t2_pnl = np.where(df.activity1>0, 18.9+df.activity1*(df.price2-df.price1), avbl_stock*(df.price2-217.76))
df['disposition2'] = -1*df['activity2']*df['price2']/t2_pnl


t2_pnl
43/157:
# ## INV RATIO: traded_volume/permissible_volume
# maxbuy1 = math.floor(2500/219.65)
# maxsell1 = 10
# df['invratio_1'] = np.where(df.activity1>0, df.activity1/maxbuy1, df.activity1/maxsell1)

# avbl_cash = 2500-df.activity1*219.65
# avbl_stock = 10+df.activity1
# maxbuy2 = avbl_cash/df.price2
# df['invratio_2'] = np.where(df.activity2>0, df.activity2/maxbuy2, df.activity2/avbl_stock)


## DELTA IN INV_RATIO: measures change in investment behavior
# df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])


## STRENGTH OF DISPOSITION = -(inv value)/unrealizedP&L
df['disposition1'] = -1*df['activity1']*df['price1']/18.9  #18.9 => pnl at T1
t2_pnl = np.where(df.activity1>0, 18.9+df.activity1*(df.price2-df.price1), avbl_stock*(df.price2-217.76))
df['disposition2'] = -1*df['activity2']*df['price2']/t2_pnl

df
43/158:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price1'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price2'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price2'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
df['activity1'] = df['qty1']
df['qty1'] = df['activity1']+10
df['activity2'] = df['qty2']
df['qty2'] = df['activity2']+df['qty1']
df['qty0'] = 10

df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])
43/159:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price1'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price2'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price2'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
df['activity1'] = df['qty1']
df['qty1'] = df['activity1']+10
df['activity2'] = df['qty2']
df['qty2'] = df['activity2']+df['qty1']
df['qty0'] = 10
df['price0'] = 217.76

df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])
43/160:
# ## INV RATIO: traded_volume/permissible_volume
# maxbuy1 = math.floor(2500/219.65)
# maxsell1 = 10
# df['invratio_1'] = np.where(df.activity1>0, df.activity1/maxbuy1, df.activity1/maxsell1)

# avbl_cash = 2500-df.activity1*219.65
# avbl_stock = 10+df.activity1
# maxbuy2 = avbl_cash/df.price2
# df['invratio_2'] = np.where(df.activity2>0, df.activity2/maxbuy2, df.activity2/avbl_stock)


## DELTA IN INV_RATIO: measures change in investment behavior
# df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])

# Average purchase price
df['APP'] = np.where(df.activity1>0, (df.price1*df.activity1/2 + 2177.6)/avbl_stock, 217.76)
df
43/161:
# ## INV RATIO: traded_volume/permissible_volume
# maxbuy1 = math.floor(2500/219.65)
# maxsell1 = 10
# df['invratio_1'] = np.where(df.activity1>0, df.activity1/maxbuy1, df.activity1/maxsell1)

# avbl_cash = 2500-df.activity1*219.65
# avbl_stock = 10+df.activity1
# maxbuy2 = avbl_cash/df.price2
# df['invratio_2'] = np.where(df.activity2>0, df.activity2/maxbuy2, df.activity2/avbl_stock)


## DELTA IN INV_RATIO: measures change in investment behavior
# df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])

# Average purchase price
df['APP'] = np.where(df.activity1>0, (df.price1*df.activity1 + 2177.6)/avbl_stock, 217.76)
df
43/162:
# ## INV RATIO: traded_volume/permissible_volume
# maxbuy1 = math.floor(2500/219.65)
# maxsell1 = 10
# df['invratio_1'] = np.where(df.activity1>0, df.activity1/maxbuy1, df.activity1/maxsell1)

# avbl_cash = 2500-df.activity1*219.65
# avbl_stock = 10+df.activity1
# maxbuy2 = avbl_cash/df.price2
# df['invratio_2'] = np.where(df.activity2>0, df.activity2/maxbuy2, df.activity2/avbl_stock)


## DELTA IN INV_RATIO: measures change in investment behavior
# df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])

# Average purchase prices
df['APP1'] = np.where(df.activity1>0, (df.price1*df.activity1 + 2177.6)/df.qty1, 217.76)
df['APP2'] = np.where(df.activity2>0, (df.price2*df.activity2 + df.price1*df.activity1 + 2177.6)/df.qty2, df.APP1)
df
43/163:
# ## INV RATIO: traded_volume/permissible_volume
# maxbuy1 = math.floor(2500/219.65)
# maxsell1 = 10
# df['invratio_1'] = np.where(df.activity1>0, df.activity1/maxbuy1, df.activity1/maxsell1)

# avbl_cash = 2500-df.activity1*219.65
# avbl_stock = 10+df.activity1
# maxbuy2 = avbl_cash/df.price2
# df['invratio_2'] = np.where(df.activity2>0, df.activity2/maxbuy2, df.activity2/avbl_stock)


## DELTA IN INV_RATIO: measures change in investment behavior
# df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])

# Average purchase prices
df['APP1'] = np.where(df.activity1>0, (df.price1*df.activity1 + 2177.6)/df.qty1, 217.76)
df['APP2'] = np.where(df.activity2>0, (df.price2*df.activity2 + max(0,df.price1*df.activity1) + 2177.6)/df.qty2, df.APP1)
df
43/164:
# ## INV RATIO: traded_volume/permissible_volume
# maxbuy1 = math.floor(2500/219.65)
# maxsell1 = 10
# df['invratio_1'] = np.where(df.activity1>0, df.activity1/maxbuy1, df.activity1/maxsell1)

# avbl_cash = 2500-df.activity1*219.65
# avbl_stock = 10+df.activity1
# maxbuy2 = avbl_cash/df.price2
# df['invratio_2'] = np.where(df.activity2>0, df.activity2/maxbuy2, df.activity2/avbl_stock)


## DELTA IN INV_RATIO: measures change in investment behavior
# df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])

# Average purchase prices
df['APP1'] = np.where(df.activity1>0, (df.price1*df.activity1 + 2177.6)/df.qty1, 217.76)
df['APP2'] = df.apply(lambda df: (df.price2*df.activity2 + max(0,df.price1*df.activity1) + 2177.6)/df.qty2 if df.activity2>0 else df.APP1)
df
43/165:
# ## INV RATIO: traded_volume/permissible_volume
# maxbuy1 = math.floor(2500/219.65)
# maxsell1 = 10
# df['invratio_1'] = np.where(df.activity1>0, df.activity1/maxbuy1, df.activity1/maxsell1)

# avbl_cash = 2500-df.activity1*219.65
# avbl_stock = 10+df.activity1
# maxbuy2 = avbl_cash/df.price2
# df['invratio_2'] = np.where(df.activity2>0, df.activity2/maxbuy2, df.activity2/avbl_stock)


## DELTA IN INV_RATIO: measures change in investment behavior
# df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])

# Average purchase prices
df['APP1'] = np.where(df.activity1>0, (df.price1*df.activity1 + 2177.6)/df.qty1, 217.76)
df['APP2'] = df.apply(lambda df: (df.price2*df.activity2 + max(0,df.price1*df.activity1) + 2177.6)/df.qty2 if df.activity2>0 else df.APP1, axis=1)
df
43/166:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price1'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price2'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price2'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)

df['stock_t0']=10

df['purchase_t1'] = np.where(df['qty1']>0, df['qty1'], None)
df['sale_t1'] = np.where(df['qty1']<0, df['qty1'], None)
df['stock_t1'] = df['stock_t0']+df['purchase_t1']-df['sale_t1']

df['purchase_t2'] = np.where(df['qty2']>0, df['qty2'], None)
df['sale_t2'] = np.where(df['qty2']<0, df['qty2'], None)
df['stock_t2'] = df['stock_t1']+df['purchase_t2']-df['sale_t2']




df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])


df
43/167:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price1'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price2'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price2'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)

df['stock_t0']=10

df['purchase_t1'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t1'] = np.where(df['qty1']<0, df['qty1'], 0)
df['stock_t1'] = df['stock_t0']+df['purchase_t1']-df['sale_t1']

df['purchase_t2'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t2'] = np.where(df['qty2']<0, df['qty2'], 0)
df['stock_t2'] = df['stock_t1']+df['purchase_t2']-df['sale_t2']




df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])


df.columns
43/168:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price1'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price2'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price2'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)

df['stock_t0']=10

df['purchase_t1'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t1'] = np.where(df['qty1']<0, df['qty1'], 0)
df['stock_t1'] = df['stock_t0']+df['purchase_t1']-df['sale_t1']

df['purchase_t2'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t2'] = np.where(df['qty2']<0, df['qty2'], 0)
df['stock_t2'] = df['stock_t1']+df['purchase_t2']-df['sale_t2']




df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])

df = df[['experiment', 'mturkid', 'winnings',
       'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret',
       'price0', 'stock_t0', 'price1', 'purchase_t1', 'sale_t1',
       'stock_t1', 'price2', 'purchase_t2', 'sale_t2', 'stock_t2']]

df
43/169:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price1'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price2'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price2'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)

df['stock_t0']=10

df['purchase_t1'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t1'] = np.where(df['qty1']<0, df['qty1'], 0)
df['stock_t1'] = df['stock_t0']+df['purchase_t1']-df['sale_t1']

df['purchase_t2'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t2'] = np.where(df['qty2']<0, df['qty2'], 0)
df['stock_t2'] = df['stock_t1']+df['purchase_t2']-df['sale_t2']

df['avg_purch_t2'] = (2177.6 + (df.purchase_t1+df.sale_t1)*df.price1 + (df.purchase_t2+df.sale_t2)*df.price2)/df.stock_t2


df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])

df = df[['experiment', 'mturkid', 'winnings',
       'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret',
       'price0', 'stock_t0', 'price1', 'purchase_t1', 'sale_t1',
       'stock_t1', 'price2', 'purchase_t2', 'sale_t2', 'stock_t2']]

df
43/170:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price1'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price2'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price2'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)

df['stock_t0']=10

df['purchase_t1'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t1'] = np.where(df['qty1']<0, df['qty1'], 0)
df['stock_t1'] = df['stock_t0']+df['purchase_t1']+df['sale_t1']

df['purchase_t2'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t2'] = np.where(df['qty2']<0, df['qty2'], 0)
df['stock_t2'] = df['stock_t1']+df['purchase_t2']+df['sale_t2']

df['avg_purch_t2'] = (2177.6 + (df.purchase_t1+df.sale_t1)*df.price1 + (df.purchase_t2+df.sale_t2)*df.price2)/df.stock_t2


df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])

df = df[['experiment', 'mturkid', 'winnings',
       'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret',
       'price0', 'stock_t0', 'price1', 'purchase_t1', 'sale_t1',
       'stock_t1', 'price2', 'purchase_t2', 'sale_t2', 'stock_t2', 'avg_purch_t2']]

df
43/171:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price1'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price2'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price2'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)

df['stock_t0']=10

df['purchase_t1'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t1'] = np.where(df['qty1']<0, df['qty1'], 0)
df['stock_t1'] = df['stock_t0']+df['purchase_t1']+df['sale_t1']

df['purchase_t2'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t2'] = np.where(df['qty2']<0, df['qty2'], 0)
df['stock_t2'] = df['stock_t1']+df['purchase_t2']+df['sale_t2']

df['avg_purch_t1'] = (2177.6 + (df.purchase_t1)*df.price1)/(10+df.purchase_t1)


df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])

df = df[['experiment', 'mturkid', 'winnings',
       'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret',
       'price0', 'stock_t0', 'price1', 'purchase_t1', 'sale_t1',
       'stock_t1', 'price2', 'purchase_t2', 'sale_t2', 'stock_t2', 'avg_purch_t2']]

df
43/172:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price1'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price2'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price2'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)

df['stock_t0']=10

df['purchase_t1'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t1'] = np.where(df['qty1']<0, df['qty1'], 0)
df['stock_t1'] = df['stock_t0']+df['purchase_t1']+df['sale_t1']

df['purchase_t2'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t2'] = np.where(df['qty2']<0, df['qty2'], 0)
df['stock_t2'] = df['stock_t1']+df['purchase_t2']+df['sale_t2']

df['avg_purch_t1'] = (2177.6 + (df.purchase_t1)*df.price1)/(10+df.purchase_t1)


df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])

df = df[['experiment', 'mturkid', 'winnings',
       'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret',
       'price0', 'stock_t0', 'price1', 'purchase_t1', 'sale_t1',
       'stock_t1', 'price2', 'purchase_t2', 'sale_t2', 'stock_t2', 'avg_purch_t1']]

df
43/173:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price1'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price2'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price2'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)

df['stock_t0']=10

df['purchase_t1'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t1'] = np.where(df['qty1']<0, df['qty1'], 0)
df['stock_t1'] = df['stock_t0']+df['purchase_t1']+df['sale_t1']

df['purchase_t2'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t2'] = np.where(df['qty2']<0, df['qty2'], 0)
df['stock_t2'] = df['stock_t1']+df['purchase_t2']+df['sale_t2']

df['avg_purch_t1'] = (2177.6 + (df.purchase_t1)*df.price1)/(10+df.purchase_t1)
df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)


df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])

df = df[['experiment', 'mturkid', 'winnings',
       'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret',
       'price0', 'stock_t0', 'price1', 'purchase_t1', 'sale_t1',
       'stock_t1', 'price2', 'purchase_t2', 'sale_t2', 'stock_t2', 'avg_purch_t1', 'avg_purch_t2']]

df
43/174:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price1'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price2'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price2'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)

df['stock_t0']=10

df['purchase_t1'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t1'] = np.where(df['qty1']<0, df['qty1'], 0)
df['stock_t1'] = df['stock_t0']+df['purchase_t1']+df['sale_t1']

df['purchase_t2'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t2'] = np.where(df['qty2']<0, df['qty2'], 0)
df['stock_t2'] = df['stock_t1']+df['purchase_t2']+df['sale_t2']

df['avg_purch_t1'] = (2177.6 + (df.purchase_t1)*df.price1)/(10+df.purchase_t1)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)


df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])

df = df[['experiment', 'mturkid', 'winnings',
       'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret',
       'price0', 'stock_t0', 'price1', 'purchase_t1', 'sale_t1',
       'stock_t1', 'price2', 'purchase_t2', 'sale_t2', 'stock_t2', 'avg_purch_t1']]

df
43/175: df.stock_t1.value_counts()
43/176:
df['realized_gainloss'] = -1*df.sale_t2*(df.price2-df.avg_purch_t1)
df
43/177:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price10'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price20'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price20'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)

df['stock_t1']=10

df['purchase_t10'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t10'] = np.where(df['qty1']<0, df['qty1'], 0)
df['stock_t11'] = df['stock_t0']+df['purchase_t1']+df['sale_t1']

df['purchase_t20'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t20'] = np.where(df['qty2']<0, df['qty2'], 0)
df['stock_t21'] = df['stock_t1']+df['purchase_t2']+df['sale_t2']

df['avg_purch_t11'] = (2177.6 + (df.purchase_t1)*df.price1)/(10+df.purchase_t1)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)


df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])

df = df[['experiment', 'mturkid', 'winnings',
       'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret',
       'price0', 'stock_t0', 'price1', 'purchase_t1', 'sale_t1',
       'stock_t1', 'price2', 'purchase_t2', 'sale_t2', 'stock_t2', 'avg_purch_t1']]

df
43/178:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price10'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price20'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price20'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)

df['stock_t1']=10

df['purchase_t10'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t10'] = np.where(df['qty1']<0, df['qty1'], 0)
df['stock_t11'] = df['stock_t0']+df['purchase_t1']+df['sale_t1']

df['purchase_t20'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t20'] = np.where(df['qty2']<0, df['qty2'], 0)
df['stock_t21'] = df['stock_t1']+df['purchase_t2']+df['sale_t2']

df['avg_purch_t11'] = (2177.6 + (df.purchase_t1)*df.price1)/(10+df.purchase_t1)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)


df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])

# df = df[['experiment', 'mturkid', 'winnings',
#        'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret',
#        'price0', 'stock_t0', 'price1', 'purchase_t1', 'sale_t1',
#        'stock_t1', 'price2', 'purchase_t2', 'sale_t2', 'stock_t2', 'avg_purch_t1']]

df.columns
43/179:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price10'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price20'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price20'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)

df['stock_t01']=10

df['purchase_t10'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t10'] = np.where(df['qty1']<0, df['qty1'], 0)
df['stock_t11'] = df['stock_t01']+df['purchase_t10']+df['sale_t10']

df['purchase_t20'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t20'] = np.where(df['qty2']<0, df['qty2'], 0)
df['stock_t21'] = df['stock_t11']+df['purchase_t20']+df['sale_t20']

df['avg_purch_t11'] = (2177.6 + df.purchase_t10*df.price10)/(10+df.purchase_t10)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)


df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])

# df = df[['experiment', 'mturkid', 'winnings',
#        'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret',
#        'price0', 'stock_t0', 'price1', 'purchase_t1', 'sale_t1',
#        'stock_t1', 'price2', 'purchase_t2', 'sale_t2', 'stock_t2', 'avg_purch_t1']]

df.columns
43/180:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price10'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price20']=None
df['price20'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price20'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)

df['stock_t01']=10

df['purchase_t10'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t10'] = np.where(df['qty1']<0, df['qty1'], 0)
df['stock_t11'] = df['stock_t01']+df['purchase_t10']+df['sale_t10']

df['purchase_t20'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t20'] = np.where(df['qty2']<0, df['qty2'], 0)
df['stock_t21'] = df['stock_t11']+df['purchase_t20']+df['sale_t20']

df['avg_purch_t11'] = (2177.6 + df.purchase_t10*df.price10)/(10+df.purchase_t10)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)


df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])

# df = df[['experiment', 'mturkid', 'winnings',
#        'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret',
#        'price0', 'stock_t0', 'price1', 'purchase_t1', 'sale_t1',
#        'stock_t1', 'price2', 'purchase_t2', 'sale_t2', 'stock_t2', 'avg_purch_t1']]

df.columns
43/181:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price10'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price20']=None
df['price20'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price20'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)

df['stock_t01']=10

df['purchase_t10'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t10'] = np.where(df['qty1']<0, df['qty1'], 0)
df['stock_t11'] = df['stock_t01']+df['purchase_t10']+df['sale_t10']

df['purchase_t20'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t20'] = np.where(df['qty2']<0, df['qty2'], 0)
df['stock_t21'] = df['stock_t11']+df['purchase_t20']+df['sale_t20']

df['avg_purch_t11'] = (2177.6 + df.purchase_t10*df.price10)/(10+df.purchase_t10)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)


df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])

df = df[['experiment', 'mturkid', 'winnings',
       'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret', 
       'price0', 'price10', 'price20', 'stock_t01',
       'purchase_t10', 'sale_t10', 'stock_t11', 'purchase_t20', 'sale_t20',
       'stock_t21', 'avg_purch_t11']]

df.columns
43/182:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price10'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price20']=None
df['price20'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price20'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)

df['stock_t01']=10

df['purchase_t10'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t10'] = np.where(df['qty1']<0, df['qty1'], 0)
df['stock_t11'] = df['stock_t01']+df['purchase_t10']+df['sale_t10']

df['purchase_t20'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t20'] = np.where(df['qty2']<0, df['qty2'], 0)
df['stock_t21'] = df['stock_t11']+df['purchase_t20']+df['sale_t20']

df['avg_purch_t11'] = (2177.6 + df.purchase_t10*df.price10)/(10+df.purchase_t10)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)


df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])

df = df[['experiment', 'mturkid', 'winnings',
       'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret', 
       'price0', 'price10', 'price20', 'stock_t01',
       'purchase_t10', 'sale_t10', 'stock_t11', 'purchase_t20', 'sale_t20',
       'stock_t21', 'avg_purch_t11']]

df
43/183:
# people who had stock at t1, and in a profitable position at t20 - how many sold, how many held/purchased?

df[df.experiment.str.contains('profit')][df.stock_t11>0]
43/184:
# people who had stock at t1, and in a profitable position at t20 - how many sold, how many held/purchased?

w=df[df.experiment.str.contains('profit')][df.stock_t11>0]
proportion_profit_sellers = (w.sale_t20!=0).sum()/w
43/185:
# people who had stock at t1, and in a profitable position at t20 - how many sold, how many held/purchased?
w=df[df.experiment.str.contains('profit')][df.stock_t11>0]
proportion_profit_sellers = (w.sale_t20!=0).sum()/w.shape[0]
# people who had stock at t1, and in a losing position at t20 - how many sold, how many held/purchased?
l=df[df.experiment.str.contains('loss')][df.stock_t11>0]
proportion_loss_sellers = (l.sale_t20!=0).sum()/l.shape[0]


proportion_profit_sellers, proportion_loss_sellers
43/186:
# proportion of profit-sellers in wc and rc
wc = w[w.experiment.str.contains('wc')]
rc = w[w.experiment.str.contains('rc')]
(wc.sale_t20!=0).sum()/wc.shape[0], (rc.sale_t20!=0).sum()/rc.shape[0]
43/187:
# proportion of profit-sellers in wc and rc
wc = w[w.experiment.str.contains('wc')]
rc = w[w.experiment.str.contains('rc')]
print(f"WC dispositioners: {(wc.sale_t20!=0).sum()/wc.shape[0]}, RC dispositioners: {(rc.sale_t20!=0).sum()/rc.shape[0]}")
43/188:
# proportion of loss-sellers in wc and rc
wc = l[l.experiment.str.contains('wc')]
rc = l[l.experiment.str.contains('rc')]
print(f"WC dispositioners: {(wc.sale_t20!=0).sum()/wc.shape[0]}, RC dispositioners: {(rc.sale_t20!=0).sum()/rc.shape[0]}")
43/189:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price10'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price20']=None
df['price20'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price20'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)

df['stock_t01']=10

df['purchase_t10'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t10'] = np.where(df['qty1']<0, df['qty1'], 0)
df['stock_t11'] = df['stock_t01']+df['purchase_t10']+df['sale_t10']

df['purchase_t20'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t20'] = np.where(df['qty2']<0, df['qty2'], 0)
df['stock_t21'] = df['stock_t11']+df['purchase_t20']+df['sale_t20']

df['avg_purch_t11'] = (2177.6 + df.purchase_t10*df.price10)/(10+df.purchase_t10)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)

df['pnl_t20'] = np.where(df.stock_t11>10, (df.stock_t11-10)*(df.price20-df.price10)+10*(df.price20-df.price0), df.stock_t11*(df.price20-df.price0))


df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])

df = df[['experiment', 'mturkid', 'winnings',
       'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret', 
       'price0', 'price10', 'price20', 'stock_t01',
       'purchase_t10', 'sale_t10', 'stock_t11', 'purchase_t20', 'sale_t20',
       'stock_t21', 'avg_purch_t11']]

df
43/190:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price10'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price20']=None
df['price20'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price20'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)

df['stock_t01']=10

df['purchase_t10'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t10'] = np.where(df['qty1']<0, df['qty1'], 0)
df['stock_t11'] = df['stock_t01']+df['purchase_t10']+df['sale_t10']

df['purchase_t20'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t20'] = np.where(df['qty2']<0, df['qty2'], 0)
df['stock_t21'] = df['stock_t11']+df['purchase_t20']+df['sale_t20']

df['avg_purch_t11'] = (2177.6 + df.purchase_t10*df.price10)/(10+df.purchase_t10)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)

df['pnl_t20'] = np.where(df.stock_t11>10, (df.stock_t11-10)*(df.price20-df.price10)+10*(df.price20-df.price0), df.stock_t11*(df.price20-df.price0))


df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])

df = df[['experiment', 'mturkid', 'winnings',
       'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret', 
       'price0', 'price10', 'price20', 'stock_t01',
       'purchase_t10', 'sale_t10', 'stock_t11', 'purchase_t20', 'sale_t20',
       'stock_t21', 'pnl_t20']]

df
43/191:
# people who had stock at t10, and in a profitable position at t20 - how many sold, how many held/purchased?
w=df[df.pnl_t20>0][df.stock_t11>0]
proportion_profit_sellers = (w.sale_t20!=0).sum()/w.shape[0]
# people who had stock at t1, and in a losing position at t20 - how many sold, how many held/purchased?
l=df[df.pnl_t20>0][df.stock_t11>0]
proportion_loss_sellers = (l.sale_t20!=0).sum()/l.shape[0]

proportion_profit_sellers, proportion_loss_sellers
43/192:
# people who had stock at t10, and in a profitable position at t20 - how many sold, how many held/purchased?
w=df[df.pnl_t20>0][df.stock_t11>0]
proportion_profit_sellers = (w.sale_t20!=0).sum()/w.shape[0]
# people who had stock at t1, and in a losing position at t20 - how many sold, how many held/purchased?
l=df[df.pnl_t20<0][df.stock_t11>0]
proportion_loss_sellers = (l.sale_t20!=0).sum()/l.shape[0]

proportion_profit_sellers, proportion_loss_sellers
43/193:
df1 = pd.read_csv('datadump.csv')
df1.mturkid.value_counts()
43/194:
df1 = pd.read_csv('datadump.csv')
df1.mturkid.value_counts().head(10)
43/195:
df1 = pd.read_csv('datadump.csv')
l=['A1R5W4RQZTROD8', 'A15BOS6WJAPZX5', 'A16LTPBK99MZHO', 'A1AD7R2ROLDRDW']
df1[df1.mturkid.isin(l)]
43/196:
df1 = pd.read_csv('datadump.csv')
l=['A1R5W4RQZTROD8', 'A15BOS6WJAPZX5', 'A16LTPBK99MZHO', 'A1AD7R2ROLDRDW']
df1[df1.mturkid.isin(l)].sort_values('mturkid')
43/197:
df1 = pd.read_csv('datadump.csv')
l=['A1R5W4RQZTROD8', 'A15BOS6WJAPZX5', 'A16LTPBK99MZHO', 'A1AD7R2ROLDRDW']
l2=['A19A849PZMS1F7']
df1[df1.mturkid.isin(l)].sort_values('mturkid')
43/198:
df1 = pd.read_csv('datadump.csv')
l=['A1R5W4RQZTROD8', 'A15BOS6WJAPZX5', 'A16LTPBK99MZHO', 'A1AD7R2ROLDRDW']
l2=['A19A849PZMS1F7']
df1[df1.mturkid.isin(l2)].sort_values('mturkid')
43/199:
df1 = pd.read_csv('datadump.csv')
l=['A1R5W4RQZTROD8', 'A15BOS6WJAPZX5', 'A16LTPBK99MZHO', 'A1AD7R2ROLDRDW']
l2=['A1YCZENBDZ5GGZ']
df1[df1.mturkid.isin(l2)].sort_values('mturkid')
43/200:
df1 = pd.read_csv('datadump.csv')
l=['A1R5W4RQZTROD8', 'A15BOS6WJAPZX5', 'A16LTPBK99MZHO', 'A1AD7R2ROLDRDW']
l2=['A1YCZENBDZ5GGZ']
# df1[df1.mturkid.isin(l2)].sort_values('mturkid')
df1[df1.rng=='163CC9606ECA']
43/201:
df1 = pd.read_csv('datadump.csv')
l=['A1R5W4RQZTROD8', 'A15BOS6WJAPZX5', 'A16LTPBK99MZHO', 'A1AD7R2ROLDRDW']
l2=['A1YCZENBDZ5GGZ']
# df1[df1.mturkid.isin(l2)].sort_values('mturkid')
df1[df1.rng=='BC7E9FB189FF']
43/202:
df1 = pd.read_csv('datadump.csv')
l=['A1R5W4RQZTROD8', 'A15BOS6WJAPZX5', 'A16LTPBK99MZHO', 'A1AD7R2ROLDRDW']
l2=['A1YCZENBDZ5GGZ']
# df1[df1.mturkid.isin(l2)].sort_values('mturkid')
df1[df1.rng.isin(['E1648FE3C432', '6E78318E8798', '5DD6F692716A', 'FC34D494025D', '2BDDCCE8E50B', '4B96C57D8FC5', 'B7BD04DD11EE', '0E48E621F45A', 'BDC0C8BE0ECA', '4125BEF29148', '0ABB884FB557', 'E4522A3127EC', '67E5E005BE82', '3D16C86BD304', '8AD8E0D06626', '57B40CDFC525', 'DD761BD5B26B', 'AED0E845D0C1', '0D68316C2373', '99231E70978D'])]].shape
43/203:
df1 = pd.read_csv('datadump.csv')
l=['A1R5W4RQZTROD8', 'A15BOS6WJAPZX5', 'A16LTPBK99MZHO', 'A1AD7R2ROLDRDW']
l2=['A1YCZENBDZ5GGZ']
# df1[df1.mturkid.isin(l2)].sort_values('mturkid')
df1[df1.rng.isin(['E1648FE3C432', '6E78318E8798', '5DD6F692716A', 'FC34D494025D', '2BDDCCE8E50B', '4B96C57D8FC5', 'B7BD04DD11EE', '0E48E621F45A', 'BDC0C8BE0ECA', '4125BEF29148', '0ABB884FB557', 'E4522A3127EC', '67E5E005BE82', '3D16C86BD304', '8AD8E0D06626', '57B40CDFC525', 'DD761BD5B26B', 'AED0E845D0C1', '0D68316C2373', '99231E70978D'])].shape
43/204:
## INV RATIO: traded_volume/permissible_volume
maxbuy10 = math.floor(2500/219.65)
maxsell10 = 10
df['invratio_1'] = df.purchase_t10/maxbuy10
df['invratio_1'] = df.sale_t10/maxsell10

df

# avbl_cash = 2500-df.activity1*219.65
# avbl_stock = 10+df.activity1
# maxbuy2 = avbl_cash/df.price2
# df['invratio_2'] = np.where(df.activity2>0, df.activity2/maxbuy2, df.activity2/avbl_stock)


# # DELTA IN INV_RATIO: measures change in investment behavior
# df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])
43/205:
## INV RATIO: traded_volume/permissible_volume
maxbuy10 = math.floor(2500/219.65)
maxsell10 = 10
df['invratio_1'] = df.purchase_t10/maxbuy10
# df['invratio_1'] = df.sale_t10/maxsell10

df

# avbl_cash = 2500-df.activity1*219.65
# avbl_stock = 10+df.activity1
# maxbuy2 = avbl_cash/df.price2
# df['invratio_2'] = np.where(df.activity2>0, df.activity2/maxbuy2, df.activity2/avbl_stock)


# # DELTA IN INV_RATIO: measures change in investment behavior
# df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])
43/206:
## INV RATIO: traded_volume/permissible_volume
maxbuy10 = math.floor(2500/219.65)
maxsell10 = 10
df['invratio_1'] = df.purchase_t10/maxbuy10
df['invratio_1'] = np.where(df.invratio_1==0, df.sale_t10/maxsell10, df.invratio_1)

df

# avbl_cash = 2500-df.activity1*219.65
# avbl_stock = 10+df.activity1
# maxbuy2 = avbl_cash/df.price2
# df['invratio_2'] = np.where(df.activity2>0, df.activity2/maxbuy2, df.activity2/avbl_stock)


# # DELTA IN INV_RATIO: measures change in investment behavior
# df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])
43/207:
## INV RATIO: traded_volume/permissible_volume
maxbuy10 = math.floor(2500/219.65)
maxsell10 = 10
df['invratio_1'] = df.purchase_t10/maxbuy10
df['invratio_1'] = np.where(df.invratio_1==0, df.sale_t10/maxsell10, df.invratio_1)

avbl_cash = 2500 - (df.purchase_t10-df.sale_t10)*219.65
avbl_stock = 10+df.purchase_t10+df.sale_t10
maxbuy2 = avbl_cash/df.price20
df['invratio_2'] = df.purchase_t20/maxbuy2
df['invratio_2'] = np.where(df.invratio_2==0, df.sale_t20/avbl_stock, df.invratio_2)
# df['invratio_2'] = np.where(df.activity2>0, df.activity2/maxbuy2, df.activity2/avbl_stock)
df

# # DELTA IN INV_RATIO: measures change in investment behavior
# df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])
43/208: avbl_cash
43/209: (df.purchase_t10-df.sale_t10)*219.65
43/210: (df.purchase_t10-df.sale_t10)
43/211:
## INV RATIO: traded_volume/permissible_volume
maxbuy10 = math.floor(2500/219.65)
maxsell10 = 10
df['invratio_1'] = df.purchase_t10/maxbuy10
df['invratio_1'] = np.where(df.invratio_1==0, df.sale_t10/maxsell10, df.invratio_1)

avbl_cash = 2500 - (df.purchase_t10+df.sale_t10)*219.65
avbl_stock = 10+df.purchase_t10+df.sale_t10
maxbuy2 = avbl_cash/df.price20
df['invratio_2'] = df.purchase_t20/maxbuy2
df['invratio_2'] = np.where(df.invratio_2==0, df.sale_t20/avbl_stock, df.invratio_2)
# df['invratio_2'] = np.where(df.activity2>0, df.activity2/maxbuy2, df.activity2/avbl_stock)
df

# # DELTA IN INV_RATIO: measures change in investment behavior
# df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])
43/212:
## INV RATIO: traded_volume/permissible_volume
maxbuy10 = math.floor(2500/219.65)
maxsell10 = 10
df['invratio_1'] = df.purchase_t10/maxbuy10
df['invratio_1'] = np.where(df.invratio_1==0, df.sale_t10/maxsell10, df.invratio_1)

avbl_cash = 2500 - (df.purchase_t10+df.sale_t10)*219.65
avbl_stock = 10+df.purchase_t10+df.sale_t10
maxbuy2 = avbl_cash/df.price20
df['invratio_2'] = df.purchase_t20/maxbuy2
df['invratio_2'] = np.where(df.invratio_2==0, df.sale_t20/avbl_stock, df.invratio_2)

# # DELTA IN INV_RATIO: measures change in investment behavior
df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])
43/213: df
43/214: df.columns
43/215: sns.distplot(df.invratio_1)
43/216: df.invratio_1.plot.hist()
43/217: df[df.invratio_1<0]
43/218: df.invratio_1.abs()
43/219: sns.distplot(df.invratio_1.abs())
43/220: sns.distplot(df.invratio_1.abs(bins=10))
43/221: sns.distplot(df.invratio_1.abs(), bins=10)
43/222: sns.distplot(df.invratio_2.abs(), bins=10)
43/223:
# sns.distplot(df.invratio_1.abs(), bins=10)
df.invratio_1.abs().describe()
43/224: sns.distplot(df.invratio_delta, bins=10)
43/225:
# sns.distplot(df.invratio_delta, bins=10)
df.invratio_delta.describe()
43/226:
# sns.distplot(df.invratio_delta, bins=10)
df.invratio_delta.abs().describe()
43/227:
# sns.distplot(df.invratio_delta, bins=10)
df.invratio_delta.describe()
43/228:
# sns.distplot(df.invratio_1.abs(), bins=10)
# df.invratio_1.abs().describe()
sns.scatterplot(df.invratio_2, df.invratio_delta)
43/229:
# sns.distplot(df.invratio_1.abs(), bins=10)
# df.invratio_1.abs().describe()
sns.scatterplot(df.invratio_1, df.invratio_delta)
43/230:
# sns.distplot(df.invratio_1.abs(), bins=10)
# df.invratio_1.abs().describe()
sns.scatterplot(df.invratio_1, df.invratio_2)
43/231:
from sklearn.linear_model import LinearRegression
pd.options.mode.chained_assignment = None

bhvrs = get_bhvr()
X = df[['invratio_1', 'live_info']].copy()
Y1 = df[['invratio_2']].copy()


#LOOCV to predict live/watercooler
Y['pred_ir'] = None
coef1 = coef2 = intercept = 0
for ix in X.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LinearRegression().fit(xx,yy)
    coef1+=lr1.coef_[0]
    coef2+=lr1.coef_[1]
    intercept+=lr1.intercept_
    Y['pred_ir'].loc[ix]= lr1.predict(X.loc[ix][['invratio_1', 'live_info']].values.reshape(1,-1))[0]

n=len(X)-1
print(intercept/n, coef1/n, coef2/n)
43/232:
from sklearn.linear_model import LinearRegression
pd.options.mode.chained_assignment = None

# bhvrs = get_bhvr()
X = df[['invratio_1', 'live_info']].copy()
Y1 = df[['invratio_2']].copy()


#LOOCV to predict live/watercooler
Y['pred_ir'] = None
coef1 = coef2 = intercept = 0
for ix in X.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LinearRegression().fit(xx,yy)
    coef1+=lr1.coef_[0]
    coef2+=lr1.coef_[1]
    intercept+=lr1.intercept_
    Y['pred_ir'].loc[ix]= lr1.predict(X.loc[ix][['invratio_1', 'live_info']].values.reshape(1,-1))[0]

n=len(X)-1
print(intercept/n, coef1/n, coef2/n)
43/233:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price10'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price20']=None
df['price20'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price20'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)

df['stock_t01']=10

df['purchase_t10'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t10'] = np.where(df['qty1']<0, df['qty1'], 0)
df['stock_t11'] = df['stock_t01']+df['purchase_t10']+df['sale_t10']

df['purchase_t20'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t20'] = np.where(df['qty2']<0, df['qty2'], 0)
df['stock_t21'] = df['stock_t11']+df['purchase_t20']+df['sale_t20']

df['avg_purch_t11'] = (2177.6 + df.purchase_t10*df.price10)/(10+df.purchase_t10)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)

df['pnl_t20'] = np.where(df.stock_t11>10, (df.stock_t11-10)*(df.price20-df.price10)+10*(df.price20-df.price0), df.stock_t11*(df.price20-df.price0))

df['live_info'] = np.where(df.experiment.str.contains('wc'), 0, 1)
df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])

df = df[['experiment', 'mturkid', 'winnings',
       'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret', 
       'price0', 'price10', 'price20', 'stock_t01',
       'purchase_t10', 'sale_t10', 'stock_t11', 'purchase_t20', 'sale_t20',
       'stock_t21', 'pnl_t20']]

df
43/234:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price10'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price20']=None
df['price20'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price20'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)

df['stock_t01']=10

df['purchase_t10'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t10'] = np.where(df['qty1']<0, df['qty1'], 0)
df['stock_t11'] = df['stock_t01']+df['purchase_t10']+df['sale_t10']

df['purchase_t20'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t20'] = np.where(df['qty2']<0, df['qty2'], 0)
df['stock_t21'] = df['stock_t11']+df['purchase_t20']+df['sale_t20']

df['avg_purch_t11'] = (2177.6 + df.purchase_t10*df.price10)/(10+df.purchase_t10)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)

df['pnl_t20'] = np.where(df.stock_t11>10, (df.stock_t11-10)*(df.price20-df.price10)+10*(df.price20-df.price0), df.stock_t11*(df.price20-df.price0))

df['live_info'] = np.where(df.experiment.str.contains('wc'), 0, 1)
df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])

df = df[['experiment', 'mturkid', 'winnings',
       'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret', 
       'price0', 'price10', 'price20', 'stock_t01',
       'purchase_t10', 'sale_t10', 'stock_t11', 'purchase_t20', 'sale_t20',
       'stock_t21', 'pnl_t20', 'live_info']]

df
43/235:
## INV RATIO: traded_volume/permissible_volume
maxbuy10 = math.floor(2500/219.65)
maxsell10 = 10
df['invratio_1'] = df.purchase_t10/maxbuy10
df['invratio_1'] = np.where(df.invratio_1==0, df.sale_t10/maxsell10, df.invratio_1)

avbl_cash = 2500 - (df.purchase_t10+df.sale_t10)*219.65
avbl_stock = 10+df.purchase_t10+df.sale_t10
maxbuy2 = avbl_cash/df.price20
df['invratio_2'] = df.purchase_t20/maxbuy2
df['invratio_2'] = np.where(df.invratio_2==0, df.sale_t20/avbl_stock, df.invratio_2)

# # DELTA IN INV_RATIO: measures change in investment behavior
df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])
43/236:
## INV RATIO: traded_volume/permissible_volume
maxbuy10 = math.floor(2500/219.65)
maxsell10 = 10
df['invratio_1'] = df.purchase_t10/maxbuy10
df['invratio_1'] = np.where(df.invratio_1==0, df.sale_t10/maxsell10, df.invratio_1)

avbl_cash = 2500 - (df.purchase_t10+df.sale_t10)*219.65
avbl_stock = 10+df.purchase_t10+df.sale_t10
maxbuy2 = avbl_cash/df.price20
df['invratio_2'] = df.purchase_t20/maxbuy2
df['invratio_2'] = np.where(df.invratio_2==0, df.sale_t20/avbl_stock, df.invratio_2)

# # DELTA IN INV_RATIO: measures change in investment behavior
df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])
43/237:
from sklearn.linear_model import LinearRegression
pd.options.mode.chained_assignment = None

# bhvrs = get_bhvr()
X = df[['invratio_1', 'live_info']].copy()
Y1 = df[['invratio_2']].copy()


#LOOCV to predict live/watercooler
Y['pred_ir'] = None
coef1 = coef2 = intercept = 0
for ix in X.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LinearRegression().fit(xx,yy)
    coef1+=lr1.coef_[0]
    coef2+=lr1.coef_[1]
    intercept+=lr1.intercept_
    Y['pred_ir'].loc[ix]= lr1.predict(X.loc[ix][['invratio_1', 'live_info']].values.reshape(1,-1))[0]

n=len(X)-1
print(intercept/n, coef1/n, coef2/n)
43/238:
from sklearn.linear_model import LinearRegression
pd.options.mode.chained_assignment = None

# bhvrs = get_bhvr()
X = df[['invratio_1', 'live_info']].copy()
Y1 = df[['invratio_2']].copy()


#LOOCV to predict live/watercooler
Y['pred_ir'] = None
coef1 = coef2 = intercept = 0
for ix in X.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LinearRegression().fit(xx,yy)
    coef1+=lr1.coef_[0]
    coef2+=lr1.coef_[1]
    intercept+=lr1.intercept_
    Y1['pred_ir'].loc[ix]= lr1.predict(X.loc[ix][['invratio_1', 'live_info']].values.reshape(1,-1))[0]

n=len(X)-1
print(intercept/n, coef1/n, coef2/n)
43/239:
from sklearn.linear_model import LinearRegression
pd.options.mode.chained_assignment = None

# bhvrs = get_bhvr()
X = df[['invratio_1', 'live_info']].copy()
Y1 = df[['invratio_2']].copy()


#LOOCV to predict live/watercooler
Y1['pred_ir'] = None
coef1 = coef2 = intercept = 0
for ix in X.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LinearRegression().fit(xx,yy)
    coef1+=lr1.coef_[0]
    coef2+=lr1.coef_[1]
    intercept+=lr1.intercept_
    Y1['pred_ir'].loc[ix]= lr1.predict(X.loc[ix][['invratio_1', 'live_info']].values.reshape(1,-1))[0]

n=len(X)-1
print(intercept/n, coef1/n, coef2/n)
43/240: xx
43/241: xx.isnull().sum()
43/242: Y1.isnull().sum()
43/243: df.invratio_2.isnull().sum()
43/244: df.invratio_2
43/245: df.invratio_2.isnull()
43/246: df[df.invratio_2.isnull()]
43/247:
from sklearn.linear_model import LinearRegression
pd.options.mode.chained_assignment = None

# bhvrs = get_bhvr()
X = df[['invratio_1', 'live_info']].copy()
Y1 = df[['invratio_2']].copy().fillna(0)


#LOOCV to predict live/watercooler
Y1['pred_ir'] = None
coef1 = coef2 = intercept = 0
for ix in X.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LinearRegression().fit(xx,yy)
    coef1+=lr1.coef_[0]
    coef2+=lr1.coef_[1]
    intercept+=lr1.intercept_
    Y1['pred_ir'].loc[ix]= lr1.predict(X.loc[ix][['invratio_1', 'live_info']].values.reshape(1,-1))[0]

n=len(X)-1
print(intercept/n, coef1/n, coef2/n)
43/248:
X = df[['invratio_1', 'live_info']].copy()
X.isnull().sum()
43/249:
Y1 = df[['invratio_2']].copy().fillna(0)
Y1.isnull().sum()
43/250:
from sklearn.linear_model import LinearRegression
pd.options.mode.chained_assignment = None

# bhvrs = get_bhvr()
X = df[['invratio_1', 'live_info']].copy()
Y1 = df[['invratio_2']].copy().fillna(0)


#LOOCV to predict live/watercooler
Y1['pred_ir'] = None
coef1 = coef2 = intercept = 0
for ix in X.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LinearRegression().fit(xx,yy)
    coef1+=lr1.coef_[0]
    coef2+=lr1.coef_[1]
    intercept+=lr1.intercept_
    Y1['pred_ir'].loc[ix]= lr1.predict(X.loc[ix][['invratio_1', 'live_info']].values.reshape(1,-1))[0]

n=len(X)-1
print(intercept/n, coef1/n, coef2/n)
43/251:
from sklearn.linear_model import LinearRegression
pd.options.mode.chained_assignment = None

# bhvrs = get_bhvr()
X = df[['invratio_1', 'live_info']].copy()
Y1 = df[['invratio_2']].copy().fillna(0)


#LOOCV to predict live/watercooler
Y1['pred_ir'] = None
coef1 = coef2 = intercept = 0
for ix in X.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LinearRegression().fit(xx,yy)
    coef1+=lr1.coef_[0]
    coef2+=lr1.coef_[1]
    intercept+=lr1.intercept_
    Y1['pred_ir'].loc[ix]= lr1.predict(X.loc[ix][['invratio_1', 'live_info']].values.reshape(1,-1))[0]

n=len(X)-1
print(intercept/n, coef1/n, coef2/n)
43/252:
from sklearn.linear_model import LinearRegression
pd.options.mode.chained_assignment = None

# bhvrs = get_bhvr()
X = df[['invratio_1', 'live_info']].copy()
Y1 = df[['invratio_2']].copy().fillna(0)


#LOOCV to predict live/watercooler
Y1['pred_ir'] = None
coef1 = coef2 = intercept = 0
for ix in X.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]
    lr1 = LinearRegression().fit(xx,yy.fillna(0))
    coef1+=lr1.coef_[0]
    coef2+=lr1.coef_[1]
    intercept+=lr1.intercept_
    Y1['pred_ir'].loc[ix]= lr1.predict(X.loc[ix][['invratio_1', 'live_info']].values.reshape(1,-1))[0]

n=len(X)-1
print(intercept/n, coef1/n, coef2/n)
43/253: df[['invratio_2']].copy().fillna(0)
43/254:
from sklearn.linear_model import LinearRegression
pd.options.mode.chained_assignment = None

# bhvrs = get_bhvr()
X = df[['invratio_1', 'live_info']].copy()
Y1 = df[['invratio_2']].copy().fillna(0)


#LOOCV to predict live/watercooler
Y1['pred_ir'] = None
coef1 = coef2 = intercept = 0
for ix in X.index:
    xx = X[X.index!=ix]
    yy = Y1[Y1.index!=ix]['invratio_2']
    lr1 = LinearRegression().fit(xx,yy)
    coef1+=lr1.coef_[0]
    coef2+=lr1.coef_[1]
    intercept+=lr1.intercept_
    Y1['pred_ir'].loc[ix]= lr1.predict(X.loc[ix][['invratio_1', 'live_info']].values.reshape(1,-1))[0]

n=len(X)-1
print(intercept/n, coef1/n, coef2/n)
43/255: lr1.score
43/256: lr1.score()
43/257: lr1.score(X, Y1['invratio_2'])
43/258:
# sns.distplot(df.invratio_1.abs(), bins=10)
# df.invratio_1.abs().describe()
# sns.scatterplot(df.invratio_1, df.invratio_2)

sns.distplot(df.invratio_delta)
43/259:
## INV RATIO: traded_volume/permissible_volume
maxbuy10 = math.floor(2500/219.65)
maxsell10 = 10
df['invratio_1'] = df.purchase_t10/maxbuy10
df['invratio_1'] = np.where(df.invratio_1==0, df.sale_t10/maxsell10, df.invratio_1)

avbl_cash = 2500 - (df.purchase_t10+df.sale_t10)*219.65
avbl_stock = 10+df.purchase_t10+df.sale_t10
maxbuy2 = avbl_cash/df.price20
df['invratio_2'] = df.purchase_t20/maxbuy2
df['invratio_2'] = np.where(df.invratio_2==0, df.sale_t20/avbl_stock, df.invratio_2).fillna(0)

# # DELTA IN INV_RATIO: measures change in investment behavior
df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])
43/260:
## INV RATIO: traded_volume/permissible_volume
maxbuy10 = math.floor(2500/219.65)
maxsell10 = 10
df['invratio_1'] = df.purchase_t10/maxbuy10
df['invratio_1'] = np.where(df.invratio_1==0, df.sale_t10/maxsell10, df.invratio_1)

avbl_cash = 2500 - (df.purchase_t10+df.sale_t10)*219.65
avbl_stock = 10+df.purchase_t10+df.sale_t10
maxbuy2 = avbl_cash/df.price20
df['invratio_2'] = df.purchase_t20/maxbuy2
df['invratio_2'] = np.where(df.invratio_2==0, df.sale_t20/avbl_stock, df.invratio_2)
df['invratio_2'].fillna(0, inplace=True)

# # DELTA IN INV_RATIO: measures change in investment behavior
df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])
43/261:
# sns.distplot(df.invratio_1.abs(), bins=10)
# df.invratio_1.abs().describe()
# sns.scatterplot(df.invratio_1, df.invratio_2)

sns.distplot(df.invratio_delta)
43/262: sns.distplot(df[df.live_info==1].invratio_delta)
43/263: sns.distplot(df[df.live_info==0].invratio_delta)
43/264: sns.distplot(df[df.live_info==1].invratio_delta)
43/265: sns.distplot(df[df.live_info==0].invratio_delta)
43/266:
# sns.distplot(df[df.live_info==0].invratio_delta)
df[df.live_info==0].invratio_delta.describe()
43/267: df[df.live_info==1].invratio_delta.describe()
43/268:
## INV RATIO: traded_volume/permissible_volume
maxbuy10 = math.floor(2500/219.65)
maxsell10 = 10
df['invratio_1'] = df.purchase_t10/maxbuy10
df['invratio_1'] = np.where(df.invratio_1==0, df.sale_t10/maxsell10, df.invratio_1)

avbl_cash = 2500 - (df.purchase_t10+df.sale_t10)*219.65
avbl_stock = 10+df.purchase_t10+df.sale_t10
maxbuy2 = avbl_cash/df.price20
df['invratio_2'] = df.purchase_t20/maxbuy2
df['invratio_2'] = np.where(df.invratio_2==0, df.sale_t20/avbl_stock, df.invratio_2)
df['invratio_2'].fillna(0, inplace=True)

# # DELTA IN INV_RATIO: measures change in investment behavior
df['invratio_delta'] = df['invratio_2']-df['invratio_1']
43/269:
# sns.distplot(df.invratio_1.abs(), bins=10)
# df.invratio_1.abs().describe()
# sns.scatterplot(df.invratio_1, df.invratio_2)

sns.distplot(df.invratio_delta)
43/270: df[df.live_info==1].invratio_delta.describe()
43/271:
# sns.distplot(df.invratio_1.abs(), bins=10)
# df.invratio_1.abs().describe()
# sns.scatterplot(df.invratio_1, df.invratio_2)

sns.distplot(df.invratio_delta, bins=10)
43/272: df[df.live_info==0].invratio_delta.describe()
43/273:
# sns.distplot(df.invratio_1.abs(), bins=10)
# df.invratio_1.abs().describe()
# sns.scatterplot(df.invratio_1, df.invratio_2)

sns.distplot(df[df.live_info==0].invratio_delta, bins=10)
43/274:
# sns.distplot(df.invratio_1.abs(), bins=10)
# df.invratio_1.abs().describe()
# sns.scatterplot(df.invratio_1, df.invratio_2)

sns.distplot(df[df.live_info==1].invratio_delta, bins=10)
43/275:
# sns.distplot(df.invratio_1.abs(), bins=10)
# df.invratio_1.abs().describe()
# sns.scatterplot(df.invratio_1, df.invratio_2)

sns.distplot(df[df.live_info==0].invratio_delta, bins=10)
43/276: df.to_csv('results_feat.csv')
43/277:
# Vol. at T1
df.purchase_t10 + df.sale_t10.abs()
43/278:
# Vol. at T1
(df.purchase_t10 + df.sale_t10.abs()).sum()
43/279: print(f"Vol. at T1 = {(df.purchase_t10 + df.sale_t10.abs()).sum()}".)
43/280: print(f"Vol. at T1 = {(df.purchase_t10 + df.sale_t10.abs()).sum()}")
43/281: print(f"Vol. at T1 = {(df.purchase_t10 + df.sale_t10.abs()).sum()}. Shares bought = {df.purchase_t10.sum()}. Shares sold = {df.sale_t10.abs().sum()}")
43/282:
print(f"Vol. at T1 = {(df.purchase_t10 + df.sale_t10.abs()).sum()}. Shares bought = {df.purchase_t10.sum()}. Shares sold = {df.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df.purchase_t20 + df.sale_t20.abs()).sum()}. Shares bought = {df.purchase_t20.sum()}. Shares sold = {df.sale_t20.abs().sum()}")
43/283: df
43/284:
print(f"Vol. at T1 = {(df.purchase_t10 + df.sale_t10.abs()).sum()}. Shares bought = {df.purchase_t10.sum()}. Shares sold = {df.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df.purchase_t20 + df.sale_t20.abs()).sum()}. Shares bought = {df.purchase_t20.sum()}. Shares sold = {df.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df.invratio_1.mean()} ({df.invratio_1.std()})")
43/285:
print(f"Vol. at T1 = {(df.purchase_t10 + df.sale_t10.abs()).sum()}. Shares bought = {df.purchase_t10.sum()}. Shares sold = {df.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df.purchase_t20 + df.sale_t20.abs()).sum()}. Shares bought = {df.purchase_t20.sum()}. Shares sold = {df.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df.invratio_1.mean()} ({df.invratio_1.std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df.invratio_2.mean()} ({df.invratio_2.std()})")
43/286:
print(f"Vol. at T1 = {(df.purchase_t10 + df.sale_t10.abs()).sum()}. Shares bought = {df.purchase_t10.sum()}. Shares sold = {df.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df.purchase_t20 + df.sale_t20.abs()).sum()}. Shares bought = {df.purchase_t20.sum()}. Shares sold = {df.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df.invratio_1.abs().mean()} ({df.invratio_1.abs().std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df.invratio_2.abs().mean()} ({df.invratio_2.abs().std()})")
43/287:
df1 = df[df.experiment=='rc_profit']
df2 = df[df.experiment=='rc_loss']
df3 = df[df.experiment=='wc_profit']
df4 = df[df.experiment=='wc_loss']
43/288:
print(f"Vol. at T1 = {(df1.purchase_t10 + df1.sale_t10.abs()).sum()}. Shares bought = {df1.purchase_t10.sum()}. Shares sold = {df1.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df1.purchase_t20 + df1.sale_t20.abs()).sum()}. Shares bought = {df1.purchase_t20.sum()}. Shares sold = {df1.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df1.invratio_1.abs().mean()} ({df1.invratio_1.abs().std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df1.invratio_2.abs().mean()} ({df1.invratio_2.abs().std()})")
43/289:
df1 = df[df.experiment=="'rc_profit'"]
df2 = df[df.experiment=="'rc_loss'"]
df3 = df[df.experiment=="'wc_profit'"]
df4 = df[df.experiment=="'wc_loss'"]
43/290:
df1 = df[df.experiment=="'rc_profit'"]
df2 = df[df.experiment=="'rc_loss'"]
df3 = df[df.experiment=="'wc_profit'"]
df4 = df[df.experiment=="'wc_loss'"]
43/291:
print(f"Vol. at T1 = {(df1.purchase_t10 + df1.sale_t10.abs()).sum()}. Shares bought = {df1.purchase_t10.sum()}. Shares sold = {df1.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df1.purchase_t20 + df1.sale_t20.abs()).sum()}. Shares bought = {df1.purchase_t20.sum()}. Shares sold = {df1.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df1.invratio_1.abs().mean()} ({df1.invratio_1.abs().std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df1.invratio_2.abs().mean()} ({df1.invratio_2.abs().std()})")
43/292:
print(f"Vol. at T1 = {(df2.purchase_t10 + df2.sale_t10.abs()).sum()}. Shares bought = {df2.purchase_t10.sum()}. Shares sold = {df2.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df2.purchase_t20 + df2.sale_t20.abs()).sum()}. Shares bought = {df2.purchase_t20.sum()}. Shares sold = {df2.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df2.invratio_1.abs().mean()} ({df2.invratio_1.abs().std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df2.invratio_2.abs().mean()} ({df2.invratio_2.abs().std()})")
43/293:
print(f"Vol. at T1 = {(df3.purchase_t10 + df3.sale_t10.abs()).sum()}. Shares bought = {df3.purchase_t10.sum()}. Shares sold = {df3.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df3.purchase_t20 + df3.sale_t20.abs()).sum()}. Shares bought = {df3.purchase_t20.sum()}. Shares sold = {df3.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df3.invratio_1.abs().mean()} ({df3.invratio_1.abs().std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df3.invratio_2.abs().mean()} ({df3.invratio_2.abs().std()})")
43/294:
print(f"Vol. at T1 = {(df4.purchase_t10 + df4.sale_t10.abs()).sum()}. Shares bought = {df4.purchase_t10.sum()}. Shares sold = {df4.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df4.purchase_t20 + df4.sale_t20.abs()).sum()}. Shares bought = {df4.purchase_t20.sum()}. Shares sold = {df4.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df4.invratio_1.abs().mean()} ({df4.invratio_1.abs().std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df4.invratio_2.abs().mean()} ({df4.invratio_2.abs().std()})")
43/295:
print(f"Vol. at T1 = {(df.purchase_t10 + df.sale_t10.abs()).sum()}. Shares bought = {df.purchase_t10.sum()}. Shares sold = {df.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df.purchase_t20 + df.sale_t20.abs()).sum()}. Shares bought = {df.purchase_t20.sum()}. Shares sold = {df.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df.invratio_1.abs().mean()} ({df.invratio_1.abs().std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df.invratio_2.abs().mean()} ({df.invratio_2.abs().std()})")
print(f"Mean buyer caution at T2 (0= cautious. 1= risky): {df[df.invratio_2>0].invratio_2.abs().mean()} ({df[df.invratio_2>0].invratio_2.abs().std()})")
43/296:
print(f"Vol. at T1 = {(df.purchase_t10 + df.sale_t10.abs()).sum()}. Shares bought = {df.purchase_t10.sum()}. Shares sold = {df.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df.purchase_t20 + df.sale_t20.abs()).sum()}. Shares bought = {df.purchase_t20.sum()}. Shares sold = {df.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df.invratio_1.abs().mean()} ({df.invratio_1.abs().std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df.invratio_2.abs().mean()} ({df.invratio_2.abs().std()})")
print(f"Mean buyer caution at T2 (0= cautious. 1= risky): {df[df.invratio_2>0].invratio_2.abs().mean()} ({df[df.invratio_2>0].invratio_2.abs().std()})")
print(f"Mean seller caution at T2 (0= cautious. 1= risky): {df[df.invratio_2<0].invratio_2.abs().mean()} ({df[df.invratio_2<0].invratio_2.abs().std()})")
43/297:
print(f"Vol. at T1 = {(df1.purchase_t10 + df1.sale_t10.abs()).sum()}. Shares bought = {df1.purchase_t10.sum()}. Shares sold = {df1.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df1.purchase_t20 + df1.sale_t20.abs()).sum()}. Shares bought = {df1.purchase_t20.sum()}. Shares sold = {df1.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df1.invratio_1.abs().mean()} ({df1.invratio_1.abs().std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df1.invratio_2.abs().mean()} ({df1.invratio_2.abs().std()})")
print(f"Mean buyer caution at T2 (0= cautious. 1= risky): {df1[df1.invratio_2>0].invratio_2.abs().mean()} ({df1[df1.invratio_2>0].invratio_2.abs().std()})")
print(f"Mean seller caution at T2 (0= cautious. 1= risky): {df1[df1.invratio_2<0].invratio_2.abs().mean()} ({df1[df1.invratio_2<0].invratio_2.abs().std()})")
43/298:
print(f"Vol. at T1 = {(df2.purchase_t10 + df2.sale_t10.abs()).sum()}. Shares bought = {df2.purchase_t10.sum()}. Shares sold = {df2.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df2.purchase_t20 + df2.sale_t20.abs()).sum()}. Shares bought = {df2.purchase_t20.sum()}. Shares sold = {df2.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df2.invratio_1.abs().mean()} ({df2.invratio_1.abs().std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df2.invratio_2.abs().mean()} ({df2.invratio_2.abs().std()})")
print(f"Mean buyer caution at T2 (0= cautious. 1= risky): {df2[df2.invratio_2>0].invratio_2.abs().mean()} ({df2[df2.invratio_2>0].invratio_2.abs().std()})")
print(f"Mean seller caution at T2 (0= cautious. 1= risky): {df2[df2.invratio_2<0].invratio_2.abs().mean()} ({df2[df2.invratio_2<0].invratio_2.abs().std()})")
43/299:
print(f"Vol. at T1 = {(df3.purchase_t10 + df3.sale_t10.abs()).sum()}. Shares bought = {df3.purchase_t10.sum()}. Shares sold = {df3.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df3.purchase_t20 + df3.sale_t20.abs()).sum()}. Shares bought = {df3.purchase_t20.sum()}. Shares sold = {df3.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df3.invratio_1.abs().mean()} ({df3.invratio_1.abs().std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df3.invratio_2.abs().mean()} ({df3.invratio_2.abs().std()})")
print(f"Mean buyer caution at T2 (0= cautious. 1= risky): {df3[df3.invratio_2>0].invratio_2.abs().mean()} ({df3[df3.invratio_2>0].invratio_2.abs().std()})")
print(f"Mean seller caution at T2 (0= cautious. 1= risky): {df3[df3.invratio_2<0].invratio_2.abs().mean()} ({df3[df3.invratio_2<0].invratio_2.abs().std()})")
43/300:
print(f"Vol. at T1 = {(df4.purchase_t10 + df4.sale_t10.abs()).sum()}. Shares bought = {df4.purchase_t10.sum()}. Shares sold = {df4.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df4.purchase_t20 + df4.sale_t20.abs()).sum()}. Shares bought = {df4.purchase_t20.sum()}. Shares sold = {df4.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df4.invratio_1.abs().mean()} ({df4.invratio_1.abs().std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df4.invratio_2.abs().mean()} ({df4.invratio_2.abs().std()})")
print(f"Mean buyer caution at T2 (0= cautious. 1= risky): {df4[df4.invratio_2>0].invratio_2.abs().mean()} ({df4[df4.invratio_2>0].invratio_2.abs().std()})")
print(f"Mean seller caution at T2 (0= cautious. 1= risky): {df4[df4.invratio_2<0].invratio_2.abs().mean()} ({df4[df4.invratio_2<0].invratio_2.abs().std()})")
43/301:
df1.sale_t20.abs().plot.hist()
df3.sale_t20.abs().plot.hist()
43/302:
df1.sale_t20.abs().plot.hist(bins=20)
df3.sale_t20.abs().plot.hist(bins=20)
43/303:
df3.sale_t20.abs().plot.hist(bins=20)
df1.sale_t20.abs().plot.hist(bins=20)
43/304:
df3.invratio_2.abs().plot.hist(bins=20)
df1.invratio_2.abs().plot.hist(bins=20)
43/305:
# df3.invratio_2.abs().plot.hist(bins=20)
# df1.invratio_2.abs().plot.hist(bins=20)
df1.invratio_2.abs().describe()
43/306:
# df3.invratio_2.abs().plot.hist(bins=20)
# df1.invratio_2.abs().plot.hist(bins=20)
df1[df1.invratio_2>1]
43/307:
# df3.invratio_2.abs().plot.hist(bins=20)
# df1.invratio_2.abs().plot.hist(bins=20)
df1[df1.invratio_2.abs()>1]
43/308:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price10'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price20']=None
df['price20'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price20'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
df=df[df.mturkid!='A2H95JVPEKRUWA'] # invalid

df['stock_t01']=10

df['purchase_t10'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t10'] = np.where(df['qty1']<0, df['qty1'], 0)
df['stock_t11'] = df['stock_t01']+df['purchase_t10']+df['sale_t10']

df['purchase_t20'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t20'] = np.where(df['qty2']<0, df['qty2'], 0)
df['stock_t21'] = df['stock_t11']+df['purchase_t20']+df['sale_t20']

df['avg_purch_t11'] = (2177.6 + df.purchase_t10*df.price10)/(10+df.purchase_t10)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)

df['pnl_t20'] = np.where(df.stock_t11>10, (df.stock_t11-10)*(df.price20-df.price10)+10*(df.price20-df.price0), df.stock_t11*(df.price20-df.price0))

df['live_info'] = np.where(df.experiment.str.contains('wc'), 0, 1)
df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])

df = df[['experiment', 'mturkid', 'winnings',
       'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret', 
       'price0', 'price10', 'price20', 'stock_t01',
       'purchase_t10', 'sale_t10', 'stock_t11', 'purchase_t20', 'sale_t20',
       'stock_t21', 'pnl_t20', 'live_info']]

df
43/309:
df3.invratio_2.abs().plot.hist(bins=20)
df1.invratio_2.abs().plot.hist(bins=20)
# df1[df1.invratio_2.abs()>1]
43/310:
df1 = df[df.experiment=="'rc_profit'"]
df2 = df[df.experiment=="'rc_loss'"]
df3 = df[df.experiment=="'wc_profit'"]
df4 = df[df.experiment=="'wc_loss'"]
43/311:
df3.invratio_2.abs().plot.hist(bins=20)
df1.invratio_2.abs().plot.hist(bins=20)
# df1[df1.invratio_2.abs()>1]
43/312:
## INV RATIO: traded_volume/permissible_volume
maxbuy10 = math.floor(2500/219.65)
maxsell10 = 10
df['invratio_1'] = df.purchase_t10/maxbuy10
df['invratio_1'] = np.where(df.invratio_1==0, df.sale_t10/maxsell10, df.invratio_1)

avbl_cash = 2500 - (df.purchase_t10+df.sale_t10)*219.65
avbl_stock = 10+df.purchase_t10+df.sale_t10
maxbuy2 = avbl_cash/df.price20
df['invratio_2'] = df.purchase_t20/maxbuy2
df['invratio_2'] = np.where(df.invratio_2==0, df.sale_t20/avbl_stock, df.invratio_2)
df['invratio_2'].fillna(0, inplace=True)

# # DELTA IN INV_RATIO: measures change in investment behavior
df['invratio_delta'] = df['invratio_2']-df['invratio_1']
43/313:
# sns.distplot(df.invratio_1.abs(), bins=10)
# df.invratio_1.abs().describe()
# sns.scatterplot(df.invratio_1, df.invratio_2)

sns.distplot(df[df.live_info==0].invratio_delta, bins=10)
43/314:
df1 = df[df.experiment=="'rc_profit'"]
df2 = df[df.experiment=="'rc_loss'"]
df3 = df[df.experiment=="'wc_profit'"]
df4 = df[df.experiment=="'wc_loss'"]
43/315:
df3.invratio_2.abs().plot.hist(bins=20)
df1.invratio_2.abs().plot.hist(bins=20)
# df1[df1.invratio_2.abs()>1]
43/316:
sns.distplot(df.invratio_2)
sns.distplot(df1.invratio_2)
43/317:
sns.distplot(df.invratio_2, bins=10)
sns.distplot(df1.invratio_2, bins=10)
43/318:
sns.distplot(df.invratio_2, bins=10)
# sns.distplot(df1.invratio_2, bins=10)
43/319:
sns.distplot(df.invratio_2, bins=10)
sns.distplot(df1.invratio_2, bins=10)
43/320:
sns.distplot(df.invratio_2, bins=10)
sns.distplot(df2.invratio_2, bins=10)
43/321:
sns.distplot(df.invratio_2, bins=10)
sns.distplot(df3.invratio_2, bins=10)
43/322:
sns.distplot(df.invratio_2, bins=10)
sns.distplot(df1.invratio_2, bins=10)
43/323:
sns.distplot(df.invratio_2, bins=10)
sns.distplot(df3.invratio_2, bins=10)
43/324:
sns.distplot(df.invratio_2, bins=10)
sns.distplot(df4.invratio_2, bins=10)
43/325: df1.winnings.describe()
43/326: sns.distplot(df.winnings)
43/327: sns.distplot(df1.winnings)
43/328: sns.distplot(df2.winnings)
43/329:
sns.distplot(df.winnings, bins=10)
sns.distplot(df1.winnings, bins=10)
43/330:
sns.distplot(df.winnings, bins=10)
sns.distplot(df2.winnings, bins=10)
43/331:
sns.distplot(df.winnings, bins=10)
sns.distplot(df3.winnings, bins=10)
43/332:
sns.distplot(df.winnings, bins=10)
sns.distplot(df4.winnings, bins=10)
43/333:
sns.distplot(df2.winnings, bins=10)
sns.distplot(df4.winnings, bins=10)
43/334:
sns.distplot(df1.winnings, bins=10)
sns.distplot(df3.winnings, bins=10)
43/335:
sns.distplot(df2.invratio_2, bins=10)
sns.distplot(df4.invratio_2, bins=10)
43/336:
sns.distplot(df1.invratio_2, bins=10)
sns.distplot(df3.invratio_2, bins=10)
43/337:
sns.distplot(df1[df1.invratio_2>0].invratio_2, bins=10)
sns.distplot(df3[df3.invratio_2>0].invratio_2, bins=10)
43/338:
sns.distplot(df1[df1.invratio_2<0].invratio_2, bins=10)
sns.distplot(df3[df3.invratio_2<0].invratio_2, bins=10)
43/339:
sns.distplot(df1[df1.invratio_2>0].invratio_2, bins=10)
sns.distplot(df3[df3.invratio_2>0].invratio_2, bins=10)
43/340: sns.distplot(df1.focus)
43/341:
sns.distplot(df1.focus)
sns.distplot(df3.focus)
43/342:
sns.distplot(df2.focus)
sns.distplot(df4.focus)
43/343:
sns.distplot(df1.focus)
sns.distplot(df3.focus)
43/344:
sns.distplot(df1.invratio_delta, bins=10)
sns.distplot(df3.invratio_delta, bins=10)
43/345:
# people who had stock at t10, and in a profitable position at t20 - how many sold, how many held/purchased?
w=df[df.pnl_t20>0][df.stock_t11>0]
proportion_profit_sellers = (w.sale_t20!=0).sum()/w.shape[0]


# people who had stock at t1, and in a losing position at t20 - how many sold, how many held/purchased?
l=df[df.pnl_t20<0][df.stock_t11>0]
proportion_loss_sellers = (l.sale_t20!=0).sum()/l.shape[0]

proportion_profit_sellers, proportion_loss_sellers
43/346:
# no. of T2 sellers who a) were in profit b) were in loss
sellers = df[df.sale_t20>0]

(sellers.pnl_t20>0).sum(), (sellers.pnl_t20<=0).sum()
43/347: df
43/348:
# no. of T2 sellers who a) were in profit b) were in loss
sellers = df[df.sale_t20<0]

(sellers.pnl_t20>0).sum(), (sellers.pnl_t20<=0).sum()
43/349:
# no. of T1 sellers who a) were in profit b) were in loss
sellers = df[df.sale_t10<0]

df.shape[0], sellers.shape[0]
43/350:
profit_sellers = sellers[sellers.pnl_t20>0]
loss_sellers = sellers[sellers.pnl_t20<=0]

profit_sellers.experiment.value_counts()
43/351:
# no. of T2 sellers who a) were in profit b) were in loss
sellers = df[df.sale_t20<0]

(sellers.pnl_t20>0)#.sum(), (sellers.pnl_t20<=0).sum()
43/352: sellers
43/353: sellers[sellers.pnl_t20>0]
43/354: sellers[sellers.pnl_t20>0].experiment.value_counts()
43/355:
profit_sellers = sellers[sellers.pnl_t20>0]
loss_sellers = sellers[sellers.pnl_t20<=0]

profit_sellers.experiment.value_counts()
43/356:
profit_sellers = sellers[sellers.pnl_t20>0]
loss_sellers = sellers[sellers.pnl_t20<=0]

loss_sellers.experiment.value_counts()
43/357:
sellers = df[df.sale_t20<0]
buyers = df[df.purchase_t20>0]

sellers.invratio_2.mean()
43/358:
sellers = df[df.sale_t20<0]
buyers = df[df.purchase_t20>0]

sellers.invratio_2.abs().mean()
43/359:
sellers = df[df.sale_t20<0]
buyers = df[df.purchase_t20>0]

buyers.invratio_2.abs().mean()
43/360:

for dfz in [df1,df2,df3,df4]:
    print(f"Mean buyer caution at T1 : {dfz[dfz.invratio_1>0].invratio_1.abs().mean()} ({dfz[dfz.invratio_1>0].invratio_1.abs().std()})")
    print(f"Mean seller caution at T1 : {dfz[dfz.invratio_1<0].invratio_1.abs().mean()} ({dfz[dfz.invratio_1<0].invratio_1.abs().std()})")
43/361:

for dfz in [df1,df2,df3,df4]:
    print(f"Mean buyer caution at T1 : {round(dfz[dfz.invratio_1>0].invratio_1.abs().mean(),2)} ({round(dfz[dfz.invratio_1>0].invratio_1.abs().std(),2)})")
    print(f"Mean seller caution at T1 : {round(dfz[dfz.invratio_1<0].invratio_1.abs().mean(),2)} ({round(dfz[dfz.invratio_1<0].invratio_1.abs().std(),2)})")
43/362: round(dfz[dfz.invratio_1!=0].invratio_1.abs().mean(),2)
43/363: (df1.purchase_t10>0).sum()
43/364: (df1.purchase_t10!=0).sum()
43/365: (df1.purchase_t10!=0).sum(), (df1.sale_t10!=0).sum()
43/366: df1[df1.purchase_t10!=0].purchase_t10.sum(), df1[df1.sale_t10!=0].sale_t10.sum()
43/367: df.to_csv('results_feat.csv')
43/368:

for dfz in [df1,df2,df3,df4]:
    round(dfz.invratio_1.abs().mean(),2)
#     print(f"Mean buyer caution at T1 : {round(dfz[dfz.invratio_1>0].invratio_1.abs().mean(),2)} ({round(dfz[dfz.invratio_1>0].invratio_1.abs().std(),2)})")
#     print(f"Mean seller caution at T1 : {round(dfz[dfz.invratio_1<0].invratio_1.abs().mean(),2)} ({round(dfz[dfz.invratio_1<0].invratio_1.abs().std(),2)})")
43/369:

for dfz in [df1,df2,df3,df4]:
    print(f"Mean caution at T1 : {round(dfz.invratio_1.abs().mean(),2)} ({round(dfz.invratio_1.abs().std(),2)})")
#     print(f"Mean seller caution at T1 : {round(dfz[dfz.invratio_1<0].invratio_1.abs().mean(),2)} ({round(dfz[dfz.invratio_1<0].invratio_1.abs().std(),2)})")
43/370: df3.shape
43/371: df2.shape
43/372: df4.shape
43/373: sns.distplot(df2.invratio_1.abs())
43/374: sns.distplot(df2.invratio_1.abs(), bins=5)
43/375:
sns.distplot(df2.invratio_1.abs(), bins=5)
sns.distplot(df2.invratio_2.abs(), bins=5)
43/376:
sns.distplot(df4.invratio_1.abs(), bins=5)
sns.distplot(df4.invratio_2.abs(), bins=5)
43/377:
fig, ax = plt.figure()
sns.distplot(df2.invratio_1.abs(), bins=5)
sns.distplot(df2.invratio_2.abs(), bins=5)
ax.set_title("asf")
43/378:
sns.distplot(df2.invratio_1.abs(), bins=5)
sns.distplot(df2.invratio_2.abs(), bins=5, label)
43/379:
sns.distplot(df2.invratio_1.abs(), bins=5)
sns.distplot(df2.invratio_2.abs(), bins=5, label="Sdf")
43/380:
sns.distplot(df2.invratio_1.abs(), bins=5)
sns.distplot(df2.invratio_2.abs(), bins=5)
sns.plt.show()
43/381:
import matplotlib.pyplot as plt
sns.distplot(df2.invratio_1.abs(), bins=5)
sns.distplot(df2.invratio_2.abs(), bins=5)
plt.show()
43/382:
df2['invratio_delta'] = df2['invratio_2']-df2['invratio_1']
sns.distplot(df2.invratio_delta)
# sns.distplot(df2.invratio_2.abs(), bins=5)
43/383:
df4['invratio_delta'] = df4['invratio_2']-df4['invratio_1']
sns.distplot(df4.invratio_delta)
43/384:
df2['invratio_delta'] = abs(df2['invratio_2'])-abs(df2['invratio_1'])
sns.distplot(df2.invratio_delta)
# sns.distplot(df2.invratio_2.abs(), bins=5)
43/385:
df4['invratio_delta'] = abs(df4['invratio_2'])-abs(df4['invratio_1'])
sns.distplot(df4.invratio_delta)
43/386:
df1 = df[df.experiment=="'rc_profit'"]
df2 = df[df.experiment=="'rc_loss'"]
df3 = df[df.experiment=="'wc_profit'"]
df4 = df[df.experiment=="'wc_loss'"]

df1.shape, df3.shape
43/387: df3
43/388:
df2['invratio_delta'] = abs(df2['invratio_2'])-abs(df2['invratio_1'])
sns.distplot(df2.invratio_delta,bins=5)
# sns.distplot(df2.invratio_2.abs(), bins=5)
43/389:
df2['invratio_delta'] = abs(df2['invratio_2'])-abs(df2['invratio_1'])
sns.distplot(df2.invratio_delta,bins=15)
# sns.distplot(df2.invratio_2.abs(), bins=5)
43/390:
df4['invratio_delta'] = abs(df4['invratio_2'])-abs(df4['invratio_1'])
sns.distplot(df4.invratio_delta, bins=15)
43/391:
df4['invratio_delta'] = abs(df4['invratio_2'])-abs(df4['invratio_1'])
sns.distplot(df2.invratio_delta,bins=15)
sns.distplot(df4.invratio_delta, bins=15)
43/392:
## INV RATIO: traded_volume/permissible_volume
maxbuy10 = math.floor(2500/219.65)
maxsell10 = 10
df['invratio_1'] = df.purchase_t10/maxbuy10
df['invratio_1'] = np.where(df.invratio_1==0, df.sale_t10/maxsell10, df.invratio_1)

avbl_cash = 2500 - (df.purchase_t10+df.sale_t10)*219.65
avbl_stock = 10+df.purchase_t10+df.sale_t10
maxbuy2 = avbl_cash/df.price20
df['invratio_2'] = df.purchase_t20/maxbuy2
df['invratio_2'] = np.where(df.invratio_2==0, df.sale_t20/avbl_stock, df.invratio_2)
df['invratio_2'].fillna(0, inplace=True)

# # DELTA IN INV_RATIO: measures change in investment behavior
df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])
43/393: df.invratio_delta.mean()
43/394:
import scipy.stats.mannwhitneyu as mwu
x = df2.invratio_delta
y = df4.invratio_delta
mwu(x,y)
43/395:
from scipy.stats import mannwhitneyu as mwu
x = df2.invratio_delta
y = df4.invratio_delta
mwu(x,y)
43/396:
dfz = df[df.experiment.str.contains('loss')]
dfz['cond'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1.abs()
dfz['T2'] = dfz.invratio_2.abs()
sns.boxplot(x='cond', y='T1', data=dfz)
43/397:
dfz = df[df.experiment.str.contains('loss')]
dfz['cond'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1.abs()
dfz['T2'] = dfz.invratio_2.abs()
sns.boxplot(x='cond', y=['T1', 'T2'], data=dfz)
43/398:
dfz = df[df.experiment.str.contains('loss')]
dfz['cond'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1.abs()
dfz['T2'] = dfz.invratio_2.abs()
sns.boxplot(x='cond', y=[['T1', 'T2']], data=dfz)
43/399:
dfz = df[df.experiment.str.contains('loss')]
dfz['condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1.abs()
dfz['T2'] = dfz.invratio_2.abs()
sns.boxplot(x='condition', y='T1', data=dfz, order=['Live','WaterCooler'])
43/400:
dfz = df[df.experiment.str.contains('loss')]
dfz['condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1.abs()
dfz['q at T2'] = dfz.invratio_2.abs()
sns.boxplot(x='condition', y='q at T1', data=dfz, order=['Live','WaterCooler'])
43/401:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1.abs()
dfz['q at T2'] = dfz.invratio_2.abs()
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'])
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'])
43/402:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1.abs()
dfz['q at T2'] = dfz.invratio_2.abs()
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'])
plt.show()
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'])
plt.show()
43/403:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1.abs()
dfz['q at T2'] = dfz.invratio_2.abs()
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'])
plt.show()
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'])
plt.show()
43/404:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1.abs()
dfz['q at T2'] = dfz.invratio_2.abs()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# plt.show()
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
plt.show()
43/405:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['focus'] = dfz.focus


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# plt.show()
# sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
plt.show()
43/406:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['focus'] = dfz.focus


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='focus', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# plt.show()
# sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
plt.show()
43/407:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['focus'] = dfz.focus


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='focus', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# plt.show()
# sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
plt.show()
43/408:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1.abs()
dfz['q at T2'] = dfz.invratio_2.abs()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# plt.show()
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
plt.show()
43/409: df[[9, 46,53,54]]
43/410: df.ix[[9, 46,53,54]]
43/411: df.loc[[9, 46,53,54]]
43/412: df.drop(df.loc[[9, 46,53,54]])
43/413: df.drop(df.loc[[9, 46,53,54]], axis=1)
43/414: df.drop([9, 46,53,54], axis=1)
43/415: df.drop([9, 46,53,54])
43/416:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price10'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price20']=None
df['price20'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price20'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
df=df[df.mturkid!='A2H95JVPEKRUWA'] # invalid

df['stock_t01']=10

df['purchase_t10'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t10'] = np.where(df['qty1']<0, df['qty1'], 0)
df['stock_t11'] = df['stock_t01']+df['purchase_t10']+df['sale_t10']

df['purchase_t20'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t20'] = np.where(df['qty2']<0, df['qty2'], 0)
df['stock_t21'] = df['stock_t11']+df['purchase_t20']+df['sale_t20']

df['avg_purch_t11'] = (2177.6 + df.purchase_t10*df.price10)/(10+df.purchase_t10)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)

df['pnl_t20'] = np.where(df.stock_t11>10, (df.stock_t11-10)*(df.price20-df.price10)+10*(df.price20-df.price0), df.stock_t11*(df.price20-df.price0))

df['live_info'] = np.where(df.experiment.str.contains('wc'), 0, 1)
df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])


# drop inactive people (4)
df=df.drop([9,46,53,54])


df = df[['experiment', 'mturkid', 'winnings',
       'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret', 
       'price0', 'price10', 'price20', 'stock_t01',
       'purchase_t10', 'sale_t10', 'stock_t11', 'purchase_t20', 'sale_t20',
       'stock_t21', 'pnl_t20', 'live_info']]

df
43/417:
## INV RATIO: traded_volume/permissible_volume
maxbuy10 = math.floor(2500/219.65)
maxsell10 = 10
df['invratio_1'] = df.purchase_t10/maxbuy10
df['invratio_1'] = np.where(df.invratio_1==0, df.sale_t10/maxsell10, df.invratio_1)

avbl_cash = 2500 - (df.purchase_t10+df.sale_t10)*219.65
avbl_stock = 10+df.purchase_t10+df.sale_t10
maxbuy2 = avbl_cash/df.price20
df['invratio_2'] = df.purchase_t20/maxbuy2
df['invratio_2'] = np.where(df.invratio_2==0, df.sale_t20/avbl_stock, df.invratio_2)
df['invratio_2'].fillna(0, inplace=True)

# # DELTA IN INV_RATIO: measures change in investment behavior
df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])
43/418:
df1 = df[df.experiment=="'rc_profit'"]
df2 = df[df.experiment=="'rc_loss'"]
df3 = df[df.experiment=="'wc_profit'"]
df4 = df[df.experiment=="'wc_loss'"]
43/419:
print(f"Vol. at T1 = {(df1.purchase_t10 + df1.sale_t10.abs()).sum()}. Shares bought = {df1.purchase_t10.sum()}. Shares sold = {df1.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df1.purchase_t20 + df1.sale_t20.abs()).sum()}. Shares bought = {df1.purchase_t20.sum()}. Shares sold = {df1.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df1.invratio_1.abs().mean()} ({df1.invratio_1.abs().std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df1.invratio_2.abs().mean()} ({df1.invratio_2.abs().std()})")
print(f"Mean buyer caution at T2 (0= cautious. 1= risky): {df1[df1.invratio_2>0].invratio_2.abs().mean()} ({df1[df1.invratio_2>0].invratio_2.abs().std()})")
print(f"Mean seller caution at T2 (0= cautious. 1= risky): {df1[df1.invratio_2<0].invratio_2.abs().mean()} ({df1[df1.invratio_2<0].invratio_2.abs().std()})")
43/420:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price10'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price20']=None
df['price20'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price20'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
df=df[df.mturkid!='A2H95JVPEKRUWA'] # invalid

df['stock_t01']=10

df['purchase_t10'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t10'] = np.where(df['qty1']<0, df['qty1'], 0)
df['stock_t11'] = df['stock_t01']+df['purchase_t10']+df['sale_t10']

df['purchase_t20'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t20'] = np.where(df['qty2']<0, df['qty2'], 0)
df['stock_t21'] = df['stock_t11']+df['purchase_t20']+df['sale_t20']

df['avg_purch_t11'] = (2177.6 + df.purchase_t10*df.price10)/(10+df.purchase_t10)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)

df['pnl_t20'] = np.where(df.stock_t11>10, (df.stock_t11-10)*(df.price20-df.price10)+10*(df.price20-df.price0), df.stock_t11*(df.price20-df.price0))

df['live_info'] = np.where(df.experiment.str.contains('wc'), 0, 1)
df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])


# drop inactive people (4)
# df=df.drop([9,46,53,54])


df = df[['experiment', 'mturkid', 'winnings',
       'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret', 
       'price0', 'price10', 'price20', 'stock_t01',
       'purchase_t10', 'sale_t10', 'stock_t11', 'purchase_t20', 'sale_t20',
       'stock_t21', 'pnl_t20', 'live_info']]

df
43/421: df.loc([9,46,53,54])
43/422: df.ix([9,46,53,54])
43/423: df.ix[9,46,53,54]
43/424: df.loc[9,46,53,54]
43/425: df.loc[[9,46,53,54]]
43/426:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price10'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price20']=None
df['price20'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price20'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
df=df[df.mturkid!='A2H95JVPEKRUWA'] # invalid

df['stock_t01']=10

df['purchase_t10'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t10'] = np.where(df['qty1']<0, df['qty1'], 0)
df['stock_t11'] = df['stock_t01']+df['purchase_t10']+df['sale_t10']

df['purchase_t20'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t20'] = np.where(df['qty2']<0, df['qty2'], 0)
df['stock_t21'] = df['stock_t11']+df['purchase_t20']+df['sale_t20']

df['avg_purch_t11'] = (2177.6 + df.purchase_t10*df.price10)/(10+df.purchase_t10)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)

df['pnl_t20'] = np.where(df.stock_t11>10, (df.stock_t11-10)*(df.price20-df.price10)+10*(df.price20-df.price0), df.stock_t11*(df.price20-df.price0))

df['live_info'] = np.where(df.experiment.str.contains('wc'), 0, 1)
df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])


# drop inactive people (4)
# df=df.drop([9,46,53,54])


df = df[['experiment', 'mturkid', 'winnings',
       'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret', 
       'price0', 'price10', 'price20', 'stock_t01',
       'purchase_t10', 'sale_t10', 'stock_t11', 'purchase_t20', 'sale_t20',
       'stock_t21', 'pnl_t20', 'live_info']]

df
43/427:
df1 = df[df.experiment=="'rc_profit'"]
df2 = df[df.experiment=="'rc_loss'"]
df3 = df[df.experiment=="'wc_profit'"]
df4 = df[df.experiment=="'wc_loss'"]
43/428:
print(f"Vol. at T1 = {(df1.purchase_t10 + df1.sale_t10.abs()).sum()}. Shares bought = {df1.purchase_t10.sum()}. Shares sold = {df1.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df1.purchase_t20 + df1.sale_t20.abs()).sum()}. Shares bought = {df1.purchase_t20.sum()}. Shares sold = {df1.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df1.invratio_1.abs().mean()} ({df1.invratio_1.abs().std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df1.invratio_2.abs().mean()} ({df1.invratio_2.abs().std()})")
print(f"Mean buyer caution at T2 (0= cautious. 1= risky): {df1[df1.invratio_2>0].invratio_2.abs().mean()} ({df1[df1.invratio_2>0].invratio_2.abs().std()})")
print(f"Mean seller caution at T2 (0= cautious. 1= risky): {df1[df1.invratio_2<0].invratio_2.abs().mean()} ({df1[df1.invratio_2<0].invratio_2.abs().std()})")
43/429:
## INV RATIO: traded_volume/permissible_volume
maxbuy10 = math.floor(2500/219.65)
maxsell10 = 10
df['invratio_1'] = df.purchase_t10/maxbuy10
df['invratio_1'] = np.where(df.invratio_1==0, df.sale_t10/maxsell10, df.invratio_1)

avbl_cash = 2500 - (df.purchase_t10+df.sale_t10)*219.65
avbl_stock = 10+df.purchase_t10+df.sale_t10
maxbuy2 = avbl_cash/df.price20
df['invratio_2'] = df.purchase_t20/maxbuy2
df['invratio_2'] = np.where(df.invratio_2==0, df.sale_t20/avbl_stock, df.invratio_2)
df['invratio_2'].fillna(0, inplace=True)

# # DELTA IN INV_RATIO: measures change in investment behavior
df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])
43/430:
print(f"Vol. at T1 = {(df1.purchase_t10 + df1.sale_t10.abs()).sum()}. Shares bought = {df1.purchase_t10.sum()}. Shares sold = {df1.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df1.purchase_t20 + df1.sale_t20.abs()).sum()}. Shares bought = {df1.purchase_t20.sum()}. Shares sold = {df1.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df1.invratio_1.abs().mean()} ({df1.invratio_1.abs().std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df1.invratio_2.abs().mean()} ({df1.invratio_2.abs().std()})")
print(f"Mean buyer caution at T2 (0= cautious. 1= risky): {df1[df1.invratio_2>0].invratio_2.abs().mean()} ({df1[df1.invratio_2>0].invratio_2.abs().std()})")
print(f"Mean seller caution at T2 (0= cautious. 1= risky): {df1[df1.invratio_2<0].invratio_2.abs().mean()} ({df1[df1.invratio_2<0].invratio_2.abs().std()})")
43/431:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price10'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price20']=None
df['price20'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price20'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
df=df[df.mturkid!='A2H95JVPEKRUWA'] # invalid

df['stock_t01']=10

df['purchase_t10'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t10'] = np.where(df['qty1']<0, df['qty1'], 0)
df['stock_t11'] = df['stock_t01']+df['purchase_t10']+df['sale_t10']

df['purchase_t20'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t20'] = np.where(df['qty2']<0, df['qty2'], 0)
df['stock_t21'] = df['stock_t11']+df['purchase_t20']+df['sale_t20']

df['avg_purch_t11'] = (2177.6 + df.purchase_t10*df.price10)/(10+df.purchase_t10)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)

df['pnl_t20'] = np.where(df.stock_t11>10, (df.stock_t11-10)*(df.price20-df.price10)+10*(df.price20-df.price0), df.stock_t11*(df.price20-df.price0))

df['live_info'] = np.where(df.experiment.str.contains('wc'), 0, 1)
df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])


# drop inactive people (4)
df=df.drop([9,46,53,54])


df = df[['experiment', 'mturkid', 'winnings',
       'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret', 
       'price0', 'price10', 'price20', 'stock_t01',
       'purchase_t10', 'sale_t10', 'stock_t11', 'purchase_t20', 'sale_t20',
       'stock_t21', 'pnl_t20', 'live_info']]

df
43/432:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price10'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price20']=None
df['price20'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price20'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
df=df[df.mturkid!='A2H95JVPEKRUWA'] # invalid

df['stock_t01']=10

df['purchase_t10'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t10'] = np.where(df['qty1']<0, df['qty1'], 0)
df['stock_t11'] = df['stock_t01']+df['purchase_t10']+df['sale_t10']

df['purchase_t20'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t20'] = np.where(df['qty2']<0, df['qty2'], 0)
df['stock_t21'] = df['stock_t11']+df['purchase_t20']+df['sale_t20']

df['avg_purch_t11'] = (2177.6 + df.purchase_t10*df.price10)/(10+df.purchase_t10)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)

df['pnl_t20'] = np.where(df.stock_t11>10, (df.stock_t11-10)*(df.price20-df.price10)+10*(df.price20-df.price0), df.stock_t11*(df.price20-df.price0))

df['live_info'] = np.where(df.experiment.str.contains('wc'), 0, 1)
df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])


# drop inactive people (4)
df=df.drop([9,46,53,54])


df = df[['experiment', 'mturkid', 'winnings',
       'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret', 
       'price0', 'price10', 'price20', 'stock_t01',
       'purchase_t10', 'sale_t10', 'stock_t11', 'purchase_t20', 'sale_t20',
       'stock_t21', 'pnl_t20', 'live_info']]
43/433:
## INV RATIO: traded_volume/permissible_volume
maxbuy10 = math.floor(2500/219.65)
maxsell10 = 10
df['invratio_1'] = df.purchase_t10/maxbuy10
df['invratio_1'] = np.where(df.invratio_1==0, df.sale_t10/maxsell10, df.invratio_1)

avbl_cash = 2500 - (df.purchase_t10+df.sale_t10)*219.65
avbl_stock = 10+df.purchase_t10+df.sale_t10
maxbuy2 = avbl_cash/df.price20
df['invratio_2'] = df.purchase_t20/maxbuy2
df['invratio_2'] = np.where(df.invratio_2==0, df.sale_t20/avbl_stock, df.invratio_2)
df['invratio_2'].fillna(0, inplace=True)

# # DELTA IN INV_RATIO: measures change in investment behavior
df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])
43/434:
df1 = df[df.experiment=="'rc_profit'"]
df2 = df[df.experiment=="'rc_loss'"]
df3 = df[df.experiment=="'wc_profit'"]
df4 = df[df.experiment=="'wc_loss'"]
43/435:
print(f"Vol. at T1 = {(df.purchase_t10 + df.sale_t10.abs()).sum()}. Shares bought = {df.purchase_t10.sum()}. Shares sold = {df.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df.purchase_t20 + df.sale_t20.abs()).sum()}. Shares bought = {df.purchase_t20.sum()}. Shares sold = {df.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df.invratio_1.abs().mean()} ({df.invratio_1.abs().std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df.invratio_2.abs().mean()} ({df.invratio_2.abs().std()})")
print(f"Mean buyer caution at T2 (0= cautious. 1= risky): {df[df.invratio_2>0].invratio_2.abs().mean()} ({df[df.invratio_2>0].invratio_2.abs().std()})")
print(f"Mean seller caution at T2 (0= cautious. 1= risky): {df[df.invratio_2<0].invratio_2.abs().mean()} ({df[df.invratio_2<0].invratio_2.abs().std()})")
43/436:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1.abs()
dfz['q at T2'] = dfz.invratio_2.abs()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# plt.show()
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
plt.show()
43/437:
print(f"Vol. at T1 = {(df1.purchase_t10 + df1.sale_t10.abs()).sum()}. Shares bought = {df1.purchase_t10.sum()}. Shares sold = {df1.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df1.purchase_t20 + df1.sale_t20.abs()).sum()}. Shares bought = {df1.purchase_t20.sum()}. Shares sold = {df1.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df1.invratio_1.abs().mean()} ({df1.invratio_1.abs().std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df1.invratio_2.abs().mean()} ({df1.invratio_2.abs().std()})")
print(f"Mean buyer caution at T2 (0= cautious. 1= risky): {df1[df1.invratio_2>0].invratio_2.abs().mean()} ({df1[df1.invratio_2>0].invratio_2.abs().std()})")
print(f"Mean seller caution at T2 (0= cautious. 1= risky): {df1[df1.invratio_2<0].invratio_2.abs().mean()} ({df1[df1.invratio_2<0].invratio_2.abs().std()})")
43/438:
print(f"Vol. at T1 = {(df3.purchase_t10 + df3.sale_t10.abs()).sum()}. Shares bought = {df3.purchase_t10.sum()}. Shares sold = {df3.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df3.purchase_t20 + df3.sale_t20.abs()).sum()}. Shares bought = {df3.purchase_t20.sum()}. Shares sold = {df3.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df3.invratio_1.abs().mean()} ({df3.invratio_1.abs().std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df3.invratio_2.abs().mean()} ({df3.invratio_2.abs().std()})")
print(f"Mean buyer caution at T2 (0= cautious. 1= risky): {df3[df3.invratio_2>0].invratio_2.abs().mean()} ({df3[df3.invratio_2>0].invratio_2.abs().std()})")
print(f"Mean seller caution at T2 (0= cautious. 1= risky): {df3[df3.invratio_2<0].invratio_2.abs().mean()} ({df3[df3.invratio_2<0].invratio_2.abs().std()})")
43/439:
print(f"Vol. at T1 = {(df3.purchase_t10 + df3.sale_t10.abs()).sum()}. Shares bought = {df3.purchase_t10.sum()}. Shares sold = {df3.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df3.purchase_t20 + df3.sale_t20.abs()).sum()}. Shares bought = {df3.purchase_t20.sum()}. Shares sold = {df3.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df3.invratio_1.abs().mean()} ({df3.invratio_1.abs().std()})")
print(f"Mean buyer caution at T1 (0= cautious. 1= risky): {df3[df3.invratio_1>0].invratio_1.abs().mean()} ({df3[df3.invratio_1>0].invratio_1.abs().std()})")
print(f"Mean seller caution at T1 (0= cautious. 1= risky): {df3[df3.invratio_1<0].invratio_1.abs().mean()} ({df3[df3.invratio_1<0].invratio_1.abs().std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df3.invratio_2.abs().mean()} ({df3.invratio_2.abs().std()})")
print(f"Mean buyer caution at T2 (0= cautious. 1= risky): {df3[df3.invratio_2>0].invratio_2.abs().mean()} ({df3[df3.invratio_2>0].invratio_2.abs().std()})")
print(f"Mean seller caution at T2 (0= cautious. 1= risky): {df3[df3.invratio_2<0].invratio_2.abs().mean()} ({df3[df3.invratio_2<0].invratio_2.abs().std()})")
43/440:
print(f"Vol. at T1 = {(df1.purchase_t10 + df1.sale_t10.abs()).sum()}. Shares bought = {df1.purchase_t10.sum()}. Shares sold = {df1.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df1.purchase_t20 + df1.sale_t20.abs()).sum()}. Shares bought = {df1.purchase_t20.sum()}. Shares sold = {df1.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df1.invratio_1.abs().mean()} ({df1.invratio_1.abs().std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df1.invratio_2.abs().mean()} ({df1.invratio_2.abs().std()})")
print(f"Mean buyer caution at T2 (0= cautious. 1= risky): {df1.invratio_2.abs().mean()} ({df1.invratio_2.abs().std()})")
print(f"Mean seller caution at T2 (0= cautious. 1= risky): {df1.invratio_2.abs().mean()} ({df1.invratio_2.abs().std()})")
43/441:
print(f"Vol. at T1 = {(df1.purchase_t10 + df1.sale_t10.abs()).sum()}. Shares bought = {df1.purchase_t10.sum()}. Shares sold = {df1.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df1.purchase_t20 + df1.sale_t20.abs()).sum()}. Shares bought = {df1.purchase_t20.sum()}. Shares sold = {df1.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df1.invratio_1.abs().mean()} ({df1.invratio_1.abs().std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df1.invratio_2.abs().mean()} ({df1.invratio_2.abs().std()})")
print(f"Mean buyer caution at T2 (0= cautious. 1= risky): {df1[df1.invratio_2>0].invratio_2.abs().mean()} ({df1[df1.invratio_2>0].invratio_2.abs().std()})")
print(f"Mean seller caution at T2 (0= cautious. 1= risky): {df[df1.invratio_2<0].invratio_2.abs().mean()} ({df1[df1.invratio_2<0].invratio_2.abs().std()})")
43/442:
print(f"Vol. at T1 = {(df1.purchase_t10 + df1.sale_t10.abs()).sum()}. Shares bought = {df1.purchase_t10.sum()}. Shares sold = {df1.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df1.purchase_t20 + df1.sale_t20.abs()).sum()}. Shares bought = {df1.purchase_t20.sum()}. Shares sold = {df1.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df1.invratio_1.abs().mean()} ({df1.invratio_1.abs().std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df1.invratio_2.abs().mean()} ({df1.invratio_2.abs().std()})")
print(f"Mean buyer caution at T2 (0= cautious. 1= risky): {df1[df1.invratio_2>0].invratio_2.abs().mean()} ({df1[df1.invratio_2>0].invratio_2.abs().std()})")
print(f"Mean seller caution at T2 (0= cautious. 1= risky): {df1[df1.invratio_2<0].invratio_2.abs().mean()} ({df1[df1.invratio_2<0].invratio_2.abs().std()})")
43/443:
print(f"Vol. at T1 = {(df1.purchase_t10 + df1.sale_t10.abs()).sum()}. Shares bought = {df1.purchase_t10.sum()}. Shares sold = {df1.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df1.purchase_t20 + df1.sale_t20.abs()).sum()}. Shares bought = {df1.purchase_t20.sum()}. Shares sold = {df1.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df1.invratio_1.abs().mean()} ({df1.invratio_1.abs().std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df1.invratio_2.abs().mean()} ({df1.invratio_2.abs().std()})")
print(f"Mean buyer caution at T2 (0= cautious. 1= risky): {df1[df1.sale_t20==0].invratio_2.abs().mean()} ({df1[df1.sale_t20==0].invratio_2.abs().std()})")
print(f"Mean seller caution at T2 (0= cautious. 1= risky): {df1[df1.purchase_t20==0].invratio_2.abs().mean()} ({df1[df1.purchase_t20==0].invratio_2.abs().std()})")
43/444:
print(f"Vol. at T1 = {(df1.purchase_t10 + df1.sale_t10.abs()).sum()}. Shares bought = {df1.purchase_t10.sum()}. Shares sold = {df1.sale_t10.abs().sum()}")
print(f"Vol. at T2 = {(df1.purchase_t20 + df1.sale_t20.abs()).sum()}. Shares bought = {df1.purchase_t20.sum()}. Shares sold = {df1.sale_t20.abs().sum()}")

print(f"Mean caution at T1 (0= cautious. 1= risky): {df1.invratio_1.abs().mean()} ({df1.invratio_1.abs().std()})")
print(f"Mean buyer caution at T2 (0= cautious. 1= risky): {df1[df1.sale_t10==0].invratio_1.abs().mean()} ({df1[df1.sale_t10==0].invratio_1.abs().std()})")
print(f"Mean seller caution at T2 (0= cautious. 1= risky): {df1[df1.purchase_t10==0].invratio_1.abs().mean()} ({df1[df1.purchase_t10==0].invratio_1.abs().std()})")
print(f"Mean caution at T2 (0= cautious. 1= risky): {df1.invratio_2.abs().mean()} ({df1.invratio_2.abs().std()})")
print(f"Mean buyer caution at T2 (0= cautious. 1= risky): {df1[df1.sale_t20==0].invratio_2.abs().mean()} ({df1[df1.sale_t20==0].invratio_2.abs().std()})")
print(f"Mean seller caution at T2 (0= cautious. 1= risky): {df1[df1.purchase_t20==0].invratio_2.abs().mean()} ({df1[df1.purchase_t20==0].invratio_2.abs().std()})")
43/445:
for exp in df.experiment:
    print(f"########{exp}################")
    dfz = df[df.experiment==exp]
    
    t1=dfz[(dfz.purchase_t10+dfz.sale_t10)>0]
    b_t1 = t1[t1.sale_t10==0]
    s_t1 = t1[t1.purchase_t10==0]
    print(f"Traders at T1 {t1.shape[0]}")
    print(f"Trade volume T1 {t1.purchase_t10.sum()+t1.sale_t10.abs().sum()}")
    print(f"Buy volume T1 {t1.purchase_t10.sum()} ({t1.purchase_t10.sum()*100/(t1.purchase_t10.sum()+t1.sale_t10.abs().sum())}%)")
    print(f"Sell volume T1 {t1.sale_t10.sum()} ({t1.purchase_t10.sum()*100/(t1.purchase_t10.sum()+t1.sale_t10.abs().sum())}%)")
    
    print(f"Mean q {round(t1.inv_ratio1.abs().mean(),2)}, ({round(t1.inv_ratio1.abs().std(),2)})")  
    print(f"Mean buyer q {round(b_t1.inv_ratio1.abs().mean(),2)}, ({round(b_t1.inv_ratio1.abs().std(),2)})")
    print(f"Mean seller q {round(s_t1.inv_ratio1.abs().mean(),2)}, ({round(s_t1.inv_ratio1.abs().std(),2)})")    
    
    
    t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
    b_t2 = t2[t2.sale_t20==0]
    s_t2 = t2[t2.purchase_t20==0]
    print(f"Traders at T2 {t2.shape[0]}")
    print(f"Trade volume T2 {t2.purchase_t20.sum()+t2.sale_t20.abs().sum()}")
    print(f"Buy volume T2 {t2.purchase_t20.sum()} ({t2.purchase_t20.sum()*100/(t2.purchase_t20.sum()+t2.sale_t20.abs().sum())}%)")
    print(f"Sell volume T2 {t2.sale_t20.sum()} ({t2.purchase_t20.sum()*100/(t2.purchase_t20.sum()+t2.sale_t20.abs().sum())}%)")

    print(f"Mean q {round(t2.inv_ratio2.abs().mean(),2)}, ({round(t2.inv_ratio2.abs().std(),2)})")  
    print(f"Mean buyer q {round(b_t2.inv_ratio2.abs().mean(),2)}, ({round(b_t2.inv_ratio2.abs().std(),2)})")
    print(f"Mean seller q {round(s_t2.inv_ratio2.abs().mean(),2)}, ({round(s_t2.inv_ratio2.abs().std(),2)})")
43/446:
for exp in df.experiment:
    print(f"########{exp}################")
    dfz = df[df.experiment==exp]
    
    t1=dfz[(dfz.purchase_t10+dfz.sale_t10)>0]
    b_t1 = t1[t1.sale_t10==0]
    s_t1 = t1[t1.purchase_t10==0]
    print(f"Traders at T1 {t1.shape[0]}")
    print(f"Trade volume T1 {t1.purchase_t10.sum()+t1.sale_t10.abs().sum()}")
    print(f"Buy volume T1 {t1.purchase_t10.sum()} ({t1.purchase_t10.sum()*100/(t1.purchase_t10.sum()+t1.sale_t10.abs().sum())}%)")
    print(f"Sell volume T1 {t1.sale_t10.sum()} ({t1.purchase_t10.sum()*100/(t1.purchase_t10.sum()+t1.sale_t10.abs().sum())}%)")
    
    print(f"Mean q {round(t1.invratio_1.abs().mean(),2)}, ({round(t1.invratio_1.abs().std(),2)})")  
    print(f"Mean buyer q {round(b_t1.invratio_1.abs().mean(),2)}, ({round(b_t1.invratio_1.abs().std(),2)})")
    print(f"Mean seller q {round(s_t1.invratio_1.abs().mean(),2)}, ({round(s_t1.invratio_1.abs().std(),2)})")    
    
    
    t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
    b_t2 = t2[t2.sale_t20==0]
    s_t2 = t2[t2.purchase_t20==0]
    print(f"Traders at T2 {t2.shape[0]}")
    print(f"Trade volume T2 {t2.purchase_t20.sum()+t2.sale_t20.abs().sum()}")
    print(f"Buy volume T2 {t2.purchase_t20.sum()} ({t2.purchase_t20.sum()*100/(t2.purchase_t20.sum()+t2.sale_t20.abs().sum())}%)")
    print(f"Sell volume T2 {t2.sale_t20.sum()} ({t2.purchase_t20.sum()*100/(t2.purchase_t20.sum()+t2.sale_t20.abs().sum())}%)")

    print(f"Mean q {round(t2.invratio_2.abs().mean(),2)}, ({round(t2.invratio_2.abs().std(),2)})")  
    print(f"Mean buyer q {round(b_t2.invratio_2.abs().mean(),2)}, ({round(b_t2.invratio_2.abs().std(),2)})")
    print(f"Mean seller q {round(s_t2.invratio_2.abs().mean(),2)}, ({round(s_t2.invratio_2.abs().std(),2)})")
43/447:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price10'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price20']=None
df['price20'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price20'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
df=df[df.mturkid!='A2H95JVPEKRUWA'] # invalid

df['stock_t01']=10

df['purchase_t10'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t10'] = np.where(df['qty1']<0, df['qty1'], 0)
df['sale_t10'] = df['sale_t10'].abs()
df['stock_t11'] = df['stock_t01']+df['purchase_t10']+df['sale_t10']

df['purchase_t20'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t20'] = np.where(df['qty2']<0, df['qty2'], 0)
df['sale_t20'] = df['sale_t20'].abs()
df['stock_t21'] = df['stock_t11']+df['purchase_t20']+df['sale_t20']

df['avg_purch_t11'] = (2177.6 + df.purchase_t10*df.price10)/(10+df.purchase_t10)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)

df['pnl_t20'] = np.where(df.stock_t11>10, (df.stock_t11-10)*(df.price20-df.price10)+10*(df.price20-df.price0), df.stock_t11*(df.price20-df.price0))

df['live_info'] = np.where(df.experiment.str.contains('wc'), 0, 1)
df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])


# drop inactive people (4)
df=df.drop([9,46,53,54])


df = df[['experiment', 'mturkid', 'winnings',
       'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret', 
       'price0', 'price10', 'price20', 'stock_t01',
       'purchase_t10', 'sale_t10', 'stock_t11', 'purchase_t20', 'sale_t20',
       'stock_t21', 'pnl_t20', 'live_info']]
43/448:
## INV RATIO: traded_volume/permissible_volume
maxbuy10 = math.floor(2500/219.65)
maxsell10 = 10
df['invratio_1'] = df.purchase_t10/maxbuy10
df['invratio_1'] = np.where(df.invratio_1==0, df.sale_t10/maxsell10, df.invratio_1)

avbl_cash = 2500 - (df.purchase_t10+df.sale_t10)*219.65
avbl_stock = 10+df.purchase_t10+df.sale_t10
maxbuy2 = avbl_cash/df.price20
df['invratio_2'] = df.purchase_t20/maxbuy2
df['invratio_2'] = np.where(df.invratio_2==0, df.sale_t20/avbl_stock, df.invratio_2)
df['invratio_2'].fillna(0, inplace=True)

# # DELTA IN INV_RATIO: measures change in investment behavior
df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])
43/449:
for exp in df.experiment:
    print(f"########{exp}################")
    dfz = df[df.experiment==exp]
    
    t1=dfz[(dfz.purchase_t10+dfz.sale_t10>0]
    b_t1 = t1[t1.sale_t10==0]
    s_t1 = t1[t1.purchase_t10==0]
    print(f"Traders at T1 {t1.shape[0]}")
    print(f"Trade volume T1 {t1.purchase_t10.sum()+t1.sale_t10.abs().sum()}")
    print(f"Buy volume T1 {t1.purchase_t10.sum()} ({t1.purchase_t10.sum()*100/(t1.purchase_t10.sum()+t1.sale_t10.abs().sum())}%)")
    print(f"Sell volume T1 {t1.sale_t10.sum()} ({t1.sale_t10.sum()*100/(t1.purchase_t10.sum()+t1.sale_t10.abs().sum())}%)")
    
    print(f"Mean q {round(t1.invratio_1.abs().mean(),2)}, ({round(t1.invratio_1.abs().std(),2)})")  
    print(f"Mean buyer q {round(b_t1.invratio_1.abs().mean(),2)}, ({round(b_t1.invratio_1.abs().std(),2)})")
    print(f"Mean seller q {round(s_t1.invratio_1.abs().mean(),2)}, ({round(s_t1.invratio_1.abs().std(),2)})")    
    
    
    t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
    b_t2 = t2[t2.sale_t20==0]
    s_t2 = t2[t2.purchase_t20==0]
    print(f"Traders at T2 {t2.shape[0]}")
    print(f"Trade volume T2 {t2.purchase_t20.sum()+t2.sale_t20.abs().sum()}")
    print(f"Buy volume T2 {t2.purchase_t20.sum()} ({t2.purchase_t20.sum()*100/(t2.purchase_t20.sum()+t2.sale_t20.abs().sum())}%)")
    print(f"Sell volume T2 {t2.sale_t20.sum()} ({t2.purchase_t20.sum()*100/(t2.purchase_t20.sum()+t2.sale_t20.abs().sum())}%)")

    print(f"Mean q {round(t2.invratio_2.abs().mean(),2)}, ({round(t2.invratio_2.abs().std(),2)})")  
    print(f"Mean buyer q {round(b_t2.invratio_2.abs().mean(),2)}, ({round(b_t2.invratio_2.abs().std(),2)})")
    print(f"Mean seller q {round(s_t2.invratio_2.abs().mean(),2)}, ({round(s_t2.invratio_2.abs().std(),2)})")
43/450:
for exp in df.experiment:
    print(f"########{exp}################")
    dfz = df[df.experiment==exp]
    
    t1=dfz[(dfz.purchase_t10+dfz.sale_t10)>0]
    b_t1 = t1[t1.sale_t10==0]
    s_t1 = t1[t1.purchase_t10==0]
    print(f"Traders at T1 {t1.shape[0]}")
    print(f"Trade volume T1 {t1.purchase_t10.sum()+t1.sale_t10.abs().sum()}")
    print(f"Buy volume T1 {t1.purchase_t10.sum()} ({t1.purchase_t10.sum()*100/(t1.purchase_t10.sum()+t1.sale_t10.abs().sum())}%)")
    print(f"Sell volume T1 {t1.sale_t10.sum()} ({t1.sale_t10.sum()*100/(t1.purchase_t10.sum()+t1.sale_t10.abs().sum())}%)")
    
    print(f"Mean q {round(t1.invratio_1.abs().mean(),2)}, ({round(t1.invratio_1.abs().std(),2)})")  
    print(f"Mean buyer q {round(b_t1.invratio_1.abs().mean(),2)}, ({round(b_t1.invratio_1.abs().std(),2)})")
    print(f"Mean seller q {round(s_t1.invratio_1.abs().mean(),2)}, ({round(s_t1.invratio_1.abs().std(),2)})")    
    
    
    t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
    b_t2 = t2[t2.sale_t20==0]
    s_t2 = t2[t2.purchase_t20==0]
    print(f"Traders at T2 {t2.shape[0]}")
    print(f"Trade volume T2 {t2.purchase_t20.sum()+t2.sale_t20.abs().sum()}")
    print(f"Buy volume T2 {t2.purchase_t20.sum()} ({t2.purchase_t20.sum()*100/(t2.purchase_t20.sum()+t2.sale_t20.abs().sum())}%)")
    print(f"Sell volume T2 {t2.sale_t20.sum()} ({t2.purchase_t20.sum()*100/(t2.purchase_t20.sum()+t2.sale_t20.abs().sum())}%)")

    print(f"Mean q {round(t2.invratio_2.abs().mean(),2)}, ({round(t2.invratio_2.abs().std(),2)})")  
    print(f"Mean buyer q {round(b_t2.invratio_2.abs().mean(),2)}, ({round(b_t2.invratio_2.abs().std(),2)})")
    print(f"Mean seller q {round(s_t2.invratio_2.abs().mean(),2)}, ({round(s_t2.invratio_2.abs().std(),2)})")
43/451: df.invratio_2
43/452: df.ix[35,27]
43/453: df.loc[35,27]
43/454: df.loc[[35,27]]
43/455:
## INV RATIO: traded_volume/permissible_volume
maxbuy10 = math.floor(2500/219.65)
maxsell10 = 10
df['invratio_1'] = df.purchase_t10/maxbuy10
df['invratio_1'] = np.where(df.invratio_1==0, df.sale_t10/maxsell10, df.invratio_1)

avbl_cash = 2500 - (df.purchase_t10-df.sale_t10)*219.65
avbl_stock = 10+df.purchase_t10-df.sale_t10
maxbuy2 = avbl_cash/df.price20
df['invratio_2'] = df.purchase_t20/maxbuy2
df['invratio_2'] = np.where(df.invratio_2==0, df.sale_t20/avbl_stock, df.invratio_2)
df['invratio_2'].fillna(0, inplace=True)

# # DELTA IN INV_RATIO: measures change in investment behavior
df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])
43/456:
for exp in df.experiment:
    print(f"########{exp}################")
    dfz = df[df.experiment==exp]
    
    t1=dfz[(dfz.purchase_t10+dfz.sale_t10)>0]
    b_t1 = t1[t1.sale_t10==0]
    s_t1 = t1[t1.purchase_t10==0]
    print(f"Traders at T1 {t1.shape[0]}")
    print(f"Trade volume T1 {t1.purchase_t10.sum()+t1.sale_t10.abs().sum()}")
    print(f"Buy volume T1 {t1.purchase_t10.sum()} ({t1.purchase_t10.sum()*100/(t1.purchase_t10.sum()+t1.sale_t10.abs().sum())}%)")
    print(f"Sell volume T1 {t1.sale_t10.sum()} ({t1.sale_t10.sum()*100/(t1.purchase_t10.sum()+t1.sale_t10.abs().sum())}%)")
    
    print(f"Mean q {round(t1.invratio_1.abs().mean(),2)}, ({round(t1.invratio_1.abs().std(),2)})")  
    print(f"Mean buyer q {round(b_t1.invratio_1.abs().mean(),2)}, ({round(b_t1.invratio_1.abs().std(),2)})")
    print(f"Mean seller q {round(s_t1.invratio_1.abs().mean(),2)}, ({round(s_t1.invratio_1.abs().std(),2)})")    
    
    
    t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
    b_t2 = t2[t2.sale_t20==0]
    s_t2 = t2[t2.purchase_t20==0]
    print(f"Traders at T2 {t2.shape[0]}")
    print(f"Trade volume T2 {t2.purchase_t20.sum()+t2.sale_t20.abs().sum()}")
    print(f"Buy volume T2 {t2.purchase_t20.sum()} ({t2.purchase_t20.sum()*100/(t2.purchase_t20.sum()+t2.sale_t20.abs().sum())}%)")
    print(f"Sell volume T2 {t2.sale_t20.sum()} ({t2.purchase_t20.sum()*100/(t2.purchase_t20.sum()+t2.sale_t20.abs().sum())}%)")

    print(f"Mean q {round(t2.invratio_2.abs().mean(),2)}, ({round(t2.invratio_2.abs().std(),2)})")  
    print(f"Mean buyer q {round(b_t2.invratio_2.abs().mean(),2)}, ({round(b_t2.invratio_2.abs().std(),2)})")
    print(f"Mean seller q {round(s_t2.invratio_2.abs().mean(),2)}, ({round(s_t2.invratio_2.abs().std(),2)})")
43/457:
for exp in df.experiment.unique():
    print(f"########{exp}################")
    dfz = df[df.experiment==exp]
    
    t1=dfz[(dfz.purchase_t10+dfz.sale_t10)>0]
    b_t1 = t1[t1.sale_t10==0]
    s_t1 = t1[t1.purchase_t10==0]
    print(f"Traders at T1 {t1.shape[0]}")
    print(f"Trade volume T1 {t1.purchase_t10.sum()+t1.sale_t10.abs().sum()}")
    print(f"Buy volume T1 {t1.purchase_t10.sum()} ({t1.purchase_t10.sum()*100/(t1.purchase_t10.sum()+t1.sale_t10.abs().sum())}%)")
    print(f"Sell volume T1 {t1.sale_t10.sum()} ({t1.sale_t10.sum()*100/(t1.purchase_t10.sum()+t1.sale_t10.abs().sum())}%)")
    
    print(f"Mean q {round(t1.invratio_1.abs().mean(),2)}, ({round(t1.invratio_1.abs().std(),2)})")  
    print(f"Mean buyer q {round(b_t1.invratio_1.abs().mean(),2)}, ({round(b_t1.invratio_1.abs().std(),2)})")
    print(f"Mean seller q {round(s_t1.invratio_1.abs().mean(),2)}, ({round(s_t1.invratio_1.abs().std(),2)})")    
    
    
    t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
    b_t2 = t2[t2.sale_t20==0]
    s_t2 = t2[t2.purchase_t20==0]
    print(f"Traders at T2 {t2.shape[0]}")
    print(f"Trade volume T2 {t2.purchase_t20.sum()+t2.sale_t20.abs().sum()}")
    print(f"Buy volume T2 {t2.purchase_t20.sum()} ({t2.purchase_t20.sum()*100/(t2.purchase_t20.sum()+t2.sale_t20.abs().sum())}%)")
    print(f"Sell volume T2 {t2.sale_t20.sum()} ({t2.purchase_t20.sum()*100/(t2.purchase_t20.sum()+t2.sale_t20.abs().sum())}%)")

    print(f"Mean q {round(t2.invratio_2.abs().mean(),2)}, ({round(t2.invratio_2.abs().std(),2)})")  
    print(f"Mean buyer q {round(b_t2.invratio_2.abs().mean(),2)}, ({round(b_t2.invratio_2.abs().std(),2)})")
    print(f"Mean seller q {round(s_t2.invratio_2.abs().mean(),2)}, ({round(s_t2.invratio_2.abs().std(),2)})")
43/458:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1.abs()
dfz['q at T2'] = dfz.invratio_2.abs()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz[(dfz.purchase_t10+dfz.sale_t10)>0], order=['Live','WaterCooler'], ax=ax1)
# plt.show()
sns.boxplot(x='Condition', y='q at T2', data=dfz[(dfz.purchase_t20+dfz.sale_t20)>0], order=['Live','WaterCooler'], ax=ax2)
plt.show()
43/459:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1.abs()
dfz['q at T2'] = dfz.invratio_2.abs()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# plt.show()
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
plt.show()
43/460:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1.abs()
dfz['q at T2'] = dfz.invratio_2.abs()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# plt.show()
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
plt.show()
43/461: df1.invratio_1.describe()
43/462: df1.invratio_1
43/463: df1.head()
43/464:
df1 = df[df.experiment=="'rc_profit'"]
df2 = df[df.experiment=="'rc_loss'"]
df3 = df[df.experiment=="'wc_profit'"]
df4 = df[df.experiment=="'wc_loss'"]
43/465: df1.head()
43/466: df1.invratio_1.describe()
43/467:
dfz = df[df.experiment.str.contains('profit')]
dfz.invratio_1.describe()
43/468:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1.abs()
dfz['q at T2'] = dfz.invratio_2.abs()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# plt.show()
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
plt.show()
43/469: df.to_csv('results_feat.csv')
43/470:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2
print(dfz[dfz.Condition=='Live'].invratio_1.median())
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# plt.show()
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
plt.show()
43/471:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2
print(dfz[dfz.Condition=='WaterCooler'].invratio_1.median())
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# plt.show()
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
plt.show()
43/472:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2
print(dfz[dfz.Condition=='WaterCooler'].invratio_2.median())
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# plt.show()
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
plt.show()
43/473:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2
print(dfz[dfz.Condition=='Live'].invratio_2.median())
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# plt.show()
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
plt.show()
43/474:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2
print(dfz[dfz.Condition=='Live'].invratio_2.median())
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# plt.show()
# sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)

sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1)
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1)

sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2)

plt.show()
43/475:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2
print(dfz[dfz.Condition=='Live'].invratio_2.median())
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# plt.show()
# sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)

sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, hist=False)
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, hist=False)

sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, hist=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, hist=False)

plt.show()
43/476:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2
print(dfz[dfz.Condition=='Live'].invratio_2.median())
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# plt.show()
# sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)

sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, hist=False, label='T1')
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, hist=False, label='T2')

sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, hist=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, hist=False)
ax1.set_title('Live')
plt.show()
43/477:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2
print(dfz[dfz.Condition=='Live'].invratio_2.median())
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# plt.show()
# sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)

sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, hist=False, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, hist=False, label='T2', axlabel=False)

sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, hist=False, label='T1')
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, hist=False, label='T2', axlabel=False)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/478:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2
print(dfz[dfz.Condition=='Live'].invratio_2.median())
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# plt.show()
# sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)

sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, hist=False, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, hist=False, label='T2', axlabel=False)

sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, hist=False, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, hist=False, label='T2', axlabel=False)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/479:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, hist=False, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, hist=False, label='T2', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, hist=False, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, hist=False, label='T2', axlabel=False)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/480:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1, axlabel=False)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2, axlabel=False)
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, hist=False, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, hist=False, label='T2', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, hist=False, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, hist=False, label='T2', axlabel=False)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/481:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, hist=False, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, hist=False, label='T2', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, hist=False, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, hist=False, label='T2', axlabel=False)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/482:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1, palette='muted')
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, hist=False, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, hist=False, label='T2', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, hist=False, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, hist=False, label='T2', axlabel=False)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/483:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, hist=False, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, hist=False, label='T2', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, hist=False, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, hist=False, label='T2', axlabel=False)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/484:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, hist=False, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, hist=False, label='T2', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, hist=False, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, hist=False, label='T2', axlabel=False)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/485:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, hist=False, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, hist=False, label='T2', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, hist=False, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, hist=False, label='T2', axlabel=False)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/486:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, bins=6, label='T2', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, hist=False, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, hist=False, label='T2', axlabel=False)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/487:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, bins=6, label='T2', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, bins=6, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins-6, label='T2', axlabel=False)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/488:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, bins=6, label='T2', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, bins=6, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, label='T2', axlabel=False)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/489:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, bins=6, label='T2', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, bins=6, label='T1', axlabel=False, kde=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, label='T2', axlabel=False, kde=False)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/490:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, bins=6, label='T2', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, bins=6, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, label='T2', axlabel=False)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/491:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, bins=6, label='T2', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, bins=6, label='T1',)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, label='T2',)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/492:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, bins=6, label='T2', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, bins=6, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, label='T2', axlabel=False)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/493:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, bins=6, label='T2', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, bins=6, label='T1', axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, label='T2', axlabel=False, kde_kws={"label": "T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/494:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, bins=6, label='T2', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, bins=6, label='T1', axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, label='T2', axlabel=False)#, kde_kws={"label": "T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/495:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, bins=6, label='T2', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, bins=6, label='T1', axlabel=False)#, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, label='T2', axlabel=False)#, kde_kws={"label": "T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/496:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, label='T1', axlabel=False)
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, bins=6, label='T2', axlabel=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/497:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T2"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/498:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T2"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/499:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T2"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/500:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
# ax1.set_title('T1')
# ax2.set_title('T2')
# plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T2"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/501:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)

sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T2"})

sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()


sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "live"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "wc"})
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "live"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "wc"})
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()
43/502:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)

sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T2"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "live"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "wc"})
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "live"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "wc"})
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()
43/503:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)

sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T2"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "live"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "wc"})
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "live"})
# sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "wc"})
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()
43/504:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)

sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T2"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "live"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "wc"})
# sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "live"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "wc"})
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()
43/505:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)

sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T2"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "live"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "wc"})
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "live"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "wc"})
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()
43/506:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2

dftmp = dfz.copy()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='variable', y='value', data=pd.melt(dfz[dfz.Condition=='Live'][['invratio_1','invratio_2']]) ,  ax=ax1)
# sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['T1','T2'], ax=ax2)
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()



fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
ax1.set_title('T1')
ax2.set_title('T2')
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)

sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T2"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/507:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2

dftmp = dfz.copy()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='variable', y='value', data=pd.melt(dfz[dfz.Condition=='Live'][['invratio_1','invratio_2']]) ,  ax=ax1)
sns.boxplot(x='variable', y='value', data=pd.melt(dfz[dfz.Condition=='WaterCooler'][['invratio_1','invratio_2']]) ,  ax=ax1)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()



# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
# ax1.set_title('T1')
# ax2.set_title('T2')
# plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)

sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T2"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/508:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2

dftmp = dfz.copy()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='variable', y='value', data=pd.melt(dfz[dfz.Condition=='Live'][['invratio_1','invratio_2']]) ,  ax=ax1)
sns.boxplot(x='variable', y='value', data=pd.melt(dfz[dfz.Condition=='WaterCooler'][['invratio_1','invratio_2']]) ,  ax=ax2)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()



# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Condition', y='q at T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# sns.boxplot(x='Condition', y='q at T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
# ax1.set_title('T1')
# ax2.set_title('T2')
# plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)

sns.distplot(dfz[dfz.Condition=='Live']['q at T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='Live']['q at T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T2"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['q at T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/509:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

dftmp = dfz.copy()


fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='variable', y='value', data=pd.melt(dfz[dfz.Condition=='Live'][['T1','T2']]) ,  ax=ax1)
sns.boxplot(x='variable', y='value', data=pd.melt(dfz[dfz.Condition=='WaterCooler'][['T1','T2']]) ,  ax=ax2)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()



# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Condition', y='T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# sns.boxplot(x='Condition', y='T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
# ax1.set_title('T1')
# ax2.set_title('T2')
# plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)

sns.distplot(dfz[dfz.Condition=='Live']['T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='Live']['T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T2"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/510:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

dftmp = dfz.copy()
dflive = pd.melt(dfz[dfz.Condition=='Live'][['T1','T2']])
dflive.columns=['Time of Trade', 'q']
dfwc=pd.melt(dfz[dfz.Condition=='WaterCooler'][['T1','T2']])
dfwc.columns=['Time of Trade', 'q']

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Time of Trade', y='q', data=dflive,  ax=ax1)
sns.boxplot(x='Time of Trade', y='q', data=dfwc,  ax=ax2)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()



# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Condition', y='T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# sns.boxplot(x='Condition', y='T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
# ax1.set_title('T1')
# ax2.set_title('T2')
# plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)

sns.distplot(dfz[dfz.Condition=='Live']['T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='Live']['T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T2"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/511:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

dftmp = dfz.copy()
dflive = pd.melt(dfz[dfz.Condition=='Live'][['T1','T2']])
dflive.columns=['Time of Trade', 'q']
dfwc=pd.melt(dfz[dfz.Condition=='WaterCooler'][['T1','T2']])
dfwc.columns=['Time of Trade', 'q']

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Time of Trade', y='q', data=dflive,  ax=ax1)
sns.boxplot(x='Time of Trade', y='q', data=dfwc,  ax=ax2)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()



# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Condition', y='T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# sns.boxplot(x='Condition', y='T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
# ax1.set_title('T1')
# ax2.set_title('T2')
# plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)

sns.distplot(dfz[dfz.Condition=='Live']['T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='Live']['T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T2"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/512:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

dftmp = dfz.copy()
dflive = pd.melt(dfz[dfz.Condition=='Live'][['T1','T2']])
dflive.columns=['Time of Trade', 'q']
dfwc=pd.melt(dfz[dfz.Condition=='WaterCooler'][['T1','T2']])
dfwc.columns=['Time of Trade', 'q']

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Time of Trade', y='q', data=dflive,  ax=ax1)
sns.boxplot(x='Time of Trade', y='q', data=dfwc,  ax=ax2)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()



# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Condition', y='T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# sns.boxplot(x='Condition', y='T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
# ax1.set_title('T1')
# ax2.set_title('T2')
# plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
#sellers
dfz = dfz[dfz.purchase_t10==0]
sns.distplot(dfz[dfz.Condition=='Live']['T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='Live']['T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T2"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/513:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

dftmp = dfz.copy()
dflive = pd.melt(dfz[dfz.Condition=='Live'][['T1','T2']])
dflive.columns=['Time of Trade', 'q']
dfwc=pd.melt(dfz[dfz.Condition=='WaterCooler'][['T1','T2']])
dfwc.columns=['Time of Trade', 'q']

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Time of Trade', y='q', data=dflive,  ax=ax1)
sns.boxplot(x='Time of Trade', y='q', data=dfwc,  ax=ax2)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()



# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Condition', y='T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# sns.boxplot(x='Condition', y='T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
# ax1.set_title('T1')
# ax2.set_title('T2')
# plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
#sellers
dfz = dfz[dfz.purchase_t10==0]
sns.distplot(dfz[dfz.Condition=='Live']['T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T1"}, hist=False)
sns.distplot(dfz[dfz.Condition=='Live']['T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T2"}, hist=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T1"}, hist=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T2"}, hist=False)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/514:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

dftmp = dfz.copy()
dflive = pd.melt(dfz[dfz.Condition=='Live'][['T1','T2']])
dflive.columns=['Time of Trade', 'q']
dfwc=pd.melt(dfz[dfz.Condition=='WaterCooler'][['T1','T2']])
dfwc.columns=['Time of Trade', 'q']

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Time of Trade', y='q', data=dflive,  ax=ax1)
sns.boxplot(x='Time of Trade', y='q', data=dfwc,  ax=ax2)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()



# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Condition', y='T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# sns.boxplot(x='Condition', y='T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
# ax1.set_title('T1')
# ax2.set_title('T2')
# plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
#sellers
dfz = dfz[dfz.purchase_t10==0]
print(dfz.head())
sns.distplot(dfz[dfz.Condition=='Live']['T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T1"}, hist=False)
sns.distplot(dfz[dfz.Condition=='Live']['T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T2"}, hist=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T1"}, hist=False)
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T2"}, hist=False)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/515:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

dftmp = dfz.copy()
dflive = pd.melt(dfz[dfz.Condition=='Live'][['T1','T2']])
dflive.columns=['Time of Trade', 'q']
dfwc=pd.melt(dfz[dfz.Condition=='WaterCooler'][['T1','T2']])
dfwc.columns=['Time of Trade', 'q']

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Time of Trade', y='q', data=dflive,  ax=ax1)
sns.boxplot(x='Time of Trade', y='q', data=dfwc,  ax=ax2)
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()



# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Condition', y='T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# sns.boxplot(x='Condition', y='T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
# ax1.set_title('T1')
# ax2.set_title('T2')
# plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)

sns.distplot(dfz[dfz.Condition=='Live']['T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='Live']['T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T2"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/516:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
b_t2 = t2[t2.sale_t20==0]
s_t2 = t2[t2.purchase_t20==0] #sellers at t2

sns.distplot(s_t2[s_t2.Condition=='Live']['T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T2"})
sns.distplot(s_t2[s_t2.Condition=='WaterCooler']['T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T2"})
43/517:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
b_t2 = t2[t2.sale_t20==0]
s_t2 = t2[t2.purchase_t20==0] #sellers at t2

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(s_t2[s_t2.Condition=='Live']['T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T2"})
sns.distplot(s_t2[s_t2.Condition=='WaterCooler']['T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T2"})
plt.show()
43/518:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
b_t2 = t2[t2.sale_t20==0]
s_t2 = t2[t2.purchase_t20==0] #sellers at t2


sns.distplot(s_t2[s_t2.Condition=='Live']['T2'], axlabel=False, label='Live')
sns.distplot(s_t2[s_t2.Condition=='WaterCooler']['T2'], axlabel=False, label='WaterCooler')
plt.show()
43/519:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
b_t2 = t2[t2.sale_t20==0]
s_t2 = t2[t2.purchase_t20==0] #sellers at t2


sns.distplot(s_t2[s_t2.Condition=='Live']['T2'], hist=False, axlabel=False, label='Live')
sns.distplot(s_t2[s_t2.Condition=='WaterCooler']['T2'], hist=False, axlabel=False, label='WaterCooler')
plt.show()
43/520:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
b_t2 = t2[t2.sale_t20==0]
s_t2 = t2[t2.purchase_t20==0] #sellers at t2


sns.distplot(s_t2[s_t2.Condition=='Live']['T2'], hist=False, axlabel=False, label='Live')
sns.distplot(s_t2[s_t2.Condition=='WaterCooler']['T2'], hist=False, axlabel=False, label='WaterCooler')
plt.show()

s_t2[s_t2.Condition=='WaterCooler']['T2'].describe()
43/521:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
b_t2 = t2[t2.sale_t20==0]
s_t2 = t2[t2.purchase_t20==0] #sellers at t2


sns.distplot(s_t2[s_t2.Condition=='Live']['T2'], hist=False, axlabel=False, label='Live')
sns.distplot(s_t2[s_t2.Condition=='WaterCooler']['T2'], hist=False, axlabel=False, label='WaterCooler')
plt.show()

s_t2[s_t2.Condition=='Live']['T2'].describe()
43/522:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
s_t2 = t2[t2.purchase_t20==0] #sellers at t2


# sns.distplot(s_t2[s_t2.Condition=='Live']['T2'], hist=False, axlabel=False, label='Live')
# sns.distplot(s_t2[s_t2.Condition=='WaterCooler']['T2'], hist=False, axlabel=False, label='WaterCooler')
# plt.show()


sns.boxplot(x='Condition', y='T2', data=s_t2)
43/523:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
s_t2 = t2[t2.purchase_t20==0] #sellers at t2


# sns.distplot(s_t2[s_t2.Condition=='Live']['T2'], hist=False, axlabel=False, label='Live')
# sns.distplot(s_t2[s_t2.Condition=='WaterCooler']['T2'], hist=False, axlabel=False, label='WaterCooler')
# plt.show()


sns.boxplot(x='Condition', y='T2', data=s_t2, order=['Live','WaterCooler'])
43/524:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
s_t2 = t2[t2.purchase_t20==0] #sellers at t2


# sns.distplot(s_t2[s_t2.Condition=='Live']['T2'], hist=False, axlabel=False, label='Live')
# sns.distplot(s_t2[s_t2.Condition=='WaterCooler']['T2'], hist=False, axlabel=False, label='WaterCooler')
# plt.show()


sns.boxplot(x='Condition', y='T2', data=s_t2, order=['Live','WaterCooler'], hue='red')
43/525:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
s_t2 = t2[t2.purchase_t20==0] #sellers at t2


# sns.distplot(s_t2[s_t2.Condition=='Live']['T2'], hist=False, axlabel=False, label='Live')
# sns.distplot(s_t2[s_t2.Condition=='WaterCooler']['T2'], hist=False, axlabel=False, label='WaterCooler')
# plt.show()


sns.boxplot(x='Condition', y='T2', data=s_t2, order=['Live','WaterCooler'], palette='Set1')
43/526:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
s_t2 = t2[t2.purchase_t20==0] #sellers at t2


# sns.distplot(s_t2[s_t2.Condition=='Live']['T2'], hist=False, axlabel=False, label='Live')
# sns.distplot(s_t2[s_t2.Condition=='WaterCooler']['T2'], hist=False, axlabel=False, label='WaterCooler')
# plt.show()


sns.boxplot(x='Condition', y='T2', data=s_t2, order=['Live','WaterCooler'], palette='Set1')
print(s_t2[s_t2.Condition=='WaterCooler'].describe())
43/527:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
s_t2 = t2[t2.purchase_t20==0] #sellers at t2


# sns.distplot(s_t2[s_t2.Condition=='Live']['T2'], hist=False, axlabel=False, label='Live')
# sns.distplot(s_t2[s_t2.Condition=='WaterCooler']['T2'], hist=False, axlabel=False, label='WaterCooler')
# plt.show()


sns.boxplot(x='Condition', y='T2', data=s_t2, order=['Live','WaterCooler'], palette='Set1')
# print(s_t2[s_t2.Condition=='WaterCooler'].describe())
43/528:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
b_t2 = t2[t2.sale_t20==0] #buyers at t2
s_t2 = t2[t2.purchase_t20==0] #sellers at t2


# sns.distplot(s_t2[s_t2.Condition=='Live']['T2'], hist=False, axlabel=False, label='Live')
# sns.distplot(s_t2[s_t2.Condition=='WaterCooler']['T2'], hist=False, axlabel=False, label='WaterCooler')
# plt.show()


sns.boxplot(x='Condition', y='T2', data=s_t2, order=['Live','WaterCooler'], palette='Set1')

sns.boxplot(x='Condition', y='T2', data=b_t2, order=['Live','WaterCooler'], palette='Set1')
43/529:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
b_t2 = t2[t2.sale_t20==0] #buyers at t2
s_t2 = t2[t2.purchase_t20==0] #sellers at t2


# sns.distplot(s_t2[s_t2.Condition=='Live']['T2'], hist=False, axlabel=False, label='Live')
# sns.distplot(s_t2[s_t2.Condition=='WaterCooler']['T2'], hist=False, axlabel=False, label='WaterCooler')
# plt.show()


sns.boxplot(x='Condition', y='T2', data=s_t2, order=['Live','WaterCooler'], palette='Set1')
plt.show()
sns.boxplot(x='Condition', y='T2', data=b_t2, order=['Live','WaterCooler'], palette='Set1')
43/530:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
b_t2 = t2[t2.sale_t20==0] #buyers at t2
b_t2['Buyers'] = b_t2['Condition']
s_t2 = t2[t2.purchase_t20==0] #sellers at t2
s_t2['Sellers'] = s_t2['Condition']


# sns.distplot(s_t2[s_t2.Condition=='Live']['T2'], hist=False, axlabel=False, label='Live')
# sns.distplot(s_t2[s_t2.Condition=='WaterCooler']['T2'], hist=False, axlabel=False, label='WaterCooler')
# plt.show()


sns.boxplot(x='Sellers', y='T2', data=s_t2, order=['Live','WaterCooler'], palette='Set1')
plt.show()
sns.boxplot(x='Buyers', y='T2', data=b_t2, order=['Live','WaterCooler'], palette='Set1')
43/531:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
b_t2 = t2[t2.sale_t20==0] #buyers at t2
b_t2['Buyers'] = b_t2['Condition']
s_t2 = t2[t2.purchase_t20==0] #sellers at t2
s_t2['Sellers'] = s_t2['Condition']



fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Sellers', y='T2', data=s_t2, order=['Live','WaterCooler'], palette='Set1', ax=ax2)
sns.boxplot(x='Buyers', y='T2', data=b_t2, order=['Live','WaterCooler'], palette='Set1', ax=ax1)
plt.show()
43/532:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

# dftmp = dfz.copy()
# dflive = pd.melt(dfz[dfz.Condition=='Live'][['T1','T2']])
# dflive.columns=['Time of Trade', 'q']
# dfwc=pd.melt(dfz[dfz.Condition=='WaterCooler'][['T1','T2']])
# dfwc.columns=['Time of Trade', 'q']

# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Time of Trade', y='q', data=dflive,  ax=ax1)
# sns.boxplot(x='Time of Trade', y='q', data=dfwc,  ax=ax2)
# ax1.set_title('Live')
# ax2.set_title('WaterCooler')
# plt.show()



# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Condition', y='T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# sns.boxplot(x='Condition', y='T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
# ax1.set_title('T1')
# ax2.set_title('T2')
# plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)

sns.distplot(dfz[dfz.Condition=='Live']['T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='Live']['T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T2"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/533:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2

# Traders at t2
t2=dfz[(dfz.purchase_t10+dfz.sale_t10)>0]
b_t2 = t2[t2.sale_t10==0] #buyers at t1
b_t2['Buyers'] = b_t2['Condition']
s_t2 = t2[t2.purchase_t10==0] #sellers at t1
s_t2['Sellers'] = s_t2['Condition']
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Sellers', y='q at T1', data=s_t2, order=['Live','WaterCooler'], palette='Set1', ax=ax2)
sns.boxplot(x='Buyers', y='q at T1', data=b_t2, order=['Live','WaterCooler'], palette='Set1', ax=ax1)
plt.show()


# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
b_t2 = t2[t2.sale_t20==0] #buyers at t2
b_t2['Buyers'] = b_t2['Condition']
s_t2 = t2[t2.purchase_t20==0] #sellers at t2
s_t2['Sellers'] = s_t2['Condition']
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Sellers', y='q at T2', data=s_t2, order=['Live','WaterCooler'], palette='Set1', ax=ax2)
sns.boxplot(x='Buyers', y='q at T2', data=b_t2, order=['Live','WaterCooler'], palette='Set1', ax=ax1)
plt.show()
43/534:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2

# # Traders at t2
# t2=dfz[(dfz.purchase_t10+dfz.sale_t10)>0]
# b_t2 = t2[t2.sale_t10==0] #buyers at t1
# b_t2['Buyers'] = b_t2['Condition']
# s_t2 = t2[t2.purchase_t10==0] #sellers at t1
# s_t2['Sellers'] = s_t2['Condition']
# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Sellers', y='q at T1', data=s_t2, order=['Live','WaterCooler'], palette='Set1', ax=ax2)
# sns.boxplot(x='Buyers', y='q at T1', data=b_t2, order=['Live','WaterCooler'], palette='Set1', ax=ax1)
# plt.show()


# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
b_t2 = t2[t2.sale_t20==0] #buyers at t2
b_t2['Buyers'] = b_t2['Condition']
s_t2 = t2[t2.purchase_t20==0] #sellers at t2
s_t2['Sellers'] = s_t2['Condition']
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Sellers', y='q at T2', data=s_t2, order=['Live','WaterCooler'], palette='Set1', ax=ax2)
sns.boxplot(x='Buyers', y='q at T2', data=b_t2, order=['Live','WaterCooler'], palette='Set1', ax=ax1)
plt.show()
43/535:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

# dftmp = dfz.copy()
# dflive = pd.melt(dfz[dfz.Condition=='Live'][['T1','T2']])
# dflive.columns=['Time of Trade', 'q']
# dfwc=pd.melt(dfz[dfz.Condition=='WaterCooler'][['T1','T2']])
# dfwc.columns=['Time of Trade', 'q']

# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Time of Trade', y='q', data=dflive,  ax=ax1)
# sns.boxplot(x='Time of Trade', y='q', data=dfwc,  ax=ax2)
# ax1.set_title('Live')
# ax2.set_title('WaterCooler')
# plt.show()



# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Condition', y='T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# sns.boxplot(x='Condition', y='T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
# ax1.set_title('T1')
# ax2.set_title('T2')
# plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)

sns.distplot(dfz[dfz.Condition=='Live']['T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='Live']['T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "T2"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
43/536:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2

# # Traders at t2
# t2=dfz[(dfz.purchase_t10+dfz.sale_t10)>0]
# b_t2 = t2[t2.sale_t10==0] #buyers at t1
# b_t2['Buyers'] = b_t2['Condition']
# s_t2 = t2[t2.purchase_t10==0] #sellers at t1
# s_t2['Sellers'] = s_t2['Condition']
# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Sellers', y='q at T1', data=s_t2, order=['Live','WaterCooler'], palette='Set1', ax=ax2)
# sns.boxplot(x='Buyers', y='q at T1', data=b_t2, order=['Live','WaterCooler'], palette='Set1', ax=ax1)
# plt.show()


# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
b_t2 = t2[t2.sale_t20==0] #buyers at t2
b_t2['Buyers'] = b_t2['Condition']
s_t2 = t2[t2.purchase_t20==0] #sellers at t2
s_t2['Sellers'] = s_t2['Condition']
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Sellers', y='q at T2', data=s_t2, order=['Live','WaterCooler'], palette='Set1', ax=ax2)
sns.boxplot(x='Buyers', y='q at T2', data=b_t2, order=['Live','WaterCooler'], palette='Set1', ax=ax1)
plt.show()
43/537:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
b_t2 = t2[t2.sale_t20==0] #buyers at t2
b_t2['Buyers'] = b_t2['Condition']
s_t2 = t2[t2.purchase_t20==0] #sellers at t2
s_t2['Sellers'] = s_t2['Condition']
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Sellers', y='q at T2', data=s_t2, order=['Live','WaterCooler'], palette='Set1', ax=ax2)
sns.boxplot(x='Buyers', y='q at T2', data=b_t2, order=['Live','WaterCooler'], palette='Set1', ax=ax1)
plt.show()

sns.distplot(s_t2[s_t2.Condition=='Live']['q at T2'])
sns.distplot(s_t2[s_t2.Condition=='WaterCooler']['q at T2'])
plt.show()
43/538:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
b_t2 = t2[t2.sale_t20==0] #buyers at t2
b_t2['Buyers'] = b_t2['Condition']
s_t2 = t2[t2.purchase_t20==0] #sellers at t2
s_t2['Sellers'] = s_t2['Condition']
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Sellers', y='q at T2', data=s_t2, order=['Live','WaterCooler'], palette='Set1', ax=ax2)
sns.boxplot(x='Buyers', y='q at T2', data=b_t2, order=['Live','WaterCooler'], palette='Set1', ax=ax1)
plt.show()

sns.distplot(s_t2[s_t2.Condition=='Live']['q at T2'], bins=3)
sns.distplot(s_t2[s_t2.Condition=='WaterCooler']['q at T2'], bins=3)
plt.show()
43/539:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
b_t2 = t2[t2.sale_t20==0] #buyers at t2
b_t2['Buyers'] = b_t2['Condition']
s_t2 = t2[t2.purchase_t20==0] #sellers at t2
s_t2['Sellers'] = s_t2['Condition']
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Sellers', y='q at T2', data=s_t2, order=['Live','WaterCooler'], palette='Set1', ax=ax2)
sns.boxplot(x='Buyers', y='q at T2', data=b_t2, order=['Live','WaterCooler'], palette='Set1', ax=ax1)
plt.show()

sns.distplot(b_t2[b_t2.Condition=='Live']['q at T2'], bins=3)
sns.distplot(b_t2[b_t2.Condition=='WaterCooler']['q at T2'], bins=3)
plt.show()

sns.distplot(s_t2[s_t2.Condition=='Live']['q at T2'], bins=3)
sns.distplot(s_t2[s_t2.Condition=='WaterCooler']['q at T2'], bins=3)
plt.show()
43/540:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
b_t2 = t2[t2.sale_t20==0] #buyers at t2
b_t2['Buyers'] = b_t2['Condition']
s_t2 = t2[t2.purchase_t20==0] #sellers at t2
s_t2['Sellers'] = s_t2['Condition']
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Sellers', y='q at T2', data=s_t2, order=['Live','WaterCooler'], palette='Set1', ax=ax2)
sns.boxplot(x='Buyers', y='q at T2', data=b_t2, order=['Live','WaterCooler'], palette='Set1', ax=ax1)
plt.show()

# sns.distplot(b_t2[b_t2.Condition=='Live']['q at T2'], bins=3)
sns.distplot(b_t2[b_t2.Condition=='WaterCooler']['q at T2'], bins=3)
plt.show()

sns.distplot(s_t2[s_t2.Condition=='Live']['q at T2'], bins=3)
sns.distplot(s_t2[s_t2.Condition=='WaterCooler']['q at T2'], bins=3)
plt.show()
43/541:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
b_t2 = t2[t2.sale_t20==0] #buyers at t2
b_t2['Buyers'] = b_t2['Condition']
s_t2 = t2[t2.purchase_t20==0] #sellers at t2
s_t2['Sellers'] = s_t2['Condition']
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Sellers', y='q at T2', data=s_t2, order=['Live','WaterCooler'], palette='Set1', ax=ax2)
sns.boxplot(x='Buyers', y='q at T2', data=b_t2, order=['Live','WaterCooler'], palette='Set1', ax=ax1)
plt.show()

sns.distplot(b_t2[b_t2.Condition=='Live']['q at T2'], bins=3)
sns.distplot(b_t2[b_t2.Condition=='WaterCooler']['q at T2'], bins=3)
plt.show()

sns.distplot(s_t2[s_t2.Condition=='Live']['q at T2'], bins=3)
sns.distplot(s_t2[s_t2.Condition=='WaterCooler']['q at T2'], bins=3)
plt.show()
44/1: import random
44/2: l = [random.random() for x in range(100)]
44/3: mean(l)
44/4: l.mean()
44/5: avg = sum(l)/100
44/6: avg
44/7: std = (sum([(x-avg)**2 for x in l])/99)**0.5
44/8: std
44/9: mult=6
44/10: stre = [avg+(x-avg)*mult for x in l]
44/11: std2 = (sum([(x-avg)**2 for x in stre])/99)**0.5
44/12: avg2 = sum(stre)/100
44/13: avg2
44/14: std2
44/15: std2/std
45/1: import pandas
45/2: df = pandas.read_csv('AAPL_Final_Trend.csv')
45/3: df.High.stf()
45/4: df.High.std()
45/5: df.High.describe()
45/6: from scipy.stats import mannwhitneyu as mwu
45/7: rcp=[0.995567321, 1, 0.733612917, 1, 1, 0.285714286, 0.5, 0.047619048, 0, 0.047619048, 0.972418926, 0.285714286, 1, 1, 0.995567321, 1, 0.090909091, 0.384615385, 0.890606081]
45/8: wcp = [1, 0.474079677, 1, 0.474079677, 1, 1, 0.925353089, 0.5, 0.540232737, 0, 0.6, 1, 0, 0.549282152, 1, 0, 0.995567321, 0.111111111, 0.047619048, 1, 0.571428571, 1, 1, 0.476190476]
45/9: mwu(rcp, wcp)
45/10: wcp = [1, 1, 1, 1, 0.5, 0.6, 1, 1, 0.111111111, 0.047619048, 1, 0.571428571, 1, 1, 0.476190476]
45/11: rcp=[1, 1, 1, 0.285714286, 0.5, 0.047619048, 0.047619048, 0.285714286, 1, 1, 1, 0.090909091, 0.384615385]
45/12: mwu(rcp, wcp)
45/13: rcp=[0.995567321, 0.733612917, 0, 0.972418926, 0.995567321, 0.890606081]
45/14: wcp=[0.474079677, 0.474079677, 0.925353089, 0.540232737, 0.549282152, 0.995567321]
45/15: rcp.remove(0)
45/16: rcp
45/17: mwu(rcp,wcp)
45/18: rcl=[0.5, 0.463673665, 0.523809524, 0.463673665, 0.927347331, 0.142857143, 0, 1, 0.772789442, 0.461304206, 0.772789442, 0.706069428, 0.238984634, 0.953266689, 1, 0.713843559, 0.142857143, 0.927347331]
45/19: wcl=[1, 1, 1, 1, 0.927347331, 0.519963648, 1, 0.927347331, 1, 0.5, 0.968738832, 0.5, 0.476190476, 0.476190476, 0.266666667, 0.19047619, 0.525674577]
45/20: mwu(rcl,wcl)
45/21: wcl=[0.927347331, 0.519963648, 0.927347331, 0.968738832, 0.525674577]
45/22: rcl=[0.463673665, 0.463673665, 0.927347331, 0.772789442, 0.461304206, 0.772789442, 0.706069428, 0.238984634, 0.953266689, 0.713843559, 0.927347331]
45/23: mwu(rcl,wcl)
45/24: wcl=[1, 1, 1, 1, 1, 1, 0.5, 0.5, 0.476190476, 0.476190476, 0.266666667, 0.19047619]
45/25: rcl=[0.5, 0.523809524, 0.142857143, 1, 1, 0.142857143]
45/26: mwu(rcl,wcl)
45/27: len(wcl)
45/28: len(rcl)
45/29: rcl=rcl+[0]*6
45/30: rcl
45/31: mwu(rcl,wcl)
45/32: from scipy.stats import kstest
45/33: kstest(rcl, wcl)
45/34: from scipy.stats import ttest_ind as ttst
45/35: ttst(rcl, wcl)
45/36: wcl=[1, 1, 1, 1, 1, 1, 0.5, 0.5, 0.476190476, 0.476190476, 0.266666667, 0.1907619]rcl=[0.5, 0.523809524, 0.142857143, 1, 1, 0.142857143]
45/37: wcl=[1, 1, 1, 1, 1, 1, 0.5, 0.5, 0.476190476, 0.476190476, 0.266666667, 0.1907619]rcl=[0.5, 0.523809524, 0.142857143, 1, 1, 0.142857143]
45/38: wcl=[1, 1, 1, 1, 1, 1, 0.5, 0.5, 0.476190476, 0.476190476, 0.266666667, 0.1907619]rcl=[0.5, 0.523809524, 0.142857143, 1, 1, 0.142857143]
45/39: wcl=[1, 1, 1, 1, 1, 1, 0.5, 0.5, 0.476190476, 0.476190476, 0.266666667, 0.1907619]rcl=[0.5, 0.523809524, 0.142857143, 1, 1, 0.142857143]
45/40: wcl=[1, 1, 1, 1, 1, 1, 0.5, 0.5, 0.476190476, 0.476190476, 0.266666667, 0.1907619]
45/41: rcl=[0.5, 0.523809524, 0.142857143, 1, 1, 0.142857143]
45/42: ttst(rcl, wcl)
46/1:
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
46/2:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price10'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price20']=None
df['price20'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price20'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
df=df[df.mturkid!='A2H95JVPEKRUWA'] # invalid

df['stock_t01']=10

df['purchase_t10'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t10'] = np.where(df['qty1']<0, df['qty1'], 0)
df['sale_t10'] = df['sale_t10'].abs()

df['stock_t20'] = df['stock_t01']+df['purchase_t10']-df['sale_t10']
df['cash_t20'] = 2500 - (df.purchase_t10-df.sale_t10)*219.65

df['purchase_t20'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t20'] = np.where(df['qty2']<0, df['qty2'], 0)
df['sale_t20'] = df['sale_t20'].abs()

df['stock_t21'] = df['stock_t11']+df['purchase_t20']-df['sale_t20']

# df['avg_purch_t11'] = (2177.6 + df.purchase_t10*df.price10)/(10+df.purchase_t10)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)

df['pnl_t20'] = np.where(df.stock_t11>10, (df.stock_t11-10)*(df.price20-df.price10)+10*(df.price20-df.price0), df.stock_t11*(df.price20-df.price0))

df['t1_purchase'] = None
df['t1_purchase'] = np.where(df.purchase_t10!=0, 1, df.t1_purchase)
df['t1_purchase'] = np.where(df.sale_t10!=0, -1, df.t1_purchase)

df['t2_purchase'] = None
df['t2_purchase'] = np.where(df.purchase_t20!=0, 1, df.t2_purchase)
df['t2_purchase'] = np.where(df.sale_t20!=0, -1, df.t2_purchase)


df['live_info'] = np.where(df.experiment.str.contains('wc'), 0, 1)
df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])


# drop inactive people (4)
df=df.drop([9,46,53,54])


df = df[['experiment', 'mturkid', 'winnings',
       'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret', 
       'price0', 'price10', 'price20', 'stock_t01',
       'purchase_t10', 'sale_t10', 'stock_t11', 'purchase_t20', 'sale_t20',
       'stock_t21', 'pnl_t20', 'live_info']]
46/3:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price10'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price20']=None
df['price20'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price20'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
df=df[df.mturkid!='A2H95JVPEKRUWA'] # invalid

df['stock_t01']=10

df['purchase_t10'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t10'] = np.where(df['qty1']<0, df['qty1'], 0)
df['sale_t10'] = df['sale_t10'].abs()

df['stock_t20'] = df['stock_t01']+df['purchase_t10']-df['sale_t10']
df['cash_t20'] = 2500 - (df.purchase_t10-df.sale_t10)*219.65

df['purchase_t20'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t20'] = np.where(df['qty2']<0, df['qty2'], 0)
df['sale_t20'] = df['sale_t20'].abs()

df['stock_t21'] = df['stock_t20']+df['purchase_t20']-df['sale_t20']

# df['avg_purch_t11'] = (2177.6 + df.purchase_t10*df.price10)/(10+df.purchase_t10)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)

df['pnl_t20'] = np.where(df.stock_t11>10, (df.stock_t11-10)*(df.price20-df.price10)+10*(df.price20-df.price0), df.stock_t11*(df.price20-df.price0))

df['t1_purchase'] = None
df['t1_purchase'] = np.where(df.purchase_t10!=0, 1, df.t1_purchase)
df['t1_purchase'] = np.where(df.sale_t10!=0, -1, df.t1_purchase)

df['t2_purchase'] = None
df['t2_purchase'] = np.where(df.purchase_t20!=0, 1, df.t2_purchase)
df['t2_purchase'] = np.where(df.sale_t20!=0, -1, df.t2_purchase)


df['live_info'] = np.where(df.experiment.str.contains('wc'), 0, 1)
df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])


# drop inactive people (4)
df=df.drop([9,46,53,54])


df = df[['experiment', 'mturkid', 'winnings',
       'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret', 
       'price0', 'price10', 'price20', 'stock_t01',
       'purchase_t10', 'sale_t10', 'stock_t11', 'purchase_t20', 'sale_t20',
       'stock_t21', 'pnl_t20', 'live_info']]
46/4:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price10'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price20']=None
df['price20'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price20'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
df=df[df.mturkid!='A2H95JVPEKRUWA'] # invalid

df['stock_t01']=10

df['purchase_t10'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t10'] = np.where(df['qty1']<0, df['qty1'], 0)
df['sale_t10'] = df['sale_t10'].abs()

df['stock_t20'] = df['stock_t01']+df['purchase_t10']-df['sale_t10']
df['cash_t20'] = 2500 - (df.purchase_t10-df.sale_t10)*219.65

df['purchase_t20'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t20'] = np.where(df['qty2']<0, df['qty2'], 0)
df['sale_t20'] = df['sale_t20'].abs()

df['stock_t21'] = df['stock_t20']+df['purchase_t20']-df['sale_t20']

# df['avg_purch_t11'] = (2177.6 + df.purchase_t10*df.price10)/(10+df.purchase_t10)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)

df['pnl_t20'] = np.where(df.stock_t20>10, (df.stock_t20-10)*(df.price20-df.price10)+10*(df.price20-df.price0), df.stock_t20*(df.price20-df.price0))

df['t1_purchase'] = None
df['t1_purchase'] = np.where(df.purchase_t10!=0, 1, df.t1_purchase)
df['t1_purchase'] = np.where(df.sale_t10!=0, -1, df.t1_purchase)

df['t2_purchase'] = None
df['t2_purchase'] = np.where(df.purchase_t20!=0, 1, df.t2_purchase)
df['t2_purchase'] = np.where(df.sale_t20!=0, -1, df.t2_purchase)


df['live_info'] = np.where(df.experiment.str.contains('wc'), 0, 1)
df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])


# drop inactive people (4)
df=df.drop([9,46,53,54])


df = df[['experiment', 'mturkid', 'winnings',
       'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret', 
       'price0', 'price10', 'price20', 'stock_t01',
       'purchase_t10', 'sale_t10', 'stock_t11', 'purchase_t20', 'sale_t20',
       'stock_t21', 'pnl_t20', 'live_info']]
46/5:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price10'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price20']=None
df['price20'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price20'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
df=df[df.mturkid!='A2H95JVPEKRUWA'] # invalid

df['stock_t01']=10

df['purchase_t10'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t10'] = np.where(df['qty1']<0, df['qty1'], 0)
df['sale_t10'] = df['sale_t10'].abs()

df['stock_t20'] = df['stock_t01']+df['purchase_t10']-df['sale_t10']
df['cash_t20'] = 2500 - (df.purchase_t10-df.sale_t10)*219.65

df['purchase_t20'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t20'] = np.where(df['qty2']<0, df['qty2'], 0)
df['sale_t20'] = df['sale_t20'].abs()

df['stock_t21'] = df['stock_t20']+df['purchase_t20']-df['sale_t20']

# df['avg_purch_t11'] = (2177.6 + df.purchase_t10*df.price10)/(10+df.purchase_t10)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)

df['pnl_t20'] = np.where(df.stock_t20>10, (df.stock_t20-10)*(df.price20-df.price10)+10*(df.price20-df.price0), df.stock_t20*(df.price20-df.price0))

df['t1_purchase'] = None
df['t1_purchase'] = np.where(df.purchase_t10!=0, 1, df.t1_purchase)
df['t1_purchase'] = np.where(df.sale_t10!=0, -1, df.t1_purchase)

df['t2_purchase'] = None
df['t2_purchase'] = np.where(df.purchase_t20!=0, 1, df.t2_purchase)
df['t2_purchase'] = np.where(df.sale_t20!=0, -1, df.t2_purchase)


df['live_info'] = np.where(df.experiment.str.contains('wc'), 0, 1)
df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])


# drop inactive people (4)
df=df.drop([9,46,53,54])


# df = df[['experiment', 'mturkid', 'winnings',
#        'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret', 
#        'price0', 'price10', 'price20', 'stock_t01',
#        'purchase_t10', 'sale_t10', 'stock_t11', 'purchase_t20', 'sale_t20',
#        'stock_t21', 'pnl_t20', 'live_info']]
46/6:
## INV RATIO: traded_volume/permissible_volume
maxbuy10 = math.floor(2500/219.65)
maxsell10 = 10
df['invratio_1'] = df.purchase_t10/maxbuy10
df['invratio_1'] = np.where(df.invratio_1==0, df.sale_t10/maxsell10, df.invratio_1)

avbl_cash = 2500 - (df.purchase_t10-df.sale_t10)*219.65
avbl_stock = 10+df.purchase_t10-df.sale_t10
maxbuy2 = df['cash_t20']/df.price20
df['invratio_2'] = df.purchase_t20/maxbuy2
df['invratio_2'] = np.where(df.invratio_2==0, df.sale_t20/avbl_stock, df.invratio_2)
df['invratio_2'].fillna(0, inplace=True)

# # DELTA IN INV_RATIO: measures change in investment behavior
# df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])
46/7:
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
import math
46/8:
## INV RATIO: traded_volume/permissible_volume
maxbuy10 = math.floor(2500/219.65)
maxsell10 = 10
df['invratio_1'] = df.purchase_t10/maxbuy10
df['invratio_1'] = np.where(df.invratio_1==0, df.sale_t10/maxsell10, df.invratio_1)

avbl_cash = 2500 - (df.purchase_t10-df.sale_t10)*219.65
avbl_stock = 10+df.purchase_t10-df.sale_t10
maxbuy2 = df['cash_t20']/df.price20
df['invratio_2'] = df.purchase_t20/maxbuy2
df['invratio_2'] = np.where(df.invratio_2==0, df.sale_t20/avbl_stock, df.invratio_2)
df['invratio_2'].fillna(0, inplace=True)

# # DELTA IN INV_RATIO: measures change in investment behavior
# df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])
46/9:
## INV RATIO: traded_volume/permissible_volume
maxbuy10 = math.floor(2500/219.65)
maxsell10 = 10
df['invratio_1'] = df.purchase_t10/maxbuy10
df['invratio_1'] = np.where(df.invratio_1==0, df.sale_t10/maxsell10, df.invratio_1)

avbl_cash = 2500 - (df.purchase_t10-df.sale_t10)*219.65
avbl_stock = 10+df.purchase_t10-df.sale_t10
maxbuy2 = df['cash_t20']/df.price20
df['invratio_2'] = df.purchase_t20/maxbuy2
df['invratio_2'] = np.where(df.invratio_2==0, df.sale_t20/avbl_stock, df.invratio_2)
df['invratio_2'].fillna(0, inplace=True)

# # DELTA IN INV_RATIO: measures change in investment behavior
# df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])

df['invratio_1_mul'] = df.invratio_1*df.t1_purchase
df['invratio_2_mul'] = df.invratio_2*df.t2_purchase
46/10:
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
import math
from sklearn.linear_model import LinearRegression
pd.options.mode.chained_assignment = None
46/11:
spec1 = ['pnl_t20', 'invratio_1_mul', 'cash_t20', 'stock_t20']
spec2 = spec1 + ['live_info']
tgt='invratio_2_mul'

import statsmodels.formula.api as smf

preds1 = tgt+'~'+'+'.join(spec1)
preds2 = tgt+'~'+'+'.join(spec2)

model1 = smf.ols(pred1, df).fit()
print(model.summary().tables[1])
print('p-values of features:\n',model1.pvalues)

model2 = smf.ols(pred2, df).fit()
print(model.summary().tables[1])
print('p-values of features:\n',model2.pvalues)
46/12:
spec1 = ['pnl_t20', 'invratio_1_mul', 'cash_t20', 'stock_t20']
spec2 = spec1 + ['live_info']
tgt='invratio_2_mul'

import statsmodels.formula.api as smf

pred1 = tgt+'~'+'+'.join(spec1)
pred2 = tgt+'~'+'+'.join(spec2)

model1 = smf.ols(pred1, df).fit()
print(model.summary().tables[1])
print('p-values of features:\n',model1.pvalues)

model2 = smf.ols(pred2, df).fit()
print(model.summary().tables[1])
print('p-values of features:\n',model2.pvalues)
46/13:
spec1 = ['pnl_t20', 'invratio_1_mul', 'cash_t20', 'stock_t20']
spec2 = spec1 + ['live_info']
tgt='invratio_2_mul'

import statsmodels.formula.api as smf

pred1 = tgt+'~'+'+'.join(spec1)
pred2 = tgt+'~'+'+'.join(spec2)

model1 = smf.ols(pred1, df).fit()
print(model1.summary().tables[1])
print('p-values of features:\n',model1.pvalues)

model2 = smf.ols(pred2, df).fit()
print(model2.summary().tables[1])
print('p-values of features:\n',model2.pvalues)
46/14: pred1
46/15: model1
46/16: model1.aic
46/17: model1.summary()
46/18: df[spec1]
46/19:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price10'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price20']=None
df['price20'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price20'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
df=df[df.mturkid!='A2H95JVPEKRUWA'] # invalid

df['stock_t01']=10

df['purchase_t10'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t10'] = np.where(df['qty1']<0, df['qty1'], 0)
df['sale_t10'] = df['sale_t10'].abs()

df['stock_t20'] = df['stock_t01']+df['purchase_t10']-df['sale_t10']
df['cash_t20'] = 2500 - (df.purchase_t10-df.sale_t10)*219.65

df['purchase_t20'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t20'] = np.where(df['qty2']<0, df['qty2'], 0)
df['sale_t20'] = df['sale_t20'].abs()

df['stock_t21'] = df['stock_t20']+df['purchase_t20']-df['sale_t20']

# df['avg_purch_t11'] = (2177.6 + df.purchase_t10*df.price10)/(10+df.purchase_t10)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)

df['pnl_t20'] = np.where(df.stock_t20>10, (df.stock_t20-10)*(df.price20-df.price10)+10*(df.price20-df.price0), df.stock_t20*(df.price20-df.price0))

df['t1_purchase'] = 0
df['t1_purchase'] = np.where(df.purchase_t10!=0, 1, df.t1_purchase)
df['t1_purchase'] = np.where(df.sale_t10!=0, -1, df.t1_purchase)

df['t2_purchase'] = 0
df['t2_purchase'] = np.where(df.purchase_t20!=0, 1, df.t2_purchase)
df['t2_purchase'] = np.where(df.sale_t20!=0, -1, df.t2_purchase)


df['live_info'] = np.where(df.experiment.str.contains('wc'), 0, 1)
df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])


# drop inactive people (4)
df=df.drop([9,46,53,54])


# df = df[['experiment', 'mturkid', 'winnings',
#        'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret', 
#        'price0', 'price10', 'price20', 'stock_t01',
#        'purchase_t10', 'sale_t10', 'stock_t11', 'purchase_t20', 'sale_t20',
#        'stock_t21', 'pnl_t20', 'live_info']]
46/20:
## INV RATIO: traded_volume/permissible_volume
maxbuy10 = math.floor(2500/219.65)
maxsell10 = 10
df['invratio_1'] = df.purchase_t10/maxbuy10
df['invratio_1'] = np.where(df.invratio_1==0, df.sale_t10/maxsell10, df.invratio_1)

avbl_cash = 2500 - (df.purchase_t10-df.sale_t10)*219.65
avbl_stock = 10+df.purchase_t10-df.sale_t10
maxbuy2 = df['cash_t20']/df.price20
df['invratio_2'] = df.purchase_t20/maxbuy2
df['invratio_2'] = np.where(df.invratio_2==0, df.sale_t20/avbl_stock, df.invratio_2)
df['invratio_2'].fillna(0, inplace=True)

# # DELTA IN INV_RATIO: measures change in investment behavior
# df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])

df['invratio_1_mul'] = df.invratio_1*df.t1_purchase
df['invratio_2_mul'] = df.invratio_2*df.t2_purchase
46/21:
spec1 = ['pnl_t20', 'invratio_1_mul', 'cash_t20', 'stock_t20']
spec2 = spec1 + ['live_info']
tgt='invratio_2_mul'

import statsmodels.formula.api as smf

pred1 = tgt+'~'+'+'.join(spec1)
pred2 = tgt+'~'+'+'.join(spec2)

model1 = smf.ols(pred1, df).fit()
print(model1.summary().tables[1])
print('p-values of features:\n',model1.pvalues)

model2 = smf.ols(pred2, df).fit()
print(model2.summary().tables[1])
print('p-values of features:\n',model2.pvalues)
46/22: df[tgt]
46/23:
spec1 = ['pnl_t20', 'invratio_1', 'cash_t20', 'stock_t20']
spec2 = spec1 + ['live_info']
tgt='invratio_2'

import statsmodels.formula.api as smf

pred1 = tgt+'~'+'+'.join(spec1)
pred2 = tgt+'~'+'+'.join(spec2)

model1 = smf.ols(pred1, df).fit()
print(model1.summary().tables[1])
print('p-values of features:\n',model1.pvalues)

model2 = smf.ols(pred2, df).fit()
print(model2.summary().tables[1])
print('p-values of features:\n',model2.pvalues)
46/24:
spec1 = ['pnl_t20', 'invratio_1', 'cash_t20', 'stock_t20','t1_purchase']
spec2 = spec1 + ['live_info']
tgt='invratio_2'

import statsmodels.formula.api as smf

pred1 = tgt+'~'+'+'.join(spec1)
pred2 = tgt+'~'+'+'.join(spec2)

model1 = smf.ols(pred1, df).fit()
print(model1.summary().tables[1])
print('p-values of features:\n',model1.pvalues)

model2 = smf.ols(pred2, df).fit()
print(model2.summary().tables[1])
print('p-values of features:\n',model2.pvalues)
46/25:
spec1 = ['pnl_t20', 'invratio_1', 'cash_t20', 'stock_t20','t1_purchase']
spec2 = spec1 + ['live_info']
tgt='invratio_2_mul'

import statsmodels.formula.api as smf

pred1 = tgt+'~'+'+'.join(spec1)
pred2 = tgt+'~'+'+'.join(spec2)

model1 = smf.ols(pred1, df).fit()
print(model1.summary().tables[1])
print('p-values of features:\n',model1.pvalues)

model2 = smf.ols(pred2, df).fit()
print(model2.summary().tables[1])
print('p-values of features:\n',model2.pvalues)
46/26:
spec1 = ['pnl_t20', 'invratio_1_mul', 'cash_t20', 'stock_t20']
spec2 = spec1 + ['live_info']
tgt='invratio_2_mul'

import statsmodels.formula.api as smf

pred1 = tgt+'~'+'+'.join(spec1)
pred2 = tgt+'~'+'+'.join(spec2)

model1 = smf.ols(pred1, df).fit()
print(model1.summary().tables[1])
print('p-values of features:\n',model1.pvalues)

model2 = smf.ols(pred2, df).fit()
print(model2.summary().tables[1])
print('p-values of features:\n',model2.pvalues)
46/27:
spec1 = ['pnl_t20', 'invratio_1*t1_purchase', 'cash_t20', 'stock_t20']
spec2 = spec1 + ['live_info']
tgt='invratio_2_mul'

import statsmodels.formula.api as smf

pred1 = tgt+'~'+'+'.join(spec1)
pred2 = tgt+'~'+'+'.join(spec2)

model1 = smf.ols(pred1, df).fit()
print(model1.summary().tables[1])
print('p-values of features:\n',model1.pvalues)

model2 = smf.ols(pred2, df).fit()
print(model2.summary().tables[1])
print('p-values of features:\n',model2.pvalues)
46/28: model1.score()
46/29:
spec1 = ['pnl_t20', 'invratio_1*t1_purchase', 'cash_t20', 'stock_t20']
spec2 = spec1 + ['live_info']
tgt='invratio_2_mul'

import statsmodels.formula.api as smf

pred1 = tgt+'~'+'+'.join(spec1)
pred2 = tgt+'~'+'+'.join(spec2)

model1 = smf.ols(pred1, df)
res = model1.fit()
print(res.summary().tables[1])
print('p-values of features:\n',res.pvalues)
print('R2', model1.score())

model2 = smf.ols(pred2, df).fit()
print(model2.summary().tables[1])
print('p-values of features:\n',model2.pvalues)
46/30:
spec1 = ['pnl_t20', 'invratio_1*t1_purchase', 'cash_t20', 'stock_t20']
spec2 = spec1 + ['live_info']
tgt='invratio_2_mul'

import statsmodels.formula.api as smf

pred1 = tgt+'~'+'+'.join(spec1)
pred2 = tgt+'~'+'+'.join(spec2)

model1 = smf.ols(pred1, df)
res = model1.fit()
print(res.summary().tables[1])
print('p-values of features:\n',res.pvalues)
print('R2', model1.score(res.params))

model2 = smf.ols(pred2, df).fit()
print(model2.summary().tables[1])
print('p-values of features:\n',model2.pvalues)
46/31:
spec1 = ['pnl_t20', 'invratio_1*t1_purchase', 'cash_t20', 'stock_t20']
spec2 = spec1 + ['live_info']
tgt='invratio_2_mul'

import statsmodels.formula.api as smf

pred1 = tgt+'~'+'+'.join(spec1)
pred2 = tgt+'~'+'+'.join(spec2)

model1 = smf.ols(pred1, df)
res = model1.fit()
print(res.summary())
print('p-values of features:\n',res.pvalues)

model2 = smf.ols(pred2, df).fit()
print(model2.summary())
print('p-values of features:\n',model2.pvalues)
46/32:
spec1 = ['pnl_t20', 'invratio_1*t1_purchase', 'cash_t20', 'stock_t20']
spec2 = spec1 + ['live_info']
tgt='invratio_2_mul'

import statsmodels.formula.api as smf

pred1 = tgt+'~'+'+'.join(spec1)
pred2 = tgt+'~'+'+'.join(spec2)

model1 = smf.ols(pred1, df)
res = model1.fit()
print(res.summary().tables[:2])
print('p-values of features:\n',res.pvalues)

model2 = smf.ols(pred2, df).fit()
print(model2.summary())
print('p-values of features:\n',model2.pvalues)
46/33:
spec1 = ['pnl_t20', 'invratio_1*t1_purchase', 'cash_t20', 'stock_t20']
spec2 = spec1 + ['live_info']
tgt='invratio_2_mul'

import statsmodels.formula.api as smf

pred1 = tgt+'~'+'+'.join(spec1)
pred2 = tgt+'~'+'+'.join(spec2)

model1 = smf.ols(pred1, df)
res = model1.fit()
print(res.summary().tables[1])
print('p-values of features:\n',res.pvalues)

model2 = smf.ols(pred2, df).fit()
print(model2.summary())
print('p-values of features:\n',model2.pvalues)
46/34:
spec1 = ['pnl_t20', 'invratio_1*t1_purchase', 'cash_t20', 'stock_t20']
spec2 = spec1 + ['live_info']
tgt='invratio_2_mul'

import statsmodels.formula.api as smf

pred1 = tgt+'~'+'+'.join(spec1)
pred2 = tgt+'~'+'+'.join(spec2)

model1 = smf.ols(pred1, df)
res = model1.fit()
print(res.summary().tables[0], res.summary().tables[1])
print('p-values of features:\n',res.pvalues)

model2 = smf.ols(pred2, df).fit()
print(model2.summary())
print('p-values of features:\n',model2.pvalues)
46/35:
df = df[spec2+[tgt]]
df.columns
46/36:
df2 = df['pnl_t20', 'invratio_1', 't1_purchase', 'cash_t20', 'stock_t20', 'live_info', 'invratio_2_mul']
df2.columns = ['PNL_T2', 'q1', 'PurchaseFlag_T1', 'AvblCash_T2', 'StocksHeld_T2', 'LiveTrackFlag', 'q2']
46/37:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price10'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price20']=None
df['price20'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price20'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
df=df[df.mturkid!='A2H95JVPEKRUWA'] # invalid

df['stock_t01']=10

df['purchase_t10'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t10'] = np.where(df['qty1']<0, df['qty1'], 0)
df['sale_t10'] = df['sale_t10'].abs()

df['stock_t20'] = df['stock_t01']+df['purchase_t10']-df['sale_t10']
df['cash_t20'] = 2500 - (df.purchase_t10-df.sale_t10)*219.65

df['purchase_t20'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t20'] = np.where(df['qty2']<0, df['qty2'], 0)
df['sale_t20'] = df['sale_t20'].abs()

df['stock_t21'] = df['stock_t20']+df['purchase_t20']-df['sale_t20']

# df['avg_purch_t11'] = (2177.6 + df.purchase_t10*df.price10)/(10+df.purchase_t10)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)

df['pnl_t20'] = np.where(df.stock_t20>10, (df.stock_t20-10)*(df.price20-df.price10)+10*(df.price20-df.price0), df.stock_t20*(df.price20-df.price0))

df['t1_purchase'] = 0
df['t1_purchase'] = np.where(df.purchase_t10!=0, 1, df.t1_purchase)
df['t1_purchase'] = np.where(df.sale_t10!=0, -1, df.t1_purchase)

df['t2_purchase'] = 0
df['t2_purchase'] = np.where(df.purchase_t20!=0, 1, df.t2_purchase)
df['t2_purchase'] = np.where(df.sale_t20!=0, -1, df.t2_purchase)


df['live_info'] = np.where(df.experiment.str.contains('wc'), 0, 1)
df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])


# drop inactive people (4)
df=df.drop([9,46,53,54])


# df = df[['experiment', 'mturkid', 'winnings',
#        'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret', 
#        'price0', 'price10', 'price20', 'stock_t01',
#        'purchase_t10', 'sale_t10', 'stock_t11', 'purchase_t20', 'sale_t20',
#        'stock_t21', 'pnl_t20', 'live_info']]
46/38:
## INV RATIO: traded_volume/permissible_volume
maxbuy10 = math.floor(2500/219.65)
maxsell10 = 10
df['invratio_1'] = df.purchase_t10/maxbuy10
df['invratio_1'] = np.where(df.invratio_1==0, df.sale_t10/maxsell10, df.invratio_1)

avbl_cash = 2500 - (df.purchase_t10-df.sale_t10)*219.65
avbl_stock = 10+df.purchase_t10-df.sale_t10
maxbuy2 = df['cash_t20']/df.price20
df['invratio_2'] = df.purchase_t20/maxbuy2
df['invratio_2'] = np.where(df.invratio_2==0, df.sale_t20/avbl_stock, df.invratio_2)
df['invratio_2'].fillna(0, inplace=True)

# # DELTA IN INV_RATIO: measures change in investment behavior
# df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])

df['invratio_1_mul'] = df.invratio_1*df.t1_purchase
df['invratio_2_mul'] = df.invratio_2*df.t2_purchase
46/39:
df2 = df['pnl_t20', 'invratio_1', 't1_purchase', 'cash_t20', 'stock_t20', 'live_info', 'invratio_2_mul']
df2.columns = ['PNL_T2', 'q1', 'PurchaseFlag_T1', 'AvblCash_T2', 'StocksHeld_T2', 'LiveTrackFlag', 'q2']
46/40:
df2 = df[['pnl_t20', 'invratio_1', 't1_purchase', 'cash_t20', 'stock_t20', 'live_info', 'invratio_2_mul']]
df2.columns = ['PNL_T2', 'q1', 'PurchaseFlag_T1', 'AvblCash_T2', 'StocksHeld_T2', 'LiveTrackFlag', 'q2']
46/41:
spec1 = ['PNL_T2', 'q1', 'PurchaseFlag_T1', 'AvblCash_T2', 'StocksHeld_T2']
spec2 = spec1 + ['LiveTrackFlag']
tgt='q2'

import statsmodels.formula.api as smf

pred1 = tgt+'~'+'+'.join(spec1)
pred2 = tgt+'~'+'+'.join(spec2)

model1 = smf.ols(pred1, df)
res = model1.fit()
print(res.summary().tables[0], res.summary().tables[1])
print('p-values of features:\n',res.pvalues)

model2 = smf.ols(pred2, df).fit()
print(model2.summary())
print('p-values of features:\n',model2.pvalues)
46/42:
spec1 = ['PNL_T2', 'q1', 'PurchaseFlag_T1', 'AvblCash_T2', 'StocksHeld_T2']
spec2 = spec1 + ['LiveTrackFlag']
tgt='q2'

import statsmodels.formula.api as smf

pred1 = tgt+'~'+'+'.join(spec1)
pred2 = tgt+'~'+'+'.join(spec2)

model1 = smf.ols(pred1, df2)
res = model1.fit()
print(res.summary().tables[0], res.summary().tables[1])
print('p-values of features:\n',res.pvalues)

model2 = smf.ols(pred2, df2).fit()
print(model2.summary())
print('p-values of features:\n',model2.pvalues)
46/43:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

# dftmp = dfz.copy()
# dflive = pd.melt(dfz[dfz.Condition=='Live'][['T1','T2']])
# dflive.columns=['Time of Trade', 'q']
# dfwc=pd.melt(dfz[dfz.Condition=='WaterCooler'][['T1','T2']])
# dfwc.columns=['Time of Trade', 'q']

# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Time of Trade', y='q', data=dflive,  ax=ax1)
# sns.boxplot(x='Time of Trade', y='q', data=dfwc,  ax=ax2)
# ax1.set_title('Live')
# ax2.set_title('WaterCooler')
# plt.show()



# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Condition', y='T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# sns.boxplot(x='Condition', y='T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
# ax1.set_title('T1')
# ax2.set_title('T2')
# plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)

sns.distplot(dfz[dfz.Condition=='Live']['T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "q at T1"})
sns.distplot(dfz[dfz.Condition=='Live']['T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "q at T2"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "q at T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "q at T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
46/44:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

# dftmp = dfz.copy()
# dflive = pd.melt(dfz[dfz.Condition=='Live'][['T1','T2']])
# dflive.columns=['Time of Trade', 'q']
# dfwc=pd.melt(dfz[dfz.Condition=='WaterCooler'][['T1','T2']])
# dfwc.columns=['Time of Trade', 'q']

# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Time of Trade', y='q', data=dflive,  ax=ax1)
# sns.boxplot(x='Time of Trade', y='q', data=dfwc,  ax=ax2)
# ax1.set_title('Live')
# ax2.set_title('WaterCooler')
# plt.show()



# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Condition', y='T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# sns.boxplot(x='Condition', y='T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
# ax1.set_title('T1')
# ax2.set_title('T2')
# plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)

sns.distplot(dfz[dfz.Condition=='Live']['T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "q at T1"})
sns.distplot(dfz[dfz.Condition=='Live']['T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "q at T2"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "q at T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "q at T2"})
ax1.set_title('Live')
ax2.set_title('WaterCooler')
plt.show()
46/45: df.columns
46/46: df['tradingfreq']
46/47: sns.distplot(df['tradingfreq'])
46/48: df['tradingfreq'].replace(dict(zip(range(1,6), ['Never', 'Rarely', 'Occasionally', 'Often', 'Everyday'])))
46/49: sns.distplot(df['tradingfreq'].replace(dict(zip(range(1,6), ['Never', 'Rarely', 'Occasionally', 'Often', 'Everyday']))))
46/50: df['tradingfreq'].replace(dict(zip(range(1,6), ['Never', 'Rarely', 'Occasionally', 'Often', 'Everyday'])))
46/51:
df['tradingfreq'].replace(dict(zip(range(1,6), ['Never', 'Rarely', 'Occasionally', 'Often', 'Everyday'])), inplace=True)
df['confidence'].replace(dict(zip(range(1,3), ['No', 'Yes'])), inplace=True)
df['focus'].replace(dict(zip(range(1,6), ['Not at all', 'Distracted', 'Neutral', 'Attentive', 'Hawkish'])), inplace=True)
df['stateofmind'].replace(dict(zip(range(1,6), ['Anxious', 'Slightly nervous', 'Calm', 'Fairly optimistic', 'Confident!'])), inplace=True)
df['regret'].replace(dict(zip(range(1,5), ['No', 'Only Trade 1', 'Only Trade 2', 'Both Trades'])), inplace=True)
df.to_csv('result_answers'.csv)
46/52:
df['tradingfreq'].replace(dict(zip(range(1,6), ['Never', 'Rarely', 'Occasionally', 'Often', 'Everyday'])), inplace=True)
df['confidence'].replace(dict(zip(range(1,3), ['No', 'Yes'])), inplace=True)
df['focus'].replace(dict(zip(range(1,6), ['Not at all', 'Distracted', 'Neutral', 'Attentive', 'Hawkish'])), inplace=True)
df['stateofmind'].replace(dict(zip(range(1,6), ['Anxious', 'Slightly nervous', 'Calm', 'Fairly optimistic', 'Confident!'])), inplace=True)
df['regret'].replace(dict(zip(range(1,5), ['No', 'Only Trade 1', 'Only Trade 2', 'Both Trades'])), inplace=True)
df.to_csv('result_answers.csv')
46/53:
ans = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ans.tradingfreq.plot.hist()
46/54:
ans = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ans.tradingfreq.value_counts()#plot.hist()
46/55:
ans = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ans.tradingfreq.value_counts().ix['Never', 'Rarely', 'Occasionally', 'Often', 'Everyday']#plot.hist()
46/56:
ans = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ans.tradingfreq.value_counts().ix[['Never', 'Rarely', 'Occasionally', 'Often', 'Everyday']]#plot.hist()
46/57:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques=['tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']


ansdf.tradingfreq.value_counts().ix[['Never', 'Rarely', 'Occasionally', 'Often', 'Everyday']].plot.hist()
46/58:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques=['tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']


ansdf.tradingfreq.value_counts().ix[['Never', 'Rarely', 'Occasionally', 'Often', 'Everyday']]#.plot.hist()
46/59:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques=['tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']


ansdf.tradingfreq.value_counts().ix[['Never', 'Rarely', 'Occasionally', 'Often', 'Everyday']].plot.bar()
46/60:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques=['tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']

fig, ax = plt.subplots(1,4)
for c,exp in enumerate(['rc_profit','wc_profit','rc_loss','wc_loss']):
    ansdf[ansdf.experiment==exp].tradingfreq.value_counts().ix[['Never', 'Rarely', 'Occasionally', 'Often', 'Everyday']].plot.bar(ax=ax[c])
46/61:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques=['tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']

fig, ax = plt.subplots(1,4)
for c,exp in enumerate(['rc_profit','wc_profit','rc_loss','wc_loss']):
    ansdf[ansdf.experiment==exp].tradingfreq.value_counts()#.ix[['Never', 'Rarely', 'Occasionally', 'Often', 'Everyday']].plot.bar(ax=ax[c])
46/62:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques=['tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']

fig, ax = plt.subplots(1,4)
for c,exp in enumerate(['rc_profit','wc_profit','rc_loss','wc_loss']):
    print(ansdf[ansdf.experiment==exp].tradingfreq.value_counts())#.ix[['Never', 'Rarely', 'Occasionally', 'Often', 'Everyday']].plot.bar(ax=ax[c])
46/63:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques=['tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']

fig, ax = plt.subplots(1,4)
for c,exp in enumerate(["'rc_profit'","'wc_profit'","'rc_loss'","'wc_loss'"]):
    print(ansdf[ansdf.experiment==exp].tradingfreq.value_counts())#.ix[['Never', 'Rarely', 'Occasionally', 'Often', 'Everyday']].plot.bar(ax=ax[c])
46/64:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques=['tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']

fig, ax = plt.subplots(1,4)
for c,exp in enumerate(["'rc_profit'","'wc_profit'","'rc_loss'","'wc_loss'"]):
    ansdf[ansdf.experiment==exp].tradingfreq.value_counts().ix[['Never', 'Rarely', 'Occasionally', 'Often', 'Everyday']].plot.bar(ax=ax[c])
46/65:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques=['tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']

fig, ax = plt.subplots(1,4, figsize=(8,5))
for c,exp in enumerate(["'rc_profit'","'wc_profit'","'rc_loss'","'wc_loss'"]):
    ansdf[ansdf.experiment==exp].tradingfreq.value_counts().ix[['Never', 'Rarely', 'Occasionally', 'Often', 'Everyday']].plot.bar(ax=ax[c])
46/66:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques=['tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']

fig, ax = plt.subplots(1,4, figsize=(10,5))
for c,exp in enumerate(["'rc_profit'","'wc_profit'","'rc_loss'","'wc_loss'"]):
    ansdf[ansdf.experiment==exp].tradingfreq.value_counts().ix[['Never', 'Rarely', 'Occasionally', 'Often', 'Everyday']].plot.bar(ax=ax[c])
46/67:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques=['tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']
name = ['LiveTrackers-Profit', 'WaterCoolers-Profit', 'LiveTrackers-Loss', 'WaterCoolers-Loss']

fig, ax = plt.subplots(1,4, figsize=(10,5))
for c,exp in enumerate(["'rc_profit'","'wc_profit'","'rc_loss'","'wc_loss'"]):
    ax[c].set_title(name[c])
    ansdf[ansdf.experiment==exp].tradingfreq.value_counts().ix[['Never', 'Rarely', 'Occasionally', 'Often', 'Everyday']].plot.bar(ax=ax[c])
46/68:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques=['tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']
name = ['LiveTrackers-Profit', 'WaterCoolers-Profit', 'LiveTrackers-Loss', 'WaterCoolers-Loss']

fig, ax = plt.subplots(1,4, figsize=(6,5))
for c,exp in enumerate(["'rc_profit'","'wc_profit'","'rc_loss'","'wc_loss'"]):
    ax[c].set_title(name[c])
    ansdf[ansdf.experiment==exp].tradingfreq.value_counts().ix[['Never', 'Rarely', 'Occasionally', 'Often', 'Everyday']].plot.bar(ax=ax[c])
46/69:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques=['tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']
name = ['LiveTrackers-Profit', 'WaterCoolers-Profit', 'LiveTrackers-Loss', 'WaterCoolers-Loss']

fig, ax = plt.subplots(1,4, figsize=(10,3))
for c,exp in enumerate(["'rc_profit'","'wc_profit'","'rc_loss'","'wc_loss'"]):
    ax[c].set_title(name[c])
    ansdf[ansdf.experiment==exp].tradingfreq.value_counts().ix[['Never', 'Rarely', 'Occasionally', 'Often', 'Everyday']].plot.bar(ax=ax[c])
46/70:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
# ques=
name = ['LiveTrackers-Profit', 'WaterCoolers-Profit', 'LiveTrackers-Loss', 'WaterCoolers-Loss']

fig, ax = plt.subplots(1,4, figsize=(10,3))
for c,exp in enumerate(["'rc_profit'","'wc_profit'","'rc_loss'","'wc_loss'"]):
    ax[c].set_title(name[c])
    ansdf[ansdf.experiment==exp].confidence.value_counts().ix[['No', 'Yes']].plot.bar(ax=ax[c])
46/71:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
# ques=
name = ['LiveTrackers-Profit', 'WaterCoolers-Profit', 'LiveTrackers-Loss', 'WaterCoolers-Loss']

fig, ax = plt.subplots(1,4, figsize=(12,3))
for c,exp in enumerate(["'rc_profit'","'wc_profit'","'rc_loss'","'wc_loss'"]):
    ax[c].set_title(name[c])
    ansdf[ansdf.experiment==exp].confidence.value_counts().ix[['No', 'Yes']].plot.bar(ax=ax[c])
46/72:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques='focus'
ans=['Not at all', 'Distracted', 'Neutral', 'Attentive', 'Hawkish']
name = ['LiveTrackers-Profit', 'WaterCoolers-Profit', 'LiveTrackers-Loss', 'WaterCoolers-Loss']

fig, ax = plt.subplots(1,4, figsize=(12,3))
for c,exp in enumerate(["'rc_profit'","'wc_profit'","'rc_loss'","'wc_loss'"]):
    ax[c].set_title(name[c])
    ansdf[ansdf.experiment==exp][ques].value_counts().ix[ans].plot.bar(ax=ax[c])
46/73:
df['tradingfreq'].replace(dict(zip(range(1,6), ['Never', 'Rarely', 'Occasionally', 'Often', 'Everyday'])), inplace=True)
df['confidence'].replace(dict(zip(range(1,3), ['No', 'Yes'])), inplace=True)
df['focus'].replace(dict(zip(range(1,6), ['Not at all', 'Distracted', 'Neutral', 'Attentive', 'Hawkish'])), inplace=True)
df['stateofmind'].replace(dict(zip(range(1,6), ['Anxious', 'Slightly nervous', 'Calm', 'Fairly optimistic', 'Confident!'])), inplace=True)
df['regret'].replace(dict(zip(range(1,5), ['No', 'Only Trade 1', 'Only Trade 2', 'Both Trades'])), inplace=True)
df.to_csv('result_answers.csv')
46/74:
df['tradingfreq'].replace(dict(zip(range(1,6), ['Never', 'Rarely', 'Occasionally', 'Often', 'Everyday'])), inplace=True)
df['confidence'].replace(dict(zip(range(1,3), ['No', 'Yes'])), inplace=True)
df['focus'].replace(dict(zip(range(1,6), ['Not at all', 'Distracted', 'Neutral', 'Attentive', 'Hawkish'])), inplace=True)
df['stateofmind'].replace(dict(zip(range(1,6), ['Anxious', 'Slightly nervous', 'Calm', 'Fairly optimistic', 'Confident!'])), inplace=True)
df['regret'].replace(dict(zip(range(1,5), ['No', 'Only Trade 1', 'Only Trade 2', 'Both Trades'])), inplace=True)
# df.to_csv('result_answers.csv')
46/75:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques='focus'
ans=['Not at all', 'Distracted', 'Neutral', 'Attentive', 'Hawkish']
name = ['LiveTrackers-Profit', 'WaterCoolers-Profit', 'LiveTrackers-Loss', 'WaterCoolers-Loss']

fig, ax = plt.subplots(1,4, figsize=(12,3))
for c,exp in enumerate(["'rc_profit'","'wc_profit'","'rc_loss'","'wc_loss'"]):
    ax[c].set_title(name[c])
    ansdf[ansdf.experiment==exp][ques].value_counts().ix[ans].plot.bar(ax=ax[c])
46/76: ansdf.focus.value_counts()
46/77:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques='focus'
ans=['Not at all', 'Neutral', 'Attentive', 'Hawkish']
name = ['LiveTrackers-Profit', 'WaterCoolers-Profit', 'LiveTrackers-Loss', 'WaterCoolers-Loss']

fig, ax = plt.subplots(1,4, figsize=(12,3))
for c,exp in enumerate(["'rc_profit'","'wc_profit'","'rc_loss'","'wc_loss'"]):
    ax[c].set_title(name[c])
    ansdf[ansdf.experiment==exp][ques].value_counts().ix[ans].plot.bar(ax=ax[c])
46/78:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques='focus'
ans=['Not at all', 'Distracted', 'Neutral', 'Attentive', 'Hawkish']
name = ['LiveTrackers-Profit', 'WaterCoolers-Profit', 'LiveTrackers-Loss', 'WaterCoolers-Loss']

fig, ax = plt.subplots(1,4, figsize=(12,3))
for c,exp in enumerate(["'rc_profit'","'wc_profit'","'rc_loss'","'wc_loss'"]):
    ax[c].set_title(name[c])
    ax[c].set_xticklabels(ans)
    ansdf[ansdf.experiment==exp][ques].value_counts().plot.bar(ax=ax[c])
46/79:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques='focus'
ans=['Not at all', 'Distracted', 'Neutral', 'Attentive', 'Hawkish']
name = ['LiveTrackers-Profit', 'WaterCoolers-Profit', 'LiveTrackers-Loss', 'WaterCoolers-Loss']

fig, ax = plt.subplots(1,4, figsize=(12,3))
for c,exp in enumerate(["'rc_profit'","'wc_profit'","'rc_loss'","'wc_loss'"]):
    ax[c].set_title(name[c])
    ax[c].set_xticklabels(ans)
    ansdf[ansdf.experiment==exp][ques].value_counts().ix[ans].plot.bar(ax=ax[c])
46/80:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques='focus'
ans=['Not at all', 'Distracted', 'Neutral', 'Attentive', 'Hawkish']
name = ['LiveTrackers-Profit', 'WaterCoolers-Profit', 'LiveTrackers-Loss', 'WaterCoolers-Loss']

fig, ax = plt.subplots(1,4, figsize=(12,3))
for c,exp in enumerate(["'rc_profit'","'wc_profit'","'rc_loss'","'wc_loss'"]):
    ax[c].set_title(name[c])
    ax[c].set_xticklabels(ans)
    s=ansdf[ansdf.experiment==exp][ques].value_counts()
    vc = pd.Series(index=ans, data=[0]*len(ans))
    vc=s
    vc.ix[ans].plot.bar(ax=ax[c])
46/81:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques='focus'
ans=['Not at all', 'Distracted', 'Neutral', 'Attentive', 'Hawkish']
name = ['LiveTrackers-Profit', 'WaterCoolers-Profit', 'LiveTrackers-Loss', 'WaterCoolers-Loss']

fig, ax = plt.subplots(1,4, figsize=(12,3))
for c,exp in enumerate(["'rc_profit'","'wc_profit'","'rc_loss'","'wc_loss'"]):
    ax[c].set_title(name[c])
    ax[c].set_xticklabels(ans)
    s=ansdf[ansdf.experiment==exp][ques].value_counts()
    vc = pd.Series(index=ans, data=[0]*len(ans))
    vc=vc.combine(s, max, fill_value=0)
    vc.ix[ans].plot.bar(ax=ax[c])
46/82:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques='stateofmind'
ans=['Anxious', 'Slightly nervous', 'Calm', 'Fairly optimistic', 'Confident!']
name = ['LiveTrackers-Profit', 'WaterCoolers-Profit', 'LiveTrackers-Loss', 'WaterCoolers-Loss']

fig, ax = plt.subplots(1,4, figsize=(12,3))
for c,exp in enumerate(["'rc_profit'","'wc_profit'","'rc_loss'","'wc_loss'"]):
    ax[c].set_title(name[c])
    ax[c].set_xticklabels(ans)
    s=ansdf[ansdf.experiment==exp][ques].value_counts()
    vc = pd.Series(index=ans, data=[0]*len(ans))
    vc=vc.combine(s, max, fill_value=0)
    vc.ix[ans].plot.bar(ax=ax[c])
46/83:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques='regret'
ans=['No', 'Only Trade 1', 'Only Trade 2', 'Both Trades']
name = ['LiveTrackers-Profit', 'WaterCoolers-Profit', 'LiveTrackers-Loss', 'WaterCoolers-Loss']

fig, ax = plt.subplots(1,4, figsize=(12,3))
for c,exp in enumerate(["'rc_profit'","'wc_profit'","'rc_loss'","'wc_loss'"]):
    ax[c].set_title(name[c])
    ax[c].set_xticklabels(ans)
    s=ansdf[ansdf.experiment==exp][ques].value_counts()
    vc = pd.Series(index=ans, data=[0]*len(ans))
    vc=vc.combine(s, max, fill_value=0)
    vc.ix[ans].plot.bar(ax=ax[c])
48/1:
apa = pd.read_csv('ontologies\APAONTO.csv')
mf = pd.read_csv('ontologies\MF.csv')
mfo = pd.read_csv('ontologies\MFOMD.csv')
sui = pd.read_csv('ontologies\suicideo.csv')
48/2: import pandas as pd
48/3:
apa = pd.read_csv('ontologies\APAONTO.csv')
mf = pd.read_csv('ontologies\MF.csv')
mfo = pd.read_csv('ontologies\MFOMD.csv')
sui = pd.read_csv('ontologies\suicideo.csv')
48/4: apa
48/5: apa[apa['Preferred Label']=='Desertion']
48/6: apa
48/7: apa.Synonyms
48/8: apa.Synonyms.notnull().sum()
48/9: ap
48/10: apa
48/11: mf
48/12: mf.notnull().sum()
48/13: mf
48/14: mfo
48/15: sui
48/16:
# populate vocabulary
vocab_list={}
apa['Preferred Label'].unique()
48/17:
# populate vocabulary
vocab_list={}
set(apa['Preferred Label'].unique())
48/18:
# populate vocabulary
vocab_list={}
vocab_list = vocab_list.union(set(apa['Preferred Label'].unique()))
48/19:
# populate vocabulary
vocab_list=set()
vocab_list = vocab_list.union(set(apa['Preferred Label'].unique()))
48/20: apa['Alt name']
48/21: apa['Alt name'].unique()
48/22: 'AZT|Azidothymidine'.split('|')
48/23:
thes=defaultdict(dict)

def add_defn(w, dfn, onto):
    d=thes[w]
    d[onto]=dfn
    thes[w] = d

for row in apa.iterrows():
    word = row['Preferred Label']
    defn = row['Definitions']
    syn = [word]+row['Alt name'].split('|')
    for w in syn:
        add_defn(w, defn, 'apa')
48/24:
import pandas as pd
from collections import defaultdict
48/25:
apa = pd.read_csv('ontologies\APAONTO.csv') #Preferred Label, Definitions, Alt name
mf = pd.read_csv('ontologies\MF.csv')
mfo = pd.read_csv('ontologies\MFOMD.csv')
sui = pd.read_csv('ontologies\suicideo.csv')
thes=defaultdict(dict)
48/26:
def add_defn(w, dfn, onto):
    d=thes[w]
    d[onto]=dfn
    thes[w] = d

for row in apa.iterrows():
    word = row['Preferred Label']
    defn = row['Definitions']
    syn = [word]+row['Alt name'].split('|')
    for w in syn:
        add_defn(w, defn, 'apa')
48/27:
def add_defn(w, dfn, onto):
    d=thes[w]
    d[onto]=dfn
    thes[w] = d

for row in apa.iterrows():
    print(row)
    break
    word = row['Preferred Label']
    defn = row['Definitions']
    syn = [word]+row['Alt name'].split('|')
    for w in syn:
        add_defn(w, defn, 'apa')
48/28:
def add_defn(w, dfn, onto):
    d=thes[w]
    d[onto]=dfn
    thes[w] = d

for row in apa.iterrows():
    print(type(row))
    break
    word = row['Preferred Label']
    defn = row['Definitions']
    syn = [word]+row['Alt name'].split('|')
    for w in syn:
        add_defn(w, defn, 'apa')
48/29:
def add_defn(w, dfn, onto):
    d=thes[w]
    d[onto]=dfn
    thes[w] = d

for row in apa.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    syn = [word]+row[1]['Alt name'].split('|')
    for w in syn:
        add_defn(w, defn, 'apa')
48/30:
def add_defn(w, dfn, onto):
    d=thes[w]
    d[onto]=dfn
    thes[w] = d

for row in apa.iterrows():
    print(row[1])
    break
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    syn = [word]+row[1]['Alt name'].split('|')
    for w in syn:
        add_defn(w, defn, 'apa')
48/31: '|'.split('|')
48/32:
def add_defn(w, dfn, onto):
    d=thes[w]
    d[onto]=dfn
    thes[w] = d

for row in apa.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    syn = [word]+row[1]['Alt name'].fillna('').split('|')
    for w in syn:
        add_defn(w, defn, 'apa')
48/33:
def add_defn(w, dfn, onto):
    d=thes[w]
    d[onto]=dfn
    thes[w] = d

for row in apa.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    syn = [word]+row[1].fillna('')['Alt name'].split('|')
    for w in syn:
        add_defn(w, defn, 'apa')
48/34: thes
48/35:
def add_defn(w, dfn, onto):
    d=thes[w]
    d[onto]=dfn
    thes[w] = d

for row in mf.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    syn = [word]+row[1].fillna('')['Alt name'].split('|')
    for w in syn:
        add_defn(w, defn, 'mf')
48/36: mf
48/37:
def add_defn(w, dfn, onto):
    d=thes[w]
    d[onto]=dfn
    thes[w] = d

for row in mf.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    add_defn(w, defn, 'mf')
    continue
    syn = [word]+row[1].fillna('')['Alt name'].split('|')
    for w in syn:
        add_defn(w, defn, 'mf')
48/38: len(thes)
48/39: mfo
48/40: thes
48/41: mf[mf.Definitions.str.contains('The specific movement from place to pl')]
48/42: mf[mf.Definitions.fillna('').str.contains('The specific movement from place to pl')]
48/43: sui
48/44:
thes=defaultdict(dict)

def add_defn(w, dfn, onto):
    d=thes[w]
    d[onto]=dfn
    thes[w] = d


for row in apa.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    syn = [word]+row[1].fillna('')['Alt name'].split('|')
    for w in syn:
        add_defn(w, defn, 'apa')    
        
for row in mf.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    add_defn(word, defn, 'mf')
    
for row in mfo.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    syn = [word]+row[1].fillna('')['Synonyms'].split('|')
    for w in syn:
        add_defn(w, defn, 'mfo')

for row in sui.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    syn = [word]+row[1].fillna('')['alternative term'].split('\n')
    for w in syn:
        add_defn(w, defn, 'sui')
48/45: len(thes)
48/46: thes
48/47: apa
48/48:
thes=defaultdict(dict)

def add_defn(w, dfn, onto):
    d=thes[w]
    d[onto]=dfn
    thes[w] = d


for row in apa.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if not defn:
        continue
    syn = [word]+row[1].fillna('')['Alt name'].split('|')
    for w in syn:
        add_defn(w, defn, 'apa')    
        
for row in mf.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if not defn:
        continue
    add_defn(word, defn, 'mf')
    
for row in mfo.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if not defn:
        continue
    syn = [word]+row[1].fillna('')['Synonyms'].split('|')
    for w in syn:
        add_defn(w, defn, 'mfo')

for row in sui.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if not defn:
        continue
    syn = [word]+row[1].fillna('')['alternative term'].split('\n')
    for w in syn:
        add_defn(w, defn, 'sui')
48/49: len(thes)
48/50: thes
48/51: apa
48/52: apa.iloc[2]
48/53: apa.iloc[1]
48/54: apa.iloc[1]['Definitions']
48/55: type(apa.iloc[1]['Definitions'])
48/56: apa.iloc[1]['Definitions']==nan
48/57: apa.iloc[1]['Definitions']==np.nan
48/58: apa.iloc[1]['Definitions']==False
48/59: apa.iloc[1]['Definitions']==np.nan
48/60: np.nan
48/61:
import math
math.isnan(apa.iloc[1]['Definitions'])
48/62:
thes=defaultdict(dict)

def add_defn(w, dfn, onto):
    d=thes[w]
    d[onto]=dfn
    thes[w] = d


for row in apa.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if math.isnan(defn):
        continue
    syn = [word]+row[1].fillna('')['Alt name'].split('|')
    for w in syn:
        add_defn(w, defn, 'apa')    
        
for row in mf.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if math.isnan(defn):
        continue
    add_defn(word, defn, 'mf')
    
for row in mfo.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if math.isnan(defn):
        continue
    syn = [word]+row[1].fillna('')['Synonyms'].split('|')
    for w in syn:
        add_defn(w, defn, 'mfo')

for row in sui.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if math.isnan(defn):
        continue
    syn = [word]+row[1].fillna('')['alternative term'].split('\n')
    for w in syn:
        add_defn(w, defn, 'sui')
48/63:
import math
s=apa.iloc[1]['Definitions']
math.isnan(s)
48/64:
thes=defaultdict(dict)

def add_defn(w, dfn, onto):
    d=thes[w]
    d[onto]=dfn
    thes[w] = d


for row in apa.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='nan' or math.isnan(defn):
        continue
    syn = [word]+row[1].fillna('')['Alt name'].split('|')
    for w in syn:
        add_defn(w, defn, 'apa')    
        
for row in mf.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if math.isnan(defn):
        continue
    add_defn(word, defn, 'mf')
    
for row in mfo.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if math.isnan(defn):
        continue
    syn = [word]+row[1].fillna('')['Synonyms'].split('|')
    for w in syn:
        add_defn(w, defn, 'mfo')

for row in sui.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if math.isnan(defn):
        continue
    syn = [word]+row[1].fillna('')['alternative term'].split('\n')
    for w in syn:
        add_defn(w, defn, 'sui')
48/65:
thes=defaultdict(dict)

def add_defn(w, dfn, onto):
    d=thes[w]
    d[onto]=dfn
    thes[w] = d


for row in apa.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='nan':# or math.isnan(defn):
        continue
    syn = [word]+row[1].fillna('')['Alt name'].split('|')
    for w in syn:
        add_defn(w, defn, 'apa')    
        
for row in mf.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if math.isnan(defn):
        continue
    add_defn(word, defn, 'mf')
    
for row in mfo.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if math.isnan(defn):
        continue
    syn = [word]+row[1].fillna('')['Synonyms'].split('|')
    for w in syn:
        add_defn(w, defn, 'mfo')

for row in sui.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if math.isnan(defn):
        continue
    syn = [word]+row[1].fillna('')['alternative term'].split('\n')
    for w in syn:
        add_defn(w, defn, 'sui')
48/66: thes
48/67:
for row in apa.head().iterrows():
    print(row)
48/68:
for row in apa.head().iterrows():
    print(row[1])
48/69:
for row in apa.head().iterrows():
    print(row[1]['Definitions'].fillna(0))
48/70:
for row in apa.head().iterrows():
    print(row[1]['Definitions'])
48/71:
for row in apa.loc[1:].head().iterrows():
    print(row[1]['Definitions'])
48/72:
for row in apa.loc[1:].head().iterrows():
    s = row[1]['Definitions']
    if s:
        print(s)
48/73:
for row in apa.loc[1:].head().iterrows():
    s = row[1]['Definitions']
    if s==nan:
        print(s)
48/74:
for row in apa.loc[1:].head().iterrows():
    s = row[1]['Definitions']
    if s=='nan':
        print(s)
48/75:
for row in apa.loc[1:].head().iterrows():
    s = row[1]['Definitions']
    if s=='nan':
        print(s)
48/76:
for row in apa.loc[1:].head().iterrows():
    s = row[1]['Definitions']
    if s!='nan':
        print(s)
48/77:
thes=defaultdict(dict)

def add_defn(w, dfn, onto):
    d=thes[w]
    d[onto]=dfn
    thes[w] = d


for row in apa.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['Alt name'].split('|')
    for w in syn:
        add_defn(w, defn, 'apa')    
        
for row in mf.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    add_defn(word, defn, 'mf')
    
for row in mfo.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['Synonyms'].split('|')
    for w in syn:
        add_defn(w, defn, 'mfo')

for row in sui.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['alternative term'].split('\n')
    for w in syn:
        add_defn(w, defn, 'sui')
48/78:
apa = pd.read_csv('ontologies\APAONTO.csv').fillna('') #Preferred Label, Definitions, Alt name
mf = pd.read_csv('ontologies\MF.csv').fillna('')
mfo = pd.read_csv('ontologies\MFOMD.csv').fillna('')
sui = pd.read_csv('ontologies\suicideo.csv').fillna('')
48/79:
thes=defaultdict(dict)

def add_defn(w, dfn, onto):
    d=thes[w]
    d[onto]=dfn
    thes[w] = d


for row in apa.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['Alt name'].split('|')
    for w in syn:
        add_defn(w, defn, 'apa')    
        
for row in mf.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    add_defn(word, defn, 'mf')
    
for row in mfo.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['Synonyms'].split('|')
    for w in syn:
        add_defn(w, defn, 'mfo')

for row in sui.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['alternative term'].split('\n')
    for w in syn:
        add_defn(w, defn, 'sui')
48/80: thes
48/81: apa[apa.Definitions.str.contains("The difference between a child's act")]
48/82: apa.loc[6033]
48/83: apa.loc[6033]['Preferred Label']
48/84:
# word=apa.loc[6033]['Preferred Label']
# [word]+apa.loc[6033]['Alt name'].split('|')
apa.loc[6033]['Alt name'].split('|')
48/85:
word=apa.loc[6033]['Preferred Label']
[word]+apa.loc[6033]['Alt name'].split('|')
48/86:
word=apa.loc[6033]['Preferred Label']
for x in [word]+apa.loc[6033]['Alt name'].split('|'):
    print(x)
48/87:
# word=apa.loc[6033]['Preferred Label']
# for x in [word]+apa.loc[6033]['Alt name'].split('|'):
#     print(x)
apa.loc[6033]['Alt name'].split('|')
48/88:
thes=defaultdict(dict)

def add_defn(w, dfn, onto):
    if w=='':
        pass
    d=thes[w]
    d[onto]=dfn
    thes[w] = d


for row in apa.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['Alt name'].split('|')
    for w in syn:
        add_defn(w, defn, 'apa')    
        
for row in mf.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    add_defn(word, defn, 'mf')
    
for row in mfo.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['Synonyms'].split('|')
    for w in syn:
        add_defn(w, defn, 'mfo')

for row in sui.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['alternative term'].split('\n')
    for w in syn:
        add_defn(w, defn, 'sui')
48/89: len(thes)
48/90: thes
48/91: mfo[mfo.Definitions.str.contains("Addiction is a mental disease in which a person persists in the use of a mood altering substance or in a behaviour despite adverse consequences.|Addictions can include, but are not limited to, alcohol abuse, drug abuse, exercise abuse, pornography and gambling. Cl")]
48/92: len(mfo)
48/93:
thes=defaultdict(dict)

def add_defn(w, dfn, onto):
    if w=='':
        pass
    d=thes[w]
    d[onto]=dfn
    thes[w] = d


for row in apa.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['Alt name'].split('|')
    for w in syn:
        add_defn(w, defn, 'apa')    
        
for row in mf.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    add_defn(word, defn, 'mf')
    
for row in mfo.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['Synonyms'].split('|')
    for w in syn:
        add_defn(w, defn, 'mfo')

for row in sui.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['alternative term'].split('\n')
    for w in syn:
        add_defn(w, defn, 'sui')
48/94: thes
48/95:
thes=defaultdict(dict)

def add_defn(w, dfn, onto):
    if w=='':
        pass
    d=thes[w]
    d[onto]=dfn.replace('\n', ' ')
    thes[w] = d


for row in apa.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['Alt name'].split('|')
    for w in syn:
        add_defn(w, defn, 'apa')    
        
for row in mf.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    add_defn(word, defn, 'mf')
    
for row in mfo.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['Synonyms'].split('|')
    for w in syn:
        add_defn(w, defn, 'mfo')

for row in sui.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['alternative term'].split('\n')
    for w in syn:
        add_defn(w, defn, 'sui')
48/96: thes
48/97:
s='[Wikipedia: http://en.wikipedia.org/wiki/Attention-Deficit_Hyperactivity_Disorder]|The'
re.sub(r'http\S+', '', s)
48/98:
import pandas as pd
from collections import defaultdict
import math
import re
48/99:
s='[Wikipedia: http://en.wikipedia.org/wiki/Attention-Deficit_Hyperactivity_Disorder]|The'
re.sub(r'http\S+', '', s)
48/100: mfo['Definitions']
48/101: mfo['Definitions'].iloc[2]
48/102: mfo['Definitions'].iloc[3]
48/103: mfo['Definitions']
48/104: mfo['Definitions'].loc[84]
48/105: mfo['Definitions'].loc[844]
48/106:
s='[Wikipedia: http://en.wikipedia.org/wiki/Attention-Deficit_Hyperactivity_Disorder]|The'
re.sub(r'\[wikipedia: http\S+\]', '', s, flags=re.IGNORECASE)
48/107:
thes=defaultdict(dict)

def add_defn(w, dfn, onto):
    if w=='':
        pass
    d=thes[w]
    d[onto]=re.sub(r'\[wikipedia: http\S+\]', '', dfn.replace('\n', ' '), flags=re.IGNORECASE)
    thes[w] = d


for row in apa.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['Alt name'].split('|')
    for w in syn:
        add_defn(w, defn, 'apa')    
        
for row in mf.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    add_defn(word, defn, 'mf')
    
for row in mfo.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['Synonyms'].split('|')
    for w in syn:
        add_defn(w, defn, 'mfo')

for row in sui.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['alternative term'].split('\n')
    for w in syn:
        add_defn(w, defn, 'sui')
48/108: thes
48/109:
import pickle 
with open('psychThesaurus.pkl','wb') as f:
        pickle.dump(thes, f)
48/110: list(range(1,12,5))
48/111: list(range(0,12,5))
48/112:
window_size = 5


def get_studded_defn(w, dfn):
    stud=[w]
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return stud
        
        
get_studded_defn("ABC","sdfsd sdfsdf sdfsdfsrw wrwerqw uiojomp xmnvxmncbv sreiowerinxc dsjfnsklc lzxf sjklfhaweiupfzn")
48/113:
import pandas as pd
from collections import defaultdict
import math
import re
from nltk import word_tokenize
48/114:
window_size = 5


def get_studded_defn(w, dfn):
    stud=[w]
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return stud
        
        
get_studded_defn("ABC","sdfsd sdfsdf sdfsdfsrw wrwerqw uiojomp xmnvxmncbv sreiowerinxc dsjfnsklc lzxf sjklfhaweiupfzn")
48/115: thes[0]
48/116: thes[1]
48/117: thes
48/118: thes['ADHD']
48/119: thes['ADHD'].values()
48/120: list(thes['ADHD'].values())
48/121:
for x in thes['ADHD'].values():
    print(x)
48/122:
window_size = 5


def get_studded_defn(w, dfn):
    stud=[w]
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes:
        dfns = d.values()
        for dd in dfns:
            yield get_studded_defn(w,dd)

c=0
for l in dict_iter():
    if c>20:
        break
    print(l)
    c+=1
48/123:
window_size = 5


def get_studded_defn(w, dfn):
    stud=[w]
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            yield get_studded_defn(w,dd)

c=0
for l in dict_iter():
    if c>20:
        break
    print(l)
    c+=1
48/124:
def remove_stopwords(s):
    # nltk_stopwords = ['i',  'me',  'my',  'myself',  'we',  'our',  'ours',  'ourselves',  'you',  "you're",  "you've",  "you'll",  "you'd",  'your',  'yours',  'yourself',  'yourselves',  'he',  'him',  'his',  'himself',  'she',  "she's",  'her',  'hers',  'herself',  'it',  "it's",  'its',  'itself',  'they',  'them',  'their',  'theirs',  'themselves',  'what',  'which',  'who',  'whom',  'this',  'that',  "that'll",  'these',  'those',  'am',  'is',  'are',  'was',  'were',  'be',  'been',  'being',  'have',  'has',  'had',  'having',  'do',  'does',  'did',  'doing',  'a',  'an',  'the',  'and',  'but',  'if',  'or',  'because',  'as',  'until',  'while',  'of',  'at',  'by',  'for',  'with',  'about',  'against',  'between',  'into',  'through',  'during',  'before',  'after',  'above',  'below',  'to',  'from',  'up',  'down',  'in',  'out',  'on',  'off',  'over',  'under',  'again',  'further',  'then',  'once',  'here',  'there',  'when',  'where',  'why',  'how',  'all',  'any',  'both',  'each',  'few',  'more',  'most',  'other',  'some',  'such',  'no',  'nor',  'not',  'only',  'own',  'same',  'so',  'than',  'too',  'very',  's',  't',  'can',  'will',  'just',  'don',  "don't",  'should',  "should've",  'now',  'd',  'll',  'm',  'o',  're',  've',  'y',  'ain',  'aren',  "aren't",  'couldn',  "couldn't",  'didn',  "didn't",  'doesn',  "doesn't",  'hadn',  "hadn't",  'hasn',  "hasn't",  'haven',  "haven't",  'isn',  "isn't",  'ma',  'mightn',  "mightn't",  'mustn',  "mustn't",  'needn',  "needn't",  'shan',  "shan't",  'shouldn',  "shouldn't",  'wasn',  "wasn't",  'weren',  "weren't",  'won',  "won't",  'wouldn',  "wouldn't"]
    nltk_stopwords = stopwords.words('english')
    STOPWORDS = nltk_stopwords+["'s", "s"]+[x.capitalize() for x in nltk_stopwords]
    
    s = re.sub(r'([a-zA-Z])[\-\/]([a-zA-Z])', r'\1 \2', s) # split hyphenated and slashed words
    s = re.sub(r'[^A-Za-z\s]+', '', s)
    tokens = word_tokenize(s)
    okstr = ' '.join([x for x in tokens if x in ['(',')'] or x not in STOPWORDS and len(x)>1])
    return okstr


def filter_pos_tags(s):
    OUT = ['VBD', 'IN', 'VBP', 'RB', 'MD', 'VB', 'VBZ', 'VBG']
    return ' '.join([w for w,t in pos_tag(word_tokenize(s)) if t not in OUT])


def abbr_expander(s):   
    res = re.findall(r'\(\s?[A-Z]\s?[A-Za-z]+\s?\)', s) # Misses acronyms with hyphen
    outs = s
    for abbr in res:
        ix = s.index(abbr)
        abbr_clean = re.sub(r'[^A-Za-z]', '', abbr) # keep only letters, discard all other chars
        abbr_len = len(re.sub(r'[a-z]','',abbr_clean)) # Number of capital letters ~ number of tokens to look back for expansion
        if abbr_clean[-1]=='s': abbr_len-=1 # Singularing a plural acronym
        # When looking for expansion candidates, consider (-) and (/) as word-breaks
        expanded = ' '.join(s[ix-1::-1][::-1].rstrip().split(' ')[-abbr_len:]) # identify possible candidates for expansion from preivous abbr_len words
        # ABBR_LIST[abbr_clean].append(expanded)
        outs = outs.replace(' '+abbr, '') # Remove first instance of abbreviation before expanding other instances
        if abbr_clean[-1]=='s':
            outs = outs.replace(abbr_clean[:-1], expanded)
        else:
            outs = outs.replace(abbr_clean, expanded) 
        
    return outs

def clean_sent(abst):
    abst = remove_stopwords(abst)
    abst = abbr_expander(abst)
    # Make post-period uppercase chars lower
    abst = re.sub(r"(?<=\. )[A-Z]",lambda t:t.group().lower(), abst)
    abst = filter_pos_tags(abst)
    return abst
48/125:
window_size = 5


def get_studded_defn(w, dfn):
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            yield get_studded_defn(w,dd)

c=0
for l in dict_iter():
    if c>20:
        break
    print(l)
    c+=1
48/126:
import pandas as pd
from collections import defaultdict
import math
import re
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
import numpy as np
48/127:
window_size = 5


def get_studded_defn(w, dfn):
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            yield get_studded_defn(w,dd)

c=0
for l in dict_iter():
    if c>20:
        break
    print(l)
    c+=1
48/128:
window_size = 5


def get_studded_defn(w, dfn):
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('Psychlopedia.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
45/43: from nltk.corpus import wordnet as wn
48/129:
import pandas as pd
from collections import defaultdict
import math
import re
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
import numpy as np
48/130:
apa = pd.read_csv('ontologies\APAONTO.csv').fillna('') #Preferred Label, Definitions, Alt name
mf = pd.read_csv('ontologies\MF.csv').fillna('')
mfo = pd.read_csv('ontologies\MFOMD.csv').fillna('')
sui = pd.read_csv('ontologies\suicideo.csv').fillna('')
48/131:
thes=defaultdict(dict)

def add_defn(w, dfn, onto):
    if w=='':
        pass
    d=thes[w]
    d[onto]=re.sub(r'\[wikipedia: http\S+\]', '', dfn.replace('\n', ' '), flags=re.IGNORECASE)
    thes[w] = d


for row in apa.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['Alt name'].split('|')
    for w in syn:
        add_defn(w, defn, 'apa')    
        
for row in mf.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    add_defn(word, defn, 'mf')
    
for row in mfo.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['Synonyms'].split('|')
    for w in syn:
        add_defn(w, defn, 'mfo')

for row in sui.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['alternative term'].split('\n')
    for w in syn:
        add_defn(w, defn, 'sui')
        
import pickle 
with open('psychThesaurusDict.pkl','wb') as f:
        pickle.dump(thes, f)
48/132:
def remove_stopwords_punct(s):
    # nltk_stopwords = ['i',  'me',  'my',  'myself',  'we',  'our',  'ours',  'ourselves',  'you',  "you're",  "you've",  "you'll",  "you'd",  'your',  'yours',  'yourself',  'yourselves',  'he',  'him',  'his',  'himself',  'she',  "she's",  'her',  'hers',  'herself',  'it',  "it's",  'its',  'itself',  'they',  'them',  'their',  'theirs',  'themselves',  'what',  'which',  'who',  'whom',  'this',  'that',  "that'll",  'these',  'those',  'am',  'is',  'are',  'was',  'were',  'be',  'been',  'being',  'have',  'has',  'had',  'having',  'do',  'does',  'did',  'doing',  'a',  'an',  'the',  'and',  'but',  'if',  'or',  'because',  'as',  'until',  'while',  'of',  'at',  'by',  'for',  'with',  'about',  'against',  'between',  'into',  'through',  'during',  'before',  'after',  'above',  'below',  'to',  'from',  'up',  'down',  'in',  'out',  'on',  'off',  'over',  'under',  'again',  'further',  'then',  'once',  'here',  'there',  'when',  'where',  'why',  'how',  'all',  'any',  'both',  'each',  'few',  'more',  'most',  'other',  'some',  'such',  'no',  'nor',  'not',  'only',  'own',  'same',  'so',  'than',  'too',  'very',  's',  't',  'can',  'will',  'just',  'don',  "don't",  'should',  "should've",  'now',  'd',  'll',  'm',  'o',  're',  've',  'y',  'ain',  'aren',  "aren't",  'couldn',  "couldn't",  'didn',  "didn't",  'doesn',  "doesn't",  'hadn',  "hadn't",  'hasn',  "hasn't",  'haven',  "haven't",  'isn',  "isn't",  'ma',  'mightn',  "mightn't",  'mustn',  "mustn't",  'needn',  "needn't",  'shan',  "shan't",  'shouldn',  "shouldn't",  'wasn',  "wasn't",  'weren',  "weren't",  'won',  "won't",  'wouldn',  "wouldn't"]
    nltk_stopwords = stopwords.words('english')
    STOPWORDS = nltk_stopwords+["'s", "s"]+[x.capitalize() for x in nltk_stopwords]
    
    
    s = re.sub(r'[^A-Za-z\s]+', '', s)
    tokens = word_tokenize(s)
    okstr = ' '.join([x for x in tokens if x in ['(',')'] or x not in STOPWORDS and len(x)>1])
    return okstr


def filter_pos_tags(s):
    OUT = ['VBD', 'IN', 'VBP', 'RB', 'MD', 'VB', 'VBZ', 'VBG']
    return ' '.join([w for w,t in pos_tag(word_tokenize(s)) if t not in OUT])


def abbr_expander(s):   
    res = re.findall(r'\(\s?[A-Z]\s?[A-Za-z]+\s?\)', s) # Misses acronyms with hyphen
    outs = s
    for abbr in res:
        ix = s.index(abbr)
        abbr_clean = re.sub(r'[^A-Za-z]', '', abbr) # keep only letters, discard all other chars
        abbr_len = len(re.sub(r'[a-z]','',abbr_clean)) # Number of capital letters ~ number of tokens to look back for expansion
        if abbr_clean[-1]=='s': abbr_len-=1 # Singularing a plural acronym
        # When looking for expansion candidates, consider (-) and (/) as word-breaks
        expanded = ' '.join(s[ix-1::-1][::-1].rstrip().split(' ')[-abbr_len:]) # identify possible candidates for expansion from preivous abbr_len words
        # ABBR_LIST[abbr_clean].append(expanded)
        outs = outs.replace(' '+abbr, '') # Remove first instance of abbreviation before expanding other instances
        if abbr_clean[-1]=='s':
            outs = outs.replace(abbr_clean[:-1], expanded)
        else:
            outs = outs.replace(abbr_clean, expanded) 
        
    return outs

def clean_sent(abst):
    abst = re.sub(r'([a-zA-Z])[\-\/]([a-zA-Z])', r'\1 \2', abst) # split hyphenated and slashed words
    abst = abbr_expander(abst)
    
    # Make post-period and first word uppercase chars lower
    abst = re.sub(r"(?<=\. )[A-Z]",lambda t:t.group().lower(), abst)
    abst = abst[0].lower()+abst[1:]
    
    abst = filter_pos_tags(abst)
    abst=remove_stopwords_punct(abst)
    return abst
48/133:
window_size = 10


def get_studded_defn(w, dfn):
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            yield get_studded_defn(w.lower(),dd)

gen=dict_iter()
with open('psychlopedia_train.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
48/134:
window_size = 10


def get_studded_defn(w, dfn):
    w=clean_sent(w)
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
48/135:
window_size = 10


def get_studded_defn(w, dfn):
    
    w=clean_sent(w)
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                pass
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
48/136:
window_size = 10


def get_studded_defn(w, dfn):
    
    w=clean_sent(w)
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
45/44: l=list(range(5))
45/45: l
45/46: l[-1::-1]
45/47: l[-1:3:-1]
45/48: iter([1,2])
51/1: from str_helper import *
51/2: l=["OBJECTIVE: Life-threatening danger is assumed to produce in tandem increases in both vigilance toward threat and stress-related symptoms but no data test the validity of this assumption. The authors examined associations in real time among imminent life-threatening danger stress-related symptoms and vigilance. METHOD: Symptoms of posttraumatic stress disorder (PTSD) depression and anxiety were measured in a civilian population (N=131) as a function of war-related stress operationalized as the time available for seeking cover from rocket attack. A computerized measure of threat-related vigilance using a classic dot-probe attention task was also collected. RESULTS: PTSD symptoms depression and anxiety increased as a function of war-related threat. Acute proximal threat was associated with avoidance of rather than vigilance toward negative valence information. For participants within rocket range the magnitude of threat bias varied with the magnitude of distress symptoms such that as bias away from threat increased distress symptoms increased. CONCLUSIONS: These data challenge current thinking about the role of attention in stress responding. Attentional threat avoidance may reduce the acute impact of imminent threat but this may come at a price in terms of an elevated risk for psychopathology.", "Learned safety a learning process in which a cue becomes associated with the absence of threat is disrupted in individuals with post-traumatic stress disorder (PTSD). A bi-directional relationship exists between smoking and PTSD and one potential explanation is that nicotine-associated changes in cognition facilitate PTSD emotional dysregulation by disrupting safety associations. Therefore we investigated whether nicotine would disrupt learned safety by enhancing fear associated with a safety cue. In the present study C57BL/6 mice were administered acute or chronic nicotine and trained over three days in a differential backward trace conditioning paradigm consisting of five trials of a forward conditioned stimulus (CS)+ (Light) co-terminating with a footshock unconditioned stimulus followed by a backward CS- (Tone) presented 20 s after cessation of the unconditioned stimulus. Summation testing found that acute nicotine disrupted learned safety but chronic nicotine had no effect. Another group of animals administered acute nicotine showed fear when presented with the backward CS (Light) alone indicating the formation of a maladaptive fear association with the backward CS. Finally we investigated the brain regions involved by administering nicotine directly into the dorsal hippocampus ventral hippocampus and prelimbic cortex. Infusion of nicotine into the dorsal hippocampus disrupted safety learning.", "There is mounting evidence that single administrations of glucocorticoids may acutely reduce human fear. We previously reported that administration of cortisol acutely reduced non-spatial selective attention to fearful faces and likewise reduced preferential processing of fearful faces in a spatial working memory task. Here we report the acute effects of 40 mg cortisol (administered in a double-blind placebo-controlled crossover design) on a different experimental task for measuring threat-selective attention. Twenty healthy young males had to localize a target which was presented in a peripheral location that was either gazed at or not by a preceding dynamic happy or fearful face. This reliable method has been used repeatedly to demonstrate fear-driven selective attention. Present results showed that after placebo as usual the fearful gaze cues caused stronger orienting of attention than happy faces. Cortisol abolished this typical anxious response pattern but only in low anxious participants. These data provide evidence that cortisol acutely influences also spatial threat-selective attention. Possible neuroendocrine mechanisms are discussed.", "Although the long-acting opiate methadone is commonly used to treat drug addiction relatively little is known about the effects of withdrawal from this drug in preclinical models. The current study examined affective neuroendocrine and somatic signs of withdrawal from the longer-acting methadone derivative l-alpha-acetylmethydol (LAAM) in rats. Anxiety-like behavior during both spontaneous and antagonist-precipitated withdrawal was measured by potentiation of the startle reflex. Withdrawal elevated corticosterone and somatic signs and blunted circadian variations in baseline startle responding. In addition fear to an explicit Pavlovian conditioned stimulus (fear-potentiated startle) was enhanced. These data suggest that anxiety-like behavior as measured using potentiated startle responding does not emerge spontaneously during withdrawal from chronic opiate exposure - in contrast to withdrawal from acute drug exposure - but rather is manifested as exaggerated fear in response to explicit threat cues.", "Late-latency auditory and somatosensory evoked potentials are sensitive to some centrally acting drugs and to certain psychological interventions. In this experiment we compared the effects of acute doses of a benzodiazepine diazepam and an H(1) histamine receptor-blocking sedative diphenhydramine on auditory and somatosensory evoked potentials within the latency range 100-500 ms in a fear conditioning paradigm. Twelve healthy males (18-30 years) participated in three sessions at weekly intervals in which they received diazepam 10mg diphenhydramine 75 mg and placebo in a balanced double-blind crossover protocol. One hundred and twenty min after diphenhydramine or 60 min after diazepam they underwent an 8 min recording period in which auditory evoked potentials elicited by 40 ms 95 dB[A] 1 kHz tones and somatosensory evoked potentials elicited by a mildly painful electric shock (1.8 mA 50 ms) were recorded at Cz (vertex). Each session consisted of four blocks of trials in which either the sound pulse or the shock was presented. Alternate blocks were designated SAFE or THREAT ('context' conditions); in THREAT blocks subjects were warned that shocks would be delivered via electrodes placed on the wrist (electrodes were removed during SAFE blocks). In one SAFE and one THREAT block the sound stimuli and shocks (shocks were delivered only in the THREAT block) were preceded by a 2 s conditioned stimulus (CS: a red light) ('cue' condition). Diazepam but not diphenhydramine reduced the amplitude of the P2 auditory evoked potential. The THREAT context was associated with increased N1 and reduced N2 potential amplitudes. The CS had no effect on the amplitudes but markedly reduced the latencies of the N1 P2 and N2 potentials under the THREAT condition. Diazepam reduced the amplitudes of the somatosensory potential evoked by the shock; the CS shortened the latencies of the later components of the response. Diazepam and diphenhydramine were approximately equi-sedative in the doses used in this experiment as judged by visual analogue self-rating scales. The results indicate that the suppression of late-latency auditory and somatosensory evoked potentials by diazepam is not simply a reflection of sedation. Late-latency evoked potentials can be modified by an aversive CS but the components that are sensitive to the CS are different from those that are sensitive to diazepam.", "BACKGROUND: Recent laboratory studies have shown that acute alcohol intoxication selectively and effectively dampens aversive responding to uncertain threat. An emerging hypothesis is that individuals who exhibit heightened reactivity to uncertain threat may be especially motivated to use alcohol to dampen their distress setting the stage for negative reinforcement processes to drive excessive alcohol use. However no study to date has directly examined whether current problematic drinkers exhibit heightened reactivity to uncertain threat as would be expected. METHODS: The present study was therefore designed to examine the association between current problematic alcohol use and reactivity to uncertain threat during sobriety in two independent samples. In Study 1 (n=221) and Study 2 (n=74) adult participants completed the same well-validated threat-of-shock task which separately probes responses to temporally predictable and unpredictable threat. Startle potentiation was measured as an index of aversive responding. Problematic alcohol use was defined as number of binge episodes within the past 30days in Study 1 and total scores on a self-report measure of hazardous drinking in Study 2. RESULTS: As hypothesized across both studies greater levels of problematic drinking were associated with greater startle potentiation to unpredictable threat. In Study 2 hazardous drinking scores were also positively associated with startle potentiation to predictable threat. CONCLUSIONS: The findings are notably consistent with the notion that heightened reactivity to uncertain threat is an important individual difference factor associated with the onset and/or maintenance of problematic drinking behaviors and may therefore be a novel prevention and intervention target.", "Deep brain stimulation (DBS) has in some cases been associated with significant psychological effects and/or personality change. These effects occur sometimes as acute changes experienced intraoperatively or during the initial setting of the stimulator and sometimes as longer term progressive changes in the months following surgery. Sometimes they are the intended outcome of treatment and in other cases they are an unintended side-effect. In all of these circumstances some patients and caregivers have described the psychological effects of DBS as frightening or disconcerting. I trace the source of these negative reactions to the fear that stimulation-related psychological and personality changes represent a threat to personal identity and agency. This issue has implications both for philosophical theories of personal identity and agency and for clinical concerns. A narrative account of personal identity is developed to illuminate the nature of the threat to identity and agency DBS potentially poses and to suggest steps that might be taken to mitigate and avoid these threats.", "Peritraumatic responses aside from dissociation have been understudied in acute trauma populations. Participants were 172 female rape 68 assault and 80 robbery victims recruited through formal reporting agencies and assessed 1 month after the crime. Despite substantial overlap across crimes rape victims reported more emotional responses reflecting fear detachment shame and more nonactive behavioral responses. Regression analysis examining the prediction of perceived threat by peritraumatic responses and crime variables indicated that increased duration of crimes; decreased calmness; increased fear; numbing; use of begging pleading and crying; and attempts to reason with the perpetrator(s) were all significantly associated with increased appraisal of threat.", "Excessive stress exposure often leads to emotional dysfunction characterized by disruptions in healthy emotional learning expression and regulation processes. A prefrontal cortex (PFC)-amygdala circuit appears to underlie these important emotional processes. However limited human neuroimaging research has investigated whether these brain regions underlie the altered emotional function that develops with stress. Therefore the present study used functional magnetic resonance imaging (fMRI) to investigate stress-induced changes in PFC-amygdala function during Pavlovian fear conditioning. Participants completed a variant of the Montreal Imaging Stress Task (MIST) followed (25min later) by a Pavlovian fear conditioning task during fMRI. Self-reported stress to the MIST was used to identify three stress-reactivity groups (Low Medium and High). Psychophysiological behavioral and fMRI signal responses were compared between the three stress-reactivity groups during fear conditioning. Fear learning indexed via participant expectation of the unconditioned stimulus during conditioning increased with stress reactivity. Further the High stress-reactivity group demonstrated greater autonomic arousal (i.e. skin conductance response SCR) to both conditioned and unconditioned stimuli compared to the Low and Medium stress-reactivity groups. Finally the High stress group did not regulate the emotional response to threat. More specifically the High stress-reactivity group did not show a negative relationship between conditioned and unconditioned SCRs. Stress-induced changes in these emotional processes paralleled changes in dorsolateral dorsomedial and ventromedial PFC function. These findings demonstrate that acute stress facilitates fear learning enhances autonomic arousal and impairs emotion regulation and suggests these stress-induced changes in emotional function are mediated by the PFC.", "BACKGROUND: Leukemia and hematopoietic stem cell transplantation (HSCT) create physical psychological social and spiritual distresses in patients. Understanding this threatening situation in adults with leukemia undergoing HSCT will assist health care professionals in providing holistic care to the patients. OBJECTIVES: The aim of the present study was exploring the perceived threat in adults with leukemia undergoing HSCT. PATIENTS AND METHODS: This article is part of a longitudinal qualitative study which used the grounded theory approach and was conducted in 2009-2011. Ten adults with acute leukemia scheduled for HSCT were recruited from the Hematology-Oncology Research Center and Stem Cell Transplantation Shariati Hospital in Tehran Iran. A series of pre-transplant and post-transplant in-depth interviews were held in the hospital's HSCT wards. Totally 18 interviews were conducted. Three written narratives were also obtained from the participants. The Corbin and Strauss approach was used to analyze the data. RESULTS: Perceived threat was one of the main categories that emerged from the data. This category included four subcategories inattention to the signs and symptoms doubt and anxiety perception of danger and time limitation and change of life conditions which occurred in linear progression over time. CONCLUSION: Suffering from leukemia and experiencing HSCT are events that are uniquely perceived by patients. This threatening situation can significantly effect perception of patients and cause temporary or permanent alterations in patients' lives. Health care professionals can help these patients by deeper understanding of their experiences and effective interventions.", "This study examined the effects of oral administration of 20mg hydrocortisone on baseline and fear-potentiated startle in 63 male veterans with or without PTSD. The procedure was based on a two-session within-subject design in which acoustic startle eyeblink responses were recorded during intervals of threat or no threat of electric shock. Results showed that the magnitude of the difference between startle responses recorded during anticipation of imminent shock compared to safe periods was reduced after hydrocortisone administration relative to placebo. This effect did not vary as a function of PTSD group nor were there were any significant group differences in other indices startle amplitude. Findings suggest that the acute elevations in systemic cortisol produced by hydrocortisone administration may have fear-inhibiting effects. This finding may have implications for understanding the role of hypothalamic-pituitary-adrenal (HPA)-axis function in vulnerability and resilience to traumatic stress.", "Two things are worth remembering about an aversive event: What made it happen? What made it cease? If a stimulus precedes an aversive event it becomes a signal for threat and will later elicit behavior indicating conditioned fear. However if the stimulus is presented upon cessation of the aversive event it elicits behavior indicating conditioned relief. What are the neuronal bases for such learning? Using functional magnetic resonance imaging (fMRI) in humans we found that a fear-conditioned stimulus activates amygdala but not striatum whereas a relief-conditioned stimulus activates striatum but not amygdala. Correspondingly acute inactivation of amygdala or of ventral striatum in rats respectively abolished only conditioned fear or only conditioned relief. Thus the behaviorally opponent memories supported by onset and offset of aversive events engage and require fear and reward networks respectively. This may explain attraction to stimuli associated with the cessation of trauma or of panic attacks.", "Serotonin is strongly implicated in the mammalian stress response but surprisingly little is known about its mode of action. Recent data suggest that serotonin can inhibit aversive responding in humans but this remains underspecified. In particular data in rodents suggest that global serotonin depletion may specifically increase long-duration bed nucleus of the stria terminalis (BNST)-mediated aversive responses (ie anxiety) but not short-duration BNST-independent responses (ie fear). Here we extend these findings to humans. In a balanced placebo-controlled crossover design healthy volunteers (n=20) received a controlled diet with and without the serotonin precursor tryptophan (acute tryptophan depletion; ATD). Aversive states were indexed by translational acoustic startle measures. Fear and anxiety were operationally defined as the increase in startle reactivity during short- and long-duration threat periods evoked by predictable shock (fear-potentiated startle) and by the context in which the shocks were administered (anxiety-potentiated startle) respectively. ATD significantly increased long-duration anxiety-potentiated startle but had no effect on short-duration fear-potentiated startle. These results suggest that serotonin depletion in humans selectively increases anxiety but not fear. Current translational frameworks support the proposition that ATD thus disinhibits dorsal raphe-originating serotonergic control of corticotropin-releasing hormone-mediated excitation of the BNST. This generates a candidate neuropharmacological mechanism by which depleted serotonin may increase response to sustained threats alongside clear implications for our understanding of the manifestation and treatment of mood and anxiety disorders.", "Posttraumatic stress disorder (PTSD) is associated with dysfunction of the neural circuitry that supports fear learning and memory processes. However much of what is known about neural dysfunction in PTSD is based on research in chronic PTSD populations. Less is known about neural function that supports fear learning acutely following trauma exposure. Determining the acute effects of trauma exposure on brain function would provide new insight into the neural processes that mediate the cognitive-affective dysfunction associated with PTSD. Therefore the present study investigated neural activity that supports fear learning and memory processes in recently Trauma-Exposed (TE) and Non-Trauma-Exposed (NTE) participants. Participants completed a Pavlovian fear conditioning procedure during functional magnetic resonance imaging (fMRI). During fMRI participants' threat expectancy was continuously monitored. NTE participants showed greater threat expectancy during warning than safety cues while no difference was observed in the TE group. This finding suggests TE participants overgeneralized the fear association to the safety cue. Further only the TE group showed a negative relationship between fMRI signal responses within dorsomedial prefrontal cortex (PFC) and threat expectancy during safety cues. These results suggest the dorsomedial PFC mediates overgeneralization of learned fear as an acute result of trauma exposure. Finally neural activity within the PFC and inferior parietal lobule showed a negative relationship with PTSD symptom severity assessed three months posttrauma. Thus neural activity measured acutely following trauma exposure predicted future PTSD symptom severity. The present findings elucidate the acute effects of trauma exposure on cognitive-affective function and provide new insight into the neural mechanisms of PTSD.", "As a step toward addressing limitations in the current psychiatric diagnostic system the National Institute of Mental Health recently developed the Research Domain Criteria (RDoC) to stimulate integrative research-spanning self-report behavior neural circuitry and molecular/genetic mechanisms-on core psychological processes implicated in mental illness. Here we use the RDoC conceptualization to review research on threat responses reward processing and their interaction. The first section of the manuscript highlights the pivotal role of exaggerated threat responses-mediated by circuits connecting the frontal cortex amygdala and midbrain-in anxiety and reviews data indicating that genotypic variation in the serotonin system is associated with hyperactivity in this circuitry which elevates the risk for anxiety and mood disorders. In the second section we describe mounting evidence linking anhedonic behavior to deficits in psychological functions that rely heavily on dopamine signaling especially cost/benefit decision making and reward learning. The third section covers recent studies that document negative effects of acute threats and chronic stress on reward responses in humans. The mechanisms underlying such effects are unclear but the fourth section reviews new optogenetic data in rodents indicating that GABAergic inhibition of midbrain dopamine neurons driven by activation of the habenula may play a fundamental role in stress-induced anhedonia. In addition to its basic scientific value a better understanding of interactions between the neural systems that mediate threat and reward responses may offer relief from the burdensome condition of anxious depression.", "In today's world mass-media and online social networks present us with unprecedented exposure to second-hand vicarious experiences and thereby the chance of forming associations between previously innocuous events (e.g. being in a subway station) and aversive outcomes (e.g. footage or verbal reports from a violent terrorist attack) without direct experience. Such social threat or fear learning can have dramatic consequences as manifested in acute stress symptoms and maladaptive fears. However most research has so far focused on socially acquired threat responses that are expressed as increased arousal rather than active behavior. In three experiments (n = 120) we examined the effect of indirect experiences on behaviors by establishing a link between social threat learning and instrumental decision making. We contrasted learning from direct experience (i.e. Pavlovian conditioning) (experiment 1) against two common forms of social threat learning-social observation (experiment 2) and verbal instruction (experiment 3)-and how this learning transferred to subsequent instrumental decision making using behavioral experiments and computational modeling. We found that both types of social threat learning transfer to decision making in a strong and surprisingly inflexible manner. Notably computational modeling indicated that the transfer of observational and instructed threat learning involved different computational mechanisms. Our results demonstrate the strong influence of others' expressions of fear on one's own decisions and have important implications for understanding both healthy and pathological human behaviors resulting from the indirect exposure to threatening events.", "Cognitive theories of anxiety psychopathology cite biased attention towards threat as a central vulnerability and maintaining factor. However many studies have found threat bias indices to have poor reliability and have failed to observe the theorized relationship between threat bias and anxiety symptoms; this may be due to the non-unitary nature of threat bias and the influence of state-level variables on its expression. Accumulating data suggests that state anxious mood is important for the robust expression of threat bias and for relations to emerge between threat bias and symptoms though this possibility has not been experimentally tested. Eye-tracking was used to assess multiple forms of threat bias (i.e. early vigilance sustained attention facilitated engagement delayed disengagement) thought to be related to anxiety. A non-clinical sample (N = 165) was recruited to test the hypothesis that biased attention towards threat but not dysphoric or positive emotional stimuli during an anxious mood induction but not at a pre-stress baseline would prospectively predict greater worry symptoms on days in which more naturalistic stressors occurred. Results revealed the hypothesized moderation effect for sustained attention towards threat after the mood induction but not at baseline though sustained attention towards dysphoric stimuli also moderated the effect of stressors on worry. Worry-relevant sustained attention towards negative emotional stimuli may be a partially mood-context dependent phenomenon.", "The behavioral analysis of laboratory rats is usually confined to the level of overt behavior like locomotion behavioral inhibition instrumental responses and others. Apart from such visible outcome however behaviorally relevant information can also be obtained when analyzing the animals' ultrasonic vocalization which is typically emitted in highly motivational situations like 22-kHz calls in response to acute or conditioned threat. To further investigate such vocalizations and their relationship with overt behavior we tested male Wistar rats in a paradigm of Pavlovian fear conditioning where a tone stimulus (CS) was preceding an aversive foot-shock (US) in a distinct environment. Importantly the shock dose was varied between groups (0-1.1 mA) and its acute and conditioned outcome were determined. The analysis of visible behavior confirms the usefulness of immobility as a measure of fear conditioning especially when higher shock doses were used. Rearing and grooming on the other hand were more useful to detect conditioned effects with lower shock levels. Ultrasonic vocalization occurred less consistently than changes in overt behavior; however dose-response relationships were also observed during the phase of conditioning for example in latency call rate and lengths intervals between calls and sound amplitude. Furthermore total calling time (and rate) were highly correlated with overt behavior namely behavioral inhibition as measured through immobility. These correlations were observed during the phase of fear conditioning and the subsequent tests. Importantly conditioned effects in overt behavior were observed both to the context and to the CS presented in this context whereas conditioned vocalization to the context was not observed (except for one rat). In support and extent of previous results the present data show that a detailed analysis of ultrasonic vocalization can substantially broaden and refine the spectrum of analysis in behavioral work with rats since it can provide information about situational- state- and subject-dependent factors which are partly distinct from what is visible to the experimenter.", "Social fearfulness is expressed on a continuum of severity from moderate distress to incapacitating fear. The present article focuses on the brain states associated with this broad dimension of social anxiety in humans. In total 70 published studies are summarized documenting the neural correlates of social anxiety during states of rest threat-related cognitive-affective activation and acute symptom provocation. Neural exaggeration in limbic (amygdala) and paralimbic (insula) regions appears to be associated with functional outcomes involving increased attention for and processing of social threat. Evidence is also reviewed showing that social anxiety is characterized by atypical functional connectivity in certain brain networks. Despite a higher prevalence of social anxiety disorder among females males have been overrepresented in the published clinical studies (constituting approximately 56% of the total participants). We evaluate the prospects of nonhuman animal models of social anxiety and discuss several promising directions for future research. The review highlights the need to adopt an integrative network-based approach to the study of the neural substrates underlying social anxiety.", "Exposure to a deadly threat an adult male rat induced the release of corticosterone in 14-day-old rat pups. The endocrine stress response was decreased when the pups were reunited with their mother immediately after exposure. These findings demonstrate that social variables can reduce the consequences of an aversive experience.", "Recent findings obtained in patients with phobias or trauma-related anxiety disorders raise doubts concerning the interrelation between acute fear relief during an exposure-based therapeutic session and beneficial treatment progress. In a mouse model explicit for exposure therapy we challenge the view that within-session fear reduction is the turning point for relearning of a stimulus-threat association. Even though within-session extinction of auditory-cued fear memory was identical for prolonged and spaced tone presentations only the latter caused between-session extinction. Furthermore spaced tone presentations led to between-session extinction even in the complete absence of within-session extinction as observed for remote fear memories and in case of abolished cannabinoid receptor type 1 signaling. Induction of between-session extinction was accompanied by an increase in the number of c-Fos-positive neurons within the basolateral amygdala the cingulate cortex and the dentate gyrus independent of the level of within-session extinction. Together our findings demonstrate that within-session extinction is neither sufficient nor essential for between-session extinction thus calling for a reconsideration of current concepts underlying exposure-based therapies.", "Psychophysiological measures of fear expression provide observable intermediate phenotypes of fear-related symptoms. Research Domain Criteria (RDoC) advocate using neurobiological intermediate phenotypes that provide dimensional correlates of psychopathology. Negative Valence Systems in the RDoC matrix include the construct of acute threat which can be measured on a physiological level using potentiation of the acoustic startle reflex assessed via electromyography recordings of the orbicularis oculi muscle. Impairments in extinction of fear-potentiated startle due to high levels of fear (termed fear load) during the early phases of extinction have been observed in posttraumatic stress disorder (PTSD). The goals of the current work were to examine dimensional associations between fear-related symptoms of PTSD and fear load variables to test their validity as an intermediate phenotype. We examined extinction of fear-potentiated startle in a cohort (n=269) of individuals with a broad range of civilian trauma exposure (range 0-13 traumatic events per person mean=3.5). Based on previously reported findings we hypothesized that fear load would be significantly associated with intrusion and fear memories of an index traumatic event. The results indicated that early extinction was correlated with intrusive thoughts (p=0.0007) and intense physiological reactions to trauma reminders (p=0.036). Degree of adult or childhood trauma exposure and depression severity were not associated with fear load. After controlling for age sex race income level of prior trauma and level of fear conditioning fear load during extinction was still significantly predictive of intrusive thoughts (p=0.004). The significance of these findings is that they support dimensional associations with symptom severity rather than diagnostic category and as such fear load may emerge as a transdiagnostic intermediate phenotype expressed across fear-related disorders (e.g. specific phobia social phobia).", "Development of the hypothalamic-pituitary-adrenal (HPA) axis is influenced by external factors during early life in mammals which optimizes adult function for predicted conditions. We have hypothesized that adolescence represents a sensitive period for the development of some aspects of adult stress response regulation. This was based on prior work showing that repeated exposure of rats to a stressor across an adolescent period increases fearfulness in a novel environment in adulthood and results in lower levels of dopamine receptor subtype-2 protein in prefrontal cortex. Here we further our investigation of both acute and long-term effects of repeated adolescent stressor exposure on physiological (i.e. corticosterone) and behavioral (i.e. defensive behavior) measures of stress responding in male and female rats. Furthermore we compared outcomes with those following identical manipulations administered in early adulthood and found that animals exposed to cues of predation threat during adolescence showed the most robust defensive responses to a homotypic stressor encountered in adulthood. Peer interaction during control manipulation in adolescence was identified as an important individual characteristic mediating development of adult defensive strategies.", "AIMS: This study was primarily aimed for developing and testing a valid and reliable instrument that measures perceived threat of the risk for graft rejection after organ transplantation. A second aim was to report descriptive data regarding graft rejection and Health-Related Quality of Life. BACKGROUND: The most serious risk connected with transplantations besides infection is graft rejection. DESIGN: Non experimental descriptive involving instrument development and psychometric assessment. METHOD: Questionnaires about perceived threat of the risk for graft rejection and Health-Related Quality of Life were mailed to 229 OTRs between 19-65 years old. The items were formed from a previous interview study. Patients were transplanted with a kidney a liver or a heart and/or a lung. All patients with follow-up time of one year +/- three months and three years +/- three months were included. RESULTS: With an 81% response rate the study comprised of 185 OTRs who had received either a kidney (n = 117) a liver (n = 39) or heart or lung (n = 29). Three homogenous factors of perceived threat for graft rejection were revealed labelled 'intrusive anxiety' 'graft-related threat' and 'lack of control'. Tests of internal consistency showed good item-scale convergent and discriminatory validity. A majority of the OTRs scored low levels for 'intrusive anxiety'. The kidney transplant recipients experienced more 'graft-related threat' by acute graft rejection than those transplanted with a liver heart or lung. CONCLUSION: In conclusion this study suggests that it is possible to measure the perceived threat of the risk for graft rejection in three homogenous factors. Relevance to clinical practice. The instrument perceived threat of the risk for graft rejection might be usable to measure the impact of fear of graft rejection to predict needs of pedagogical intervention strategies to reduce fear and to improve Health-Related Quality of Life related to graft rejection.", "Recent evidence suggests that the steroid hormone testosterone can decrease the functional coupling between orbitofrontal cortex (OFC) and amygdala. Theoretically this decoupling has been linked to a testosterone-driven increase of goal-directed behaviour in case of threat but this has never been studied directly. Therefore we placed twenty-two women in dynamically changing situations of escapable and inescapable threat after a within-subject placebo controlled testosterone administration. Using functional magnetic resonance imaging (fMRI) we provide evidence that testosterone activates the left lateral OFC (LOFC) in preparation of active goal-directed escape and decouples this OFC area from a subcortical threat system including the central-medial amygdala hypothalamus and periaqueductal gray. This LOFC decoupling was specific to threatening situations a point that was further emphasized by an absence of such decoupling in a second experiment focused on resting-state connectivity. These results not only confirm that testosterone administration decouples the LOFC from the subcortical threat system but also show that this is specifically the case in response to acute threat and ultimately leads to an increase in LOFC activity when the participant prepares a goal-directed action to escape. Together these results for the first time provide a detailed understanding of functional brain alterations induced by testosterone under threat conditions and corroborate and extend the view that testosterone prepares the brain for goal-directed action in case of threat.", "Serotonin reuptake inhibitors may increase symptoms of anxiety immediately following treatment initiation. The present study examined whether acute citalopram increased fear-potentiated startle to predictable and/or unpredictable shocks in healthy subjects. Eighteen healthy subjects each received two treatments placebo and 20 mg citalopram in a crossover design. Participants were exposed to three conditions including one in which predictable aversive shocks were signaled by a cue a second in which unpredictable shocks were anticipated and a third in which no shocks were administered. Changes in aversive states were investigated using acoustic startle stimuli. Citalopram did not affect baseline startle. However the phasic startle potentiation to the threat cue in the predictable condition was robustly increased by acute citalopram. The sustained startle potentiation in the unpredictable conditions was also increased by citalopram but only when the drug was given during the first session. These results indicate that a single dose of citalopram is not anxiogenic in itself but can exacerbate the expression of fear and anxiety.", "Phan et al. (2008) recently reported that an acute dose of oral THC is associated with a decrease in threat-related amygdala reactivity during a social threat stimulus task. However to date those findings have not been replicated and have not been extended to clinical studies involving smoked rather than oral cannabis. In this study we hypothesized that level of cannabis smoked by participants in our treatment study would be inversely related to the level of threat-related amygdala reactivity. Subjects were recruited from among participants in our double-blind placebo-controlled trial of fluoxetine in comorbid youth with cannabis dependence/major depression. The threat-related amygdala reactivity task used by Hariri et al. (2009) was completed during BOLD fMRI scans at study baseline and then again 12 weeks later at the end of the trial. Data are available from six subjects with pre-and post-treatment fMRI data. During the course of the study five of the six subjects demonstrated a decrease in their level of cannabis use with a mean decrease of 64% and those persons all demonstrated an increase in their level of amygdala reactivity. One subject demonstrated an increase in their level of cannabis use (a 79% increase) during the treatment trial and that person demonstrated a decrease in their level of amygdala reactivity. Thus a higher level of cannabis use was consistently associated with a lower level of amygdala reactivity across all subjects (matched pairs t = 2.70 df = 5 p < 0.05 two-tailed). These findings are consistent with the reports by Phan et al. (2008) and Hariri et al. (2009) suggesting that cannabinoids have an inhibitory effect on threat-related amygdala reactivity.", "Stress broadly affects the ability to regulate emotions and may contribute to generalization of threat-related behaviors to harmless stimuli. Behavioral generalization also tends to increase over time as memory precision for recent events gives way to more gist-like representations. Thus acute stress coupled with a delay in time from a negative experience may be a strong predictor of the transition from normal to generalized fear expression. Here we investigated the effect of a single-episode acute stressor on generalization of aversive learning when stress is administered either immediately after an aversive learning event or following a delay. In a between-subjects design healthy adult volunteers underwent threat (fear) conditioning using a tone-conditioned stimulus paired with an electric shock to the wrist and another tone not paired with shock. Behavioral generalization was tested to a range of novel tones either on the same day (experiment 1) or 24 h later (experiment 2) and was preceded by either an acute stress induction or a control task. Anticipatory sympathetic arousal [i.e. skin conductance responses (SCRs)] and explicit measures of shock expectancy served as dependent measures. Stress administered shortly after threat conditioning did not affect behavioral generalization. In contrast stress administered following a delay led to heightened arousal and increased generalization of SCRs and explicit measures of shock expectancy. These findings show that acute stress increases generalization of older but not recent threat memories and have clinical relevance to understanding overgeneralization characteristics of anxiety and stress-related disorders.", "OBJECTIVE: To examine whether exposure to curve versus sharp contours in the built healthcare setting produces systematic and identifiable patterns of amygdala activation and behavioral response in healthy adults. BACKGROUND: Recent studies in cognitive neuroscience suggest that humans prefer objects with a curved contour compared with objects that have pointed features and a sharp-angled contour. An implicit perception of threat represented by sharp objects in humans was hypothesized to explain this bias. METHOD: The study adopted a within-subject experimental design where 36 subjects (representing three age-groups and both sexes) were exposed to a randomized order of 312 real-life images (objects interiors exteriors landscape and a set of control images). Amygdala activation was simultaneously captured using functional magnetic resonance imaging technology. Subjects' preference (like/dislike) data were also collected while in the scanner. Data were collected in 2013. RESULTS: In case of images depicting landscape and healthcare objects brain scans show significant higher amygdala activation associated with sharp contours. However in relation to images depicting hospital interiors and exterior envelops brain scans show significant higher amygdala activation associated with curve contours. These activations pertain to exposure during the precognitive stages of the subjects' perception. CONCLUSION: Hospital forms do have systematic impact on fear response during precognitive stages of human perception. Whether this first impression colors the subsequent experience of an actual patient with real illness or injury is unknown.", "The NIMH Research Domain Criteria (RDoC) initiative aims to describe key dimensional constructs underlying mental function across multiple units of analysis-from genes to observable behaviors-in order to better understand psychopathology. The acute threat (fear) construct of the RDoC Negative Valence System has been studied extensively from a translational perspective and is highly pertinent to numerous psychiatric conditions including anxiety and trauma-related disorders. We examined genetic contributions to the construct of acute threat at two units of analysis within the RDoC framework: (1) neural circuits and (2) physiology. Specifically we focused on genetic influences on activation patterns of frontolimbic neural circuitry and on startle skin conductance and heart rate responses. Research on the heritability of activation in threat-related frontolimbic neural circuitry is lacking but physiological indicators of acute threat have been found to be moderately heritable (35-50%). Genetic studies of the neural circuitry and physiology of acute threat have almost exclusively relied on the candidate gene method and as in the broader psychiatric genetics literature most findings have failed to replicate. The most robust support has been demonstrated for associations between variation in the serotonin transporter (SLC6A4) and catechol-O-methyltransferase (COMT) genes with threat-related neural activation and physiological responses. However unbiased genome-wide approaches using very large samples are needed for gene discovery and these can be accomplished with collaborative consortium-based research efforts such as those of the Psychiatric Genomics Consortium (PGC) and Enhancing Neuro Imaging Genetics through Meta-Analysis (ENIGMA) Consortium.", "Patients with generalized social anxiety disorder (GSAD) exhibit heightened activation of the amygdala in response to social cues conveying threat (eg fearful/angry faces). The neuropeptide oxytocin (OXT) decreases anxiety and stress facilitates social encounters and attenuates amygdala reactivity to threatening faces in healthy subjects. The goal of this study was to examine the effects of OXT on fear-related amygdala reactivity in GSAD and matched healthy control (CON) subjects. In a functional magnetic resonance imaging study utilizing a double-blind placebo-controlled within-subjects design we measured amygdala activation to an emotional face matching task of fearful angry and happy faces following acute intranasal administration of OXT (24 IU or 40.32 mug) and placebo in 18 GSAD and 18 CON subjects. Both the CON and GSAD groups activated bilateral amygdala to all emotional faces during placebo with the GSAD group exhibiting hyperactivity specifically to fearful faces in bilateral amygdala compared with the CON group. OXT had no effect on amygdala activity to emotional faces in the CON group but attenuated the heightened amygdala reactivity to fearful faces in the GSAD group such that the hyperactivity observed during the placebo session was no longer evident following OXT (ie normalization). These findings suggest that OXT has a specific effect on fear-related amygdala activity particularly when the amygdala is hyperactive such as in GSAD thereby providing a brain-based mechanism of the impact of OXT in modulating the exaggerated processing of social signals of threat in patients with pathological anxiety.", "The ability to adaptively regulate responses to the proximity of potential danger is critical to survival and imbalance in this system may contribute to psychopathology. The bed nucleus of the stria terminalis (BNST) is implicated in defensive responding during uncertain threat anticipation whereas the amygdala may drive responding upon more acute danger. This functional dissociation between the BNST and amygdala is however controversial and human evidence scarce. Here we used data from two independent functional magnetic resonance imaging studies [n = 108 males and n = 70 (45 females)] to probe how coordination between the BNST and amygdala may regulate responses during shock anticipation and actual shock confrontation. In a subset of participants from Sample 2 (n = 48) we demonstrate that anticipation and confrontation evoke bradycardic and tachycardic responses respectively. Further we show that in each sample when going from shock anticipation to the moment of shock confrontation neural activity shifted from a region anatomically consistent with the BNST toward the amygdala. Comparisons of functional connectivity during threat processing showed overlapping yet also consistently divergent functional connectivity profiles for the BNST and amygdala. Finally childhood maltreatment levels predicted amygdala but not BNST hyperactivity during shock anticipation. Our results support an evolutionary conserved defensive distance-dependent dynamic balance between BNST and amygdala activity. Shifts in this balance may enable shifts in defensive reactions via the demonstrated differential functional connectivity. Our results indicate that early life stress may tip the neural balance toward acute threat responding and via that route predispose for affective disorder.SIGNIFICANCE STATEMENT Previously proposed differential contributions of the BNST and amygdala to fear and anxiety have been recently debated. Despite the significance of understanding their contributions to defensive reactions there is a paucity of human studies that directly compared these regions on activity and connectivity during threat processing. We show strong evidence for a dissociable role of the BNST and amygdala in threat processing by demonstrating in two large participant samples that they show a distinct temporal signature of threat responding as well as a discriminable pattern of functional connections and differential sensitivity to early life threat.", "Fear generalization occurs when a response previously acquired with a threatening stimulus is transferred to a similar one. However it could be maladaptive when stimuli that do not represent a real threat are appraised as dangerous which is a hallmark of several anxiety disorders. Stress exposure is a major risk factor for the occurrence of anxiety disorders and it is well established that it influences different phases of fear memory; nevertheless its impact on the generalization of contextual fear memories has been less studied. In the present work we have characterized the impact of acute restraint stress prior to contextual fear conditioning on the generalization of this fear memory and the role of the GABAergic signaling within the basolateral amygdala complex (BLA) on the stress modulatory effects. We have found that a single stress exposure promoted the generalization of this memory trace to a different context that was well discriminated in unstressed conditioned animals. Moreover this effect was dependent on the formation of a contextual associative memory and on the testing order (i.e. conditioning context first vs generalization context first). Furthermore we observed that increasing GABA-A signaling by intra-BLA midazolam administration prior to the stressful session exposure prevented the generalization of fear memory whereas intra-BLA administration of the GABA-A antagonist (Bicuculline) prior to fear conditioning induced the generalization of fear memory in unstressed rats. We concluded that stress exposure prior to contextual fear conditioning promotes the generalization of fear memory and that the GABAergic transmission within the BLA has a critical role in this phenomenon.", "OBJECTIVE: The aim of this study was to test the hypothesis that greater global and situational relationship satisfaction would reduce the negative impact of threatening information on acute pain. DESIGN: An experimental design was used to manipulate threat and elicit acute pain via a cold pressor task. SETTING: The study was completed in a research laboratory at a large urban university in the Midwestern USA. SUBJECTS: Participants were 134 couples in which at least one individual was an undergraduate student. METHODS: After administration of a global relationship satisfaction measure couples were randomly assigned to either receive high or low threatening information about the painful task. Following the threat manipulation couples discussed the upcoming task and rated their satisfaction with the interaction (i.e. situational relationship satisfaction). The designated pain participant then completed the painful task alone. RESULTS: The threat manipulation altered couples' perceived threat of pain. Situational relationship satisfaction moderated the effect of threat on pain trajectories such that situational relationship satisfaction predicted less pain intensity at an earlier point in the task for the low threat condition than the high threat condition. Greater global relationship satisfaction predicted greater likelihood of task completion among those in the low threat condition whereas it was unrelated to task completion in the high threat condition. Greater global relationship satisfaction also predicted lower pain intensity throughout the task. CONCLUSIONS: These findings demonstrate that the interpersonal context is independently related to acute pain and may also alter the effect of threatening information on acute pain.", "Chronically elevated HPA activity has often been associated with fear and anxiety but there is evidence that single administrations of glucocorticoids may acutely reduce fear. Moreover peri-traumatic cortisol elevation may protect against development of post-traumatic stress disorder. Hypervigilant processing of threat information plays a role in anxiety disorders and although relations with HPA functioning have been established causality of these relations remains unclear. Presently self-reported anxiety and response time patterns on a masked emotional Stroop task with fearful faces were measured in 20 healthy young men after double-blind placebo-controlled oral administration of 40 mg cortisol. The masked fearful Stroop task measures vocal colornaming response latencies for pictures of neutral and fearful faces presented below the threshold for conscious perception. Results showed increased response times on trials for fearful compared to neutral faces after placebo but this emotional Stroop effect was acutely abolished by cortisol administration. This effect was most pronounced in subjects with heightened anxiety levels. This is the first evidence showing that exogenous cortisol acutely reduces anxiety-driven selective attention to threat. These results extend earlier findings of acute fear reduction after glucocorticoid administration. This suggests interactions of HPA functioning and vigilant attention in the pathogenesis of anxiety disorders. Possible neuroendocrine mechanisms of action are discussed.", "The use of fear conditioning and extinction paradigms to examine intermediate phenotypes of anxiety and stress-related disorders has facilitated the identification of neurobiological mechanisms that underlie specific components of abnormal psychological functioning. Across species acute pharmacologic manipulation of the endogenous cannabinoid system has provided evidence of its critical role in fear extinction but the effects of chronic cannabis on extinction are relatively understudied. In rats chronic cannabinoid administration impairs fear extinction in a drug-free state. Here we examine whether chronic cannabis use is associated with impaired fear extinction in humans. Participants were healthy chronic cannabis users (n = 20) and nonuser controls with minimal lifetime cannabis use (n = 20) matched on age sex and race who all screened negative for psychiatric disorders. A 2-day differential fear conditioning paradigm was used to test the hypothesis that chronic cannabis use would be associated with impaired extinction of the skin conductance response. Consistent with hypotheses chronic cannabis use was associated with reduced within-session extinction of skin conductance response on Day 1 (d = 0.78) and between-session extinction on Day 2 (d = 0.76). Unexpectedly cannabis use was also associated with reduced subjective differentiation between threat and safety stimuli during conditioning. Replication and translation of findings are necessary to test potential mechanisms directly and examine whether impairments can be reversed pharmacologically or after a period of cannabis abstinence. (PsycINFO Database Record", "RATIONALE: In this study we used functional magnetic resonance imaging (fMRI) to examine the effects of acute tryptophan depletion (ATD) a well-recognised method for inducing transient cerebral serotonin depletion on brain activation to fearful faces. OBJECTIVES: We predicted that ATD would increase the responsiveness of the amygdala to fearful faces as a function of individual variation in threat sensitivity. METHODS: Twelve healthy male volunteers received a tryptophan depleting drink or a tryptophan balancing amino acid drink (placebo) in a double-blind crossover design. Five hours after drink ingestion participants were scanned whilst viewing fearful happy and neutral faces. RESULTS: Consistent with previous findings fearful faces induced significant signal change in the bilateral amygdala/hippocampus as well as the fusiform face area and the right dorsolateral prefrontal cortex. Furthermore ATD modulated amygdala/hippocampus activation in response to fearful relative to happy faces as a function of self-reported threat sensitivity (as measured with the Behavioral Inhibition Scale; Carver CS White TL (1994) Behavioral inhibition behavioral activation and affective responses to impending reward and punishment: the BIS/BAS scales. J Pers Soc Psychol 67:319-333). CONCLUSION: The data support the hypothesis that individual variation in threat sensitivity interacts with manipulation of 5-HT function to bias the processing of amygdala-dependent threat-relevant stimuli.", "Fear promotes adaptive responses to threats. However when the level of fear is not proportional to the level of threat maladaptive fear-related behaviors characteristic of anxiety disorders result. Post-traumatic stress disorder develops in response to a traumatic event and patients often show sensitized reactions to mild stressors associated with the trauma. Stress-enhanced fear learning (SEFL) is a rodent model of this sensitized responding in which exposure to a 15-shock stressor nonassociatively enhances subsequent fear conditioning training with only a single trial. We examined the role of corticosterone (CORT) in SEFL. Administration of the CORT synthesis blocker metyrapone prior to the stressor but not at time points after attenuated SEFL. Moreover CORT co-administered with metyrapone rescued SEFL. However CORT alone without the stressor was not sufficient to produce SEFL. In these same animals we then looked for correlates of SEFL in terms of changes in excitatory receptor expression. Western blot analysis of the basolateral amygdala (BLA) revealed an increase in the GluA1 AMPA receptor subunit that correlated with SEFL. Thus CORT is permissive to trauma-induced changes in BLA function.", "Acute pain informs the individual that there is an imminent threat of body damage and is associated with the urge to escape and avoid. Fear learning takes place when neutral stimuli receive the propensity to predict the occurrence of pain and when defensive responses are initiated in anticipation of potential threats to the integrity of the body. Fear-avoidance models have been put forward featuring the role of individual differences in catastrophic interpretations of pain in the modulation of learning and avoidance. Based on extensive literature on fear reduction in anxiety disorders; cognitive-behavioral treatments have been developed and applied to patients with chronic pain reporting substantial pain-related fear. In this article we discuss mechanisms underlying the acquisition the assessment and extinction of pain-related fear through the cognitive-behavioral treatment of pain-related fear. Finally we provide a number of critical notes and directions for future research in the field of chronic pain and pain-related fear."]
51/3: l
51/4: tfidfDist(l)
51/5: import importlib as imp
51/6: imp.reload(str_helper)
51/7: from str_helper import *
51/8: tfidfDist(l)
51/9: import str_helper as s
51/10: s.tfidfDist(l)
51/11: imp.reload(s)
51/12: s.tfidfDist(l)
51/13: from nltk import FreqDist, word_tokenize
51/14: FreqDist(word_tokenize(l[0]))
51/15: list(FreqDist(word_tokenize(l[0])).items())
51/16: imp.reload(s)
51/17: l
51/18: s.tfidfDist(l)
51/19: l2 = list(map(s.clean_sent, l))
51/20: l2
51/21: a=s.tfidfDist(l)[-1]
51/22: b=s.tfidfDist(l2)[-1]
51/23: a
51/24: b
51/25: l[-1]
51/26: l2[-1]
51/27: import math
51/28: a[-1:math.floor(0.8*len(a)):-1]
51/29: b[-1:math.floor(0.8*len(b)):-1]
51/30: b[:math.floor(0.8*len(b))]
51/31: a[:math.floor(0.8*len(a))]
48/137:
window_size = 10


def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
52/1: import str_helper as sh
53/1:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

# dftmp = dfz.copy()
# dflive = pd.melt(dfz[dfz.Condition=='Live'][['T1','T2']])
# dflive.columns=['Time of Trade', 'q']
# dfwc=pd.melt(dfz[dfz.Condition=='WaterCooler'][['T1','T2']])
# dfwc.columns=['Time of Trade', 'q']

# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Time of Trade', y='q', data=dflive,  ax=ax1)
# sns.boxplot(x='Time of Trade', y='q', data=dfwc,  ax=ax2)
# ax1.set_title('Live')
# ax2.set_title('WaterCooler')
# plt.show()



# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Condition', y='T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# sns.boxplot(x='Condition', y='T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
# ax1.set_title('T1')
# ax2.set_title('T2')
# plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)

sns.distplot(dfz[dfz.Condition=='Live']['T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "q at T1"})
sns.distplot(dfz[dfz.Condition=='Live']['T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "q at T2"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "q at T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "q at T2"})
ax1.set_title('LiveTrackers')
ax2.set_title('Hindsighters')
plt.show()
53/2:
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
import math
from sklearn.linear_model import LinearRegression
pd.options.mode.chained_assignment = None
53/3:
df = pd.read_csv('datadump.csv')
df = df.loc[8:]
df['price0'] = 217.76
df['price10'] = 219.65
src=pd.read_csv('AAPL_Final_Trend.csv')
df['price20']=None
df['price20'][df.experiment.str.contains("profit")] = src.loc[491]['High']
df['price20'][df.experiment.str.contains("loss")] = src.loc[502]['High']
df=df.fillna(0)
df=df[df.mturkid!='A2H95JVPEKRUWA'] # invalid

df['stock_t01']=10

df['purchase_t10'] = np.where(df['qty1']>0, df['qty1'], 0)
df['sale_t10'] = np.where(df['qty1']<0, df['qty1'], 0)
df['sale_t10'] = df['sale_t10'].abs()

df['stock_t20'] = df['stock_t01']+df['purchase_t10']-df['sale_t10']
df['cash_t20'] = 2500 - (df.purchase_t10-df.sale_t10)*219.65

df['purchase_t20'] = np.where(df['qty2']>0, df['qty2'], 0)
df['sale_t20'] = np.where(df['qty2']<0, df['qty2'], 0)
df['sale_t20'] = df['sale_t20'].abs()

df['stock_t21'] = df['stock_t20']+df['purchase_t20']-df['sale_t20']

# df['avg_purch_t11'] = (2177.6 + df.purchase_t10*df.price10)/(10+df.purchase_t10)
# df['avg_purch_t2'] = (2177.6 + (df.purchase_t1)*df.price1 + (df.purchase_t2)*df.price2)/(10+df.purchase_t1+df.purchase_t2)

df['pnl_t20'] = np.where(df.stock_t20>10, (df.stock_t20-10)*(df.price20-df.price10)+10*(df.price20-df.price0), df.stock_t20*(df.price20-df.price0))

df['t1_purchase'] = 0
df['t1_purchase'] = np.where(df.purchase_t10!=0, 1, df.t1_purchase)
df['t1_purchase'] = np.where(df.sale_t10!=0, -1, df.t1_purchase)

df['t2_purchase'] = 0
df['t2_purchase'] = np.where(df.purchase_t20!=0, 1, df.t2_purchase)
df['t2_purchase'] = np.where(df.sale_t20!=0, -1, df.t2_purchase)


df['live_info'] = np.where(df.experiment.str.contains('wc'), 0, 1)
df=df.sort_values(['mturkid', 'created_at'])
df = df.drop_duplicates(['mturkid'])


# drop inactive people (4)
df=df.drop([9,46,53,54])


# df = df[['experiment', 'mturkid', 'winnings',
#        'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret', 
#        'price0', 'price10', 'price20', 'stock_t01',
#        'purchase_t10', 'sale_t10', 'stock_t11', 'purchase_t20', 'sale_t20',
#        'stock_t21', 'pnl_t20', 'live_info']]
53/4:
## INV RATIO: traded_volume/permissible_volume
maxbuy10 = math.floor(2500/219.65)
maxsell10 = 10
df['invratio_1'] = df.purchase_t10/maxbuy10
df['invratio_1'] = np.where(df.invratio_1==0, df.sale_t10/maxsell10, df.invratio_1)

avbl_cash = 2500 - (df.purchase_t10-df.sale_t10)*219.65
avbl_stock = 10+df.purchase_t10-df.sale_t10
maxbuy2 = df['cash_t20']/df.price20
df['invratio_2'] = df.purchase_t20/maxbuy2
df['invratio_2'] = np.where(df.invratio_2==0, df.sale_t20/avbl_stock, df.invratio_2)
df['invratio_2'].fillna(0, inplace=True)

# # DELTA IN INV_RATIO: measures change in investment behavior
# df['invratio_delta'] = abs(df['invratio_2'])-abs(df['invratio_1'])

df['invratio_1_mul'] = df.invratio_1*df.t1_purchase
df['invratio_2_mul'] = df.invratio_2*df.t2_purchase
53/5:
df1 = df[df.experiment=="'rc_profit'"]
df2 = df[df.experiment=="'rc_loss'"]
df3 = df[df.experiment=="'wc_profit'"]
df4 = df[df.experiment=="'wc_loss'"]
53/6:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

# dftmp = dfz.copy()
# dflive = pd.melt(dfz[dfz.Condition=='Live'][['T1','T2']])
# dflive.columns=['Time of Trade', 'q']
# dfwc=pd.melt(dfz[dfz.Condition=='WaterCooler'][['T1','T2']])
# dfwc.columns=['Time of Trade', 'q']

# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Time of Trade', y='q', data=dflive,  ax=ax1)
# sns.boxplot(x='Time of Trade', y='q', data=dfwc,  ax=ax2)
# ax1.set_title('Live')
# ax2.set_title('WaterCooler')
# plt.show()



# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Condition', y='T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# sns.boxplot(x='Condition', y='T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
# ax1.set_title('T1')
# ax2.set_title('T2')
# plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)

sns.distplot(dfz[dfz.Condition=='Live']['T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "q at T1"})
sns.distplot(dfz[dfz.Condition=='Live']['T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "q at T2"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "q at T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "q at T2"})
ax1.set_title('LiveTrackers')
ax2.set_title('Hindsighters')
plt.show()
53/7:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'LiveTrackers', 'Hindsighters')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
b_t2 = t2[t2.sale_t20==0] #buyers at t2
b_t2['Buyers'] = b_t2['Condition']
s_t2 = t2[t2.purchase_t20==0] #sellers at t2
s_t2['Sellers'] = s_t2['Condition']
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Sellers', y='q at T2', data=s_t2, order=['LiveTrackers', 'Hindsighters'], palette='Set1', ax=ax2)
sns.boxplot(x='Buyers', y='q at T2', data=b_t2, order=['LiveTrackers', 'Hindsighters'], palette='Set1', ax=ax1)
plt.show()

sns.distplot(b_t2[b_t2.Condition=='LiveTrackers']['q at T2'], bins=3)
sns.distplot(b_t2[b_t2.Condition=='Hindsighters']['q at T2'], bins=3)
plt.show()

sns.distplot(s_t2[s_t2.Condition=='LiveTrackers']['q at T2'], bins=3)
sns.distplot(s_t2[s_t2.Condition=='Hindsighters']['q at T2'], bins=3)
plt.show()
53/8:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'LiveTrackers', 'Hindsighters')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
b_t2 = t2[t2.sale_t20==0] #buyers at t2
b_t2['Buyers'] = b_t2['Condition']
s_t2 = t2[t2.purchase_t20==0] #sellers at t2
s_t2['Sellers'] = s_t2['Condition']
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Sellers', y='q at T2', data=s_t2, order=['LiveTrackers', 'Hindsighters'], palette='Set1', ax=ax2)
sns.boxplot(x='Buyers', y='q at T2', data=b_t2, order=['LiveTrackers', 'Hindsighters'], palette='Set1', ax=ax1)
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(b_t2[b_t2.Condition=='LiveTrackers']['q at T2'], bins=3, ax=ax1)
sns.distplot(b_t2[b_t2.Condition=='Hindsighters']['q at T2'], bins=3, ax=ax1)
# plt.show()

sns.distplot(s_t2[s_t2.Condition=='LiveTrackers']['q at T2'], bins=3, ax=ax2)
sns.distplot(s_t2[s_t2.Condition=='Hindsighters']['q at T2'], bins=3, ax=ax2)
plt.show()
53/9:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'LiveTrackers', 'Hindsighters')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
b_t2 = t2[t2.sale_t20==0] #buyers at t2
b_t2['Buyers'] = b_t2['Condition']
s_t2 = t2[t2.purchase_t20==0] #sellers at t2
s_t2['Sellers'] = s_t2['Condition']
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Sellers', y='q at T2', data=s_t2, order=['LiveTrackers', 'Hindsighters'], palette='Set1', ax=ax2)
sns.boxplot(x='Buyers', y='q at T2', data=b_t2, order=['LiveTrackers', 'Hindsighters'], palette='Set1', ax=ax1)
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(b_t2[b_t2.Condition=='LiveTrackers']['q at T2'], bins=3, ax=ax1, palette='Set1')
sns.distplot(b_t2[b_t2.Condition=='Hindsighters']['q at T2'], bins=3, ax=ax1, palette='Set1')
# plt.show()

sns.distplot(s_t2[s_t2.Condition=='LiveTrackers']['q at T2'], bins=3, ax=ax2)
sns.distplot(s_t2[s_t2.Condition=='Hindsighters']['q at T2'], bins=3, ax=ax2)
plt.show()
53/10:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'Live', 'WaterCooler')
dfz['T1'] = dfz.invratio_1
dfz['T2'] = dfz.invratio_2

# dftmp = dfz.copy()
# dflive = pd.melt(dfz[dfz.Condition=='Live'][['T1','T2']])
# dflive.columns=['Time of Trade', 'q']
# dfwc=pd.melt(dfz[dfz.Condition=='WaterCooler'][['T1','T2']])
# dfwc.columns=['Time of Trade', 'q']

# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Time of Trade', y='q', data=dflive,  ax=ax1)
# sns.boxplot(x='Time of Trade', y='q', data=dfwc,  ax=ax2)
# ax1.set_title('Live')
# ax2.set_title('WaterCooler')
# plt.show()



# fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
# sns.boxplot(x='Condition', y='T1', data=dfz, order=['Live','WaterCooler'], ax=ax1)
# sns.boxplot(x='Condition', y='T2', data=dfz, order=['Live','WaterCooler'], ax=ax2)
# ax1.set_title('T1')
# ax2.set_title('T2')
# plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)

sns.distplot(dfz[dfz.Condition=='Live']['T1'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "q at T1"})
sns.distplot(dfz[dfz.Condition=='Live']['T2'], ax=ax1, bins=6, axlabel=False, kde_kws={"label": "q at T2"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T1'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "q at T1"})
sns.distplot(dfz[dfz.Condition=='WaterCooler']['T2'], ax=ax2, bins=6, axlabel=False, kde_kws={"label": "q at T2"})
ax1.set_title('LiveTrackers')
ax2.set_title('Hindsighters')
plt.show()
53/11:
dfz = df[df.experiment.str.contains('profit')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'LiveTrackers', 'Hindsighters')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
b_t2 = t2[t2.sale_t20==0] #buyers at t2
b_t2['Buyers'] = b_t2['Condition']
s_t2 = t2[t2.purchase_t20==0] #sellers at t2
s_t2['Sellers'] = s_t2['Condition']
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Sellers', y='q at T2', data=s_t2, order=['LiveTrackers', 'Hindsighters'], palette='Set1', ax=ax2)
sns.boxplot(x='Buyers', y='q at T2', data=b_t2, order=['LiveTrackers', 'Hindsighters'], palette='Set1', ax=ax1)
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(b_t2[b_t2.Condition=='LiveTrackers']['q at T2'], bins=3, ax=ax1, palette='Set1')
sns.distplot(b_t2[b_t2.Condition=='Hindsighters']['q at T2'], bins=3, ax=ax1, palette='Set1')
# plt.show()

sns.distplot(s_t2[s_t2.Condition=='LiveTrackers']['q at T2'], bins=3, ax=ax2)
sns.distplot(s_t2[s_t2.Condition=='Hindsighters']['q at T2'], bins=3, ax=ax2)
plt.show()
53/12:
dfz = df[df.experiment.str.contains('loss')]
dfz['Condition'] = np.where(dfz.experiment.str.contains('rc'), 'LiveTrackers', 'Hindsighters')
dfz['q at T1'] = dfz.invratio_1
dfz['q at T2'] = dfz.invratio_2


# Traders at t2
t2=dfz[(dfz.purchase_t20+dfz.sale_t20)>0]
b_t2 = t2[t2.sale_t20==0] #buyers at t2
b_t2['Buyers'] = b_t2['Condition']
s_t2 = t2[t2.purchase_t20==0] #sellers at t2
s_t2['Sellers'] = s_t2['Condition']
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.boxplot(x='Sellers', y='q at T2', data=s_t2, order=['LiveTrackers', 'Hindsighters'], palette='Set1', ax=ax2)
sns.boxplot(x='Buyers', y='q at T2', data=b_t2, order=['LiveTrackers', 'Hindsighters'], palette='Set1', ax=ax1)
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
sns.distplot(b_t2[b_t2.Condition=='LiveTrackers']['q at T2'], bins=3, ax=ax1, palette='Set1')
sns.distplot(b_t2[b_t2.Condition=='Hindsighters']['q at T2'], bins=3, ax=ax1, palette='Set1')
# plt.show()

sns.distplot(s_t2[s_t2.Condition=='LiveTrackers']['q at T2'], bins=3, ax=ax2)
sns.distplot(s_t2[s_t2.Condition=='Hindsighters']['q at T2'], bins=3, ax=ax2)
plt.show()
53/13:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques='regret'
ans=['No', 'Only Trade 1', 'Only Trade 2', 'Both Trades']
name = ['LiveTrackers-Profit', 'Hindsighters-Profit', 'LiveTrackers-Loss', 'Hindsighters-Loss']

fig, ax = plt.subplots(1,4, figsize=(12,3))
for c,exp in enumerate(["'rc_profit'","'wc_profit'","'rc_loss'","'wc_loss'"]):
    ax[c].set_title(name[c])
    ax[c].set_xticklabels(ans)
    s=ansdf[ansdf.experiment==exp][ques].value_counts()
    vc = pd.Series(index=ans, data=[0]*len(ans))
    vc=vc.combine(s, max, fill_value=0)
    vc.ix[ans].plot.bar(ax=ax[c])
53/14:
df['tradingfreq'].replace(dict(zip(range(1,6), ['Never', 'Rarely', 'Occasionally', 'Often', 'Everyday'])), inplace=True)
df['confidence'].replace(dict(zip(range(1,3), ['No', 'Yes'])), inplace=True)
df['focus'].replace(dict(zip(range(1,6), ['Not at all', 'Distracted', 'Neutral', 'Attentive', 'Hawkish'])), inplace=True)
df['stateofmind'].replace(dict(zip(range(1,6), ['Anxious', 'Slightly nervous', 'Calm', 'Fairly optimistic', 'Confident!'])), inplace=True)
df['regret'].replace(dict(zip(range(1,5), ['No', 'Only Trade 1', 'Only Trade 2', 'Both Trades'])), inplace=True)
# df.to_csv('result_answers.csv')
53/15:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques='regret'
ans=['No', 'Only Trade 1', 'Only Trade 2', 'Both Trades']
name = ['LiveTrackers-Profit', 'Hindsighters-Profit', 'LiveTrackers-Loss', 'Hindsighters-Loss']

fig, ax = plt.subplots(1,4, figsize=(12,3))
for c,exp in enumerate(["'rc_profit'","'wc_profit'","'rc_loss'","'wc_loss'"]):
    ax[c].set_title(name[c])
    ax[c].set_xticklabels(ans)
    s=ansdf[ansdf.experiment==exp][ques].value_counts()
    vc = pd.Series(index=ans, data=[0]*len(ans))
    vc=vc.combine(s, max, fill_value=0)
    vc.ix[ans].plot.bar(ax=ax[c])
53/16:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques='stateofmind'
ans=['Anxious', 'Slightly nervous', 'Calm', 'Fairly optimistic', 'Confident!']
name = ['LiveTrackers-Profit', 'Hindsighters-Profit', 'LiveTrackers-Loss', 'Hindsighters-Loss']

fig, ax = plt.subplots(1,4, figsize=(12,3))
for c,exp in enumerate(["'rc_profit'","'wc_profit'","'rc_loss'","'wc_loss'"]):
    ax[c].set_title(name[c])
    ax[c].set_xticklabels(ans)
    s=ansdf[ansdf.experiment==exp][ques].value_counts()
    vc = pd.Series(index=ans, data=[0]*len(ans))
    vc=vc.combine(s, max, fill_value=0)
    vc.ix[ans].plot.bar(ax=ax[c])
53/17:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques='focus'
ans=['Not at all', 'Distracted', 'Neutral', 'Attentive', 'Hawkish']
name = ['LiveTrackers-Profit', 'Hindsighters-Profit', 'LiveTrackers-Loss', 'Hindsighters-Loss']

fig, ax = plt.subplots(1,4, figsize=(12,3))
for c,exp in enumerate(["'rc_profit'","'wc_profit'","'rc_loss'","'wc_loss'"]):
    ax[c].set_title(name[c])
    ax[c].set_xticklabels(ans)
    s=ansdf[ansdf.experiment==exp][ques].value_counts()
    vc = pd.Series(index=ans, data=[0]*len(ans))
    vc=vc.combine(s, max, fill_value=0)
    vc.ix[ans].plot.bar(ax=ax[c])
53/18:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques='confidence'
ans=['No', 'Yes']
name = ['LiveTrackers-Profit', 'Hindsighters-Profit', 'LiveTrackers-Loss', 'Hindsighters-Loss']

fig, ax = plt.subplots(1,4, figsize=(12,3))
for c,exp in enumerate(["'rc_profit'","'wc_profit'","'rc_loss'","'wc_loss'"]):
    ax[c].set_title(name[c])
    ax[c].set_xticklabels(ans)
    s=ansdf[ansdf.experiment==exp][ques].value_counts()
    vc = pd.Series(index=ans, data=[0]*len(ans))
    vc=vc.combine(s, max, fill_value=0)
    vc.ix[ans].plot.bar(ax=ax[c])
53/19:
ansdf = df[['experiment', 'tradingfreq', 'confidence', 'focus', 'stateofmind', 'regret']]
ques='tradingfreq'
ans=['Never', 'Rarely', 'Occasionally', 'Often', 'Everyday']
name = ['LiveTrackers-Profit', 'Hindsighters-Profit', 'LiveTrackers-Loss', 'Hindsighters-Loss']

fig, ax = plt.subplots(1,4, figsize=(12,3))
for c,exp in enumerate(["'rc_profit'","'wc_profit'","'rc_loss'","'wc_loss'"]):
    ax[c].set_title(name[c])
    ax[c].set_xticklabels(ans)
    s=ansdf[ansdf.experiment==exp][ques].value_counts()
    vc = pd.Series(index=ans, data=[0]*len(ans))
    vc=vc.combine(s, max, fill_value=0)
    vc.ix[ans].plot.bar(ax=ax[c])
54/1:
import pandas as pd
from collections import defaultdict
import math
import re
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
import numpy as np
from str_helper import *
54/2:
apa = pd.read_csv('ontologies\APAONTO.csv').fillna('') #Preferred Label, Definitions, Alt name
mf = pd.read_csv('ontologies\MF.csv').fillna('')
mfo = pd.read_csv('ontologies\MFOMD.csv').fillna('')
sui = pd.read_csv('ontologies\suicideo.csv').fillna('')
54/3:
thes=defaultdict(dict)

def add_defn(w, dfn, onto):
    if w=='':
        pass
    d=thes[w]
    d[onto]=re.sub(r'\[wikipedia: http\S+\]', '', dfn.replace('\n', ' '), flags=re.IGNORECASE)
    thes[w] = d


for row in apa.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['Alt name'].split('|')
    for w in syn:
        add_defn(w, defn, 'apa')    
        
for row in mf.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    add_defn(word, defn, 'mf')
    
for row in mfo.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['Synonyms'].split('|')
    for w in syn:
        add_defn(w, defn, 'mfo')

for row in sui.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['alternative term'].split('\n')
    for w in syn:
        add_defn(w, defn, 'sui')
        
import pickle 
with open('psychThesaurusDict.pkl','wb') as f:
        pickle.dump(thes, f)
54/4:
window_size = 10


def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train_verbose.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
55/1:
import pandas as pd
from collections import defaultdict
import math
import re
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
import numpy as np
from str_helper import *
55/2:
apa = pd.read_csv('ontologies\APAONTO.csv').fillna('') #Preferred Label, Definitions, Alt name
mf = pd.read_csv('ontologies\MF.csv').fillna('')
mfo = pd.read_csv('ontologies\MFOMD.csv').fillna('')
sui = pd.read_csv('ontologies\suicideo.csv').fillna('')
55/3:
thes=defaultdict(dict)

def add_defn(w, dfn, onto):
    if w=='':
        pass
    d=thes[w]
    d[onto]=re.sub(r'\[wikipedia: http\S+\]', '', dfn.replace('\n', ' '), flags=re.IGNORECASE)
    thes[w] = d


for row in apa.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['Alt name'].split('|')
    for w in syn:
        add_defn(w, defn, 'apa')    
        
for row in mf.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    add_defn(word, defn, 'mf')
    
for row in mfo.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['Synonyms'].split('|')
    for w in syn:
        add_defn(w, defn, 'mfo')

for row in sui.iterrows():
    word = row[1]['Preferred Label']
    defn = row[1]['Definitions']
    if defn=='':
        continue
    syn = [word]+row[1]['alternative term'].split('\n')
    for w in syn:
        add_defn(w, defn, 'sui')
        
import pickle 
with open('psychThesaurusDict.pkl','wb') as f:
        pickle.dump(thes, f)
55/4:
window_size = 10


def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train_verbose.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
55/5:
window_size = 10


def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train_verbose.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
55/6:
window_size = 10


def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train_verbose.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
56/1:
import pandas as pd
from collections import defaultdict
import math
import re
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
import numpy as np
from str_helper import *
56/2:
window_size = 10


def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train_verbose.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
56/3:
window_size = 10


with open('psychThesaurusDict.pkl','rb') as f:
        thes = pickle.load(f)      

def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train_verbose.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
56/4:
import pandas as pd
from collections import defaultdict
import math
import re
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
import numpy as np
from str_helper import *
import pickle
56/5:
window_size = 10


with open('psychThesaurusDict.pkl','rb') as f:
        thes = pickle.load(f)      

def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train_verbose.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
56/6: list('abc')
56/7:
window_size = 10


with open('psychThesaurusDict.pkl','rb') as f:
        thes = pickle.load(f)      

def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train_verbose.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
57/1:
import pandas as pd
from collections import defaultdict
import math
import re
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
import numpy as np
from str_helper import *
import pickle
57/2:
import pandas as pd
from collections import defaultdict
import math
import re
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
import numpy as np
from str_helper import *
import pickle
57/3:
window_size = 10


with open('psychThesaurusDict.pkl','rb') as f:
        thes = pickle.load(f)      

def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train_verbose.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
58/1:
import pandas as pd
from collections import defaultdict
import math
import re
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
import numpy as np
from str_helper import *
import pickle
58/2:
window_size = 10


with open('psychThesaurusDict.pkl','rb') as f:
        thes = pickle.load(f)      

def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train_verbose.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
58/3: list('abc')
58/4:
import pandas as pd
from collections import defaultdict
import math
import re
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
import numpy as np
from str_helper import *
import pickle
58/5:
window_size = 10


with open('psychThesaurusDict.pkl','rb') as f:
        thes = pickle.load(f)      

def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train_verbose.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
59/1:
import pandas as pd
from collections import defaultdict
import math
import re
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
import numpy as np
from str_helper import *
import pickle
59/2:
window_size = 10


with open('psychThesaurusDict.pkl','rb') as f:
        thes = pickle.load(f)      

def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train_verbose.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
59/3: list('abc')
60/1:
import pandas as pd
from collections import defaultdict
import math
import re
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
import numpy as np
from str_helper import *
import pickle
60/2:
window_size = 10


with open('psychThesaurusDict.pkl','rb') as f:
        thes = pickle.load(f)      

def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train_verbose.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
61/1:
import pandas as pd
from collections import defaultdict
import math
import re
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
import numpy as np
from str_helper import *
import pickle
61/2:
window_size = 10


with open('psychThesaurusDict.pkl','rb') as f:
        thes = pickle.load(f)      

def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train_verbose.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
62/1:
import pandas as pd
from collections import defaultdict
import math
import re
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
import numpy as np
from str_helper import *
import pickle
62/2:
window_size = 10


with open('psychThesaurusDict.pkl','rb') as f:
        thes = pickle.load(f)      

def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train_verbose.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
63/1:
import pandas as pd
from collections import defaultdict
import math
import re
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
import numpy as np
from str_helper import *
import pickle
63/2:
window_size = 10


with open('psychThesaurusDict.pkl','rb') as f:
        thes = pickle.load(f)      

def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train_verbose.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
63/3: list('abc')
63/4: map(lower, list('abc'))
63/5: 'abc'.lower()
64/1:
import pandas as pd
from collections import defaultdict
import math
import re
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
import numpy as np
from str_helper import *
import pickle
64/2:
window_size = 10


with open('psychThesaurusDict.pkl','rb') as f:
        thes = pickle.load(f)      

def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train_verbose.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
64/3: 'abc'.lower()
65/1:
import pandas as pd
from collections import defaultdict
import math
import re
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
import numpy as np
from str_helper import *
import pickle
65/2:
window_size = 10


with open('psychThesaurusDict.pkl','rb') as f:
        thes = pickle.load(f)      

def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train_verbose.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
66/1:
import pandas as pd
from collections import defaultdict
import math
import re
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
import numpy as np
from str_helper import *
import pickle
66/2:
window_size = 10


with open('psychThesaurusDict.pkl','rb') as f:
        thes = pickle.load(f)      

def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train_verbose.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
66/3: 'abc'.lower()
66/4:
def split_words(abst):
    return re.sub(r'([a-zA-Z])[\-\/\|]([a-zA-Z])', r'\1 \2', abst)

split_words('MGI.|Attention')
66/5:
def split_words(abst):
    return re.sub(r'([a-zA-Z])[\-\/|]([a-zA-Z])', r'\1 \2', abst)

split_words('MGI.|Attention')
66/6:
def split_words(abst):
    return re.sub(r'([a-zA-Z])[\|\-\/]([a-zA-Z])', r'\1 \2', abst)

split_words('MGI.|Attention')
66/7:
def split_words(abst):
    return re.sub(r'([a-zA-Z])[^a-zA-Z]([a-zA-Z])', r'\1 \2', abst)

split_words('MGI.|Attention')
66/8:
def split_words(abst):
    return re.sub(r'([a-zA-Z])[^a-zA-Z]+([a-zA-Z])', r'\1 \2', abst)

split_words('MGI.|Attention')
67/1:
import pandas as pd
from collections import defaultdict
import math
import re
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
import numpy as np
from str_helper import *
import pickle
67/2:
window_size = 10


with open('psychThesaurusDict.pkl','rb') as f:
        thes = pickle.load(f)      

def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train_verbose.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
67/3:
def split_words(abst):
    return re.sub(r'([a-zA-Z])[^a-zA-Z]+([a-zA-Z])', r'\1 \2', abst)

split_words('MGI.|Attention')
68/1:
import pandas as pd
from collections import defaultdict
import math
import re
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
import numpy as np
from str_helper import *
import pickle
68/2:
window_size = 10


with open('psychThesaurusDict.pkl','rb') as f:
        thes = pickle.load(f)      

def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train_verbose.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
68/3:
def split_words(abst):
    return re.sub(r'([a-zA-Z])[^a-zA-Z]+([a-zA-Z])', r'\1 \2', abst)

split_words('MGI.|Attention')
69/1:
import pandas as pd
from collections import defaultdict
import math
import re
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
import numpy as np
from str_helper import *
import pickle
69/2:
window_size = 10


with open('psychThesaurusDict.pkl','rb') as f:
        thes = pickle.load(f)      

def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train_verbose.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
69/3:
def split_words(abst):
    return re.sub(r'([a-zA-Z])[^a-zA-Z]+([a-zA-Z])', r'\1 \2', abst)

split_words('MGI.|Attention')
70/1:
import pandas as pd
from collections import defaultdict
import math
import re
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
import numpy as np
from str_helper import *
import pickle
70/2:
window_size = 10


with open('psychThesaurusDict.pkl','rb') as f:
        thes = pickle.load(f)      

def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train_verbose.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
70/3:
def split_words(abst):
    return re.sub(r'([a-zA-Z])[^a-zA-Z]+([a-zA-Z])', r'\1 \2', abst)

split_words('MGI.|Attention')
70/4:
from nltk import word_tokenize, pos_tag, FreqDist
from nltk.corpus import stopwords
import io
import pandas as pd
import re
import numpy as np
import math


def split_words_aggro(abst):
    return re.sub(r'([a-zA-Z])[^a-zA-Z]+([a-zA-Z])', r'\1 \2', abst) # split words. delims = all non-words

def split_words(abst):
    return re.sub(r'([a-zA-Z])[\-\/]([a-zA-Z])', r'\1 \2', abst) # split words, delims = -,/

def remove_stopwords(s):
#   nltk_stopwords = ['i',  'me',  'my',  'myself',  'we',  'our',  'ours',  'ourselves',  'you',  "you're",  "you've",  "you'll",  "you'd",  'your',  'yours',  'yourself',  'yourselves',  'he',  'him',  'his',  'himself',  'she',  "she's",  'her',  'hers',  'herself',  'it',  "it's",  'its',  'itself',  'they',  'them',  'their',  'theirs',  'themselves',  'what',  'which',  'who',  'whom',  'this',  'that',  "that'll",  'these',  'those',  'am',  'is',  'are',  'was',  'were',  'be',  'been',  'being',  'have',  'has',  'had',  'having',  'do',  'does',  'did',  'doing',  'a',  'an',  'the',  'and',  'but',  'if',  'or',  'because',  'as',  'until',  'while',  'of',  'at',  'by',  'for',  'with',  'about',  'against',  'between',  'into',  'through',  'during',  'before',  'after',  'above',  'below',  'to',  'from',  'up',  'down',  'in',  'out',  'on',  'off',  'over',  'under',  'again',  'further',  'then',  'once',  'here',  'there',  'when',  'where',  'why',  'how',  'all',  'any',  'both',  'each',  'few',  'more',  'most',  'other',  'some',  'such',  'no',  'nor',  'not',  'only',  'own',  'same',  'so',  'than',  'too',  'very',  's',  't',  'can',  'will',  'just',  'don',  "don't",  'should',  "should've",  'now',  'd',  'll',  'm',  'o',  're',  've',  'y',  'ain',  'aren',  "aren't",  'couldn',  "couldn't",  'didn',  "didn't",  'doesn',  "doesn't",  'hadn',  "hadn't",  'hasn',  "hasn't",  'haven',  "haven't",  'isn',  "isn't",  'ma',  'mightn',  "mightn't",  'mustn',  "mustn't",  'needn',  "needn't",  'shan',  "shan't",  'shouldn',  "shouldn't",  'wasn',  "wasn't",  'weren',  "weren't",  'won',  "won't",  'wouldn',  "wouldn't"]
    nltk_stopwords = stopwords.words('english')
    STOPWORDS = nltk_stopwords+["'s", "s"]+[x.capitalize() for x in nltk_stopwords]
    tokens = word_tokenize(s)
    okstr = ' '.join([x for x in tokens if x in ['(',')'] or x not in STOPWORDS and len(x)>1])
    return okstr

def remove_punct(s):
    s = re.sub(r'[^A-Za-z\-]+', '', s)
    return s



def filter_pos_tags(s):
    OUT = ['VBD', 'IN', 'VBP', 'RB', 'MD', 'VB', 'VBZ', 'VBG']
    return ' '.join([w for w,t in pos_tag(word_tokenize(s)) if t not in OUT])


def tokenize(s):
    return word_tokenize(s)


def abbr_expander(s):   
    res = re.findall(r'\(\s?[A-Z]\s?[A-Za-z]+\s?\)', s) # Misses acronyms with hyphen
           
    outs = s
    for abbr in res:
        ix = s.index(abbr)
        abbr_clean = re.sub(r'[^A-Za-z]', '', abbr) # keep only letters, discard all other chars
        abbr_len = len(re.sub(r'[a-z]','',abbr_clean)) # Number of capital letters ~ number of tokens to look back for expansion
        if abbr_clean[-1]=='s': abbr_len-=1 # Singularing a plural acronym
        # When looking for expansion candidates, consider (-) and (/) as word-breaks
        candidate_l = split_words_aggro(s[ix-1::-1][::-1].rstrip()).split(' ')[-abbr_len:] # identify possible candidates for expansion from preivous abbr_len words
        expansion_l = [] 
        
        aix=0
        wix=0
        print(abbr_clean, candidate_l)
        while aix<len(abbr_clean) and wix<len(candidate_l):
            if abbr_clean[aix].lower()==candidate_l[wix][0].lower():
                expansion_l.append(candidate_l[wix])
                aix+=1
                wix+=1
            elif abbr[aix].lower() in candidate_l[max(0,wix-1)].lower():
                aix+=1
            else:
                wix+=1

        if len(expansion_l)>0.4*abbr_len:
            expanded = ' '.join(expansion_l)
            outs = outs.replace(' '+abbr, '') # Remove first instance of abbreviation before expanding other instances
            if abbr_clean[-1]=='s':
                outs = outs.replace(abbr_clean[:-1], expanded)
            else:
                outs = outs.replace(abbr_clean, expanded) 
            print(abbr,expanded)

    return outs



def clean_sent(abst):
    abst = split_words(abst)
    abst = abbr_expander(abst)
    
    # Make post-period and first word uppercase chars lower
    abst = re.sub(r"(?<=\. )[A-Z]",lambda t:t.group().lower(), abst)
    abst = abst[0].lower()+abst[1:]
    abst = remove_punct(abst)
    
#     abst = filter_pos_tags(abst)
#     abst=remove_stopwords(abst)

    return abst


def sim(v1, v2):
    return 1-cosine(v1,v2)



def idf(doclist):
    N = len(doclist)
    vocab = list(FreqDist(word_tokenize(' '.join(doclist))).keys())
    idfd={}
    for w in vocab:
        idfd[w] = math.log(N/max(1,sum([1 for d in doclist if w in d])), 10)
    return idfd

def tfidfDist(doclist):
    idf_lookup = idf(doclist)
    docs_tfidf = []
    for doc in doclist:
        token_fd = list(FreqDist(word_tokenize(doc)).items())
        tfidf = [(w, freq*idf_lookup[w]) for w,freq in token_fd]
        docs_tfidf.append(sorted(tfidf, key=lambda x:x[1], reverse=True))
    return docs_tfidf


def filter_bottom_tfidf(doclist, cutoff_pcile=0.2):
    tfidf_vals = tfidfDist(doclist)
    filt_docs=[]

    for c,doc in enumerate(doclist):
        tf = tfidf_vals[c]
        allowed = [w for w,n in tf[:math.ceil((1-cutoff_pcile)*len(tf))]]
        filt_docs.append(' '.join([w for w in word_tokenize(doc) if w in allowed]))

    return filt_docs
70/5: clean_sent("sdfds SDFSDF Sdcxcsqw")
70/6:
from nltk import word_tokenize, pos_tag, FreqDist
from nltk.corpus import stopwords
import io
import pandas as pd
import re
import numpy as np
import math


def split_words_aggro(abst):
    return re.sub(r'([a-zA-Z])[^a-zA-Z]+([a-zA-Z])', r'\1 \2', abst) # split words. delims = all non-words

def split_words(abst):
    return re.sub(r'([a-zA-Z])[\-\/]([a-zA-Z])', r'\1 \2', abst) # split words, delims = -,/

def remove_stopwords(s):
#   nltk_stopwords = ['i',  'me',  'my',  'myself',  'we',  'our',  'ours',  'ourselves',  'you',  "you're",  "you've",  "you'll",  "you'd",  'your',  'yours',  'yourself',  'yourselves',  'he',  'him',  'his',  'himself',  'she',  "she's",  'her',  'hers',  'herself',  'it',  "it's",  'its',  'itself',  'they',  'them',  'their',  'theirs',  'themselves',  'what',  'which',  'who',  'whom',  'this',  'that',  "that'll",  'these',  'those',  'am',  'is',  'are',  'was',  'were',  'be',  'been',  'being',  'have',  'has',  'had',  'having',  'do',  'does',  'did',  'doing',  'a',  'an',  'the',  'and',  'but',  'if',  'or',  'because',  'as',  'until',  'while',  'of',  'at',  'by',  'for',  'with',  'about',  'against',  'between',  'into',  'through',  'during',  'before',  'after',  'above',  'below',  'to',  'from',  'up',  'down',  'in',  'out',  'on',  'off',  'over',  'under',  'again',  'further',  'then',  'once',  'here',  'there',  'when',  'where',  'why',  'how',  'all',  'any',  'both',  'each',  'few',  'more',  'most',  'other',  'some',  'such',  'no',  'nor',  'not',  'only',  'own',  'same',  'so',  'than',  'too',  'very',  's',  't',  'can',  'will',  'just',  'don',  "don't",  'should',  "should've",  'now',  'd',  'll',  'm',  'o',  're',  've',  'y',  'ain',  'aren',  "aren't",  'couldn',  "couldn't",  'didn',  "didn't",  'doesn',  "doesn't",  'hadn',  "hadn't",  'hasn',  "hasn't",  'haven',  "haven't",  'isn',  "isn't",  'ma',  'mightn',  "mightn't",  'mustn',  "mustn't",  'needn',  "needn't",  'shan',  "shan't",  'shouldn',  "shouldn't",  'wasn',  "wasn't",  'weren',  "weren't",  'won',  "won't",  'wouldn',  "wouldn't"]
    nltk_stopwords = stopwords.words('english')
    STOPWORDS = nltk_stopwords+["'s", "s"]+[x.capitalize() for x in nltk_stopwords]
    tokens = word_tokenize(s)
    okstr = ' '.join([x for x in tokens if x in ['(',')'] or x not in STOPWORDS and len(x)>1])
    return okstr

def remove_punct(s):
    s = re.sub(r'[^A-Za-z\-]+', '', s)
    return s



def filter_pos_tags(s):
    OUT = ['VBD', 'IN', 'VBP', 'RB', 'MD', 'VB', 'VBZ', 'VBG']
    return ' '.join([w for w,t in pos_tag(word_tokenize(s)) if t not in OUT])


def tokenize(s):
    return word_tokenize(s)


def abbr_expander(s):   
    res = re.findall(r'\(\s?[A-Z]\s?[A-Za-z]+\s?\)', s) # Misses acronyms with hyphen
           
    outs = s
    for abbr in res:
        ix = s.index(abbr)
        abbr_clean = re.sub(r'[^A-Za-z]', '', abbr) # keep only letters, discard all other chars
        abbr_len = len(re.sub(r'[a-z]','',abbr_clean)) # Number of capital letters ~ number of tokens to look back for expansion
        if abbr_clean[-1]=='s': abbr_len-=1 # Singularing a plural acronym
        # When looking for expansion candidates, consider (-) and (/) as word-breaks
        candidate_l = split_words_aggro(s[ix-1::-1][::-1].rstrip()).split(' ')[-abbr_len:] # identify possible candidates for expansion from preivous abbr_len words
        expansion_l = [] 
        
        aix=0
        wix=0
        print(abbr_clean, candidate_l)
        while aix<len(abbr_clean) and wix<len(candidate_l):
            if abbr_clean[aix].lower()==candidate_l[wix][0].lower():
                expansion_l.append(candidate_l[wix])
                aix+=1
                wix+=1
            elif abbr[aix].lower() in candidate_l[max(0,wix-1)].lower():
                aix+=1
            else:
                wix+=1

        if len(expansion_l)>0.4*abbr_len:
            expanded = ' '.join(expansion_l)
            outs = outs.replace(' '+abbr, '') # Remove first instance of abbreviation before expanding other instances
            if abbr_clean[-1]=='s':
                outs = outs.replace(abbr_clean[:-1], expanded)
            else:
                outs = outs.replace(abbr_clean, expanded) 
            print(abbr,expanded)

    return outs



def clean_sent(abst):
    abst = split_words(abst)
#     abst = abbr_expander(abst)
    
    # Make post-period and first word uppercase chars lower
    abst = re.sub(r"(?<=\. )[A-Z]",lambda t:t.group().lower(), abst)
    abst = abst[0].lower()+abst[1:]
    abst = remove_punct(abst)
    
#     abst = filter_pos_tags(abst)
#     abst=remove_stopwords(abst)

    return abst


def sim(v1, v2):
    return 1-cosine(v1,v2)



def idf(doclist):
    N = len(doclist)
    vocab = list(FreqDist(word_tokenize(' '.join(doclist))).keys())
    idfd={}
    for w in vocab:
        idfd[w] = math.log(N/max(1,sum([1 for d in doclist if w in d])), 10)
    return idfd

def tfidfDist(doclist):
    idf_lookup = idf(doclist)
    docs_tfidf = []
    for doc in doclist:
        token_fd = list(FreqDist(word_tokenize(doc)).items())
        tfidf = [(w, freq*idf_lookup[w]) for w,freq in token_fd]
        docs_tfidf.append(sorted(tfidf, key=lambda x:x[1], reverse=True))
    return docs_tfidf


def filter_bottom_tfidf(doclist, cutoff_pcile=0.2):
    tfidf_vals = tfidfDist(doclist)
    filt_docs=[]

    for c,doc in enumerate(doclist):
        tf = tfidf_vals[c]
        allowed = [w for w,n in tf[:math.ceil((1-cutoff_pcile)*len(tf))]]
        filt_docs.append(' '.join([w for w in word_tokenize(doc) if w in allowed]))

    return filt_docs
70/7: clean_sent("sdfds SDFSDF Sdcxcsqw")
70/8: clean_sent("sdfds SDFSDF Sdcxcsqw")
70/9:
from nltk import word_tokenize, pos_tag, FreqDist
from nltk.corpus import stopwords
import io
import pandas as pd
import re
import numpy as np
import math


def split_words_aggro(abst):
    return re.sub(r'([a-zA-Z])[^a-zA-Z]+([a-zA-Z])', r'\1 \2', abst) # split words. delims = all non-words

def split_words(abst):
    return re.sub(r'([a-zA-Z])[\-\/]([a-zA-Z])', r'\1 \2', abst) # split words, delims = -,/

def remove_stopwords(s):
#   nltk_stopwords = ['i',  'me',  'my',  'myself',  'we',  'our',  'ours',  'ourselves',  'you',  "you're",  "you've",  "you'll",  "you'd",  'your',  'yours',  'yourself',  'yourselves',  'he',  'him',  'his',  'himself',  'she',  "she's",  'her',  'hers',  'herself',  'it',  "it's",  'its',  'itself',  'they',  'them',  'their',  'theirs',  'themselves',  'what',  'which',  'who',  'whom',  'this',  'that',  "that'll",  'these',  'those',  'am',  'is',  'are',  'was',  'were',  'be',  'been',  'being',  'have',  'has',  'had',  'having',  'do',  'does',  'did',  'doing',  'a',  'an',  'the',  'and',  'but',  'if',  'or',  'because',  'as',  'until',  'while',  'of',  'at',  'by',  'for',  'with',  'about',  'against',  'between',  'into',  'through',  'during',  'before',  'after',  'above',  'below',  'to',  'from',  'up',  'down',  'in',  'out',  'on',  'off',  'over',  'under',  'again',  'further',  'then',  'once',  'here',  'there',  'when',  'where',  'why',  'how',  'all',  'any',  'both',  'each',  'few',  'more',  'most',  'other',  'some',  'such',  'no',  'nor',  'not',  'only',  'own',  'same',  'so',  'than',  'too',  'very',  's',  't',  'can',  'will',  'just',  'don',  "don't",  'should',  "should've",  'now',  'd',  'll',  'm',  'o',  're',  've',  'y',  'ain',  'aren',  "aren't",  'couldn',  "couldn't",  'didn',  "didn't",  'doesn',  "doesn't",  'hadn',  "hadn't",  'hasn',  "hasn't",  'haven',  "haven't",  'isn',  "isn't",  'ma',  'mightn',  "mightn't",  'mustn',  "mustn't",  'needn',  "needn't",  'shan',  "shan't",  'shouldn',  "shouldn't",  'wasn',  "wasn't",  'weren',  "weren't",  'won',  "won't",  'wouldn',  "wouldn't"]
    nltk_stopwords = stopwords.words('english')
    STOPWORDS = nltk_stopwords+["'s", "s"]+[x.capitalize() for x in nltk_stopwords]
    tokens = word_tokenize(s)
    okstr = ' '.join([x for x in tokens if x in ['(',')'] or x not in STOPWORDS and len(x)>1])
    return okstr

def remove_punct(s):
    s = re.sub(r'[^A-Za-z\-]+', '', s)
    return s



def filter_pos_tags(s):
    OUT = ['VBD', 'IN', 'VBP', 'RB', 'MD', 'VB', 'VBZ', 'VBG']
    return ' '.join([w for w,t in pos_tag(word_tokenize(s)) if t not in OUT])


def tokenize(s):
    return word_tokenize(s)


def abbr_expander(s):   
    res = re.findall(r'\(\s?[A-Z]\s?[A-Za-z]+\s?\)', s) # Misses acronyms with hyphen
           
    outs = s
    for abbr in res:
        ix = s.index(abbr)
        abbr_clean = re.sub(r'[^A-Za-z]', '', abbr) # keep only letters, discard all other chars
        abbr_len = len(re.sub(r'[a-z]','',abbr_clean)) # Number of capital letters ~ number of tokens to look back for expansion
        if abbr_clean[-1]=='s': abbr_len-=1 # Singularing a plural acronym
        # When looking for expansion candidates, consider (-) and (/) as word-breaks
        candidate_l = split_words_aggro(s[ix-1::-1][::-1].rstrip()).split(' ')[-abbr_len:] # identify possible candidates for expansion from preivous abbr_len words
        expansion_l = [] 
        
        aix=0
        wix=0
        print(abbr_clean, candidate_l)
        while aix<len(abbr_clean) and wix<len(candidate_l):
            if abbr_clean[aix].lower()==candidate_l[wix][0].lower():
                expansion_l.append(candidate_l[wix])
                aix+=1
                wix+=1
            elif abbr[aix].lower() in candidate_l[max(0,wix-1)].lower():
                aix+=1
            else:
                wix+=1

        if len(expansion_l)>0.4*abbr_len:
            expanded = ' '.join(expansion_l)
            outs = outs.replace(' '+abbr, '') # Remove first instance of abbreviation before expanding other instances
            if abbr_clean[-1]=='s':
                outs = outs.replace(abbr_clean[:-1], expanded)
            else:
                outs = outs.replace(abbr_clean, expanded) 
            print(abbr,expanded)

    return outs




def sim(v1, v2):
    return 1-cosine(v1,v2)



def idf(doclist):
    N = len(doclist)
    vocab = list(FreqDist(word_tokenize(' '.join(doclist))).keys())
    idfd={}
    for w in vocab:
        idfd[w] = math.log(N/max(1,sum([1 for d in doclist if w in d])), 10)
    return idfd

def tfidfDist(doclist):
    idf_lookup = idf(doclist)
    docs_tfidf = []
    for doc in doclist:
        token_fd = list(FreqDist(word_tokenize(doc)).items())
        tfidf = [(w, freq*idf_lookup[w]) for w,freq in token_fd]
        docs_tfidf.append(sorted(tfidf, key=lambda x:x[1], reverse=True))
    return docs_tfidf


def filter_bottom_tfidf(doclist, cutoff_pcile=0.2):
    tfidf_vals = tfidfDist(doclist)
    filt_docs=[]

    for c,doc in enumerate(doclist):
        tf = tfidf_vals[c]
        allowed = [w for w,n in tf[:math.ceil((1-cutoff_pcile)*len(tf))]]
        filt_docs.append(' '.join([w for w in word_tokenize(doc) if w in allowed]))

    return filt_docs
70/10:
def c(abst):
    abst = split_words(abst)
    abst = abbr_expander(abst)
    
    # Make post-period and first word uppercase chars lower
    abst = re.sub(r"(?<=\. )[A-Z]",lambda t:t.group().lower(), abst)
    abst = abst[0].lower()+abst[1:]
    abst = remove_punct(abst)
    
#     abst = filter_pos_tags(abst)
#     abst=remove_stopwords(abst)

    return abst


c("sdfds SDFSDF Sdcxcsqw")
70/11:
def c(abst):
    abst = split_words(abst)
#     abst = abbr_expander(abst)
    
    # Make post-period and first word uppercase chars lower
    abst = re.sub(r"(?<=\. )[A-Z]",lambda t:t.group().lower(), abst)
    abst = abst[0].lower()+abst[1:]
    abst = remove_punct(abst)
    
#     abst = filter_pos_tags(abst)
#     abst=remove_stopwords(abst)

    return abst


c("sdfds SDFSDF Sdcxcsqw")
70/12:
def c(abst):
#     abst = split_words(abst)
    abst = abbr_expander(abst)
    
    # Make post-period and first word uppercase chars lower
    abst = re.sub(r"(?<=\. )[A-Z]",lambda t:t.group().lower(), abst)
    abst = abst[0].lower()+abst[1:]
    abst = remove_punct(abst)
    
#     abst = filter_pos_tags(abst)
#     abst=remove_stopwords(abst)

    return abst


c("sdfds SDFSDF Sdcxcsqw")
70/13:
def c(abst):
    abst = split_words(abst)
    abst = abbr_expander(abst)
    
    # Make post-period and first word uppercase chars lower
    abst = re.sub(r"(?<=\. )[A-Z]",lambda t:t.group().lower(), abst)
    abst = abst[0].lower()+abst[1:]
#     abst = remove_punct(abst)
    
#     abst = filter_pos_tags(abst)
#     abst=remove_stopwords(abst)

    return abst


c("sdfds SDFSDF Sdcxcsqw")
70/14:
from nltk import word_tokenize, pos_tag, FreqDist
from nltk.corpus import stopwords
import io
import pandas as pd
import re
import numpy as np
import math


def split_words_aggro(abst):
    return re.sub(r'([a-zA-Z])[^a-zA-Z]+([a-zA-Z])', r'\1 \2', abst) # split words. delims = all non-words

def split_words(abst):
    return re.sub(r'([a-zA-Z])[\-\/]([a-zA-Z])', r'\1 \2', abst) # split words, delims = -,/

def remove_stopwords(s):
#   nltk_stopwords = ['i',  'me',  'my',  'myself',  'we',  'our',  'ours',  'ourselves',  'you',  "you're",  "you've",  "you'll",  "you'd",  'your',  'yours',  'yourself',  'yourselves',  'he',  'him',  'his',  'himself',  'she',  "she's",  'her',  'hers',  'herself',  'it',  "it's",  'its',  'itself',  'they',  'them',  'their',  'theirs',  'themselves',  'what',  'which',  'who',  'whom',  'this',  'that',  "that'll",  'these',  'those',  'am',  'is',  'are',  'was',  'were',  'be',  'been',  'being',  'have',  'has',  'had',  'having',  'do',  'does',  'did',  'doing',  'a',  'an',  'the',  'and',  'but',  'if',  'or',  'because',  'as',  'until',  'while',  'of',  'at',  'by',  'for',  'with',  'about',  'against',  'between',  'into',  'through',  'during',  'before',  'after',  'above',  'below',  'to',  'from',  'up',  'down',  'in',  'out',  'on',  'off',  'over',  'under',  'again',  'further',  'then',  'once',  'here',  'there',  'when',  'where',  'why',  'how',  'all',  'any',  'both',  'each',  'few',  'more',  'most',  'other',  'some',  'such',  'no',  'nor',  'not',  'only',  'own',  'same',  'so',  'than',  'too',  'very',  's',  't',  'can',  'will',  'just',  'don',  "don't",  'should',  "should've",  'now',  'd',  'll',  'm',  'o',  're',  've',  'y',  'ain',  'aren',  "aren't",  'couldn',  "couldn't",  'didn',  "didn't",  'doesn',  "doesn't",  'hadn',  "hadn't",  'hasn',  "hasn't",  'haven',  "haven't",  'isn',  "isn't",  'ma',  'mightn',  "mightn't",  'mustn',  "mustn't",  'needn',  "needn't",  'shan',  "shan't",  'shouldn',  "shouldn't",  'wasn',  "wasn't",  'weren',  "weren't",  'won',  "won't",  'wouldn',  "wouldn't"]
    nltk_stopwords = stopwords.words('english')
    STOPWORDS = nltk_stopwords+["'s", "s"]+[x.capitalize() for x in nltk_stopwords]
    tokens = word_tokenize(s)
    okstr = ' '.join([x for x in tokens if x in ['(',')'] or x not in STOPWORDS and len(x)>1])
    return okstr

def remove_punct(s):
    s = re.sub(r'[^A-Za-z\-\s]+', '', s)
    return s



def filter_pos_tags(s):
    OUT = ['VBD', 'IN', 'VBP', 'RB', 'MD', 'VB', 'VBZ', 'VBG']
    return ' '.join([w for w,t in pos_tag(word_tokenize(s)) if t not in OUT])


def tokenize(s):
    return word_tokenize(s)


def abbr_expander(s):   
    res = re.findall(r'\(\s?[A-Z]\s?[A-Za-z]+\s?\)', s) # Misses acronyms with hyphen
           
    outs = s
    for abbr in res:
        ix = s.index(abbr)
        abbr_clean = re.sub(r'[^A-Za-z]', '', abbr) # keep only letters, discard all other chars
        abbr_len = len(re.sub(r'[a-z]','',abbr_clean)) # Number of capital letters ~ number of tokens to look back for expansion
        if abbr_clean[-1]=='s': abbr_len-=1 # Singularing a plural acronym
        # When looking for expansion candidates, consider (-) and (/) as word-breaks
        candidate_l = split_words_aggro(s[ix-1::-1][::-1].rstrip()).split(' ')[-abbr_len:] # identify possible candidates for expansion from preivous abbr_len words
        expansion_l = [] 
        
        aix=0
        wix=0
        print(abbr_clean, candidate_l)
        while aix<len(abbr_clean) and wix<len(candidate_l):
            if abbr_clean[aix].lower()==candidate_l[wix][0].lower():
                expansion_l.append(candidate_l[wix])
                aix+=1
                wix+=1
            elif abbr[aix].lower() in candidate_l[max(0,wix-1)].lower():
                aix+=1
            else:
                wix+=1

        if len(expansion_l)>0.4*abbr_len:
            expanded = ' '.join(expansion_l)
            outs = outs.replace(' '+abbr, '') # Remove first instance of abbreviation before expanding other instances
            if abbr_clean[-1]=='s':
                outs = outs.replace(abbr_clean[:-1], expanded)
            else:
                outs = outs.replace(abbr_clean, expanded) 
            print(abbr,expanded)

    return outs




def sim(v1, v2):
    return 1-cosine(v1,v2)



def idf(doclist):
    N = len(doclist)
    vocab = list(FreqDist(word_tokenize(' '.join(doclist))).keys())
    idfd={}
    for w in vocab:
        idfd[w] = math.log(N/max(1,sum([1 for d in doclist if w in d])), 10)
    return idfd

def tfidfDist(doclist):
    idf_lookup = idf(doclist)
    docs_tfidf = []
    for doc in doclist:
        token_fd = list(FreqDist(word_tokenize(doc)).items())
        tfidf = [(w, freq*idf_lookup[w]) for w,freq in token_fd]
        docs_tfidf.append(sorted(tfidf, key=lambda x:x[1], reverse=True))
    return docs_tfidf


def filter_bottom_tfidf(doclist, cutoff_pcile=0.2):
    tfidf_vals = tfidfDist(doclist)
    filt_docs=[]

    for c,doc in enumerate(doclist):
        tf = tfidf_vals[c]
        allowed = [w for w,n in tf[:math.ceil((1-cutoff_pcile)*len(tf))]]
        filt_docs.append(' '.join([w for w in word_tokenize(doc) if w in allowed]))

    return filt_docs
70/15:
def c(abst):
    abst = split_words(abst)
    abst = abbr_expander(abst)
    
    # Make post-period and first word uppercase chars lower
    abst = re.sub(r"(?<=\. )[A-Z]",lambda t:t.group().lower(), abst)
    abst = abst[0].lower()+abst[1:]
    abst = remove_punct(abst)
    
#     abst = filter_pos_tags(abst)
#     abst=remove_stopwords(abst)

    return abst


c("sdfds SDFSDF Sdcxcsqw")
71/1:
import pandas as pd
from collections import defaultdict
import math
import re
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
import numpy as np
from str_helper import *
import pickle
71/2:
window_size = 10


with open('psychThesaurusDict.pkl','rb') as f:
        thes = pickle.load(f)      

def get_studded_defn(w, dfn):
    
    w=clean_sent(w.lower())
    stud=[w]
    dfn=clean_sent(dfn)
    tok = word_tokenize(dfn)
    for i in range(0, len(tok), window_size):
        stud += tok[i:i+window_size]+[w]
    return ' '.join(stud)

def dict_iter():
    for w,d in thes.items():
        dfns = d.values()
        for dd in dfns:
            if len(w)<2:
                continue
            yield get_studded_defn(w,dd)

gen=dict_iter()
with open('psychlopedia_train_verbose.txt','w') as f:
    for l in gen:
        f.write(l+'\n')
71/3:
def split_words(abst):
    return re.sub(r'([a-zA-Z])[^a-zA-Z]+([a-zA-Z])', r'\1 \2', abst)

split_words('MGI.|Attention')
74/1:
"""
{left_x, bottom_y, width, height}
"""

def main(r1, r2):
    
    def outside_rect(x,y,r):
        if x<r['left_x'] or x>(r['left_x']+r['width']) or y<r['bottom_y'] or y>(r['bottom_y']+r['height']):
            return True

    def turt(x,y):
        if outside_rect(x,y,r1):
            return 0
        if outside_rect(x,y,r2):
            flag = 0
        else:
            flag = 1

        return turt(x,y+1)+turt(x+1,y)+area

    # check for no intersection - if all vertices are outside
    r2_pts = [(r2['left_x'], r2['bottom_y']), \
                (r2['left_x']+r2['width'], r2['bottom_y']) \
                (r2['left_x'], r2['bottom_y']+r2['length']) \
                (r2['left_x']+r2['width'], r2['bottom_y']+r2['length'])]
    out_flag = True
    for x,y in r2_pts:
        out_flag = out_flag and outside_rect(x,y,r1)

    if out_flag:
        return 0
    else:
        return turt(r1['left_x'], r1['bottom_y'])
74/2:
"""
{left_x, bottom_y, width, height}
"""

def main(r1, r2):
    
    def outside_rect(x,y,r):
        if x<r['left_x'] or x>(r['left_x']+r['width']) or y<r['bottom_y'] or y>(r['bottom_y']+r['height']):
            return True

    def turt(x,y):
        if outside_rect(x,y,r1):
            return 0
        if outside_rect(x,y,r2):
            flag = 0
        else:
            flag = 1

        return turt(x,y+1)+turt(x+1,y)+area

    # check for no intersection - if all vertices are outside
    r2_pts = [(r2['left_x'], r2['bottom_y']), \
                (r2['left_x']+r2['width'], r2['bottom_y']) \
                (r2['left_x'], r2['bottom_y']+r2['height']) \
                (r2['left_x']+r2['width'], r2['bottom_y']+r2['height'])]
    out_flag = True
    for x,y in r2_pts:
        out_flag = out_flag and outside_rect(x,y,r1)

    if out_flag:
        return 0
    else:
        return turt(r1['left_x'], r1['bottom_y'])
74/3:
r1={'left_x':1, 'bottom_y':1, 'width':4, 'height':4}
r2={'left_x':3, 'bottom_y':2, 'width':4, 'height':2}
main(r1,r2)
74/4:
"""
{left_x, bottom_y, width, height}
"""

def main(r1, r2):
    
    def outside_rect(x,y,r):
        if x<r['left_x'] or x>(r['left_x']+r['width']) or y<r['bottom_y'] or y>(r['bottom_y']+r['height']):
            return True

    def turt(x,y):
        if outside_rect(x,y,r1):
            return 0
        if outside_rect(x,y,r2):
            flag = 0
        else:
            flag = 1

        return turt(x,y+1)+turt(x+1,y)+area

    # check for no intersection - if all vertices are outside
    r2_pts = [(r2['left_x'], r2['bottom_y']), \
                (r2['left_x']+r2['width'], r2['bottom_y']), \
                (r2['left_x'], r2['bottom_y']+r2['height']), \
                (r2['left_x']+r2['width'], r2['bottom_y']+r2['height'])]
    out_flag = True
    for x,y in r2_pts:
        out_flag = out_flag and outside_rect(x,y,r1)

    if out_flag:
        return 0
    else:
        return turt(r1['left_x'], r1['bottom_y'])
74/5:
r1={'left_x':1, 'bottom_y':1, 'width':4, 'height':4}
r2={'left_x':3, 'bottom_y':2, 'width':4, 'height':2}
main(r1,r2)
74/6:
"""
{left_x, bottom_y, width, height}
"""

def main(r1, r2):
    
    def outside_rect(x,y,r):
        if x<r['left_x'] or x>(r['left_x']+r['width']) or y<r['bottom_y'] or y>(r['bottom_y']+r['height']):
            return True

    def turt(x,y):
        if outside_rect(x,y,r1):
            return 0
        flag=1
        if outside_rect(x,y,r2):
            flag = 0
        return turt(x,y+1)+turt(x+1,y)+flag

    # check for no intersection - if all vertices are outside
    r2_pts = [(r2['left_x'], r2['bottom_y']), \
                (r2['left_x']+r2['width'], r2['bottom_y']), \
                (r2['left_x'], r2['bottom_y']+r2['height']), \
                (r2['left_x']+r2['width'], r2['bottom_y']+r2['height'])]
    out_flag = True
    for x,y in r2_pts:
        out_flag = out_flag and outside_rect(x,y,r1)

    if out_flag:
        return 0
    else:
        return turt(r1['left_x'], r1['bottom_y'])
74/7:
"""
{left_x, bottom_y, width, height}
"""

def main(r1, r2):
    
    def outside_rect(x,y,r):
        if x<r['left_x'] or x>(r['left_x']+r['width']) or y<r['bottom_y'] or y>(r['bottom_y']+r['height']):
            return True

    def turt(x,y):
        if outside_rect(x,y,r1):
            return 0
        flag=1
        if outside_rect(x,y,r2):
            flag = 0
        return turt(x,y+1)+turt(x+1,y)+flag

    # check for no intersection - if all vertices are outside
    r2_pts = [(r2['left_x'], r2['bottom_y']), \
                (r2['left_x']+r2['width'], r2['bottom_y']), \
                (r2['left_x'], r2['bottom_y']+r2['height']), \
                (r2['left_x']+r2['width'], r2['bottom_y']+r2['height'])]
    out_flag = True
    for x,y in r2_pts:
        out_flag = out_flag and outside_rect(x,y,r1)

    if out_flag:
        return 0
    else:
        return turt(r1['left_x'], r1['bottom_y'])
74/8:
"""
{left_x, bottom_y, width, height}
"""

def main(r1, r2):
    
    def outside_rect(x,y,r):
        if x<r['left_x'] or x>(r['left_x']+r['width']) or y<r['bottom_y'] or y>(r['bottom_y']+r['height']):
            return True

    def turt(x,y):
        if outside_rect(x,y,r1):
            return 0
        flag = 1
        if outside_rect(x,y,r2):
            flag = 0
        return turt(x,y+1)+turt(x+1,y)+area

    # check for no intersection - if all vertices are outside
    r2_pts = [(r2['left_x'], r2['bottom_y']), \
                (r2['left_x']+r2['width'], r2['bottom_y']) \
                (r2['left_x'], r2['bottom_y']+r2['length']) \
                (r2['left_x']+r2['width'], r2['bottom_y']+r2['length'])]
    out_flag = True
    for x,y in r2_pts:
        out_flag = out_flag and outside_rect(x,y,r1)

    if out_flag:
        return 0
    else:
        return turt(r1['left_x'], r1['bottom_y'])
74/9:
r1={'left_x':1, 'bottom_y':1, 'width':4, 'height':4}
r2={'left_x':3, 'bottom_y':2, 'width':4, 'height':2}
main(r1,r2)
74/10:
"""
{left_x, bottom_y, width, height}
"""

def main(r1, r2):
    
    def outside_rect(x,y,r):
        if x<r['left_x'] or x>(r['left_x']+r['width']) or y<r['bottom_y'] or y>(r['bottom_y']+r['height']):
            return True

    def turt(x,y):
        if outside_rect(x,y,r1):
            return 0
        flag = 1
        if outside_rect(x,y,r2):
            flag = 0
        return turt(x,y+1)+turt(x+1,y)+area

    # check for no intersection - if all vertices are outside
    r2_pts = [(r2['left_x'], r2['bottom_y']), \
                (r2['left_x']+r2['width'], r2['bottom_y']) \
                (r2['left_x'], r2['bottom_y']+r2['height']) \
                (r2['left_x']+r2['width'], r2['bottom_y']+r2['height'])]
    out_flag = True
    for x,y in r2_pts:
        out_flag = out_flag and outside_rect(x,y,r1)

    if out_flag:
        return 0
    else:
        return turt(r1['left_x'], r1['bottom_y'])
74/11:
r1={'left_x':1, 'bottom_y':1, 'width':4, 'height':4}
r2={'left_x':3, 'bottom_y':2, 'width':4, 'height':2}
main(r1,r2)
74/12:
"""
{left_x, bottom_y, width, height}
"""

def main(r1, r2):
    
    def outside_rect(x,y,r):
        if x<r['left_x'] or x>(r['left_x']+r['width']) or y<r['bottom_y'] or y>(r['bottom_y']+r['height']):
            return True

    def turt(x,y):
        if outside_rect(x,y,r1):
            return 0
        flag = 1
        if outside_rect(x,y,r2):
            flag = 0
        return turt(x,y+1)+turt(x+1,y)+area

    # check for no intersection - if all vertices are outside
    r2_pts = [(r2['left_x'], r2['bottom_y']), \
                (r2['left_x']+r2['width'], r2['bottom_y']), \
                (r2['left_x'], r2['bottom_y']+r2['height']), \
                (r2['left_x']+r2['width'], r2['bottom_y']+r2['height'])]
    out_flag = True
    for x,y in r2_pts:
        out_flag = out_flag and outside_rect(x,y,r1)

    if out_flag:
        return 0
    else:
        return turt(r1['left_x'], r1['bottom_y'])
74/13:
r1={'left_x':1, 'bottom_y':1, 'width':4, 'height':4}
r2={'left_x':3, 'bottom_y':2, 'width':4, 'height':2}
main(r1,r2)
74/14:
"""
{left_x, bottom_y, width, height}
"""

def main(r1, r2):
    
    def outside_rect(x,y,r):
        if x<r['left_x'] or x>(r['left_x']+r['width']) or y<r['bottom_y'] or y>(r['bottom_y']+r['height']):
            return True

    def turt(x,y):
        if outside_rect(x,y,r1):
            return 0
        flag = 1
        if outside_rect(x,y,r2):
            flag = 0
        return turt(x,y+1)+turt(x+1,y)+flag

    # check for no intersection - if all vertices are outside
    r2_pts = [(r2['left_x'], r2['bottom_y']), \
                (r2['left_x']+r2['width'], r2['bottom_y']), \
                (r2['left_x'], r2['bottom_y']+r2['height']), \
                (r2['left_x']+r2['width'], r2['bottom_y']+r2['height'])]
    out_flag = True
    for x,y in r2_pts:
        out_flag = out_flag and outside_rect(x,y,r1)

    if out_flag:
        return 0
    else:
        return turt(r1['left_x'], r1['bottom_y'])
74/15:
r1={'left_x':1, 'bottom_y':1, 'width':4, 'height':4}
r2={'left_x':3, 'bottom_y':2, 'width':4, 'height':2}
main(r1,r2)
74/16:
"""
{left_x, bottom_y, width, height}
"""

def main(r1, r2):
    
    def outside_rect(x,y,r):
        if x<r['left_x'] or x>(r['left_x']+r['width']) or y<r['bottom_y'] or y>(r['bottom_y']+r['height']):
            return True

    def turt(x,y):
        print(x,y)
        if outside_rect(x,y,r1):
            return 0
        flag = 1
        if outside_rect(x,y,r2):
            flag = 0
        return turt(x,y+1)+turt(x+1,y)+flag

    # check for no intersection - if all vertices are outside
    r2_pts = [(r2['left_x'], r2['bottom_y']), \
                (r2['left_x']+r2['width'], r2['bottom_y']), \
                (r2['left_x'], r2['bottom_y']+r2['height']), \
                (r2['left_x']+r2['width'], r2['bottom_y']+r2['height'])]
    out_flag = True
    for x,y in r2_pts:
        out_flag = out_flag and outside_rect(x,y,r1)

    if out_flag:
        return 0
    else:
        return turt(r1['left_x'], r1['bottom_y'])
74/17:
r1={'left_x':1, 'bottom_y':1, 'width':4, 'height':4}
r2={'left_x':3, 'bottom_y':2, 'width':4, 'height':2}
main(r1,r2)
74/18:
"""
{left_x, bottom_y, width, height}
"""

def main(r1, r2):
    
    def outside_rect(x,y,r):
        if x<r['left_x'] or x>(r['left_x']+r['width']) or y<r['bottom_y'] or y>(r['bottom_y']+r['height']):
            return True

    def turt(x,y):
        print(x,y)
        if outside_rect(x,y,r1):
            return 0
        flag = 0
        if not outside_rect(x,y,r2):
            flag = 1
        return turt(x,y+1)+turt(x+1,y)+flag

    # check for no intersection - if all vertices are outside
    r2_pts = [(r2['left_x'], r2['bottom_y']), \
                (r2['left_x']+r2['width'], r2['bottom_y']), \
                (r2['left_x'], r2['bottom_y']+r2['height']), \
                (r2['left_x']+r2['width'], r2['bottom_y']+r2['height'])]
    out_flag = True
    for x,y in r2_pts:
        out_flag = out_flag and outside_rect(x,y,r1)

    if out_flag:
        return 0
    else:
        return turt(r1['left_x'], r1['bottom_y'])
74/19:
r1={'left_x':1, 'bottom_y':1, 'width':4, 'height':4}
r2={'left_x':3, 'bottom_y':2, 'width':4, 'height':2}
main(r1,r2)
74/20:
def rect_inter(r1,r2):
    left1 = r1['left_x']
    right1 = left1+r1['width']
    bottom1 = r1['bottom_y']
    up1 = bottom1+r1['height']
    
    left2 = r2['left_x']
    right2 = left2+r2['width']
    bottom2 = r2['bottom_y']
    up2 = bottom2+r2['height']
    
    left3 = max(left1, left2)
    right3 = min(right1, right2)
    up3 = min(up1, up2)
    bottom3 = max(bottom1, bottom2)
    
    if left3<right3 and bottom3<up3:
        return (right3-left3)*(up3-bottom3)
    else:
        return 0
74/21: rect_inter(r1,r2)
74/22:
class Stack:
    def __init__(iter):
        self.stack = list(iter)
    
    def push(x):
        self.stack.append(x)
    
    def pop():
        popped = self.stack.top()      
        self.stack = self.stack[:-1]
        return popped
    
    def top():
        return self.stack[-1]

class MinStack:
    def __init__():
        self.stack = Stack()
        self.mins = Stack([999999])
    
    def push(x):
        if x<self.mins.top():
            self.mins.push(x)
        self.stack.push(x)
    
    def pop():
        popped = self.stack.pop()
        if popped == self.mins.top():
            self.mins.pop()
        return popped
    
    def getMin():
        return self.mins.pop()
    
    
obj = MinStack()
obj.push(-2)
obj.push(0)
obj.push(-3)
assert(obj.getMin()==-3)
obj.pop()
assert(obj.top()==0)
assert(obj.getMin()==-2)
74/23:
class Stack:
    def __init__(self,iter):
        self.stack = list(iter)
    
    def push(x):
        self.stack.append(x)
    
    def pop():
        popped = self.stack.top()      
        self.stack = self.stack[:-1]
        return popped
    
    def top():
        return self.stack[-1]

class MinStack:
    def __init__(self):
        self.stack = Stack()
        self.mins = Stack([999999])
    
    def push(x):
        if x<self.mins.top():
            self.mins.push(x)
        self.stack.push(x)
    
    def pop():
        popped = self.stack.pop()
        if popped == self.mins.top():
            self.mins.pop()
        return popped
    
    def getMin():
        return self.mins.pop()
    
    
obj = MinStack()
obj.push(-2)
obj.push(0)
obj.push(-3)
assert(obj.getMin()==-3)
obj.pop()
assert(obj.top()==0)
assert(obj.getMin()==-2)
74/24:
class Stack:
    def __init__(self,iter):
        self.stack = list(iter)
    
    def push(x):
        self.stack.append(x)
    
    def pop():
        popped = self.stack.top()      
        self.stack = self.stack[:-1]
        return popped
    
    def top():
        return self.stack[-1]

class MinStack:
    def __init__(self):
        self.stack = Stack([])
        self.mins = Stack([999999])
    
    def push(x):
        if x<self.mins.top():
            self.mins.push(x)
        self.stack.push(x)
    
    def pop():
        popped = self.stack.pop()
        if popped == self.mins.top():
            self.mins.pop()
        return popped
    
    def getMin():
        return self.mins.pop()
    
    
obj = MinStack()
obj.push(-2)
obj.push(0)
obj.push(-3)
assert(obj.getMin()==-3)
obj.pop()
assert(obj.top()==0)
assert(obj.getMin()==-2)
74/25:
class Stack:
    def __init__(self,iter):
        self.stack = list(iter)
    
    def push(self, x):
        self.stack.append(x)
    
    def pop(self):
        popped = self.stack.top()      
        self.stack = self.stack[:-1]
        return popped
    
    def top(self):
        return self.stack[-1]

class MinStack:
    def __init__(self):
        self.stack = Stack([])
        self.mins = Stack([999999])
    
    def push(self, x):
        if x<self.mins.top():
            self.mins.push(x)
        self.stack.push(x)
    
    def pop(self):
        popped = self.stack.pop()
        if popped == self.mins.top():
            self.mins.pop()
        return popped
    
    def getMin(self):
        return self.mins.pop()
    
    
obj = MinStack()
obj.push(-2)
obj.push(0)
obj.push(-3)
assert(obj.getMin()==-3)
obj.pop()
assert(obj.top()==0)
assert(obj.getMin()==-2)
74/26:
class Stack:
    def __init__(self,iter):
        self.stack = list(iter)
    
    def push(self, x):
        self.stack.append(x)
    
    def pop(self):
        popped = self.stack[-1]     
        self.stack = self.stack[:-1]
        return popped
    
    def top(self):
        return self.stack[-1]

class MinStack:
    def __init__(self):
        self.stack = Stack([])
        self.mins = Stack([999999])
    
    def push(self, x):
        if x<self.mins.top():
            self.mins.push(x)
        self.stack.push(x)
    
    def pop(self):
        popped = self.stack.pop()
        if popped == self.mins.top():
            self.mins.pop()
        return popped
    
    def getMin(self):
        return self.mins.pop()
    
    
obj = MinStack()
obj.push(-2)
obj.push(0)
obj.push(-3)
assert(obj.getMin()==-3)
obj.pop()
assert(obj.top()==0)
assert(obj.getMin()==-2)
74/27:
class Stack:
    def __init__(self,iter):
        self.stack = list(iter)
    
    def push(self, x):
        self.stack.append(x)
    
    def pop(self):
        popped = self.stack[-1]     
        self.stack = self.stack[:-1]
        return popped
    
    def top(self):
        return self.stack[-1]

class MinStack(Stack):
    def __init__(self):
        self.stack = Stack([])
        self.mins = Stack([999999])
    
    def push(self, x):
        if x<self.mins.top():
            self.mins.push(x)
        self.stack.push(x)
    
    def pop(self):
        popped = self.stack.pop()
        if popped == self.mins.top():
            self.mins.pop()
        return popped
    
    def getMin(self):
        return self.mins.pop()
    
    
obj = MinStack()
obj.push(-2)
obj.push(0)
obj.push(-3)
assert(obj.getMin()==-3)
obj.pop()
assert(obj.top()==0)
assert(obj.getMin()==-2)
74/28:
class Stack:
    def __init__(self,iter):
        self.stack = list(iter)
    
    def push(self, x):
        self.stack.append(x)
    
    def pop(self):
        popped = self.stack[-1]     
        self.stack = self.stack[:-1]
        return popped
    
    def top(self):
        return self.stack[-1]

class MinStack():
    def __init__(self):
        self.stack = Stack([])
        self.mins = Stack([999999])
    
    def push(self, x):
        if x<self.mins.top():
            self.mins.push(x)
        self.stack.push(x)
    
    def pop(self):
        popped = self.stack.pop()
        if popped == self.mins.top():
            self.mins.pop()
        return popped
    
    def getMin(self):
        return self.mins.pop()
    
    def top(self):
        return self.stack.top()
    
    
obj = MinStack()
obj.push(-2)
obj.push(0)
obj.push(-3)
assert(obj.getMin()==-3)
obj.pop()
assert(obj.top()==0)
assert(obj.getMin()==-2)
74/29:
class Stack:
    def __init__(self,iter):
        self.stack = list(iter)
    
    def push(self, x):
        self.stack.append(x)
    
    def pop(self):
        popped = self.stack[-1]     
        self.stack = self.stack[:-1]
        return popped
    
    def top(self):
        return self.stack[-1]

class MinStack():
    def __init__(self):
        self.stack = Stack([])
        self.mins = Stack([999999])
    
    def push(self, x):
        if x<self.mins.top():
            self.mins.push(x)
        self.stack.push(x)
    
    def pop(self):
        popped = self.stack.pop()
        if popped == self.mins.top():
            self.mins.pop()
        return popped
    
    def getMin(self):
        return self.mins.pop()
    
    def top(self):
        return self.stack.top()
    
    
obj = MinStack()
obj.push(-2)
obj.push(0)
obj.push(-1)
assert(obj.getMin()==-2)
assert(obj.top()==-1)
print(obj.pop())
assert(obj.getMin()==-2)
    
["MinStack","push","push","push","getMin","top","pop","getMin"]
[[],[-2],[0],[-1],[],[],[],[]]
74/30:
class Stack:
    def __init__(self,iter):
        self.stack = list(iter)
    
    def push(self, x):
        self.stack.append(x)
    
    def pop(self):
        popped = self.stack[-1]     
        self.stack = self.stack[:-1]
        return popped
    
    def top(self):
        return self.stack[-1]

class MinStack():
    def __init__(self):
        self.stack = Stack([])
        self.mins = Stack([999999])
    
    def push(self, x):
        if x<self.mins.top():
            self.mins.push(x)
        self.stack.push(x)
    
    def pop(self):
        popped = self.stack.pop()
        if popped == self.mins.top():
            print("popping min")
            self.mins.pop()
        return popped
    
    def getMin(self):
        return self.mins.pop()
    
    def top(self):
        return self.stack.top()
    
    
obj = MinStack()
obj.push(-2)
obj.push(0)
obj.push(-1)
assert(obj.getMin()==-2)
assert(obj.top()==-1)
print(obj.pop())
assert(obj.getMin()==-2)
    
["MinStack","push","push","push","getMin","top","pop","getMin"]
[[],[-2],[0],[-1],[],[],[],[]]
74/31:
class Stack:
    def __init__(self,iter):
        self.stack = list(iter)
    
    def push(self, x):
        self.stack.append(x)
    
    def pop(self):
        popped = self.stack[-1]     
        self.stack = self.stack[:-1]
        return popped
    
    def top(self):
        return self.stack[-1]

class MinStack():
    def __init__(self):
        self.stack = Stack([])
        self.mins = Stack([999999])
    
    def push(self, x):
        if x<self.mins.top():
            self.mins.push(x)
        self.stack.push(x)
    
    def pop(self):
        popped = self.stack.pop()
        if popped == self.mins.top():
            print("popping min")
            self.mins.pop()
        return popped
    
    def getMin(self):
        return self.mins.pop()
    
    def top(self):
        return self.stack.top()
    
    
obj = MinStack()
obj.push(-2)
obj.push(0)
obj.push(-1)
assert(obj.getMin()==-2)
assert(obj.top()==-1)
print(obj.pop())
print(obje.getMin())
assert(obj.getMin()==-2)
    
["MinStack","push","push","push","getMin","top","pop","getMin"]
[[],[-2],[0],[-1],[],[],[],[]]
74/32:
class Stack:
    def __init__(self,iter):
        self.stack = list(iter)
    
    def push(self, x):
        self.stack.append(x)
    
    def pop(self):
        popped = self.stack[-1]     
        self.stack = self.stack[:-1]
        return popped
    
    def top(self):
        return self.stack[-1]

class MinStack():
    def __init__(self):
        self.stack = Stack([])
        self.mins = Stack([999999])
    
    def push(self, x):
        if x<self.mins.top():
            self.mins.push(x)
        self.stack.push(x)
    
    def pop(self):
        popped = self.stack.pop()
        if popped == self.mins.top():
            print("popping min")
            self.mins.pop()
        return popped
    
    def getMin(self):
        return self.mins.pop()
    
    def top(self):
        return self.stack.top()
    
    
obj = MinStack()
obj.push(-2)
obj.push(0)
obj.push(-1)
assert(obj.getMin()==-2)
assert(obj.top()==-1)
print(obj.pop())
print(obj.getMin())
assert(obj.getMin()==-2)
    
["MinStack","push","push","push","getMin","top","pop","getMin"]
[[],[-2],[0],[-1],[],[],[],[]]
74/33:
class Stack:
    def __init__(self,iter):
        self.stack = list(iter)
    
    def push(self, x):
        self.stack.append(x)
    
    def pop(self):
        popped = self.stack[-1]     
        self.stack = self.stack[:-1]
        return popped
    
    def top(self):
        return self.stack[-1]

class MinStack():
    def __init__(self):
        self.stack = Stack([])
        self.mins = Stack([999999])
    
    def push(self, x):
        if x<self.mins.top():
            self.mins.push(x)
        self.stack.push(x)
    
    def pop(self):
        popped = self.stack.pop()
        if popped == self.mins.top():
            print("popping min")
            self.mins.pop()
        return popped
    
    def getMin(self):
        return self.mins.top()
    
    def top(self):
        return self.stack.top()
    
    
obj = MinStack()
obj.push(-2)
obj.push(0)
obj.push(-1)
assert(obj.getMin()==-2)
assert(obj.top()==-1)
print(obj.pop())
print(obj.getMin())
assert(obj.getMin()==-2)
    
["MinStack","push","push","push","getMin","top","pop","getMin"]
[[],[-2],[0],[-1],[],[],[],[]]
74/34:
l=[]
l[-1]
74/35:
import sys
print(sys.maxsize)
76/1:
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
from torch.autograd import Variable
from torch.nn import Parameter
from torch import Tensor
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
import pandas as pd
import math
import numpy as np
import time

cuda = True if torch.cuda.is_available() else False
    
Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor    

torch.manual_seed(125)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(125)
    
if cuda:
    device = torch.device('cuda')
else:
    device = torch.device('cpu')
76/2:
#Reading the csv
final_df = pd.read_csv('longitudinal_master.csv').drop(columns = ['Unnamed: 0'])
76/3:
#Creating binary values of charges (high or low)
threshold = final_df['AMOUNT'].quantile(0.7)
final_df['AMOUNT'] = final_df.apply(lambda row : 1 if row['AMOUNT'] > threshold else 0, axis=1)
76/4:
#Getting the data ready in window size
window = 8
X = []
Y = []
input_size = len(final_df.drop(columns=['AUTO_ID', 'window', 'AMOUNT', 'CUM_AMOUNT']).columns)
for i in final_df.AUTO_ID.unique():
    data_int = final_df[final_df.AUTO_ID == i]
    if i%500 == 0:
        print(i)
    for j in range(data_int.index[0], data_int.index[-1] - window+1):
        X.append(data_int.loc[j:j+window-1, :].drop(columns=['AUTO_ID', 'window', 'AMOUNT', 'CUM_AMOUNT']).values.tolist())
        Y.append(data_int.loc[j+window]['AMOUNT'])
# import pickle
# X = pickle.load(open('long_X.pkl', 'rb'))
# Y = pickle.load(open('long_Y.pkl', 'rb'))
76/5:
#Getting the data ready in window size
window = 8
X = []
Y = []
input_size = len(final_df.drop(columns=['AUTO_ID', 'window', 'AMOUNT', 'CUM_AMOUNT']).columns)
for i in final_df.AUTO_ID.unique():
    data_int = final_df[final_df.AUTO_ID == i]
    if i%500 == 0:
        print(i)
    for j in range(data_int.index[0], data_int.index[-1] - window+1):
        X.append(data_int.loc[j:j+window-1, :].drop(columns=['AUTO_ID', 'window', 'AMOUNT', 'CUM_AMOUNT']).values.tolist())
        Y.append(data_int.loc[j+window]['AMOUNT'])
    break
# import pickle
# X = pickle.load(open('long_X.pkl', 'rb'))
# Y = pickle.load(open('long_Y.pkl', 'rb'))
76/6: X
76/7: X[0]
76/8: len(X[0])
76/9: len(X[1])
76/10: len(X)
76/11: len(X[1])
76/12:
#Getting the data ready in window size
window = 8
X = []
Y = []
input_size = len(final_df.drop(columns=['AUTO_ID', 'window', 'AMOUNT', 'CUM_AMOUNT']).columns)
for i, data_int in enumerate(final_df.groupby('AUTO_ID')):
#     data_int = final_df[final_df.AUTO_ID == i]
    if i%500 == 0:
        print(i)
    for j in range(data_int.index[0], data_int.index[-1] - window+1):
        X.append(data_int.loc[j:j+window-1, :].drop(columns=['AUTO_ID', 'window', 'AMOUNT', 'CUM_AMOUNT']).values.tolist())
        Y.append(data_int.loc[j+window]['AMOUNT'])
# import pickle
# X = pickle.load(open('long_X.pkl', 'rb'))
# Y = pickle.load(open('long_Y.pkl', 'rb'))
76/13:
#Getting the data ready in window size
window = 8
X = []
Y = []
input_size = len(final_df.drop(columns=['AUTO_ID', 'window', 'AMOUNT', 'CUM_AMOUNT']).columns)

counter=0
for data_int in final_df.groupby('AUTO_ID'):
    if counter%500 == 0:
        print(i)
    for j in range(data_int.index[0], data_int.index[-1] - window+1):
        X.append(data_int.loc[j:j+window-1, :].drop(columns=['AUTO_ID', 'window', 'AMOUNT', 'CUM_AMOUNT']).values.tolist())
        Y.append(data_int.loc[j+window]['AMOUNT'])
# import pickle
# X = pickle.load(open('long_X.pkl', 'rb'))
# Y = pickle.load(open('long_Y.pkl', 'rb'))
76/14:
#Getting the data ready in window size
window = 8
X = []
Y = []
input_size = len(final_df.drop(columns=['AUTO_ID', 'window', 'AMOUNT', 'CUM_AMOUNT']).columns)

counter=0
for data_int in final_df.groupby('AUTO_ID'):
    print(data_int)
    if counter%500 == 0:
        print(i)
    for j in range(data_int.index[0], data_int.index[-1] - window+1):
        X.append(data_int.loc[j:j+window-1, :].drop(columns=['AUTO_ID', 'window', 'AMOUNT', 'CUM_AMOUNT']).values.tolist())
        Y.append(data_int.loc[j+window]['AMOUNT'])
# import pickle
# X = pickle.load(open('long_X.pkl', 'rb'))
# Y = pickle.load(open('long_Y.pkl', 'rb'))
76/15:
#Getting the data ready in window size
window = 8
X = []
Y = []
input_size = len(final_df.drop(columns=['AUTO_ID', 'window', 'AMOUNT', 'CUM_AMOUNT']).columns)

counter=0
for aid, data_int in final_df.groupby('AUTO_ID'):
    if counter%500 == 0:
        print(i)
    for j in range(data_int.index[0], data_int.index[-1] - window+1):
        X.append(data_int.loc[j:j+window-1, :].drop(columns=['AUTO_ID', 'window', 'AMOUNT', 'CUM_AMOUNT']).values.tolist())
        Y.append(data_int.loc[j+window]['AMOUNT'])
# import pickle
# X = pickle.load(open('long_X.pkl', 'rb'))
# Y = pickle.load(open('long_Y.pkl', 'rb'))
76/16:
#Getting the data ready in window size
window = 8
X = []
Y = []
input_size = len(final_df.drop(columns=['AUTO_ID', 'window', 'AMOUNT', 'CUM_AMOUNT']).columns)

counter=0
for aid, data_int in final_df.groupby('AUTO_ID'):
    if counter%500 == 0:
        print(i)
    for j in range(data_int.index[0], data_int.index[-1] - window+1):
        X.append(data_int.loc[j:j+window-1, :].drop(columns=['AUTO_ID', 'window', 'AMOUNT', 'CUM_AMOUNT']).values.tolist())
        Y.append(data_int.loc[j+window]['AMOUNT'])
    counter+=1
# import pickle
# X = pickle.load(open('long_X.pkl', 'rb'))
# Y = pickle.load(open('long_Y.pkl', 'rb'))
76/17:
import pickle
with open('longX.nparr.pkl', 'wb') as f:
    pickle.dump(np.array(X), f)

with open('longY.nparr.pkl', 'wb') as f:
    pickle.dump(np.array(Y), f)
    
# X = pickle.load(open('long_X.pkl', 'rb'))
# Y = pickle.load(open('long_Y.pkl', 'rb'))
76/18:
#Create an array of the data
X_arr = np.array(X)
Y_arr = np.array(Y)

#Reshaping the array
X_arr = X_arr.reshape(-1, window, input_size)
Y_arr = Y_arr.reshape(-1, 1)
76/19: len(X_arr)
76/20: len(X)
76/21: len(Y)
76/22:
import pickle

# with open('longX.nparr.pkl', 'wb') as f:
#     pickle.dump(np.array(X), f)
# with open('longY.nparr.pkl', 'wb') as f:
#     pickle.dump(np.array(Y), f)
    
X_arr = pickle.load(open('longX.nparr.pkl', 'rb'))
Y_arr = pickle.load(open('longY.nparr.pkl', 'rb'))
76/23:
#Reshaping the array
X_arr = X_arr.reshape(-1, window, input_size)
Y_arr = Y_arr.reshape(-1, 1)
77/1:
import torchvision
import torchvision.transforms as transforms
77/2:
BATCH_SIZE = 64

# list all transformations
transform = transforms.Compose(
    [transforms.ToTensor()])

# download and load training dataset
trainset = torchvision.datasets.MNIST(root='./data', train=True,
                                        download=True, transform=transform)
77/3: trainset
77/4:
trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,
                                          shuffle=True, num_workers=2)
77/5: import torch
77/6:
trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,
                                          shuffle=True, num_workers=2)
77/7: trainloader
78/1:
## Code taken from https://blog.floydhub.com/gru-with-pytorch/

import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
from torch.autograd import Variable
from torch.nn import Parameter
from torch import Tensor
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
import pandas as pd
import math
import numpy as np
import time

cuda = True if torch.cuda.is_available() else False
    
Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor    

torch.manual_seed(125)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(125)
    
if cuda:
    device = torch.device('cuda')
else:
    device = torch.device('cpu')
77/8: len(trainloader)
77/9: trainloader[0]
77/10: next(trainloader)
77/11:
for x,i in trainloader:
    print(x, i)
    break
77/12:
for x,y in trainloader:
    print(x.size(), y.size())
    break
79/1:
import numpy as np
x = np.sin(0.01 * time) + 0.2 * np.random.normal(size=T)
79/2:
import numpy as np
time = np.arange(0, 1000)
x = np.sin(0.01 * time) + 0.2 * np.random.normal(size=T)
79/3:
import numpy as np
T=1000
time = np.arange(0, T)
x = np.sin(0.01 * time) + 0.2 * np.random.normal(size=T)
79/4: x
79/5:
tau=4
features = np.zeros((T-tau, tau))
for i in range(tau):
    features[:, i] = x[i: T-tau+i]
# labels = x[tau:]
79/6: features
79/7:
tau=4
features = np.zeros((T-tau, tau))
for i in range(tau):
    features[:, i] = x[i: T-tau+i]
labels = x[tau:]
79/8: features.shape
79/9: labels.shape
79/10: labels
79/11: features
80/1:
import torch 

t = torch.zeros(1, n_letters)
80/2:
import torch 

t = torch.zeros(1, 5)
80/3: t
80/4: t[0]
80/5: t[0][0]
80/6: t[0]
80/7: t[1]
80/8: t
81/1: DATA_PATH = '.\\datasets\\registry_data'
81/2:
psych_med_prescrip = pd.read_csv(os.path.join(DATA_PATH, 'deid_IBD_Registry_BA1951_Medications_2018-07-05-09-47-15.csv'), parse_dates=['ORDERING_DATE'])
psych_med_prescrip.drop(['Unnamed: 0', 'SIMPLE_GENERIC_NAME', 'PHARM_CLASS_C', 'PHARM_CLASS', 'PHARM_SUBCLASS_C', 'PHARM_SUB_CLASS'], axis=1, inplace=True)
81/3:
import os
import pandas as pd
import numpy as np
81/4:
psych_med_prescrip = pd.read_csv(os.path.join(DATA_PATH, 'deid_IBD_Registry_BA1951_Medications_2018-07-05-09-47-15.csv'), parse_dates=['ORDERING_DATE'])
psych_med_prescrip.drop(['Unnamed: 0', 'SIMPLE_GENERIC_NAME', 'PHARM_CLASS_C', 'PHARM_CLASS', 'PHARM_SUBCLASS_C', 'PHARM_SUB_CLASS'], axis=1, inplace=True)
81/5:
psych_med_prescrip = pd.read_csv(os.path.join(DATA_PATH, 'deid_IBD_Registry_BA1951_Medications_2018-07-05-09-47-15.csv'), parse_dates=['ORDERING_DATE'])
psych_med_prescrip.drop(['Unnamed: 0', 'SIMPLE_GENERIC_NAME', 'PHARM_CLASS_C', 'PHARM_CLASS', 'PHARM_SUBCLASS_C', 'PHARM_SUB_CLASS'], axis=1, inplace=True)
psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.THERAPEUTIC_CLASS=='PSYCHOTHERAPEUTIC DRUGS']
81/6: psych_med_prescrip.head(10)
81/7: psych_med_prescrip.sample(5)
81/8: psych_med_prescrip.END_DATE.isnull().sum()
81/9: psych_med_prescrip.sample(5)
81/10: psych_med_prescrip.MED_ID.value_counts()
81/11: psych_med_prescrip.MED_ID.value_counts().plot.hist()
81/12:
%matplotlib inline
import os
import pandas as pd
import numpy as np
81/13: psych_med_prescrip.MED_ID.value_counts().plot.hist()
81/14:
med_counts = psych_med_prescrip.MED_ID.value_counts()
allowed_meds = med_counts[med_counts>50]
81/15:
psych_med_prescrip = pd.read_csv(os.path.join(DATA_PATH, 'deid_IBD_Registry_BA1951_Medications_2018-07-05-09-47-15.csv'), parse_dates=['ORDERING_DATE'])
# psych_med_prescrip.drop(['Unnamed: 0', 'SIMPLE_GENERIC_NAME', 'PHARM_CLASS_C', 'PHARM_CLASS', 'PHARM_SUBCLASS_C', 'PHARM_SUB_CLASS'], axis=1, inplace=True)

psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.THERAPEUTIC_CLASS=='PSYCHOTHERAPEUTIC DRUGS']
psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.END_DATE.notnull()] # filter no end_date rows

# check: should we drop uncommon med_ids?
81/16: psych_med_prescrip.sample(5)
81/17: psych_med_prescrip.PHARM_CLASS.value_counts().plot.hist()
81/18: psych_med_prescrip.PHARM_CLASS.value_counts()
81/19: psych_med_prescrip.PHARM_CLASS.value_counts().T.plot.hist()
81/20: psych_med_prescrip.PHARM_CLASS.value_counts().values.plot.hist()
81/21: psych_med_prescrip.PHARM_CLASS.value_counts().sort_index().plot.hist()
81/22: psych_med_prescrip.PHARM_CLASS.value_counts()
81/23: psych_med_prescrip.PHARM_CLASS.plot.hist()
81/24: psych_med_prescrip.PHARM_CLASS.value_counts().plot()
81/25: psych_med_prescrip.PHARM_CLASS.value_counts()
81/26: psych_med_prescrip[['PHARM_CLASS_C', 'PHARM_CLASS']].unique()
81/27: psych_med_prescrip[['PHARM_CLASS_C', 'PHARM_CLASS']].drop_duplicates()
81/28: psych_med_prescrip[['PHARM_CLASS_C', 'PHARM_CLASS']].drop_duplicates().values()
81/29: psych_med_prescrip[['PHARM_CLASS_C', 'PHARM_CLASS']].drop_duplicates().values
81/30: psych_med_prescrip.PHARM_CLASS.value_counts()
81/31:
IN_DATA_PATH = '.\\datasets\\raw_registry_data'
OUT_DATA_PATH = '.\\datasets\\clean_registry_data'
81/32:
psych_med_prescrip = pd.read_csv(os.path.join(DATA_PATH, 'deid_IBD_Registry_BA1951_Medications_2018-07-05-09-47-15.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
psych_med_prescrip.drop(['Unnamed: 0', 'SIMPLE_GENERIC_NAME', 'PHARM_CLASS_C', 'MED_ID' 'PHARM_SUBCLASS_C', 'PHARM_SUB_CLASS'], axis=1, inplace=True)
psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.THERAPEUTIC_CLASS=='PSYCHOTHERAPEUTIC DRUGS']
psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.END_DATE.notnull() and psych_med_prescrip.ORDERING_DATE.notnull()] # filter no end_date rows
psych_med_prescrip.to_csv(os.path.join(OUT_DATA_PATH,'psych_rx.csv'))
# check: can we aggregate PHARM_CLASS (or keep only the top 5 common ones)
81/33:
psych_med_prescrip = pd.read_csv(os.path.join(IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Medications_2018-07-05-09-47-15.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
psych_med_prescrip.drop(['Unnamed: 0', 'SIMPLE_GENERIC_NAME', 'PHARM_CLASS_C', 'MED_ID' 'PHARM_SUBCLASS_C', 'PHARM_SUB_CLASS'], axis=1, inplace=True)
psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.THERAPEUTIC_CLASS=='PSYCHOTHERAPEUTIC DRUGS']
psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.END_DATE.notnull() and psych_med_prescrip.ORDERING_DATE.notnull()] # filter no end_date rows
psych_med_prescrip.to_csv(os.path.join(OUT_DATA_PATH,'psych_rx.csv'))
# check: can we aggregate PHARM_CLASS (or keep only the top 5 common ones)
81/34:
psych_med_prescrip = pd.read_csv(os.path.join(IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Medications_2018-07-05-09-47-15.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
psych_med_prescrip.drop(['Unnamed: 0', 'SIMPLE_GENERIC_NAME', 'PHARM_CLASS_C', 'MED_ID', 'PHARM_SUBCLASS_C', 'PHARM_SUB_CLASS'], axis=1, inplace=True)
psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.THERAPEUTIC_CLASS=='PSYCHOTHERAPEUTIC DRUGS']
psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.END_DATE.notnull() and psych_med_prescrip.ORDERING_DATE.notnull()] # filter no end_date rows
psych_med_prescrip.to_csv(os.path.join(OUT_DATA_PATH,'psych_rx.csv'))
# check: can we aggregate PHARM_CLASS (or keep only the top 5 common ones)
81/35:
psych_med_prescrip = pd.read_csv(os.path.join(IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Medications_2018-07-05-09-47-15.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
psych_med_prescrip.drop(['Unnamed: 0', 'SIMPLE_GENERIC_NAME', 'PHARM_CLASS_C', 'MED_ID', 'PHARM_SUBCLASS_C', 'PHARM_SUB_CLASS'], axis=1, inplace=True)
psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.THERAPEUTIC_CLASS=='PSYCHOTHERAPEUTIC DRUGS']
psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.END_DATE.notnull() & psych_med_prescrip.ORDERING_DATE.notnull()] # filter no end_date rows
psych_med_prescrip.to_csv(os.path.join(OUT_DATA_PATH,'psych_rx.csv'))
# check: can we aggregate PHARM_CLASS (or keep only the top 5 common ones)
81/36: psych_med_prescrip.columns
81/37:
psych_med_prescrip = pd.read_csv(os.path.join(IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Medications_2018-07-05-09-47-15.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.THERAPEUTIC_CLASS=='PSYCHOTHERAPEUTIC DRUGS']
psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.END_DATE.notnull() & psych_med_prescrip.ORDERING_DATE.notnull()] # filter no end_date rows
# DROP EXTRA COLUMNS
psych_med_prescrip.drop(['Unnamed: 0', 'SIMPLE_GENERIC_NAME', 'PHARM_CLASS_C', 'MED_ID', 'PHARM_SUBCLASS_C', 'PHARM_SUB_CLASS', 'MED_NAME', 'START_DATE', 'THERA_CLASS_C', 'THERAPEUTIC_CLASS', 'PHARM_CLASS', 'DESCRIPTION', 'SIG'], axis=1, inplace=True)
psych_med_prescrip.to_csv(os.path.join(OUT_DATA_PATH,'psych_rx.csv'))

# check: can we aggregate PHARM_CLASS (or keep only the top 5 common ones)
81/38:
med_df = pd.read_csv('rawdata\\filtered_meds2018.csv', parse_dates=['ORDERING_DATE', 'END_DATE'])
med_df = med_df[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
med_df = med_df[med_df.GROUP!='Vitamin_D']

med_df = med_df.drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'])
# med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])
81/39:
med_df = pd.read_csv(os.path.join(IN_DATA_PATH,'filtered_meds2018.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
med_df = med_df[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
med_df = med_df[med_df.GROUP!='Vitamin_D']

med_df = med_df.drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'])
# med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])
81/40:
x=med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])
x[x.AUTO_ID==0]
81/41:
x=med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])
x=med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'END_DATE', 'GROUP'])
x[x.AUTO_ID==0]
81/42:
x=med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])
x=x.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'END_DATE', 'GROUP'])
x[x.AUTO_ID==0]
81/43:
y = x.loc[[0,5,7,12,13]]
y
81/44:
y = x.loc[[0,5,7,12,13]]
e = y.END_DATE
o = y.ORDERING_DATE

e, o
81/45:
y = x.loc[[0,5,7,12,13]]
e = list(y.END_DATE)
o = list(y.ORDERING_DATE)

e, o
81/46:
y = x.loc[[0,5,7,12,13]]
e = list(y.END_DATE)
o = list(y.ORDERING_DATE)
o,e
81/47:
y = x.loc[[0,5,7,12,13]]
e = list(y.END_DATE)
o = list(y.ORDERING_DATE)

list(zip(o[1:],e[:-1]))
81/48:
y = x.loc[[0,5,7,12,13]]
e = list(y.END_DATE)
o = list(y.ORDERING_DATE)

y, list(zip(o[1:],e[:-1]))
81/49:
y = x.loc[[0,5,7,12,13]]
e = list(y.END_DATE)
o = list(y.ORDERING_DATE)

y, list(zip(e[:-1],o[1:]))
81/50:
y = x.loc[[0,5,7,12,13]]
e = list(y.END_DATE)
o = list(y.ORDERING_DATE)

start_loc = 0
end_loc = 0
start_dates = [o[0]]
end_dates=[]

for d1, d2 in zip(e[:-1],o[1:]):
    if d1==d2:
        pass
    else:
        end_dates.append(d1)
        start_loc=end_loc+1
        start_dates.append(d2)
    end_loc+=1
end_dates.append(e[-1])
81/51: start_dates, end_dates
81/52:
y = x.loc[[4,25,26,27,29]]
e = list(y.END_DATE)
o = list(y.ORDERING_DATE)

start_loc = 0
end_loc = 0
start_dates = [o[0]]
end_dates=[]

for d1, d2 in zip(e[:-1],o[1:]):
    if d1==d2:
        pass
    else:
        end_dates.append(d1)
        start_loc=end_loc+1
        start_dates.append(d2)
    end_loc+=1
end_dates.append(e[-1])
81/53: start_dates, end_dates
81/54: y
81/55: y, list(zip(e[:-1],o[1:]))
81/56:
y = x.loc[[4,25,26,27,29]]
e = list(y.END_DATE)
o = list(y.ORDERING_DATE)

start_loc = 0
end_loc = 0
start_dates = [o[0]]
end_dates=[]

for end_tm1, start_t0 in zip(e[:-1],o[1:]):
    if end_tm1==start_t0:
        pass
    else:
        end_dates.append(end_tm1)
        start_loc=end_loc+1
        start_dates.append(start_t0)
    end_loc+=1
end_dates.append(e[-1])
81/57: start_dates, end_dates
81/58: list(zip(start_dates, end_dates))
81/59:
med_df = pd.read_csv(os.path.join(IN_DATA_PATH,'filtered_meds2018.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
med_df = med_df[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
med_df = med_df[med_df.END_DATE.notnull() & med_df.ORDERING_DATE.notnull()] # filter no end_date rows
med_df = med_df[med_df.GROUP!='Vitamin_D']

med_df = med_df.drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'])

# drop redundant rows like (Apr-Dec, Apr-Nov) and (Jan-Sept, Mar-Sept) ==> second row is redundant in both tuples 
med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])
med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'END_DATE', 'GROUP'])
81/60:
def merge_contiguous_events(y):    
    e = list(y.END_DATE)
    o = list(y.ORDERING_DATE)

    start_loc = 0
    end_loc = 0
    start_dates = [o[0]]
    end_dates=[]

    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1!=start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e[-1])
    return list(zip(start_dates, end_dates))
81/61: med_df.shape
81/62: x=med_df.head(1000).groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
81/63: x
81/64:
psych_med_prescrip = pd.read_csv(os.path.join(IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Medications_2018-07-05-09-47-15.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.THERAPEUTIC_CLASS=='PSYCHOTHERAPEUTIC DRUGS']
psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.END_DATE.notnull() & psych_med_prescrip.ORDERING_DATE.notnull()] # filter no end_date rows
psych_med_prescrip['GROUP'] = 'Psych'
psych_med_prescrip = psych_med_prescrip[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
psych_med_prescrip.to_csv(os.path.join(OUT_DATA_PATH,'psych_rx.csv'))

# check: can we aggregate PHARM_CLASS (or keep only the top 5 common ones)
81/65:
def merge_contiguous_events(y):    
    e = list(y.END_DATE)
    o = list(y.ORDERING_DATE)

    start_loc = 0
    end_loc = 0
    start_dates = [o[0]]
    end_dates=[]

    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e[-1])
    return list(zip(start_dates, end_dates))
81/66:
med_df = pd.read_csv(os.path.join(IN_DATA_PATH,'filtered_meds2018.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
med_df = med_df[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
med_df = med_df[med_df.END_DATE.notnull() & med_df.ORDERING_DATE.notnull()] # filter no end_date rows
med_df = med_df[med_df.GROUP!='Vitamin_D']
med_df = pd.concat([med_df, psych_med_prescrip])

# drop redundant rows like (Apr-Dec, Apr-Nov) and (Jan-Sept, Mar-Sept) ==> second row is redundant in both tuples 
med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])
med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'END_DATE', 'GROUP'])
81/67: x=med_df.head(10000).groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
81/68: x
81/69: x[-1]
81/70: x.iloc[-1]
81/71: x
81/72: x.loc[816]
81/73: x.loc[816, 'Psych']
81/74: med_df.head().END_DATE[:-1]
81/75: list(zip(med_df.head().END_DATE[:-1], med_df.head().END_DATE[:-1]))
81/76:
def merge_contiguous_events(y):    
    e = y.END_DATE
    o = y.ORDERING_DATE

    start_loc = 0
    end_loc = 0
    start_dates = [o[0]]
    end_dates=[]

    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e[-1])
    return list(zip(start_dates, end_dates))
81/77:
med_df = pd.read_csv(os.path.join(IN_DATA_PATH,'filtered_meds2018.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
med_df = med_df[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
med_df = med_df[med_df.END_DATE.notnull() & med_df.ORDERING_DATE.notnull()] # filter no end_date rows
med_df = med_df[med_df.GROUP!='Vitamin_D']
med_df = pd.concat([med_df, psych_med_prescrip])

# drop redundant rows like (Apr-Dec, Apr-Nov) and (Jan-Sept, Mar-Sept) ==> second row is redundant in both tuples 
med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])
med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'END_DATE', 'GROUP'])

med_df.to_csv(os.path.join(OUT_DATA_PATH, 'all_rx.csv'))

intervals = med_df.groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
intervals.to_csv(os.path.join(OUT_DATA_PATH, 'rx_intervals.csv'))
81/78:
o=med_df.head().END_DATE
o[0]
81/79:
def merge_contiguous_events(y):    
    e = y.END_DATE
    o = y.ORDERING_DATE

    start_loc = 0
    end_loc = 0
    try:
        start_dates = [o[0]]
    except KeyError:
        print(y)
    end_dates=[]

    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e[-1])
    return list(zip(start_dates, end_dates))
81/80:
med_df = pd.read_csv(os.path.join(IN_DATA_PATH,'filtered_meds2018.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
med_df = med_df[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
med_df = med_df[med_df.END_DATE.notnull() & med_df.ORDERING_DATE.notnull()] # filter no end_date rows
med_df = med_df[med_df.GROUP!='Vitamin_D']
med_df = pd.concat([med_df, psych_med_prescrip])

# drop redundant rows like (Apr-Dec, Apr-Nov) and (Jan-Sept, Mar-Sept) ==> second row is redundant in both tuples 
med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])
med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'END_DATE', 'GROUP'])

med_df.to_csv(os.path.join(OUT_DATA_PATH, 'all_rx.csv'))

intervals = med_df.groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
intervals.to_csv(os.path.join(OUT_DATA_PATH, 'rx_intervals.csv'))
81/81:
# med_df = pd.read_csv(os.path.join(IN_DATA_PATH,'filtered_meds2018.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
# med_df = med_df[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
# med_df = med_df[med_df.END_DATE.notnull() & med_df.ORDERING_DATE.notnull()] # filter no end_date rows
# med_df = med_df[med_df.GROUP!='Vitamin_D']
# med_df = pd.concat([med_df, psych_med_prescrip])

# # drop redundant rows like (Apr-Dec, Apr-Nov) and (Jan-Sept, Mar-Sept) ==> second row is redundant in both tuples 
# med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])
# med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'END_DATE', 'GROUP'])

# med_df.to_csv(os.path.join(OUT_DATA_PATH, 'all_rx.csv'))

intervals = med_df.groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
intervals.to_csv(os.path.join(OUT_DATA_PATH, 'rx_intervals.csv'))
81/82:
x = med_df[med_df.AUTO_ID==0]
x.groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
81/83:
def merge_contiguous_events(y):    
    e = y.END_DATE
    o = y.ORDERING_DATE

    start_loc = 0
    end_loc = 0
    
    start_dates = [o[0]]
    end_dates=[]

    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e[-1])
    return list(zip(start_dates, end_dates))
81/84:
x = med_df[med_df.AUTO_ID==0]
x.groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
81/85:
x = med_df[med_df.AUTO_ID==0]
y = x[x.GROUP=='5_ASA']
# x.groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
81/86: y
81/87: y.ORDERING_DATE
81/88: y.ORDERING_DATE[0]
81/89: y.ORDERING_DATE.iloc[0]
81/90: y.ORDERING_DATE[1:]
81/91:
def merge_contiguous_events(y):    
    e = y.END_DATE
    o = y.ORDERING_DATE

    start_loc = 0
    end_loc = 0
    
    start_dates = [o.iloc[0]]
    end_dates=[]

    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    return list(zip(start_dates, end_dates))
81/92:
# med_df = pd.read_csv(os.path.join(IN_DATA_PATH,'filtered_meds2018.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
# med_df = med_df[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
# med_df = med_df[med_df.END_DATE.notnull() & med_df.ORDERING_DATE.notnull()] # filter no end_date rows
# med_df = med_df[med_df.GROUP!='Vitamin_D']
# med_df = pd.concat([med_df, psych_med_prescrip])

# # drop redundant rows like (Apr-Dec, Apr-Nov) and (Jan-Sept, Mar-Sept) ==> second row is redundant in both tuples 
# med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])
# med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'END_DATE', 'GROUP'])

# med_df.to_csv(os.path.join(OUT_DATA_PATH, 'all_rx.csv'))

intervals = med_df.groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
intervals.to_csv(os.path.join(OUT_DATA_PATH, 'rx_intervals.csv'))
81/93:
# med_df = pd.read_csv(os.path.join(IN_DATA_PATH,'filtered_meds2018.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
# med_df = med_df[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
# med_df = med_df[med_df.END_DATE.notnull() & med_df.ORDERING_DATE.notnull()] # filter no end_date rows
# med_df = med_df[med_df.GROUP!='Vitamin_D']
# med_df = pd.concat([med_df, psych_med_prescrip])

# # drop redundant rows like (Apr-Dec, Apr-Nov) and (Jan-Sept, Mar-Sept) ==> second row is redundant in both tuples 
# med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])
# med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'END_DATE', 'GROUP'])

# med_df.to_csv(os.path.join(OUT_DATA_PATH, 'all_rx.csv'))

# intervals = med_df.groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
intervals.columns = ['AUTO_ID', 'DRUG_CLASS', 'ACTIVE_DURING']
intervals.to_csv(os.path.join(OUT_DATA_PATH, 'rx_intervals.csv'))
81/94: x.groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
81/95: x.groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events).flatten_index()
81/96: x.groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events).reset_index()
81/97:
# med_df = pd.read_csv(os.path.join(IN_DATA_PATH,'filtered_meds2018.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
# med_df = med_df[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
# med_df = med_df[med_df.END_DATE.notnull() & med_df.ORDERING_DATE.notnull()] # filter no end_date rows
# med_df = med_df[med_df.GROUP!='Vitamin_D']
# med_df = pd.concat([med_df, psych_med_prescrip])

# # drop redundant rows like (Apr-Dec, Apr-Nov) and (Jan-Sept, Mar-Sept) ==> second row is redundant in both tuples 
# med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])
# med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'END_DATE', 'GROUP'])

# med_df.to_csv(os.path.join(OUT_DATA_PATH, 'all_rx.csv'))

# intervals = med_df.groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events).reset_index()
intervals = intervals.reset_index()
intervals.columns = ['AUTO_ID', 'DRUG_CLASS', 'ACTIVE_DURING']
intervals.to_csv(os.path.join(OUT_DATA_PATH, 'rx_intervals.csv'))
81/98:
labs_og = pd.read_csv('rawdata\\filtered_labs.csv', parse_dates=['ORDER_DATE','RESULT_DATE'])
labs_og = labs_og.drop(['Unnamed: 0', 'Unnamed: 0.1', 'PROC_CODE', 'PROC_NAME', 'ORDER_STATUS', 'CPT_CODE', 'LAB_COMP_ID'], axis=1, inplace=True)
labs_og.columns
81/99:
labs_og = pd.read_csv(os.path.join(IN_DATA_PATH,'filtered_labs.csv'), parse_dates=['ORDER_DATE','RESULT_DATE'])
labs_og = labs_og.drop(['Unnamed: 0', 'Unnamed: 0.1', 'PROC_CODE', 'PROC_NAME', 'ORDER_STATUS', 'CPT_CODE', 'LAB_COMP_ID'], axis=1, inplace=True)
labs_og.columns
81/100:
labs_og = pd.read_csv(os.path.join(IN_DATA_PATH,'filtered_labs.csv'), parse_dates=['ORDER_DATE','RESULT_DATE'])
labs_og.drop(['Unnamed: 0', 'Unnamed: 0.1', 'PROC_CODE', 'PROC_NAME', 'ORDER_STATUS', 'CPT_CODE', 'LAB_COMP_ID'], axis=1, inplace=True)
labs_og.columns
81/101:
labs = pd.read_csv('rawdata\\filtered_labs.csv', parse_dates=['ORDER_DATE','RESULT_DATE'])
labs = labs[['AUTO_ID', 'ORDER_DATE', 'RESULT_DATE', 'ORD_NUM_VALUE', 'REF_LOW', 'REF_HIGH', 'REF_NORMAL_VALS', 'REF_UNIT',
       'RESULT_FLAG', 'GROUP']]

# drop invalid rows
labs = labs.drop(labs.index[np.logical_or(labs['ORD_NUM_VALUE'].isnull(), labs['ORD_NUM_VALUE']>=9999999)]) # Drop rows with invalid readings

# === clean messy readings data ====
# copy REF_LOW values with <,- to REF_NORMAL_VALS. WILL PROCESS THEM 
labs['REF_NORMAL_VALS'] = np.where(labs['REF_LOW'].str.contains(r'[<-]')==True, labs['REF_LOW'], labs['REF_NORMAL_VALS'])
# replace invalid values with nan
labs['REF_HIGH'] = np.where(labs['REF_HIGH'].str.contains(r'[a-zA-Z<]')==True, np.nan, labs['REF_HIGH'])
labs['REF_LOW'] = np.where(labs['REF_LOW'].str.contains(r'[a-zA-Z<\-]')==True, np.nan, labs['REF_LOW'])
# drop garbage rows
labs.drop(labs[labs['REF_NORMAL_VALS'].str.contains(r'[a-zA-Z]')==True].index, inplace=True)
# extract LOW, HIGH by splitting REF_NORMAL_VALS
labs[['NEW_LOW','NEW_HIGH']] = labs[labs['REF_NORMAL_VALS'].str.contains('-')==True]['REF_NORMAL_VALS'].str.split('-').apply(pd.Series, 1) # solve cases where REF_LOW is like 0-10
labs['NEW_LOW'] = np.where(np.logical_and(labs['REF_LOW'].isnull(), labs['REF_NORMAL_VALS'].str.contains(r'[<=]')==True), -999, labs.NEW_LOW) # solve cases where REF_LOW is like '<TAU', anything below TAU is normal => x is normal if -999 < x < TAU. 
labs['NEW_HIGH'] = np.where(labs['REF_NORMAL_VALS'].str.contains(r'[<=]'), labs['REF_NORMAL_VALS'].str.replace(r'[<=]',''), labs.NEW_HIGH) # NEW_LOW = -999, NEW_HIGH = TAU
# put REF_X into NEW_X
labs['NEW_LOW'] = np.where(labs.NEW_LOW.isnull(), labs['REF_LOW'], labs.NEW_LOW)
labs['NEW_HIGH'] = np.where(labs.NEW_HIGH.isnull(), labs['REF_HIGH'], labs.NEW_HIGH)
labs['NEW_HIGH'] = labs['NEW_HIGH'].astype(float)
labs['NEW_LOW'] = labs['NEW_LOW'].astype(float)    
labs['REF_UNIT'] = labs['REF_UNIT'].str.lower()

# ALL_LABS = labs
# ALL_LABS['NEW_FLAG'] = np.nan
# ALL_LABS['NEW_FLAG'] = np.where(ALL_LABS['ORD_NUM_VALUE'] < ALL_LABS.NEW_LOW, -1, ALL_LABS.NEW_FLAG)
# ALL_LABS['NEW_FLAG'] = np.where(ALL_LABS['ORD_NUM_VALUE'] > ALL_LABS.NEW_HIGH, 1, ALL_LABS.NEW_FLAG) 
# ALL_LABS.to_pickle('rawdata\\Indiv_Labs_Data.pkl')
81/102:
labs = pd.read_csv(os.path.join(IN_DATA_PATH,'filtered_labs.csv'), parse_dates=['ORDER_DATE','RESULT_DATE'])
labs = labs[['AUTO_ID', 'ORDER_DATE', 'RESULT_DATE', 'ORD_NUM_VALUE', 'REF_LOW', 'REF_HIGH', 'REF_NORMAL_VALS', 'REF_UNIT',
       'RESULT_FLAG', 'GROUP']]

# drop invalid rows
labs = labs.drop(labs.index[np.logical_or(labs['ORD_NUM_VALUE'].isnull(), labs['ORD_NUM_VALUE']>=9999999)]) # Drop rows with invalid readings

# === clean messy readings data ====
# copy REF_LOW values with <,- to REF_NORMAL_VALS. WILL PROCESS THEM 
labs['REF_NORMAL_VALS'] = np.where(labs['REF_LOW'].str.contains(r'[<-]')==True, labs['REF_LOW'], labs['REF_NORMAL_VALS'])
# replace invalid values with nan
labs['REF_HIGH'] = np.where(labs['REF_HIGH'].str.contains(r'[a-zA-Z<]')==True, np.nan, labs['REF_HIGH'])
labs['REF_LOW'] = np.where(labs['REF_LOW'].str.contains(r'[a-zA-Z<\-]')==True, np.nan, labs['REF_LOW'])
# drop garbage rows
labs.drop(labs[labs['REF_NORMAL_VALS'].str.contains(r'[a-zA-Z]')==True].index, inplace=True)
# extract LOW, HIGH by splitting REF_NORMAL_VALS
labs[['NEW_LOW','NEW_HIGH']] = labs[labs['REF_NORMAL_VALS'].str.contains('-')==True]['REF_NORMAL_VALS'].str.split('-').apply(pd.Series, 1) # solve cases where REF_LOW is like 0-10
labs['NEW_LOW'] = np.where(np.logical_and(labs['REF_LOW'].isnull(), labs['REF_NORMAL_VALS'].str.contains(r'[<=]')==True), -999, labs.NEW_LOW) # solve cases where REF_LOW is like '<TAU', anything below TAU is normal => x is normal if -999 < x < TAU. 
labs['NEW_HIGH'] = np.where(labs['REF_NORMAL_VALS'].str.contains(r'[<=]'), labs['REF_NORMAL_VALS'].str.replace(r'[<=]',''), labs.NEW_HIGH) # NEW_LOW = -999, NEW_HIGH = TAU
# put REF_X into NEW_X
labs['NEW_LOW'] = np.where(labs.NEW_LOW.isnull(), labs['REF_LOW'], labs.NEW_LOW)
labs['NEW_HIGH'] = np.where(labs.NEW_HIGH.isnull(), labs['REF_HIGH'], labs.NEW_HIGH)
labs['NEW_HIGH'] = labs['NEW_HIGH'].astype(float)
labs['NEW_LOW'] = labs['NEW_LOW'].astype(float)    
labs['REF_UNIT'] = labs['REF_UNIT'].str.lower()

# ALL_LABS = labs
# ALL_LABS['NEW_FLAG'] = np.nan
# ALL_LABS['NEW_FLAG'] = np.where(ALL_LABS['ORD_NUM_VALUE'] < ALL_LABS.NEW_LOW, -1, ALL_LABS.NEW_FLAG)
# ALL_LABS['NEW_FLAG'] = np.where(ALL_LABS['ORD_NUM_VALUE'] > ALL_LABS.NEW_HIGH, 1, ALL_LABS.NEW_FLAG) 
# ALL_LABS.to_pickle('rawdata\\Indiv_Labs_Data.pkl')
81/103: labs
81/104: labs.ORDER_DATE.isnull().sum()
81/105: labs.RESULT_DATE.isnull().sum()
81/106:
labs = pd.read_csv(os.path.join(IN_DATA_PATH,'filtered_labs.csv'), parse_dates=['ORDER_DATE','RESULT_DATE'])
labs = labs[['AUTO_ID', 'ORDER_DATE', 'RESULT_DATE', 'ORD_NUM_VALUE', 'REF_LOW', 'REF_HIGH', 'REF_NORMAL_VALS', 'REF_UNIT',
       'RESULT_FLAG', 'GROUP']]

# drop invalid rows
labs = labs.drop(labs.index[np.logical_or(labs['ORD_NUM_VALUE'].isnull(), labs['ORD_NUM_VALUE']>=9999999)]) # Drop rows with invalid readings

# === clean messy readings data ====
# copy REF_LOW values with <,- to REF_NORMAL_VALS. WILL PROCESS THEM 
labs['REF_NORMAL_VALS'] = np.where(labs['REF_LOW'].str.contains(r'[<-]')==True, labs['REF_LOW'], labs['REF_NORMAL_VALS'])
# replace invalid values with nan
labs['REF_HIGH'] = np.where(labs['REF_HIGH'].str.contains(r'[a-zA-Z<]')==True, np.nan, labs['REF_HIGH'])
labs['REF_LOW'] = np.where(labs['REF_LOW'].str.contains(r'[a-zA-Z<\-]')==True, np.nan, labs['REF_LOW'])
# drop garbage rows
labs.drop(labs[labs['REF_NORMAL_VALS'].str.contains(r'[a-zA-Z]')==True].index, inplace=True)
# extract LOW, HIGH by splitting REF_NORMAL_VALS
labs[['NEW_LOW','NEW_HIGH']] = labs[labs['REF_NORMAL_VALS'].str.contains('-')==True]['REF_NORMAL_VALS'].str.split('-').apply(pd.Series, 1) # solve cases where REF_LOW is like 0-10
labs['NEW_LOW'] = np.where(np.logical_and(labs['REF_LOW'].isnull(), labs['REF_NORMAL_VALS'].str.contains(r'[<=]')==True), -999, labs.NEW_LOW) # solve cases where REF_LOW is like '<TAU', anything below TAU is normal => x is normal if -999 < x < TAU. 
labs['NEW_HIGH'] = np.where(labs['REF_NORMAL_VALS'].str.contains(r'[<=]'), labs['REF_NORMAL_VALS'].str.replace(r'[<=]',''), labs.NEW_HIGH) # NEW_LOW = -999, NEW_HIGH = TAU
# put REF_X into NEW_X
labs['NEW_LOW'] = np.where(labs.NEW_LOW.isnull(), labs['REF_LOW'], labs.NEW_LOW)
labs['NEW_HIGH'] = np.where(labs.NEW_HIGH.isnull(), labs['REF_HIGH'], labs.NEW_HIGH)
labs['NEW_HIGH'] = labs['NEW_HIGH'].astype(float)
labs['NEW_LOW'] = labs['NEW_LOW'].astype(float)    
labs['REF_UNIT'] = labs['REF_UNIT'].str.lower()

labs['RESULT_FLAG'] = 'Normal'
labs['RESULT_FLAG'] = np.where(labs['ORD_NUM_VALUE'] < labs.NEW_LOW, 'Low', ALL_LABS.NEW_FLAG)
labs['RESULT_FLAG'] = np.where(labs['ORD_NUM_VALUE'] > labs.NEW_HIGH, 'High', ALL_LABS.NEW_FLAG)

labs[['AUTO_ID', 'RESULT_DATE', 'RESULT_FLAG', 'GROUP', 'NEW_LOW', 'NEW_HIGH', 'ORD_NUM_VALUE']]
81/107: labs[['AUTO_ID', 'RESULT_DATE', 'RESULT_FLAG', 'GROUP', 'NEW_LOW', 'NEW_HIGH', 'ORD_NUM_VALUE']]
81/108:
labs = pd.read_csv(os.path.join(IN_DATA_PATH,'filtered_labs.csv'), parse_dates=['ORDER_DATE','RESULT_DATE'])
labs = labs[['AUTO_ID', 'ORDER_DATE', 'RESULT_DATE', 'ORD_NUM_VALUE', 'REF_LOW', 'REF_HIGH', 'REF_NORMAL_VALS', 'REF_UNIT',
       'RESULT_FLAG', 'GROUP']]

# drop invalid rows
labs = labs.drop(labs.index[np.logical_or(labs['ORD_NUM_VALUE'].isnull(), labs['ORD_NUM_VALUE']>=9999999)]) # Drop rows with invalid readings

# === clean messy readings data ====
# copy REF_LOW values with <,- to REF_NORMAL_VALS. WILL PROCESS THEM 
labs['REF_NORMAL_VALS'] = np.where(labs['REF_LOW'].str.contains(r'[<-]')==True, labs['REF_LOW'], labs['REF_NORMAL_VALS'])
# replace invalid values with nan
labs['REF_HIGH'] = np.where(labs['REF_HIGH'].str.contains(r'[a-zA-Z<]')==True, np.nan, labs['REF_HIGH'])
labs['REF_LOW'] = np.where(labs['REF_LOW'].str.contains(r'[a-zA-Z<\-]')==True, np.nan, labs['REF_LOW'])
# drop garbage rows
labs.drop(labs[labs['REF_NORMAL_VALS'].str.contains(r'[a-zA-Z]')==True].index, inplace=True)
# extract LOW, HIGH by splitting REF_NORMAL_VALS
labs[['NEW_LOW','NEW_HIGH']] = labs[labs['REF_NORMAL_VALS'].str.contains('-')==True]['REF_NORMAL_VALS'].str.split('-').apply(pd.Series, 1) # solve cases where REF_LOW is like 0-10
labs['NEW_LOW'] = np.where(np.logical_and(labs['REF_LOW'].isnull(), labs['REF_NORMAL_VALS'].str.contains(r'[<=]')==True), -999, labs.NEW_LOW) # solve cases where REF_LOW is like '<TAU', anything below TAU is normal => x is normal if -999 < x < TAU. 
labs['NEW_HIGH'] = np.where(labs['REF_NORMAL_VALS'].str.contains(r'[<=]'), labs['REF_NORMAL_VALS'].str.replace(r'[<=]',''), labs.NEW_HIGH) # NEW_LOW = -999, NEW_HIGH = TAU
# put REF_X into NEW_X
labs['NEW_LOW'] = np.where(labs.NEW_LOW.isnull(), labs['REF_LOW'], labs.NEW_LOW)
labs['NEW_HIGH'] = np.where(labs.NEW_HIGH.isnull(), labs['REF_HIGH'], labs.NEW_HIGH)
labs['NEW_HIGH'] = labs['NEW_HIGH'].astype(float)
labs['NEW_LOW'] = labs['NEW_LOW'].astype(float)    
labs['REF_UNIT'] = labs['REF_UNIT'].str.lower()

labs['RESULT_FLAG'] = 'Normal'
labs['RESULT_FLAG'] = np.where(labs['ORD_NUM_VALUE'] < labs.NEW_LOW, 'Low', ALL_LABS.NEW_FLAG)
labs['RESULT_FLAG'] = np.where(labs['ORD_NUM_VALUE'] > labs.NEW_HIGH, 'High', ALL_LABS.NEW_FLAG)

labs[['AUTO_ID', 'RESULT_DATE', 'RESULT_FLAG', 'GROUP', 'NEW_LOW', 'NEW_HIGH', 'ORD_NUM_VALUE']].to_csv(os.path.join(OUT_DATA_PATH, 'labs.csv'))
81/109:
labs = pd.read_csv(os.path.join(IN_DATA_PATH,'filtered_labs.csv'), parse_dates=['ORDER_DATE','RESULT_DATE'])
labs = labs[['AUTO_ID', 'ORDER_DATE', 'RESULT_DATE', 'ORD_NUM_VALUE', 'REF_LOW', 'REF_HIGH', 'REF_NORMAL_VALS', 'REF_UNIT',
       'RESULT_FLAG', 'GROUP']]

# drop invalid rows
labs = labs.drop(labs.index[np.logical_or(labs['ORD_NUM_VALUE'].isnull(), labs['ORD_NUM_VALUE']>=9999999)]) # Drop rows with invalid readings

# === clean messy readings data ====
# copy REF_LOW values with <,- to REF_NORMAL_VALS. WILL PROCESS THEM 
labs['REF_NORMAL_VALS'] = np.where(labs['REF_LOW'].str.contains(r'[<-]')==True, labs['REF_LOW'], labs['REF_NORMAL_VALS'])
# replace invalid values with nan
labs['REF_HIGH'] = np.where(labs['REF_HIGH'].str.contains(r'[a-zA-Z<]')==True, np.nan, labs['REF_HIGH'])
labs['REF_LOW'] = np.where(labs['REF_LOW'].str.contains(r'[a-zA-Z<\-]')==True, np.nan, labs['REF_LOW'])
# drop garbage rows
labs.drop(labs[labs['REF_NORMAL_VALS'].str.contains(r'[a-zA-Z]')==True].index, inplace=True)
# extract LOW, HIGH by splitting REF_NORMAL_VALS
labs[['NEW_LOW','NEW_HIGH']] = labs[labs['REF_NORMAL_VALS'].str.contains('-')==True]['REF_NORMAL_VALS'].str.split('-').apply(pd.Series, 1) # solve cases where REF_LOW is like 0-10
labs['NEW_LOW'] = np.where(np.logical_and(labs['REF_LOW'].isnull(), labs['REF_NORMAL_VALS'].str.contains(r'[<=]')==True), -999, labs.NEW_LOW) # solve cases where REF_LOW is like '<TAU', anything below TAU is normal => x is normal if -999 < x < TAU. 
labs['NEW_HIGH'] = np.where(labs['REF_NORMAL_VALS'].str.contains(r'[<=]'), labs['REF_NORMAL_VALS'].str.replace(r'[<=]',''), labs.NEW_HIGH) # NEW_LOW = -999, NEW_HIGH = TAU
# put REF_X into NEW_X
labs['NEW_LOW'] = np.where(labs.NEW_LOW.isnull(), labs['REF_LOW'], labs.NEW_LOW)
labs['NEW_HIGH'] = np.where(labs.NEW_HIGH.isnull(), labs['REF_HIGH'], labs.NEW_HIGH)
labs['NEW_HIGH'] = labs['NEW_HIGH'].astype(float)
labs['NEW_LOW'] = labs['NEW_LOW'].astype(float)    
labs['REF_UNIT'] = labs['REF_UNIT'].str.lower()

labs['RESULT_FLAG'] = 'Normal'
labs['RESULT_FLAG'] = np.where(labs['ORD_NUM_VALUE'] < labs.NEW_LOW, 'Low', labs['RESULT_FLAG'])
labs['RESULT_FLAG'] = np.where(labs['ORD_NUM_VALUE'] > labs.NEW_HIGH, 'High', labs['RESULT_FLAG'])

labs[['AUTO_ID', 'RESULT_DATE', 'RESULT_FLAG', 'GROUP', 'NEW_LOW', 'NEW_HIGH', 'ORD_NUM_VALUE']].to_csv(os.path.join(OUT_DATA_PATH, 'labs.csv'))
81/110:
# labs = pd.read_csv(os.path.join(IN_DATA_PATH,'filtered_labs.csv'), parse_dates=['ORDER_DATE','RESULT_DATE'])
# labs = labs[['AUTO_ID', 'ORDER_DATE', 'RESULT_DATE', 'ORD_NUM_VALUE', 'REF_LOW', 'REF_HIGH', 'REF_NORMAL_VALS', 'REF_UNIT',
#        'RESULT_FLAG', 'GROUP']]

# # drop invalid rows
# labs = labs.drop(labs.index[np.logical_or(labs['ORD_NUM_VALUE'].isnull(), labs['ORD_NUM_VALUE']>=9999999)]) # Drop rows with invalid readings

# # === clean messy readings data ====
# # copy REF_LOW values with <,- to REF_NORMAL_VALS. WILL PROCESS THEM 
# labs['REF_NORMAL_VALS'] = np.where(labs['REF_LOW'].str.contains(r'[<-]')==True, labs['REF_LOW'], labs['REF_NORMAL_VALS'])
# # replace invalid values with nan
# labs['REF_HIGH'] = np.where(labs['REF_HIGH'].str.contains(r'[a-zA-Z<]')==True, np.nan, labs['REF_HIGH'])
# labs['REF_LOW'] = np.where(labs['REF_LOW'].str.contains(r'[a-zA-Z<\-]')==True, np.nan, labs['REF_LOW'])
# # drop garbage rows
# labs.drop(labs[labs['REF_NORMAL_VALS'].str.contains(r'[a-zA-Z]')==True].index, inplace=True)
# # extract LOW, HIGH by splitting REF_NORMAL_VALS
# labs[['NEW_LOW','NEW_HIGH']] = labs[labs['REF_NORMAL_VALS'].str.contains('-')==True]['REF_NORMAL_VALS'].str.split('-').apply(pd.Series, 1) # solve cases where REF_LOW is like 0-10
# labs['NEW_LOW'] = np.where(np.logical_and(labs['REF_LOW'].isnull(), labs['REF_NORMAL_VALS'].str.contains(r'[<=]')==True), -999, labs.NEW_LOW) # solve cases where REF_LOW is like '<TAU', anything below TAU is normal => x is normal if -999 < x < TAU. 
# labs['NEW_HIGH'] = np.where(labs['REF_NORMAL_VALS'].str.contains(r'[<=]'), labs['REF_NORMAL_VALS'].str.replace(r'[<=]',''), labs.NEW_HIGH) # NEW_LOW = -999, NEW_HIGH = TAU
# # put REF_X into NEW_X
# labs['NEW_LOW'] = np.where(labs.NEW_LOW.isnull(), labs['REF_LOW'], labs.NEW_LOW)
# labs['NEW_HIGH'] = np.where(labs.NEW_HIGH.isnull(), labs['REF_HIGH'], labs.NEW_HIGH)
# labs['NEW_HIGH'] = labs['NEW_HIGH'].astype(float)
# labs['NEW_LOW'] = labs['NEW_LOW'].astype(float)    
# labs['REF_UNIT'] = labs['REF_UNIT'].str.lower()

labs['RESULT_FLAG'] = 'Normal'
labs['RESULT_FLAG'] = np.where(labs['ORD_NUM_VALUE'] < labs.NEW_LOW, 'Low', labs['RESULT_FLAG'])
labs['RESULT_FLAG'] = np.where(labs['ORD_NUM_VALUE'] > labs.NEW_HIGH, 'High', labs['RESULT_FLAG'])

labs[['AUTO_ID', 'RESULT_DATE', 'RESULT_FLAG', 'GROUP', 'NEW_LOW', 'NEW_HIGH', 'ORD_NUM_VALUE']].to_csv(os.path.join(OUT_DATA_PATH, 'labs.csv'))
81/111:
# labs = pd.read_csv(os.path.join(IN_DATA_PATH,'filtered_labs.csv'), parse_dates=['ORDER_DATE','RESULT_DATE'])
# labs = labs[['AUTO_ID', 'ORDER_DATE', 'RESULT_DATE', 'ORD_NUM_VALUE', 'REF_LOW', 'REF_HIGH', 'REF_NORMAL_VALS', 'REF_UNIT',
#        'RESULT_FLAG', 'GROUP']]

# # drop invalid rows
# labs = labs.drop(labs.index[np.logical_or(labs['ORD_NUM_VALUE'].isnull(), labs['ORD_NUM_VALUE']>=9999999)]) # Drop rows with invalid readings

# # === clean messy readings data ====
# # copy REF_LOW values with <,- to REF_NORMAL_VALS. WILL PROCESS THEM 
# labs['REF_NORMAL_VALS'] = np.where(labs['REF_LOW'].str.contains(r'[<-]')==True, labs['REF_LOW'], labs['REF_NORMAL_VALS'])
# # replace invalid values with nan
# labs['REF_HIGH'] = np.where(labs['REF_HIGH'].str.contains(r'[a-zA-Z<]')==True, np.nan, labs['REF_HIGH'])
# labs['REF_LOW'] = np.where(labs['REF_LOW'].str.contains(r'[a-zA-Z<\-]')==True, np.nan, labs['REF_LOW'])
# # drop garbage rows
# labs.drop(labs[labs['REF_NORMAL_VALS'].str.contains(r'[a-zA-Z]')==True].index, inplace=True)
# # extract LOW, HIGH by splitting REF_NORMAL_VALS
# labs[['NEW_LOW','NEW_HIGH']] = labs[labs['REF_NORMAL_VALS'].str.contains('-')==True]['REF_NORMAL_VALS'].str.split('-').apply(pd.Series, 1) # solve cases where REF_LOW is like 0-10
# labs['NEW_LOW'] = np.where(np.logical_and(labs['REF_LOW'].isnull(), labs['REF_NORMAL_VALS'].str.contains(r'[<=]')==True), -999, labs.NEW_LOW) # solve cases where REF_LOW is like '<TAU', anything below TAU is normal => x is normal if -999 < x < TAU. 
# labs['NEW_HIGH'] = np.where(labs['REF_NORMAL_VALS'].str.contains(r'[<=]'), labs['REF_NORMAL_VALS'].str.replace(r'[<=]',''), labs.NEW_HIGH) # NEW_LOW = -999, NEW_HIGH = TAU
# # put REF_X into NEW_X
# labs['NEW_LOW'] = np.where(labs.NEW_LOW.isnull(), labs['REF_LOW'], labs.NEW_LOW)
# labs['NEW_HIGH'] = np.where(labs.NEW_HIGH.isnull(), labs['REF_HIGH'], labs.NEW_HIGH)
# labs['NEW_HIGH'] = labs['NEW_HIGH'].astype(float)
# labs['NEW_LOW'] = labs['NEW_LOW'].astype(float)    
# labs['REF_UNIT'] = labs['REF_UNIT'].str.lower()

# labs['RESULT_FLAG'] = 'Normal'
# labs['RESULT_FLAG'] = np.where(labs['ORD_NUM_VALUE'] < labs.NEW_LOW, 'Low', labs['RESULT_FLAG'])
# labs['RESULT_FLAG'] = np.where(labs['ORD_NUM_VALUE'] > labs.NEW_HIGH, 'High', labs['RESULT_FLAG'])

labs[['AUTO_ID', 'ORDER_DATE', 'RESULT_DATE', 'RESULT_FLAG', 'GROUP', 'NEW_LOW', 'NEW_HIGH', 'ORD_NUM_VALUE']].to_csv(os.path.join(OUT_DATA_PATH, 'labs.csv'))
81/112:
# labs = pd.read_csv(os.path.join(IN_DATA_PATH,'filtered_labs.csv'), parse_dates=['ORDER_DATE','RESULT_DATE'])
# labs = labs[['AUTO_ID', 'ORDER_DATE', 'RESULT_DATE', 'ORD_NUM_VALUE', 'REF_LOW', 'REF_HIGH', 'REF_NORMAL_VALS', 'REF_UNIT',
#        'RESULT_FLAG', 'GROUP']]

# # drop invalid rows
# labs = labs.drop(labs.index[np.logical_or(labs['ORD_NUM_VALUE'].isnull(), labs['ORD_NUM_VALUE']>=9999999)]) # Drop rows with invalid readings

# # === clean messy readings data ====
# # copy REF_LOW values with <,- to REF_NORMAL_VALS. WILL PROCESS THEM 
# labs['REF_NORMAL_VALS'] = np.where(labs['REF_LOW'].str.contains(r'[<-]')==True, labs['REF_LOW'], labs['REF_NORMAL_VALS'])
# # replace invalid values with nan
# labs['REF_HIGH'] = np.where(labs['REF_HIGH'].str.contains(r'[a-zA-Z<]')==True, np.nan, labs['REF_HIGH'])
# labs['REF_LOW'] = np.where(labs['REF_LOW'].str.contains(r'[a-zA-Z<\-]')==True, np.nan, labs['REF_LOW'])
# # drop garbage rows
# labs.drop(labs[labs['REF_NORMAL_VALS'].str.contains(r'[a-zA-Z]')==True].index, inplace=True)
# # extract LOW, HIGH by splitting REF_NORMAL_VALS
# labs[['NEW_LOW','NEW_HIGH']] = labs[labs['REF_NORMAL_VALS'].str.contains('-')==True]['REF_NORMAL_VALS'].str.split('-').apply(pd.Series, 1) # solve cases where REF_LOW is like 0-10
# labs['NEW_LOW'] = np.where(np.logical_and(labs['REF_LOW'].isnull(), labs['REF_NORMAL_VALS'].str.contains(r'[<=]')==True), -999, labs.NEW_LOW) # solve cases where REF_LOW is like '<TAU', anything below TAU is normal => x is normal if -999 < x < TAU. 
# labs['NEW_HIGH'] = np.where(labs['REF_NORMAL_VALS'].str.contains(r'[<=]'), labs['REF_NORMAL_VALS'].str.replace(r'[<=]',''), labs.NEW_HIGH) # NEW_LOW = -999, NEW_HIGH = TAU
# # put REF_X into NEW_X
# labs['NEW_LOW'] = np.where(labs.NEW_LOW.isnull(), labs['REF_LOW'], labs.NEW_LOW)
# labs['NEW_HIGH'] = np.where(labs.NEW_HIGH.isnull(), labs['REF_HIGH'], labs.NEW_HIGH)
# labs['NEW_HIGH'] = labs['NEW_HIGH'].astype(float)
# labs['NEW_LOW'] = labs['NEW_LOW'].astype(float)    
# labs['REF_UNIT'] = labs['REF_UNIT'].str.lower()

# labs['RESULT_FLAG'] = 'Normal'
# labs['RESULT_FLAG'] = np.where(labs['ORD_NUM_VALUE'] < labs.NEW_LOW, 'Low', labs['RESULT_FLAG'])
# labs['RESULT_FLAG'] = np.where(labs['ORD_NUM_VALUE'] > labs.NEW_HIGH, 'High', labs['RESULT_FLAG'])

labs[['AUTO_ID', 'ORDER_DATE', 'RESULT_DATE', 'RESULT_FLAG', 'GROUP', 'NEW_LOW', 'NEW_HIGH', 'ORD_NUM_VALUE']].to_csv(os.path.join(OUT_DATA_PATH, 'labs.csv'))
81/113:
# labs = pd.read_csv(os.path.join(IN_DATA_PATH,'filtered_labs.csv'), parse_dates=['ORDER_DATE','RESULT_DATE'])
# labs = labs[['AUTO_ID', 'ORDER_DATE', 'RESULT_DATE', 'ORD_NUM_VALUE', 'REF_LOW', 'REF_HIGH', 'REF_NORMAL_VALS', 'REF_UNIT',
#        'RESULT_FLAG', 'GROUP']]

# # drop invalid rows
# labs = labs.drop(labs.index[np.logical_or(labs['ORD_NUM_VALUE'].isnull(), labs['ORD_NUM_VALUE']>=9999999)]) # Drop rows with invalid readings

# # === clean messy readings data ====
# # copy REF_LOW values with <,- to REF_NORMAL_VALS. WILL PROCESS THEM 
# labs['REF_NORMAL_VALS'] = np.where(labs['REF_LOW'].str.contains(r'[<-]')==True, labs['REF_LOW'], labs['REF_NORMAL_VALS'])
# # replace invalid values with nan
# labs['REF_HIGH'] = np.where(labs['REF_HIGH'].str.contains(r'[a-zA-Z<]')==True, np.nan, labs['REF_HIGH'])
# labs['REF_LOW'] = np.where(labs['REF_LOW'].str.contains(r'[a-zA-Z<\-]')==True, np.nan, labs['REF_LOW'])
# # drop garbage rows
# labs.drop(labs[labs['REF_NORMAL_VALS'].str.contains(r'[a-zA-Z]')==True].index, inplace=True)
# # extract LOW, HIGH by splitting REF_NORMAL_VALS
# labs[['NEW_LOW','NEW_HIGH']] = labs[labs['REF_NORMAL_VALS'].str.contains('-')==True]['REF_NORMAL_VALS'].str.split('-').apply(pd.Series, 1) # solve cases where REF_LOW is like 0-10
# labs['NEW_LOW'] = np.where(np.logical_and(labs['REF_LOW'].isnull(), labs['REF_NORMAL_VALS'].str.contains(r'[<=]')==True), -999, labs.NEW_LOW) # solve cases where REF_LOW is like '<TAU', anything below TAU is normal => x is normal if -999 < x < TAU. 
# labs['NEW_HIGH'] = np.where(labs['REF_NORMAL_VALS'].str.contains(r'[<=]'), labs['REF_NORMAL_VALS'].str.replace(r'[<=]',''), labs.NEW_HIGH) # NEW_LOW = -999, NEW_HIGH = TAU
# # put REF_X into NEW_X
# labs['NEW_LOW'] = np.where(labs.NEW_LOW.isnull(), labs['REF_LOW'], labs.NEW_LOW)
# labs['NEW_HIGH'] = np.where(labs.NEW_HIGH.isnull(), labs['REF_HIGH'], labs.NEW_HIGH)
# labs['NEW_HIGH'] = labs['NEW_HIGH'].astype(float)
# labs['NEW_LOW'] = labs['NEW_LOW'].astype(float)    
# labs['REF_UNIT'] = labs['REF_UNIT'].str.lower()

# labs['RESULT_FLAG'] = 'Normal'
# labs['RESULT_FLAG'] = np.where(labs['ORD_NUM_VALUE'] < labs.NEW_LOW, 'Low', labs['RESULT_FLAG'])
# labs['RESULT_FLAG'] = np.where(labs['ORD_NUM_VALUE'] > labs.NEW_HIGH, 'High', labs['RESULT_FLAG'])

labs = labs[['AUTO_ID', 'RESULT_DATE', 'RESULT_FLAG', 'GROUP', 'NEW_LOW', 'NEW_HIGH', 'ORD_NUM_VALUE']].drop_duplicates(['AUTO_ID', 'RESULT_DATE', 'RESULT_FLAG', 'GROUP'])
labs.to_csv(os.path.join(OUT_DATA_PATH, 'labs.csv'))
81/114:
# labs = pd.read_csv(os.path.join(IN_DATA_PATH,'filtered_labs.csv'), parse_dates=['ORDER_DATE','RESULT_DATE'])
# labs = labs[['AUTO_ID', 'ORDER_DATE', 'RESULT_DATE', 'ORD_NUM_VALUE', 'REF_LOW', 'REF_HIGH', 'REF_NORMAL_VALS', 'REF_UNIT',
#        'RESULT_FLAG', 'GROUP']]

# # drop invalid rows
# labs = labs.drop(labs.index[np.logical_or(labs['ORD_NUM_VALUE'].isnull(), labs['ORD_NUM_VALUE']>=9999999)]) # Drop rows with invalid readings

# # === clean messy readings data ====
# # copy REF_LOW values with <,- to REF_NORMAL_VALS. WILL PROCESS THEM 
# labs['REF_NORMAL_VALS'] = np.where(labs['REF_LOW'].str.contains(r'[<-]')==True, labs['REF_LOW'], labs['REF_NORMAL_VALS'])
# # replace invalid values with nan
# labs['REF_HIGH'] = np.where(labs['REF_HIGH'].str.contains(r'[a-zA-Z<]')==True, np.nan, labs['REF_HIGH'])
# labs['REF_LOW'] = np.where(labs['REF_LOW'].str.contains(r'[a-zA-Z<\-]')==True, np.nan, labs['REF_LOW'])
# # drop garbage rows
# labs.drop(labs[labs['REF_NORMAL_VALS'].str.contains(r'[a-zA-Z]')==True].index, inplace=True)
# # extract LOW, HIGH by splitting REF_NORMAL_VALS
# labs[['NEW_LOW','NEW_HIGH']] = labs[labs['REF_NORMAL_VALS'].str.contains('-')==True]['REF_NORMAL_VALS'].str.split('-').apply(pd.Series, 1) # solve cases where REF_LOW is like 0-10
# labs['NEW_LOW'] = np.where(np.logical_and(labs['REF_LOW'].isnull(), labs['REF_NORMAL_VALS'].str.contains(r'[<=]')==True), -999, labs.NEW_LOW) # solve cases where REF_LOW is like '<TAU', anything below TAU is normal => x is normal if -999 < x < TAU. 
# labs['NEW_HIGH'] = np.where(labs['REF_NORMAL_VALS'].str.contains(r'[<=]'), labs['REF_NORMAL_VALS'].str.replace(r'[<=]',''), labs.NEW_HIGH) # NEW_LOW = -999, NEW_HIGH = TAU
# # put REF_X into NEW_X
# labs['NEW_LOW'] = np.where(labs.NEW_LOW.isnull(), labs['REF_LOW'], labs.NEW_LOW)
# labs['NEW_HIGH'] = np.where(labs.NEW_HIGH.isnull(), labs['REF_HIGH'], labs.NEW_HIGH)
# labs['NEW_HIGH'] = labs['NEW_HIGH'].astype(float)
# labs['NEW_LOW'] = labs['NEW_LOW'].astype(float)    
# labs['REF_UNIT'] = labs['REF_UNIT'].str.lower()

# labs['RESULT_FLAG'] = 'Normal'
# labs['RESULT_FLAG'] = np.where(labs['ORD_NUM_VALUE'] < labs.NEW_LOW, 'Low', labs['RESULT_FLAG'])
# labs['RESULT_FLAG'] = np.where(labs['ORD_NUM_VALUE'] > labs.NEW_HIGH, 'High', labs['RESULT_FLAG'])

labs = labs[['AUTO_ID', 'RESULT_DATE', 'RESULT_FLAG', 'GROUP', 'NEW_LOW', 'NEW_HIGH', 'ORD_NUM_VALUE']].drop_duplicates(['AUTO_ID', 'RESULT_DATE', 'RESULT_FLAG', 'GROUP'])
labs.to_csv(os.path.join(OUT_DATA_PATH, 'labs.csv'))
81/115:
enc_df = pd.read_csv(os.path.join(IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'CONSULT'})
ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE']]
# ENC_LONG.to_pickle('longitudinal\\longitudinal_encounters.pkl')
81/116: pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'])
81/117: pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC_')
81/118: pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC')
81/119:
enc_df = pd.read_csv(os.path.join(IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'CONSULT'})
ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE']]
pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC').to_csv(os.path.join(OUT_DATA_PATH, 'encounters.csv'))
81/120:
sibdq = pd.read_csv(os.path.join(IN_DATA_PATH, 'deid_IBD_Registry_BA1951_SIBDQ_Pain_Questionnaires_2018-07-05-10-02-02.csv', parse_dates=['CONTACT_DATE']).drop(['Unnamed: 0', 'ENC_TYPE'], axis=1).dropna(how='all')
sibdq.drop(sibdq.index[sibdq.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
sibdq.AUTO_ID = pd.to_numeric(sibdq.AUTO_ID).astype(int)
sibdq['PAIN_TODAY'] = sibdq['PAIN_TODAY'].fillna('No')

#compute missing total scores
q_cols = [x for x in sibdq.columns if x.startswith('Q')]
# parse responses as integers
sibdq[q_cols] = sibdq[q_cols].fillna('0').applymap(lambda st: int(st[0]))
# fill empty totals
null_scores_ix = sibdq[sibdq.SIBDQ_TOTAL_SCORE.isnull()].index
sibdq.loc[null_scores_ix, 'SIBDQ_TOTAL_SCORE'] = sibdq.loc[null_scores_ix][q_cols].sum(axis=1).astype(int)
sibdq.SIBDQ_TOTAL_SCORE = sibdq.SIBDQ_TOTAL_SCORE.astype(int)
sibdq['PAIN_TODAY'] = sibdq['PAIN_TODAY'].replace({'Yes':1, 'No':-1})
81/121:
sibdq = pd.read_csv(os.path.join(IN_DATA_PATH, 'deid_IBD_Registry_BA1951_SIBDQ_Pain_Questionnaires_2018-07-05-10-02-02.csv'), parse_dates=['CONTACT_DATE']).drop(['Unnamed: 0', 'ENC_TYPE'], axis=1).dropna(how='all')
sibdq.drop(sibdq.index[sibdq.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
sibdq.AUTO_ID = pd.to_numeric(sibdq.AUTO_ID).astype(int)
sibdq['PAIN_TODAY'] = sibdq['PAIN_TODAY'].fillna('No')

#compute missing total scores
q_cols = [x for x in sibdq.columns if x.startswith('Q')]
# parse responses as integers
sibdq[q_cols] = sibdq[q_cols].fillna('0').applymap(lambda st: int(st[0]))
# fill empty totals
null_scores_ix = sibdq[sibdq.SIBDQ_TOTAL_SCORE.isnull()].index
sibdq.loc[null_scores_ix, 'SIBDQ_TOTAL_SCORE'] = sibdq.loc[null_scores_ix][q_cols].sum(axis=1).astype(int)
sibdq.SIBDQ_TOTAL_SCORE = sibdq.SIBDQ_TOTAL_SCORE.astype(int)
sibdq['PAIN_TODAY'] = sibdq['PAIN_TODAY'].replace({'Yes':1, 'No':-1})
81/122: sibdq
81/123:
sibdq = pd.read_csv(os.path.join(IN_DATA_PATH, 'deid_IBD_Registry_BA1951_SIBDQ_Pain_Questionnaires_2018-07-05-10-02-02.csv'), parse_dates=['CONTACT_DATE']).drop(['Unnamed: 0', 'ENC_TYPE'], axis=1).dropna(how='all')
sibdq.drop(sibdq.index[sibdq.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
sibdq.AUTO_ID = pd.to_numeric(sibdq.AUTO_ID).astype(int)
sibdq['PAIN_TODAY'] = sibdq['PAIN_TODAY'].fillna('No')

#compute missing total scores
q_cols = [x for x in sibdq.columns if x.startswith('Q')]
# parse responses as integers
sibdq[q_cols] = sibdq[q_cols].fillna('0').applymap(lambda st: int(st[0]))
# fill empty totals
null_scores_ix = sibdq[sibdq.SIBDQ_TOTAL_SCORE.isnull()].index
sibdq.loc[null_scores_ix, 'SIBDQ_TOTAL_SCORE'] = sibdq.loc[null_scores_ix][q_cols].sum(axis=1).astype(int)
sibdq.SIBDQ_TOTAL_SCORE = sibdq.SIBDQ_TOTAL_SCORE.astype(int)
sibdq['PAIN_TODAY'] = sibdq['PAIN_TODAY'].replace({'Yes':1, 'No':0})
81/124: sibdq
81/125:
sibdq = pd.read_csv(os.path.join(IN_DATA_PATH, 'deid_IBD_Registry_BA1951_SIBDQ_Pain_Questionnaires_2018-07-05-10-02-02.csv'), parse_dates=['CONTACT_DATE']).drop(['Unnamed: 0', 'ENC_TYPE'], axis=1).dropna(how='all')
sibdq.drop(sibdq.index[sibdq.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
sibdq.AUTO_ID = pd.to_numeric(sibdq.AUTO_ID).astype(int)
sibdq['PAIN_TODAY'] = sibdq['PAIN_TODAY'].fillna('No')

#compute missing total scores
q_cols = [x for x in sibdq.columns if x.startswith('Q')]
# parse responses as integers
sibdq[q_cols] = sibdq[q_cols].fillna('0').applymap(lambda st: int(st[0]))
# fill empty totals
null_scores_ix = sibdq[sibdq.SIBDQ_TOTAL_SCORE.isnull()].index
sibdq.loc[null_scores_ix, 'SIBDQ_TOTAL_SCORE'] = sibdq.loc[null_scores_ix][q_cols].sum(axis=1).astype(int)
sibdq.SIBDQ_TOTAL_SCORE = sibdq.SIBDQ_TOTAL_SCORE.astype(int)
sibdq['PAIN_TODAY'] = sibdq['PAIN_TODAY'].replace({'Yes':1, 'No':0})
sibdq.to_csv(os.path.join(OUT_DATA_PATH, 'sibdq.csv'))
81/126:
def get_inflation_data():
    #http://www.usinflationcalculator.com/inflation/current-inflation-rates/
    #I took it from the December column as it is explained on the website (last 12 months inflation)
    #year,inflation
    inflationrates={2005:3.4,
                    2006:2.5,
                    2007:4.1,
                    2008:0.1,
                    2009:2.7,
                    2010:1.5,
                    2011:3.0,
                    2012:1.7,
                    2013:1.5,
                    2014:0.8,
                    2015:0.7,
                    2016:2.1,
                    2017:2.1 } #last update: last charges made in 2018, so need to take into account only till 2017
    for (year,rate) in inflationrates.items():
        inflationrates[year]=rate*0.01+1
    
    #calculate coefficients    
    lastyear=np.max(list(inflationrates.keys()))
    inflation_coeff={lastyear+1:1.0}
    for year in range(lastyear, 2005, -1):
        inflation_coeff[year]=inflation_coeff[year+1]*inflationrates[year]
    return inflation_coeff
81/127:
inflation_coeff = get_inflation_data()
#INPATIENT
inpatient_charges_txn = pd.read_csv("rawdata\\deid_IP_Charges_Aug_2018_12_6_18.csv",thousands=r',', parse_dates=['ADMISSION DATE', 'DISCHARGE DATE']).drop('Unnamed: 0', axis=1)
inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
# inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
#adjust for inflation
inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL CHARGES']*inflation_coeff[row['DISCHARGE DATE'].year], axis=1)
inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)
inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION DATE', 'DISCHARGE DATE', 'INPATIENT_CHARGES']]

inpatient_charges
81/128:
inflation_coeff = get_inflation_data()
#INPATIENT
inpatient_charges_txn = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION DATE', 'DISCHARGE DATE']).drop('Unnamed: 0', axis=1)
inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
# inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
#adjust for inflation
inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL CHARGES']*inflation_coeff[row['DISCHARGE DATE'].year], axis=1)
inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)
inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION DATE', 'DISCHARGE DATE', 'INPATIENT_CHARGES']]

inpatient_charges
81/129: inpatient_charges['DISCHARGE DATE'] - inpatient_charges['ADMISSION DATE']
81/130: (inpatient_charges['DISCHARGE DATE'] - inpatient_charges['ADMISSION DATE']).dt.days
81/131:
inflation_coeff = get_inflation_data()
#INPATIENT
inpatient_charges_txn = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION DATE', 'DISCHARGE DATE']).drop('Unnamed: 0', axis=1)
inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
# inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
#adjust for inflation
inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE DATE'].year], axis=1)
inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

inpatient_charges['DAYS_ADMITTED'] = (inpatient_charges['DISCHARGE_DATE'] - inpatient_charges['ADMISSION_DATE']).dt.days
inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'INPATIENT_CHARGES']]

inpatient_charges
81/132:
inflation_coeff = get_inflation_data()
#INPATIENT
inpatient_charges_txn = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
# inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
#adjust for inflation
inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE DATE'].year], axis=1)
inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

inpatient_charges['DAYS_ADMITTED'] = (inpatient_charges['DISCHARGE_DATE'] - inpatient_charges['ADMISSION_DATE']).dt.days
inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'INPATIENT_CHARGES']]

inpatient_charges
81/133:
inflation_coeff = get_inflation_data()
#INPATIENT
inpatient_charges_txn = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
#adjust for inflation
inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE DATE'].year], axis=1)
inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

inpatient_charges['DAYS_ADMITTED'] = (inpatient_charges['DISCHARGE_DATE'] - inpatient_charges['ADMISSION_DATE']).dt.days
inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'INPATIENT_CHARGES']]

inpatient_charges
81/134:
inflation_coeff = get_inflation_data()
#INPATIENT
inpatient_charges_txn = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
#adjust for inflation
inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

inpatient_charges['DAYS_ADMITTED'] = (inpatient_charges['DISCHARGE_DATE'] - inpatient_charges['ADMISSION_DATE']).dt.days
inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'INPATIENT_CHARGES']]

inpatient_charges
81/135:
inflation_coeff = get_inflation_data()
#INPATIENT
inpatient_charges_txn = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
#adjust for inflation
inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

inpatient_charges['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'INPATIENT_CHARGES']]

inpatient_charges
81/136:
inflation_coeff = get_inflation_data()
#INPATIENT
inpatient_charges_txn = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
#adjust for inflation
inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'INPATIENT_CHARGES']]

inpatient_charges
81/137:
inflation_coeff = get_inflation_data()
#INPATIENT
inpatient_charges_txn = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
#adjust for inflation
inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]

inpatient_charges
81/138:
inflation_coeff = get_inflation_data()
#INPATIENT
inpatient_charges_txn = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
#adjust for inflation
inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]

inpatient_charges.to_csv(os.path.join(OUT_DATA_PATH, 'inpatient_charges.csv'))
81/139:
# OUTPATIENT
outpatient_charges_transactions = pd.read_csv("rawdata\\deid_OP_charges_2018.csv", parse_dates=['ORIG_SERVICE_DATE'])
outpatient_charges=outpatient_charges_transactions[outpatient_charges_transactions['AMOUNT']>0]
outpatient_charges.drop(outpatient_charges[outpatient_charges.AUTO_ID=='NO_MATCH'].index, inplace=True)
outpatient_charges = outpatient_charges[outpatient_charges.DX.notnull()]
outpatient_charges.AUTO_ID=outpatient_charges.AUTO_ID.astype(int)

outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
outpatient_charges
# outpatient_charges = outpatient_charges[['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT']]
81/140:
# OUTPATIENT
outpatient_charges_transactions = pd.read_csv(os.path.join(IN_DATA_PATH, "rawdata\\deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
outpatient_charges=outpatient_charges_transactions[outpatient_charges_transactions['AMOUNT']>0]
outpatient_charges.drop(outpatient_charges[outpatient_charges.AUTO_ID=='NO_MATCH'].index, inplace=True)
outpatient_charges = outpatient_charges[outpatient_charges.DX.notnull()]
outpatient_charges.AUTO_ID=outpatient_charges.AUTO_ID.astype(int)

outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
outpatient_charges
# outpatient_charges = outpatient_charges[['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT']]
81/141:
# OUTPATIENT
outpatient_charges_transactions = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
outpatient_charges=outpatient_charges_transactions[outpatient_charges_transactions['AMOUNT']>0]
outpatient_charges.drop(outpatient_charges[outpatient_charges.AUTO_ID=='NO_MATCH'].index, inplace=True)
outpatient_charges = outpatient_charges[outpatient_charges.DX.notnull()]
outpatient_charges.AUTO_ID=outpatient_charges.AUTO_ID.astype(int)

outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
outpatient_charges
# outpatient_charges = outpatient_charges[['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT']]
81/142:
# # OUTPATIENT
# outpatient_charges_transactions = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
# outpatient_charges=outpatient_charges_transactions[outpatient_charges_transactions['AMOUNT']>0]
# outpatient_charges.drop(outpatient_charges[outpatient_charges.AUTO_ID=='NO_MATCH'].index, inplace=True)
# outpatient_charges = outpatient_charges[outpatient_charges.DX.notnull()]
# outpatient_charges.AUTO_ID=outpatient_charges.AUTO_ID.astype(int)

# outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
op = outpatient_charges.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE']).AMOUNT.sum().reset_index()
op
# outpatient_charges = outpatient_charges[['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT']]
81/143:
# # OUTPATIENT
# outpatient_charges_transactions = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
# outpatient_charges=outpatient_charges_transactions[outpatient_charges_transactions['AMOUNT']>0]
# outpatient_charges.drop(outpatient_charges[outpatient_charges.AUTO_ID=='NO_MATCH'].index, inplace=True)
# outpatient_charges = outpatient_charges[outpatient_charges.DX.notnull()]
# outpatient_charges.AUTO_ID=outpatient_charges.AUTO_ID.astype(int)

# outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
op = outpatient_charges.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE']).AMOUNT.sum().reset_index()
op.to_csv(os.path.join(OUT_DATA_PATH, 'outpatient_charges.csv'))
81/144: op[op.AUTO_ID==0]
81/145: enc_df[enc_df.AUTO_ID==0]
81/146: labs[labs.AUTO_ID==0]
81/147: op[op.AUTO_ID==0].head()
81/148: labs[labs.AUTO_ID==0].head()
81/149: inpatient_charges[inpatient_charges.AUTO_ID==0].head()
81/150: outpatient_charges[outpatient_charges.AUTO_ID==0]
81/151: outpatient_charges[outpatient_charges.AUTO_ID==0].iloc[7:]
81/152: outpatient_charges = normalize_dx_codes(outpatient_charges)
81/153:
def normalize_dx_codes(opc):
    opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
    opc2 = opc[~opc.index.isin(opc1.index)]

    dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
    dx = dx.droplevel(-1)
    dx.name = 'DX'
    del opc1['DX']
    opc1 = opc1.join(dx)
    opc1 = opc1[opc2.columns]
    opc = pd.concat([opc1, opc2])
    
    opc['DX2'] = opc.DX.str.replace('.', '')
    icd = pd.read_csv('rawdata\\icd9_codes.csv')
    del icd['Unnamed: 3']
    opc3 = opc.merge(icd, 'left', left_on='DX2', right_on='DIAGNOSIS CODE')
    
    return opc3
81/154: outpatient_charges = normalize_dx_codes(outpatient_charges)
81/155:
op0 = outpatient_charges[outpatient_charges.AUTO_ID==0]
op0 = normalize_dx_codes(op0)
81/156: outpatient_charges[outpatient_charges.AUTO_ID==0]
81/157:
# outpatient_charges[outpatient_charges.AUTO_ID==0]
opc = outpatient_charges

opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
opc2 = opc[~opc.index.isin(opc1.index)]

dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
dx = dx.droplevel(-1)
dx.name = 'DX'
del opc1['DX']
opc1 = opc1.join(dx)
opc1 = opc1[opc2.columns]
opc = pd.concat([opc1, opc2])

opc
81/158:
# outpatient_charges[outpatient_charges.AUTO_ID==0]
opc = outpatient_charges

opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
opc2 = opc[~opc.index.isin(opc1.index)]

dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
dx = dx.droplevel(-1)
dx.name = 'DX'
del opc1['DX']
opc1 = opc1.join(dx)
opc1 = opc1[opc2.columns]
opc = pd.concat([opc1, opc2])

opc.DX.value_counts()
81/159: opc.head().DX.between(10, 1000)
81/160: opc.head().DX.astype(float).between(10, 1000)
81/161:
tup = (630, 780)
opc.head().DX.astype(float).between(tup)
81/162:
tup = (630, 780)
opc.head().DX.astype(float).between(*tup)
81/163: np.logical_or([True, False, True])
81/164: np.logical_or(True, False, True)
81/165:
x=[True, False, True]
np.logical_or(x==True)
81/166:
x=[True, False, True]
any(x)
81/167:
x=[False, False, True]
any(x)
81/168:
x=[False, False, False]
any(x)
81/169:
bad_codes = [(630, 780), (460, 520), (280, 290)]
opc.DX = opc.DX.astype(float)
81/170:
bad_codes = [(630, 780), (460, 520), (280, 290)]
opc_num = opc[~(opc.DX.startswith('V') | opc.DX.startswith('E'))]
opc_num.DX = opc_num.DX.astype(float)
81/171:
bad_codes = [(630, 780), (460, 520), (280, 290)]
opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
opc_num.DX = opc_num.DX.astype(float)
81/172:
bad_codes = [(630, 780), (460, 520), (280, 290)]
opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E') | opc.DX=='')]
opc_num.DX = opc_num.DX.astype(float)
81/173: opc
81/174: opc.DX.isnull().sum()
81/175:
bad_codes = [(630, 780), (460, 520), (280, 290)]
opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
opc_num.DX = opc_num.DX.astype(float)
81/176: opc_num.DX.isnull().sum()
81/177: opc_num.DX
81/178: opc_num.DX==''
81/179: opc[opc_num.DX=='']
81/180: opc_num[opc_num.DX=='']
81/181: opc = opc[opc.DX!='']
81/182:
bad_codes = [(630, 780), (460, 520), (280, 290)]
opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
opc_num.DX = opc_num.DX.astype(float)
81/183: [opc_num.head().DX.between(*tup) for tup in bad_codes]
81/184: [opc_num.head().DX.between(*tup).values for tup in bad_codes]
81/185: [any(opc_num.head().DX.between(*tup).values) for tup in bad_codes]
81/186: [opc_num.head().DX.between(*tup).values for tup in bad_codes]
81/187: [opc_num.head().DX.between(*tup).values for tup in bad_codes].T
81/188: np.array([opc_num.head().DX.between(*tup).values for tup in bad_codes]).T
81/189: map(any, np.array([opc_num.head().DX.between(*tup).values for tup in bad_codes]).T)
81/190: list(map(any, np.array([opc_num.head().DX.between(*tup).values for tup in bad_codes]).T))
81/191:
bad_rows = list(map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_codes]).T))
bad_df = opc[bad_rows]
bad_df
81/192:
bad_rows = list(map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_codes]).T))
bad_df = opc_num[bad_rows]
bad_df
81/193:
def separate_dx_codes(opc):
    opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
    opc2 = opc[~opc.index.isin(opc1.index)]
    dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
    dx = dx.droplevel(-1)
    dx.name = 'DX'
    del opc1['DX']
    opc1 = opc1.join(dx)
    opc1 = opc1[opc2.columns]
    opc = pd.concat([opc1, opc2])
    return opc


# OUTPATIENT
outpatient_charges_transactions = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
outpatient_charges=outpatient_charges_transactions[outpatient_charges_transactions['AMOUNT']>0]
outpatient_charges.drop(outpatient_charges[outpatient_charges.AUTO_ID=='NO_MATCH'].index, inplace=True)
outpatient_charges = outpatient_charges[outpatient_charges.DX.notnull()]
outpatient_charges.AUTO_ID=outpatient_charges.AUTO_ID.astype(int)

outpatient_charges = normalize_dx_codes(outpatient_charges)
# filter out DX we KNOW are unrelated
opc = outpatient_charges[outpatient_charges.DX!='']
bad_codes = [(630, 780), (460, 520), (280, 290)]
opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
opc_num.DX = opc_num.DX.astype(float)
opc_alpha = opc[~opc_num.index]
bad_rows = list(map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_codes]).T)) # True if DX falls in any bad_code ranges
opc_ok = opc_num[~bad_rows]
outpatient_charges_transactions = pd.concat([opc_ok, opc_alpha]).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT'])


outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE']).AMOUNT.sum().reset_index()
op.to_csv(os.path.join(OUT_DATA_PATH, 'outpatient_charges.csv'))
81/194:
def separate_dx_codes(opc):
    opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
    opc2 = opc[~opc.index.isin(opc1.index)]
    dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
    dx = dx.droplevel(-1)
    dx.name = 'DX'
    del opc1['DX']
    opc1 = opc1.join(dx)
    opc1 = opc1[opc2.columns]
    opc = pd.concat([opc1, opc2])
    return opc


# # OUTPATIENT
# outpatient_charges_transactions = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
# outpatient_charges=outpatient_charges_transactions[outpatient_charges_transactions['AMOUNT']>0]
# outpatient_charges.drop(outpatient_charges[outpatient_charges.AUTO_ID=='NO_MATCH'].index, inplace=True)
# outpatient_charges = outpatient_charges[outpatient_charges.DX.notnull()]
# outpatient_charges.AUTO_ID=outpatient_charges.AUTO_ID.astype(int)

outpatient_charges = separate_dx_codes(outpatient_charges)
# filter out DX we KNOW are unrelated
opc = outpatient_charges[outpatient_charges.DX!='']
bad_codes = [(630, 780), (460, 520), (280, 290)]
opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
opc_num.DX = opc_num.DX.astype(float)
opc_alpha = opc[~opc_num.index]
bad_rows = list(map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_codes]).T)) # True if DX falls in any bad_code ranges
opc_ok = opc_num[~bad_rows]
outpatient_charges_transactions = pd.concat([opc_ok, opc_alpha]).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT'])


outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE']).AMOUNT.sum().reset_index()
op.to_csv(os.path.join(OUT_DATA_PATH, 'outpatient_charges.csv'))
81/195: bad_rows
81/196:
def separate_dx_codes(opc):
    opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
    opc2 = opc[~opc.index.isin(opc1.index)]
    dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
    dx = dx.droplevel(-1)
    dx.name = 'DX'
    del opc1['DX']
    opc1 = opc1.join(dx)
    opc1 = opc1[opc2.columns]
    opc = pd.concat([opc1, opc2])
    return opc


# # OUTPATIENT
# outpatient_charges_transactions = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
# outpatient_charges=outpatient_charges_transactions[outpatient_charges_transactions['AMOUNT']>0]
# outpatient_charges.drop(outpatient_charges[outpatient_charges.AUTO_ID=='NO_MATCH'].index, inplace=True)
# outpatient_charges = outpatient_charges[outpatient_charges.DX.notnull()]
# outpatient_charges.AUTO_ID=outpatient_charges.AUTO_ID.astype(int)

# outpatient_charges = separate_dx_codes(outpatient_charges)
# # filter out DX we KNOW are unrelated
# opc = outpatient_charges[outpatient_charges.DX!='']
# bad_codes = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
# opc_num.DX = opc_num.DX.astype(float)
opc_alpha = opc[~opc.index.isin(opc_num.index)]
bad_rows = list(map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_codes]).T)) # True if DX falls in any bad_code ranges
opc_ok = opc_num[~bad_rows]
outpatient_charges_transactions = pd.concat([opc_ok, opc_alpha]).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT'])


outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE']).AMOUNT.sum().reset_index()
op.to_csv(os.path.join(OUT_DATA_PATH, 'outpatient_charges.csv'))
81/197:
def separate_dx_codes(opc):
    opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
    opc2 = opc[~opc.index.isin(opc1.index)]
    dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
    dx = dx.droplevel(-1)
    dx.name = 'DX'
    del opc1['DX']
    opc1 = opc1.join(dx)
    opc1 = opc1[opc2.columns]
    opc = pd.concat([opc1, opc2])
    return opc


# # OUTPATIENT
# outpatient_charges_transactions = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
# outpatient_charges=outpatient_charges_transactions[outpatient_charges_transactions['AMOUNT']>0]
# outpatient_charges.drop(outpatient_charges[outpatient_charges.AUTO_ID=='NO_MATCH'].index, inplace=True)
# outpatient_charges = outpatient_charges[outpatient_charges.DX.notnull()]
# outpatient_charges.AUTO_ID=outpatient_charges.AUTO_ID.astype(int)

# outpatient_charges = separate_dx_codes(outpatient_charges)
# # filter out DX we KNOW are unrelated
# opc = outpatient_charges[outpatient_charges.DX!='']
# bad_codes = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
# opc_num.DX = opc_num.DX.astype(float)
opc_alpha = opc[~opc.index.isin(opc_num.index)]
bad_rows = list(map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_codes]).T)) # True if DX falls in any bad_code ranges
opc_ok = opc_num[~pd.Series(bad_rows)]
outpatient_charges_transactions = pd.concat([opc_ok, opc_alpha]).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT'])


outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE']).AMOUNT.sum().reset_index()
op.to_csv(os.path.join(OUT_DATA_PATH, 'outpatient_charges.csv'))
81/198: outpatient_charges_transactions
81/199: outpatient_charges_transactions.shape
81/200: outpatient_charges_transactions
81/201: outpatient_charges_transactions[outpatient_charges_transactions.AUTO_ID==0]
81/202: bad_rows
81/203: ~pd.Series(bad_rows)
81/204: opc_num
81/205: bad_rows
81/206: np.array([opc_num.DX.between(*tup).values for tup in bad_code_ranges]).T
81/207:
bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
np.array([opc_num.DX.between(*tup).values for tup in bad_code_ranges]).T
81/208:
bad_code_ranges = [(630, 780), (460, 520), (280, 290)]

np.array([opc_num.DX.between(tup[0],tup[1]) for tup in bad_code_ranges]).T
81/209:
bad_code_ranges = [(630, 780), (460, 520), (280, 290)]

np.array([opc_num.DX.between(tup[0],tup[1]) for tup in bad_code_ranges]).
81/210:
bad_code_ranges = [(630, 780), (460, 520), (280, 290)]

np.array([opc_num.DX.between(tup[0],tup[1]) for tup in bad_code_ranges])
81/211:
bad_code_ranges = [(630, 780), (460, 520), (280, 290)]

# np.array([opc_num.DX.between(tup[0],tup[1]) for tup in bad_code_ranges])
opc_num.DX.between(*bad_code_ranges[0])
81/212:
bad_code_ranges = [(630, 780), (460, 520), (280, 290)]

# np.array([opc_num.DX.between(tup[0],tup[1]) for tup in bad_code_ranges])
opc_num#.DX.between(*bad_code_ranges[0])
81/213:
def separate_dx_codes(opc):
    opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
    opc2 = opc[~opc.index.isin(opc1.index)]
    dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
    dx = dx.droplevel(-1)
    dx.name = 'DX'
    del opc1['DX']
    opc1 = opc1.join(dx)
    opc1 = opc1[opc2.columns]
    opc = pd.concat([opc1, opc2]).reset_index()
    return opc


# OUTPATIENT
outpatient = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
outpatient = outpatient[outpatient['AMOUNT']>0]
outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)

outpatient = separate_dx_codes(outpatient)
opc = outpatient[outpatient.DX!='']
bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
opc_alpha = opc[~opc.index.isin(opc_num.index)]

opc_num.DX = opc_num.DX.astype(float)
bad_rows = list(map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_code_ranges]).T)) # True if DX falls in any bad_code ranges
opc_ok = opc_num[~pd.Series(bad_rows)]
outpatient_charges_transactions = pd.concat([opc_ok, opc_alpha]).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT'])


outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE']).AMOUNT.sum().reset_index()
op.to_csv(os.path.join(OUT_DATA_PATH, 'outpatient_charges.csv'))
81/214: outpatient
81/215:
l = [True, False, True]
map(not, l)
81/216:
l = [True, False, True]
not(l)
81/217:
l = [True, False, True]
[not x for x in l]
81/218:
# def separate_dx_codes(opc):
#     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
#     opc2 = opc[~opc.index.isin(opc1.index)]
#     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
#     dx = dx.droplevel(-1)
#     dx.name = 'DX'
#     del opc1['DX']
#     opc1 = opc1.join(dx)
#     opc1 = opc1[opc2.columns]
#     opc = pd.concat([opc1, opc2]).reset_index()
#     return opc


# # OUTPATIENT
# outpatient = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
# outpatient = outpatient[outpatient['AMOUNT']>0]
# outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
# outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
# outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)

# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
# opc_alpha = opc[~opc.index.isin(opc_num.index)]

# opc_num.DX = opc_num.DX.astype(float)
# bad_rows = list(map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_code_ranges]).T)) # True if DX falls in any bad_code ranges
opc_ok = opc_num[[not x for x in bad_rows]]
outpatient_charges_transactions = pd.concat([opc_ok, opc_alpha]).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT'])


outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE']).AMOUNT.sum().reset_index()
op.to_csv(os.path.join(OUT_DATA_PATH, 'outpatient_charges.csv'))
81/219: outpatient_charges_transactions
81/220:
# def separate_dx_codes(opc):
#     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
#     opc2 = opc[~opc.index.isin(opc1.index)]
#     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
#     dx = dx.droplevel(-1)
#     dx.name = 'DX'
#     del opc1['DX']
#     opc1 = opc1.join(dx)
#     opc1 = opc1[opc2.columns]
#     opc = pd.concat([opc1, opc2]).reset_index()
#     return opc


# # OUTPATIENT
# outpatient = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
# outpatient = outpatient[outpatient['AMOUNT']>0]
# outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
# outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
# outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)

# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
# opc_alpha = opc[~opc.index.isin(opc_num.index)]

# opc_num.DX = opc_num.DX.astype(float)
# bad_rows = list(map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_code_ranges]).T)) # True if DX falls in any bad_code ranges
# opc_ok = opc_num[[not x for x in bad_rows]]
# outpatient_charges_transactions = pd.concat([opc_ok, opc_alpha]).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT'])


# outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT', 'DX', 'PROC_NAME' ]).AMOUNT.sum().reset_index()
op.to_csv(os.path.join(OUT_DATA_PATH, 'outpatient_charges.csv'))
81/221:
# def separate_dx_codes(opc):
#     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
#     opc2 = opc[~opc.index.isin(opc1.index)]
#     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
#     dx = dx.droplevel(-1)
#     dx.name = 'DX'
#     del opc1['DX']
#     opc1 = opc1.join(dx)
#     opc1 = opc1[opc2.columns]
#     opc = pd.concat([opc1, opc2]).reset_index()
#     return opc


# # OUTPATIENT
# outpatient = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
# outpatient = outpatient[outpatient['AMOUNT']>0]
# outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
# outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
# outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)

# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
# opc_alpha = opc[~opc.index.isin(opc_num.index)]

# opc_num.DX = opc_num.DX.astype(float)
# bad_rows = list(map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_code_ranges]).T)) # True if DX falls in any bad_code ranges
# opc_ok = opc_num[[not x for x in bad_rows]]
# outpatient_charges_transactions = pd.concat([opc_ok, opc_alpha]).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT'])


# outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE', 'DX', 'PROC_NAME' ]).AMOUNT.sum().reset_index()
op.to_csv(os.path.join(OUT_DATA_PATH, 'outpatient_charges.csv'))
81/222:
# def separate_dx_codes(opc):
#     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
#     opc2 = opc[~opc.index.isin(opc1.index)]
#     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
#     dx = dx.droplevel(-1)
#     dx.name = 'DX'
#     del opc1['DX']
#     opc1 = opc1.join(dx)
#     opc1 = opc1[opc2.columns]
#     opc = pd.concat([opc1, opc2]).reset_index()
#     return opc


# # OUTPATIENT
# outpatient = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
# outpatient = outpatient[outpatient['AMOUNT']>0]
# outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
# outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
# outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)

# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
# opc_alpha = opc[~opc.index.isin(opc_num.index)]

# opc_num.DX = opc_num.DX.astype(float)
# bad_rows = list(map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_code_ranges]).T)) # True if DX falls in any bad_code ranges
# opc_ok = opc_num[[not x for x in bad_rows]]
# outpatient_charges_transactions = pd.concat([opc_ok, opc_alpha]).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT'])


# outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE' ]).AMOUNT.sum().reset_index()
op.to_csv(os.path.join(OUT_DATA_PATH, 'outpatient_charges.csv'))
81/223: l = labs[labs.AUTO_ID==0]
81/224:
l = labs[labs.AUTO_ID==0]
l
81/225: ip = inpatient_charges[inpatient_charges.AUTO_ID==0]
81/226:
ip = inpatient_charges[inpatient_charges.AUTO_ID==0]
ip
81/227:
ip = inpatient_charges[inpatient_charges.AUTO_ID==0]
inpatient_charges
81/228:
ip = inpatient_charges[inpatient_charges.AUTO_ID==0]
inpatient_charges.AUTO_ID.min()
81/229:
ip = inpatient_charges[inpatient_charges.AUTO_ID==1]
# inpatient_charges.AUTO_ID.min()
81/230:
ip = inpatient_charges[inpatient_charges.AUTO_ID==1]
# inpatient_charges.AUTO_ID.min()
ip
81/231:
ip = inpatient_charges[inpatient_charges.AUTO_ID==1]
# inpatient_charges.AUTO_ID.min()
op[op.AUTO_ID==1]
81/232: ip
81/233: labs[labs.AUTO_ID==1]
83/1:
%matplotlib inline
import pandas as pd
import os
83/2:
ip = pd.read_csv('datasets/clean_registry_data/inpatient_charges.csv')
op = pd.read_csv('datasets/clean_registry_data/outpatient_charges.csv')
83/3: ip.head()
83/4: op.head()
83/5: ip.columns
83/6:
ip = ip[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE','DAYS_ADMITTED', 'INPATIENT_CHARGES']]
ip.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE','DAYS_ADMITTED', 'AMOUNT']
ip.head()
83/7:
ip = ip[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE','DAYS_ADMITTED', 'INPATIENT_CHARGES']]
ip.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE','DAYS_ADMITTED', 'AMOUNT']
ip.to_csv('datasets/clean_registry_data/inpatient_charges.csv', ignore_index=True)
83/8:
# ip = ip[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE','DAYS_ADMITTED', 'INPATIENT_CHARGES']]
# ip.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE','DAYS_ADMITTED', 'AMOUNT']
ip.to_csv('datasets/clean_registry_data/inpatient_charges.csv', ignore_index=True)
83/9:
# ip = ip[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE','DAYS_ADMITTED', 'INPATIENT_CHARGES']]
# ip.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE','DAYS_ADMITTED', 'AMOUNT']
ip.to_csv('datasets/clean_registry_data/inpatient_charges.csv', index=False)
83/10:
op['ADMISSION_DATE'] = op['ORIG_SERVICE_DATE']
op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
op['DAYS_ADMITTED'] = 0
op = op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE','DAYS_ADMITTED', 'AMOUNT']]
op.head()
83/11:
# op['ADMISSION_DATE'] = op['ORIG_SERVICE_DATE']
# op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
# op['DAYS_ADMITTED'] = 0
# op = op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE','DAYS_ADMITTED', 'AMOUNT']]
op.to_csv('datasets/clean_registry_data/outpatient_charges.csv', index=False)
83/12:
ip = pd.read_csv('datasets/clean_registry_data/inpatient_charges.csv')
op = pd.read_csv('datasets/clean_registry_data/outpatient_charges.csv')
83/13: ip.head()
83/14: op.head()
83/15: tot = pd.concat([ip, op]).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
83/16:
# tot = pd.concat([ip, op]).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
tot.head()
83/17:
# tot = pd.concat([ip, op]).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
83/18:
# tot = pd.concat([ip, op]).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
# tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
tot.head(10)
83/19:
# tot = pd.concat([ip, op]).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
# tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
tot.head(100)
83/20:
# tot = pd.concat([ip, op]).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
# tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
tot.loc[12:30]
83/21:
# tot = pd.concat([ip, op]).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
# tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
tot.iloc[12:30]
83/22:
x=tot[tot.AUTO_ID==0]
x['ADMISSION_DATE'].dt.month+'-'+x['ADMISSION_DATE'].dt.year.astype(str)
83/23:
x=tot[tot.AUTO_ID==0]
d = pd.to_datetime(x['ADMISSION_DATE'])
d.dt.month+'-'+d.dt.year.astype(str)
83/24:
x=tot[tot.AUTO_ID==0]
d = pd.to_datetime(x['ADMISSION_DATE'])
print(f"{d.dt.month} {d.dt.year.astype(str)}")
83/25:
x=tot[tot.AUTO_ID==0]
d = pd.to_datetime(x['ADMISSION_DATE'])
d.dt.month.str.cat(d.dt.year.astype(str), sep='-')
83/26:
x=tot[tot.AUTO_ID==0]
d = pd.to_datetime(x['ADMISSION_DATE'])
d.dt.month
83/27:
x=tot[tot.AUTO_ID==0]
d = pd.to_datetime(x['ADMISSION_DATE'])
d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
83/28:
%matplotlib inline
import pandas as pd
import os
import matplotlib.pyplot as plt
83/29:
df=tot[tot.AUTO_ID==0]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
plt.plot(x, y)
83/30:
aids = df.AUTO_ID.values.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
plt.plot(x, y)
83/31:
aids = df.AUTO_ID.unique().values
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
plt.plot(x, y)
83/32:
aids = df.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
plt.plot(x, y)
83/33:
aids = df.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
plt.plot(x, y)
83/34:
aids = df.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
plt.plot(x, y)
83/35: np.random.randint(0,10)
83/36: np.random.randint(0,len(aids))
83/37: len(aids)
83/38:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
plt.plot(x, y)
83/39:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
plt.plot(x, y)
83/40:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
plt.plot(x, y)
83/41:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
plt.plot(x, y)
83/42:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
plt.plot(x, y)
83/43:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
plt.plot(x, y)
83/44:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
plt.plot(x, y)
83/45:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
plt.plot(x, y)
83/46:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
plt.plot(x, y)
83/47:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
plt.plot(x, y)
83/48:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
plt.plot(x, y)
83/49:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
plt.plot(x, y)
83/50:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
plt.plot(x, y)
83/51:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
plt.plot(x, y)
83/52:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
plt.plot(x, y)
83/53:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
plt.plot(x, y)
83/54:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
plt.plot(x, y)
83/55:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
titl = (d.max()-d.min()).months
plt.plot(x, y, title=f"Months: {titl}")
83/56:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
titl = (d.max()-d.min()).days//30
plt.plot(x, y, title=f"Months: {titl}")
83/57:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
titl = (d.max()-d.min()).days//30
px.line(x, y, title=f"Months: {titl}")
83/58:
%matplotlib inline
import pandas as pd
import os
import matplotlib.pyplot as plt
import plotly.express as px
83/59:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
titl = (d.max()-d.min()).days//30
px.line(x, y, title=f"Months: {titl}").show()
83/60:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
titl = (d.max()-d.min()).days//30
plt.plot(x, y)
plt.title(f"Months: {titl}")
83/61:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
titl = (d.max()-d.min()).days//30
plt.plot(x, y)
plt.title(f"Months: {titl}")
83/62:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
titl = (d.max()-d.min()).days//30
plt.plot(x, y)
plt.title(f"Months: {titl}")
83/63:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
titl = (d.max()-d.min()).days//30
plt.plot(x, y)
plt.title(f"Months: {titl}")
83/64:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
titl = (d.max()-d.min()).days//30
plt.plot(x, y)
plt.title(f"Months: {titl}")
83/65:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
titl = (d.max()-d.min()).days//30
plt.plot(x, y)
plt.title(f"Months: {titl}")
83/66:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
titl = (d.max()-d.min()).days//30
plt.plot(x, y)
plt.title(f"Months: {titl}")
83/67:
aids = tot.AUTO_ID.unique()
aid = aids[np.random.randint(0,len(aids))]

df=tot[tot.AUTO_ID==aid]
d = pd.to_datetime(df['ADMISSION_DATE'])
x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
y = df['CUMULATIVE']
titl = (d.max()-d.min()).days//30
plt.plot(x, y)
plt.title(f"Months: {titl}")
83/68:




aids = tot.AUTO_ID.unique()




fig, axes = plt.subplots(3,3, figsize = (12,12))

for i in range(9):
    aid = aids[np.random.randint(0,len(aids))]
    df=tot[tot.AUTO_ID==aid]
    d = pd.to_datetime(df['ADMISSION_DATE'])
    x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
    y = df['CUMULATIVE']
    titl = (d.max()-d.min()).days//30
    axes[i].plot(x, y)
    axes[i].title(f"Months: {titl}")
plt.show()
83/69: axes[0]
83/70:




aids = tot.AUTO_ID.unique()




fig, axes = plt.subplots(3,3, figsize = (12,12))

for r in range(3):
    for c in range(3):
        aid = aids[np.random.randint(0,len(aids))]
        df=tot[tot.AUTO_ID==aid]
        d = pd.to_datetime(df['ADMISSION_DATE'])
        x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
        y = df['CUMULATIVE']
        titl = (d.max()-d.min()).days//30
        axes[r,c].plot(x, y)
        axes[r,c].title(f"Months: {titl}")
plt.show()
83/71:




aids = tot.AUTO_ID.unique()




fig, axes = plt.subplots(3,3, figsize = (12,12))

for r in range(3):
    for c in range(3):
        aid = aids[np.random.randint(0,len(aids))]
        df=tot[tot.AUTO_ID==aid]
        d = pd.to_datetime(df['ADMISSION_DATE'])
        x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
        y = df['CUMULATIVE']
        titl = (d.max()-d.min()).days//30
        axes[r,c].plot(x, y)
        axes[r,c].set_title(f"Months: {titl}")
plt.show()
83/72:




aids = tot.AUTO_ID.unique()




fig, axes = plt.subplots(3,3, figsize = (12,12))

for r in range(3):
    for c in range(3):
        aid = aids[np.random.randint(0,len(aids))]
        df=tot[tot.AUTO_ID==aid]
        d = pd.to_datetime(df['ADMISSION_DATE'])
        x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
        y = df['CUMULATIVE']
        titl = (d.max()-d.min()).days//30
        axes[r,c].plot(x, y)
        axes[r,c].set_title(f"Months: {titl}")
plt.show()
83/73:




aids = tot.AUTO_ID.unique()




fig, axes = plt.subplots(3,3, figsize = (12,12))

for r in range(3):
    for c in range(3):
        aid = aids[np.random.randint(0,len(aids))]
        df=tot[tot.AUTO_ID==aid]
        d = pd.to_datetime(df['ADMISSION_DATE'])
        x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
        y = df['CUMULATIVE']
        titl = (d.max()-d.min()).days//30
        axes[r,c].plot(x, y)
        axes[r,c].set_title(f"AID: {aid}, Months: {titl}")
plt.show()
83/74: x = tot[tot.AUTO_ID==2690]
83/75: tot[tot.AUTO_ID==2690]
83/76:
%matplotlib inline
import pandas as pd
import os
import matplotlib.pyplot as plt
83/77: tot.to_csv('datasets/clean_registry_data/total_charges.csv', index=False)
83/78: tot[tot.AUTO_ID==1133]
83/79: tot[tot.AUTO_ID==1170]
83/80: tot[tot.DAYS_ADMITTED==0].AMOUNT.describe()
83/81: tot[tot.DAYS_ADMITTED==0].AMOUNT.plot()
83/82: tot[tot.DAYS_ADMITTED==0].AMOUNT.plot.hist()
83/83: tot[tot.DAYS_ADMITTED==0].AMOUNT.describe()
83/84: tot[tot.DAYS_ADMITTED==0].sort_values('AMOUNT', ascending=False)
83/85:
op = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
op[op.AUTO_ID==2203]
83/86:
%matplotlib inline
import pandas as pd
import os
import matplotlib.pyplot as plt
import config as cfg
83/87:
op = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
op[op.AUTO_ID==2203]
83/88:
op = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
op[op.AUTO_ID==1606]
83/89:
op = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
ip = pd.read_csv('datasets/clean_registry_data/inpatient_charges.csv')
ip[ip.AUTO_ID==1606]
83/90:
ip = pd.read_csv('datasets/clean_registry_data/inpatient_charges.csv')
op = pd.read_csv('datasets/clean_registry_data/outpatient_charges.csv')
tot = pd.concat([ip, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
tot.to_csv(os.path.join(OUT_DATA_PATH, 'total_charges.csv'), index=False)
83/91:
ip = pd.read_csv('datasets/clean_registry_data/inpatient_charges.csv')
op = pd.read_csv('datasets/clean_registry_data/outpatient_charges.csv')
tot = pd.concat([ip, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)
83/92: tot[tot.DAYS_ADMITTED==0].sort_values('AMOUNT', ascending=False)
83/93:
op = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
ip = pd.read_csv('datasets/clean_registry_data/inpatient_charges.csv')
tot[tot.AUTO_ID==1606]
83/94:
ip = pd.read_csv('datasets/clean_registry_data/inpatient_charges.csv')
op = pd.read_csv('datasets/clean_registry_data/outpatient_charges.csv')
tot = pd.concat([ip, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)
83/95: op[op.AUTO_ID==1606]
83/96: op[op.AUTO_ID==1606].AMOUNT.sum()
83/97:
rop = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
op[op.AUTO_ID==1606]
83/98:
rop = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
op[op.AUTO_ID==1606].AMOUNT.max()
83/99:
rop = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
rop[op.AUTO_ID==1606].AMOUNT.max()
83/100:
rop = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
rop[rop.AUTO_ID==1606].AMOUNT.max()
83/101:
rop = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
rop[rop.AUTO_ID=='1606']#.AMOUNT.max()
83/102:
rop = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
rop[rop.AUTO_ID=='2203']#.AMOUNT.max()
83/103:
rop = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
rop[rop.AUTO_ID=='2203'].sort_values('AMOUNT', ascending=False)
83/104:
rop = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
rop[rop.AUTO_ID=='2203' & rop.ORIG_SERVICE_DATE=='2017-03-27']
83/105:
rop = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
rop[rop.AUTO_ID=='2203' & (rop.ORIG_SERVICE_DATE=='2017-03-27')]
83/106:
rop = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
rop[rop.AUTO_ID=='2203'] [rop.ORIG_SERVICE_DATE=='2017-03-27']
83/107: ip[ip.AUTO_ID==528]
83/108: ip[ip.AUTO_ID==0]
83/109: ip[ip.AUTO_ID==2]
83/110: op[op.AUTO_ID==2]
83/111: op[op.AUTO_ID==4]
83/112: rip = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
83/113: rip = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
83/114: rip[rip.AUTO_ID==4]
83/115: rip[rip.AUTO_ID=='4']
83/116: rip[rip.AUTO_ID=='16']
84/1:
%matplotlib inline
import os
import pandas as pd
import numpy as np
import config as cfg
84/2:
def get_inflation_data():
    #http://www.usinflationcalculator.com/inflation/current-inflation-rates/
    #I took it from the December column as it is explained on the website (last 12 months inflation)
    #year,inflation
    inflationrates={2005:3.4,
                    2006:2.5,
                    2007:4.1,
                    2008:0.1,
                    2009:2.7,
                    2010:1.5,
                    2011:3.0,
                    2012:1.7,
                    2013:1.5,
                    2014:0.8,
                    2015:0.7,
                    2016:2.1,
                    2017:2.1 } #last update: last charges made in 2018, so need to take into account only till 2017
    for (year,rate) in inflationrates.items():
        inflationrates[year]=rate*0.01+1
    
    #calculate coefficients    
    lastyear=np.max(list(inflationrates.keys()))
    inflation_coeff={lastyear+1:1.0}
    for year in range(lastyear, 2005, -1):
        inflation_coeff[year]=inflation_coeff[year+1]*inflationrates[year]
    return inflation_coeff


# UNRELATED CHARGES
unrelated = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'ibd_related_ip_charges.csv'))
unrelated['PAT_MRN_ID'] = unrelated['PAT_MRN_ID'].astype(int)
mrn_auto_map = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'MRN_AUTO_mapping.csv'))
mrn_auto_map['PAT_MRN_ID'] = mrn_auto_map['PAT_MRN_ID'].astype(int)
unrelated = unrelated.merge(mrn_auto_map[['AUTO_ID', 'PAT_MRN_ID']], on='PAT_MRN_ID')
84/3: unrelated
83/117: rip.shape
84/4: inpatient_charges[inpatient_charges.AUTO_ID==16]
84/5:
inpatient_charges = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'))
inpatient_charges[inpatient_charges.AUTO_ID==16]
84/6:
inflation_coeff = get_inflation_data()
inpatient_charges = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'))
inpatient_charges[inpatient_charges.AUTO_ID==16]/inflation_coeff[2011]
84/7:
inflation_coeff = get_inflation_data()
inpatient_charges = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'))
inpatient_charges[inpatient_charges.AUTO_ID==16]['AMOUNT']/inflation_coeff[2011]
84/8:
# inflation_coeff = get_inflation_data()
# #INPATIENT
# inpatient_charges_txn = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
# inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
# inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
# #adjust for inflation
# inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
# inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

# inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
# inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]
# inpatient_charges.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']

unrelated = unrelated[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'related']]
inpatient_charges = inpatient_charges.merge(unrelated, on=['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE'])

# inpatient_charges.to_csv(os.path.join(OUT_DATA_PATH, 'inpatient_charges.csv'))
84/9:
def get_inflation_data():
    #http://www.usinflationcalculator.com/inflation/current-inflation-rates/
    #I took it from the December column as it is explained on the website (last 12 months inflation)
    #year,inflation
    inflationrates={2005:3.4,
                    2006:2.5,
                    2007:4.1,
                    2008:0.1,
                    2009:2.7,
                    2010:1.5,
                    2011:3.0,
                    2012:1.7,
                    2013:1.5,
                    2014:0.8,
                    2015:0.7,
                    2016:2.1,
                    2017:2.1 } #last update: last charges made in 2018, so need to take into account only till 2017
    for (year,rate) in inflationrates.items():
        inflationrates[year]=rate*0.01+1
    
    #calculate coefficients    
    lastyear=np.max(list(inflationrates.keys()))
    inflation_coeff={lastyear+1:1.0}
    for year in range(lastyear, 2005, -1):
        inflation_coeff[year]=inflation_coeff[year+1]*inflationrates[year]
    return inflation_coeff


# UNRELATED CHARGES
unrelated = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'ibd_related_ip_charges.csv'))
unrelated['PAT_MRN_ID'] = unrelated['PAT_MRN_ID'].astype(int)
mrn_auto_map = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'MRN_AUTO_mapping.csv'))
mrn_auto_map['PAT_MRN_ID'] = mrn_auto_map['PAT_MRN_ID'].astype(int)
unrelated = unrelated.merge(mrn_auto_map[['AUTO_ID', 'PAT_MRN_ID']], on='PAT_MRN_ID')
84/10: unrelated
84/11:
# inflation_coeff = get_inflation_data()
# #INPATIENT
# inpatient_charges_txn = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
# inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
# inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
# #adjust for inflation
# inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
# inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

# inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
# inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]
# inpatient_charges.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']

unrelated = unrelated[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'related']]
inpatient_charges = inpatient_charges.merge(unrelated, on=['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE'])

# inpatient_charges.to_csv(os.path.join(OUT_DATA_PATH, 'inpatient_charges.csv'))
84/12: inpatient_charges
84/13:
def get_inflation_data():
    #http://www.usinflationcalculator.com/inflation/current-inflation-rates/
    #I took it from the December column as it is explained on the website (last 12 months inflation)
    #year,inflation
    inflationrates={2005:3.4,
                    2006:2.5,
                    2007:4.1,
                    2008:0.1,
                    2009:2.7,
                    2010:1.5,
                    2011:3.0,
                    2012:1.7,
                    2013:1.5,
                    2014:0.8,
                    2015:0.7,
                    2016:2.1,
                    2017:2.1 } #last update: last charges made in 2018, so need to take into account only till 2017
    for (year,rate) in inflationrates.items():
        inflationrates[year]=rate*0.01+1
    
    #calculate coefficients    
    lastyear=np.max(list(inflationrates.keys()))
    inflation_coeff={lastyear+1:1.0}
    for year in range(lastyear, 2005, -1):
        inflation_coeff[year]=inflation_coeff[year+1]*inflationrates[year]
    return inflation_coeff


# UNRELATED CHARGES
unrelated = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'ibd_related_ip_charges.csv'), parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE'])
unrelated['PAT_MRN_ID'] = unrelated['PAT_MRN_ID'].astype(int)
mrn_auto_map = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'MRN_AUTO_mapping.csv'))
mrn_auto_map['PAT_MRN_ID'] = mrn_auto_map['PAT_MRN_ID'].astype(int)
unrelated = unrelated.merge(mrn_auto_map[['AUTO_ID', 'PAT_MRN_ID']], on='PAT_MRN_ID')
84/14: unrelated
84/15:
# inflation_coeff = get_inflation_data()
# #INPATIENT
# inpatient_charges_txn = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
# inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
# inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
# #adjust for inflation
# inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
# inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

# inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
# inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]
# inpatient_charges.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']

unrelated = unrelated[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'related']]
inpatient_charges = inpatient_charges.merge(unrelated, on=['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE'])

# inpatient_charges.to_csv(os.path.join(OUT_DATA_PATH, 'inpatient_charges.csv'))
84/16: inpatient_charges
84/17:
inflation_coeff = get_inflation_data()
inpatient_charges = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'))
84/18:
# inflation_coeff = get_inflation_data()
# inpatient_charges = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'))
inpatient_charges.AUTO_ID
84/19:
# inflation_coeff = get_inflation_data()
# inpatient_charges = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'))
unrelated.AUTO_ID
84/20:
# inflation_coeff = get_inflation_data()
# inpatient_charges = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'))
unrelated.ADMISSION_DATE
84/21:
# inflation_coeff = get_inflation_data()
# inpatient_charges = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'))
inpatient_charges.ADMISSION_DATE
84/22:
# inflation_coeff = get_inflation_data()
inpatient_charges = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'), parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE'])
84/23:
# inflation_coeff = get_inflation_data()
# #INPATIENT
# inpatient_charges_txn = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
# inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
# inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
# #adjust for inflation
# inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
# inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

# inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
# inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]
# inpatient_charges.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']

unrelated = unrelated[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'related']]
x = inpatient_charges.merge(unrelated, on=['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE'])

# inpatient_charges.to_csv(os.path.join(OUT_DATA_PATH, 'inpatient_charges.csv'))
84/24: x
84/25:
# inflation_coeff = get_inflation_data()
# #INPATIENT
# inpatient_charges_txn = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
# inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
# inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
# #adjust for inflation
# inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
# inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

# inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
# inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]
# inpatient_charges.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']

unrelated = unrelated[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'related']]
x = inpatient_charges.merge(unrelated, how='left',  on=['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE'])

# inpatient_charges.to_csv(os.path.join(OUT_DATA_PATH, 'inpatient_charges.csv'))
84/26: x
84/27:
# inflation_coeff = get_inflation_data()
# #INPATIENT
# inpatient_charges_txn = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
# inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
# inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
# #adjust for inflation
# inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
# inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

# inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
# inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]
# inpatient_charges.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']

unrelated = unrelated[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'related']]
x = inpatient_charges.merge(unrelated, how='left',  on=['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE'])
x = x[~x.related.isin([0,2])]
# inpatient_charges.to_csv(os.path.join(OUT_DATA_PATH, 'inpatient_charges.csv'))
84/28: x
84/29: x.related.value_counts()
84/30:
# inflation_coeff = get_inflation_data()
# #INPATIENT
# inpatient_charges_txn = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
# inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
# inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
# #adjust for inflation
# inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
# inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

# inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
# inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]
# inpatient_charges.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']

unrelated = unrelated[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'related']]
x = inpatient_charges.merge(unrelated, how='left',  on=['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE'])
inpatient_charges = x[~x.related.isin([0,2])]
inpatient_charges.to_csv(os.path.join(OUT_DATA_PATH, 'inpatient_charges.csv'))
84/31:
# inflation_coeff = get_inflation_data()
# #INPATIENT
# inpatient_charges_txn = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
# inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
# inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
# #adjust for inflation
# inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
# inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

# inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
# inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]
# inpatient_charges.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']

unrelated = unrelated[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'related']]
x = inpatient_charges.merge(unrelated, how='left',  on=['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE'])
inpatient_charges = x[~x.related.isin([0,2])]
inpatient_charges.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'))
84/32:
# inflation_coeff = get_inflation_data()
inpatient_charges = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'), parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE'])
84/33:
# inflation_coeff = get_inflation_data()
# #INPATIENT
# inpatient_charges_txn = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
# inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
# inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
# #adjust for inflation
# inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
# inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

# inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
# inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]
# inpatient_charges.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']

unrelated = unrelated[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'related']]
x = inpatient_charges.merge(unrelated, how='left',  on=['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE'])
inpatient_charges = x[~x.related.isin([0,2])]
inpatient_charges.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'))
84/34:
# find op unrelated
bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
84/35: bad_proc
84/36: bad_proc['do not include'].value_counts()
84/37: bad_proc['do not include'].shape
84/38:
def separate_dx_codes(opc):
    opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
    opc2 = opc[~opc.index.isin(opc1.index)]
    dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
    dx = dx.droplevel(-1)
    dx.name = 'DX'
    del opc1['DX']
    opc1 = opc1.join(dx)
    opc1 = opc1[opc2.columns]
    opc = pd.concat([opc1, opc2]).reset_index()
    return opc


# OUTPATIENT
outpatient = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
outpatient = outpatient[outpatient['AMOUNT']>0]
outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)

# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
# opc_alpha = opc[~opc.index.isin(opc_num.index)]

# opc_num.DX = opc_num.DX.astype(float)
# bad_rows = list(map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_code_ranges]).T)) # True if DX falls in any bad_code ranges
# opc_ok = opc_num[[not x for x in bad_rows]]
# outpatient_charges_transactions = pd.concat([opc_ok, opc_alpha]).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT'])


# outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
# op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE' ]).AMOUNT.sum().reset_index()
# op['ADMISSIONS_DATE'] = op['ORIG_SERVICE_DATE']
# op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
# op['DAYS_ADMITTED'] = 0

# op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']].to_csv(os.path.join(OUT_DATA_PATH, 'outpatient_charges.csv'))



# tot = pd.concat([ip, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
# tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
# tot.to_csv(os.path.join(OUT_DATA_PATH, 'total_charges.csv'), index=False)
84/39:
# find op unrelated
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
bad_proc
84/40:
# find op unrelated
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
outpatient
84/41:
def separate_dx_codes(opc):
    opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
    opc2 = opc[~opc.index.isin(opc1.index)]
    dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
    dx = dx.droplevel(-1)
    dx.name = 'DX'
    del opc1['DX']
    opc1 = opc1.join(dx)
    opc1 = opc1[opc2.columns]
    opc = pd.concat([opc1, opc2]).reset_index()
    return opc


# OUTPATIENT
outpatient = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
outpatient = outpatient[outpatient['AMOUNT']>0]
outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)

# outpatient_bad = outpatient.merge(bad_proc, how='left', )

# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
# opc_alpha = opc[~opc.index.isin(opc_num.index)]

# opc_num.DX = opc_num.DX.astype(float)
# bad_rows = list(map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_code_ranges]).T)) # True if DX falls in any bad_code ranges
# opc_ok = opc_num[[not x for x in bad_rows]]
# outpatient_charges_transactions = pd.concat([opc_ok, opc_alpha]).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT'])


# outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
# op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE' ]).AMOUNT.sum().reset_index()
# op['ADMISSIONS_DATE'] = op['ORIG_SERVICE_DATE']
# op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
# op['DAYS_ADMITTED'] = 0

# op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']].to_csv(os.path.join(OUT_DATA_PATH, 'outpatient_charges.csv'))



# tot = pd.concat([ip, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
# tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
# tot.to_csv(os.path.join(OUT_DATA_PATH, 'total_charges.csv'), index=False)
84/42:
def separate_dx_codes(opc):
    opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
    opc2 = opc[~opc.index.isin(opc1.index)]
    dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
    dx = dx.droplevel(-1)
    dx.name = 'DX'
    del opc1['DX']
    opc1 = opc1.join(dx)
    opc1 = opc1[opc2.columns]
    opc = pd.concat([opc1, opc2]).reset_index()
    return opc


# OUTPATIENT
outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
outpatient = outpatient[outpatient['AMOUNT']>0]
outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)

# outpatient_bad = outpatient.merge(bad_proc, how='left', )

# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
# opc_alpha = opc[~opc.index.isin(opc_num.index)]

# opc_num.DX = opc_num.DX.astype(float)
# bad_rows = list(map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_code_ranges]).T)) # True if DX falls in any bad_code ranges
# opc_ok = opc_num[[not x for x in bad_rows]]
# outpatient_charges_transactions = pd.concat([opc_ok, opc_alpha]).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT'])


# outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
# op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE' ]).AMOUNT.sum().reset_index()
# op['ADMISSIONS_DATE'] = op['ORIG_SERVICE_DATE']
# op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
# op['DAYS_ADMITTED'] = 0

# op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']].to_csv(os.path.join(OUT_DATA_PATH, 'outpatient_charges.csv'))



# tot = pd.concat([ip, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
# tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
# tot.to_csv(os.path.join(OUT_DATA_PATH, 'total_charges.csv'), index=False)
84/43:
# find op unrelated
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
outpatient
84/44:
# find op unrelated
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
84/45:
# find op unrelated
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
outpatient_bad
84/46:
# find op unrelated
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
outpatient.shape
84/47:
# find op unrelated
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
outpatient['do not include'].value_counts()
84/48:
# find op unrelated
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
outpatient_bad['do not include'].value_counts()
84/49:
# find op unrelated
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
outpatient = outpatient_bad[outpatient_bad['do not include'].isnull()].drop(['do not include'],axis=1)
print(outpatient.shape)
bad_procedures = outpatient_bad[outpatient_bad['do not include'].notnull()]
bad_procedures.head()
84/50:
# find op unrelated
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
outpatient = outpatient_bad[outpatient_bad['do not include'].isnull()].drop(['do not include'],axis=1)
print(outpatient.shape)
bad_procedures = outpatient_bad[outpatient_bad['do not include'].notnull()]
bad_procedures.DX.value_counts()
84/51:
# find op unrelated
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
outpatient = outpatient_bad[outpatient_bad['do not include'].isnull()].drop(['do not include'],axis=1)
print(outpatient.shape)
bad_procedures = outpatient_bad[outpatient_bad['do not include'].notnull()]
bad_procedures.DX.value_counts().plot.hist()
84/52:
# find op unrelated
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
outpatient = outpatient_bad[outpatient_bad['do not include'].isnull()].drop(['do not include'],axis=1)
print(outpatient.shape)
bad_procedures = outpatient_bad[outpatient_bad['do not include'].notnull()]
bad_procedures.DX.value_counts()
84/53:
# find op unrelated
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
outpatient = outpatient_bad[outpatient_bad['do not include'].isnull()].drop(['do not include'],axis=1)
print(outpatient.shape)
bad_procedures = outpatient_bad[outpatient_bad['do not include'].notnull()]
bad_procedures.DX.value_counts().plot.line()
84/54:
# find op unrelated
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
outpatient = outpatient_bad[outpatient_bad['do not include'].isnull()].drop(['do not include'],axis=1)
print(outpatient.shape)
bad_procedures = outpatient_bad[outpatient_bad['do not include'].notnull()]
bad_procedures.DX.value_counts()#.plot.line()
84/55:
# find op unrelated
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
outpatient = outpatient_bad[outpatient_bad['do not include'].isnull()].drop(['do not include'],axis=1)
print(outpatient.shape)
bad_procedures = outpatient_bad[outpatient_bad['do not include'].notnull()]
bad_procedures.DX.value_counts().plot.line()
84/56:
# find op unrelated
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
outpatient = outpatient_bad[outpatient_bad['do not include'].isnull()].drop(['do not include'],axis=1)
print(outpatient.shape)
bad_procedures = outpatient_bad[outpatient_bad['do not include'].notnull()]
bad_procedures.DX.value_counts().head(20)#plot.line()
84/57:
# find op unrelated

# print(outpatient.shape)
# bad_procedures = outpatient_bad[outpatient_bad['do not include'].notnull()]
# bad_procedures.DX.value_counts().head(20)#plot.line()

print(outpatient_charges_transactions.shape)
84/58:
def separate_dx_codes(opc):
    opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
    opc2 = opc[~opc.index.isin(opc1.index)]
    dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
    dx = dx.droplevel(-1)
    dx.name = 'DX'
    del opc1['DX']
    opc1 = opc1.join(dx)
    opc1 = opc1[opc2.columns]
    opc = pd.concat([opc1, opc2]).reset_index()
    return opc


# OUTPATIENT
outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
outpatient = outpatient[outpatient['AMOUNT']>0]
outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)


# FILTERING BAD PROCEDURES 
# =======================================================================

# Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
# outpatient_charges_transactions = outpatient_bad[outpatient_bad['do not include'].isnull()].drop(['do not include'],axis=1)

# Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
# opc_alpha = opc[~opc.index.isin(opc_num.index)]
# opc_num.DX = opc_num.DX.astype(float)
# good_rows = [not x for x in map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_code_ranges]).T)] # False if DX falls in any bad_code ranges
# opc_ok = opc_num[good_rows]
# outpatient_charges_transactions = pd.concat([opc_ok, opc_alpha]).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT'])

# Method 3: No filter
outpatient_charges_transactions = outpatient

# =======================================================================



# outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
# op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE' ]).AMOUNT.sum().reset_index()
# op['ADMISSIONS_DATE'] = op['ORIG_SERVICE_DATE']
# op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
# op['DAYS_ADMITTED'] = 0

# op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']].to_csv(os.path.join(OUT_DATA_PATH, 'outpatient_charges.csv'))



# tot = pd.concat([ip, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
# tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
# tot.to_csv(os.path.join(OUT_DATA_PATH, 'total_charges.csv'), index=False)
84/59:
def separate_dx_codes(opc):
    opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
    opc2 = opc[~opc.index.isin(opc1.index)]
    dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
    dx = dx.droplevel(-1)
    dx.name = 'DX'
    del opc1['DX']
    opc1 = opc1.join(dx)
    opc1 = opc1[opc2.columns]
    opc = pd.concat([opc1, opc2]).reset_index()
    return opc


# OUTPATIENT
outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
outpatient = outpatient[outpatient['AMOUNT']>0]
outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)


# FILTERING BAD PROCEDURES 
# =======================================================================

# Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
# outpatient_charges_transactions = outpatient_bad[outpatient_bad['do not include'].isnull()].drop(['do not include'],axis=1)

# Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
# opc_alpha = opc[~opc.index.isin(opc_num.index)]
# opc_num.DX = opc_num.DX.astype(float)
# good_rows = [not x for x in map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_code_ranges]).T)] # False if DX falls in any bad_code ranges
# opc_ok = opc_num[good_rows]
# outpatient_charges_transactions = pd.concat([opc_ok, opc_alpha]).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT'])

# Method 3: No filter
outpatient_charges_transactions1 = outpatient

# =======================================================================



# outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
# op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE' ]).AMOUNT.sum().reset_index()
# op['ADMISSIONS_DATE'] = op['ORIG_SERVICE_DATE']
# op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
# op['DAYS_ADMITTED'] = 0

# op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']].to_csv(os.path.join(OUT_DATA_PATH, 'outpatient_charges.csv'))



# tot = pd.concat([ip, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
# tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
# tot.to_csv(os.path.join(OUT_DATA_PATH, 'total_charges.csv'), index=False)
84/60:
# def separate_dx_codes(opc):
#     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
#     opc2 = opc[~opc.index.isin(opc1.index)]
#     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
#     dx = dx.droplevel(-1)
#     dx.name = 'DX'
#     del opc1['DX']
#     opc1 = opc1.join(dx)
#     opc1 = opc1[opc2.columns]
#     opc = pd.concat([opc1, opc2]).reset_index()
#     return opc


# # OUTPATIENT
# outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
# outpatient = outpatient[outpatient['AMOUNT']>0]
# outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
# outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
# outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)




# FILTERING BAD PROCEDURES 
# =======================================================================

# Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
# outpatient_charges_transactions = outpatient_bad[outpatient_bad['do not include'].isnull()].drop(['do not include'],axis=1)

# Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
# opc_alpha = opc[~opc.index.isin(opc_num.index)]
# opc_num.DX = opc_num.DX.astype(float)
# good_rows = [not x for x in map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_code_ranges]).T)] # False if DX falls in any bad_code ranges
# opc_ok = opc_num[good_rows]
# outpatient_charges_transactions = pd.concat([opc_ok, opc_alpha]).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT'])

# Method 3: No filter
# outpatient_charges_transactions1 = outpatient

# =======================================================================



# outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
# op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE' ]).AMOUNT.sum().reset_index()
# op['ADMISSIONS_DATE'] = op['ORIG_SERVICE_DATE']
# op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
# op['DAYS_ADMITTED'] = 0

# op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']].to_csv(os.path.join(OUT_DATA_PATH, 'outpatient_charges.csv'))



# tot = pd.concat([ip, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
# tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
# tot.to_csv(os.path.join(OUT_DATA_PATH, 'total_charges.csv'), index=False)
84/61:
# def separate_dx_codes(opc):
#     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
#     opc2 = opc[~opc.index.isin(opc1.index)]
#     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
#     dx = dx.droplevel(-1)
#     dx.name = 'DX'
#     del opc1['DX']
#     opc1 = opc1.join(dx)
#     opc1 = opc1[opc2.columns]
#     opc = pd.concat([opc1, opc2]).reset_index()
#     return opc


# # OUTPATIENT
# outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
# outpatient = outpatient[outpatient['AMOUNT']>0]
# outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
# outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
# outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)




# FILTERING BAD PROCEDURES 
# =======================================================================

# Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
# outpatient_charges_transactions1 = outpatient_bad[outpatient_bad['do not include'].isnull()].drop(['do not include'],axis=1)

# Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
# opc_alpha = opc[~opc.index.isin(opc_num.index)]
# opc_num.DX = opc_num.DX.astype(float)
# good_rows = [not x for x in map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_code_ranges]).T)] # False if DX falls in any bad_code ranges
# opc_ok = opc_num[good_rows]
# outpatient_charges_transactions2 = pd.concat([opc_ok, opc_alpha]).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT'])

# Method 0: No filter
outpatient_charges_transactions0 = outpatient

# =======================================================================



# outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
# op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE' ]).AMOUNT.sum().reset_index()
# op['ADMISSIONS_DATE'] = op['ORIG_SERVICE_DATE']
# op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
# op['DAYS_ADMITTED'] = 0

# op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']].to_csv(os.path.join(OUT_DATA_PATH, 'outpatient_charges.csv'))



# tot = pd.concat([ip, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
# tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
# tot.to_csv(os.path.join(OUT_DATA_PATH, 'total_charges.csv'), index=False)
84/62:
# def separate_dx_codes(opc):
#     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
#     opc2 = opc[~opc.index.isin(opc1.index)]
#     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
#     dx = dx.droplevel(-1)
#     dx.name = 'DX'
#     del opc1['DX']
#     opc1 = opc1.join(dx)
#     opc1 = opc1[opc2.columns]
#     opc = pd.concat([opc1, opc2]).reset_index()
#     return opc


# # OUTPATIENT
# outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
# outpatient = outpatient[outpatient['AMOUNT']>0]
# outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
# outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
# outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)




# FILTERING BAD PROCEDURES 
# =======================================================================

# Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
outpatient_charges_transactions1 = outpatient_bad[outpatient_bad['do not include'].isnull()].drop(['do not include'],axis=1)

# Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
outpatient = separate_dx_codes(outpatient)
opc = outpatient[outpatient.DX!='']
bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
opc_alpha = opc[~opc.index.isin(opc_num.index)]
opc_num.DX = opc_num.DX.astype(float)
good_rows = [not x for x in map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_code_ranges]).T)] # False if DX falls in any bad_code ranges
opc_ok = opc_num[good_rows]
outpatient_charges_transactions2 = pd.concat([opc_ok, opc_alpha]).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT'])

# Method 0: No filter
# outpatient_charges_transactions0 = outpatient

# =======================================================================



# outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
# op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE' ]).AMOUNT.sum().reset_index()
# op['ADMISSIONS_DATE'] = op['ORIG_SERVICE_DATE']
# op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
# op['DAYS_ADMITTED'] = 0

# op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']].to_csv(os.path.join(OUT_DATA_PATH, 'outpatient_charges.csv'))



# tot = pd.concat([ip, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
# tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
# tot.to_csv(os.path.join(OUT_DATA_PATH, 'total_charges.csv'), index=False)
84/63:
# find op unrelated

# print(outpatient.shape)
# bad_procedures = outpatient_bad[outpatient_bad['do not include'].notnull()]
# bad_procedures.DX.value_counts().head(20)#plot.line()
outpatient_charges_transactions1.shape
84/64:
# find op unrelated

# print(outpatient.shape)
# bad_procedures = outpatient_bad[outpatient_bad['do not include'].notnull()]
# bad_procedures.DX.value_counts().head(20)#plot.line()
outpatient_charges_transactions0.shape, outpatient_charges_transactions1.shape, outpatient_charges_transactions2.shape
84/65:
# def separate_dx_codes(opc):
#     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
#     opc2 = opc[~opc.index.isin(opc1.index)]
#     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
#     dx = dx.droplevel(-1)
#     dx.name = 'DX'
#     del opc1['DX']
#     opc1 = opc1.join(dx)
#     opc1 = opc1[opc2.columns]
#     opc = pd.concat([opc1, opc2]).reset_index()
#     return opc


# # OUTPATIENT
# outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
# outpatient = outpatient[outpatient['AMOUNT']>0]
# outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
# outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
# outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)




# FILTERING BAD PROCEDURES 
# =======================================================================

# Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
# outpatient_charges_transactions1 = outpatient_bad[outpatient_bad['do not include'].isnull()].drop(['do not include'],axis=1)

# Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
outpatient = separate_dx_codes(outpatient)
opc = outpatient[outpatient.DX!='']
bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
opc_alpha = opc[~opc.index.isin(opc_num.index)]
opc_num.DX = opc_num.DX.astype(float)
good_rows = [not x for x in map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_code_ranges]).T)] # False if DX falls in any bad_code ranges
opc_ok = opc_num[good_rows]
outpatient_charges_transactions2 = pd.concat([opc_ok, opc_alpha]).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT'])

# Method 0: No filter
# outpatient_charges_transactions0 = outpatient

# =======================================================================



# outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
# op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE' ]).AMOUNT.sum().reset_index()
# op['ADMISSIONS_DATE'] = op['ORIG_SERVICE_DATE']
# op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
# op['DAYS_ADMITTED'] = 0

# op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']].to_csv(os.path.join(OUT_DATA_PATH, 'outpatient_charges.csv'))



# tot = pd.concat([ip, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
# tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
# tot.to_csv(os.path.join(OUT_DATA_PATH, 'total_charges.csv'), index=False)
84/66:
# find op unrelated

# print(outpatient.shape)
# bad_procedures = outpatient_bad[outpatient_bad['do not include'].notnull()]
# bad_procedures.DX.value_counts().head(20)#plot.line()
outpatient_charges_transactions0.shape, outpatient_charges_transactions1.shape, outpatient_charges_transactions2.shape
84/67:
# find op unrelated

# print(outpatient.shape)
# bad_procedures = outpatient_bad[outpatient_bad['do not include'].notnull()]
# bad_procedures.DX.value_counts().head(20)#plot.line()
outpatient_charges_transactions0.shape, outpatient_charges_transactions1.shape, outpatient_charges_transactions2.shape
84/68:
# find op unrelated

# print(outpatient.shape)
# bad_procedures = outpatient_bad[outpatient_bad['do not include'].notnull()]
# bad_procedures.DX.value_counts().head(20)#plot.line()
outpatient_charges_transactions0.shape, outpatient_charges_transactions1.shape, outpatient_charges_transactions2.shape
84/69:
def separate_dx_codes(opc):
    opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
    opc2 = opc[~opc.index.isin(opc1.index)]
    dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
    dx = dx.droplevel(-1)
    dx.name = 'DX'
    del opc1['DX']
    opc1 = opc1.join(dx)
    opc1 = opc1[opc2.columns]
    opc = pd.concat([opc1, opc2]).reset_index()
    return opc


# OUTPATIENT
outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
outpatient = outpatient[outpatient['AMOUNT']>0]
outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)




# FILTERING BAD PROCEDURES 
# =======================================================================

# Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
# outpatient_charges_transactions1 = outpatient_bad[outpatient_bad['do not include'].isnull()].drop(['do not include'],axis=1)

# Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
outpatient = separate_dx_codes(outpatient)
opc = outpatient[outpatient.DX!='']
bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
opc_alpha = opc[~opc.index.isin(opc_num.index)]
opc_num.DX = opc_num.DX.astype(float)
good_rows = [not x for x in map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_code_ranges]).T)] # False if DX falls in any bad_code ranges
opc_ok = opc_num[good_rows]
outpatient_charges_transactions2 = pd.concat([opc_ok, opc_alpha]).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT'])

# Method 0: No filter
# outpatient_charges_transactions0 = outpatient

# =======================================================================



# outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
# op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE' ]).AMOUNT.sum().reset_index()
# op['ADMISSIONS_DATE'] = op['ORIG_SERVICE_DATE']
# op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
# op['DAYS_ADMITTED'] = 0

# op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']].to_csv(os.path.join(OUT_DATA_PATH, 'outpatient_charges.csv'))



# tot = pd.concat([ip, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
# tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
# tot.to_csv(os.path.join(OUT_DATA_PATH, 'total_charges.csv'), index=False)
84/70:
# find op unrelated

# print(outpatient.shape)
# bad_procedures = outpatient_bad[outpatient_bad['do not include'].notnull()]
# bad_procedures.DX.value_counts().head(20)#plot.line()
outpatient_charges_transactions0.shape, outpatient_charges_transactions1.shape, outpatient_charges_transactions2.shape
84/71:
# def separate_dx_codes(opc):
#     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
#     opc2 = opc[~opc.index.isin(opc1.index)]
#     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
#     dx = dx.droplevel(-1)
#     dx.name = 'DX'
#     del opc1['DX']
#     opc1 = opc1.join(dx)
#     opc1 = opc1[opc2.columns]
#     opc = pd.concat([opc1, opc2]).reset_index()
#     return opc


# # OUTPATIENT
# outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
# outpatient = outpatient[outpatient['AMOUNT']>0]
# outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
# outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
# outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)




# FILTERING BAD PROCEDURES ((422943, 9), (383395, 9), (357001, 10))
# =======================================================================

# Method 0: No filter
# outpatient_charges_transactions0 = outpatient

# Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
# outpatient_charges_transactions1 = outpatient_bad[outpatient_bad['do not include'].isnull()].drop(['do not include'],axis=1)

# Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
# opc_alpha = opc[~opc.index.isin(opc_num.index)]
# opc_num.DX = opc_num.DX.astype(float)
# good_rows = [not x for x in map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_code_ranges]).T)] # False if DX falls in any bad_code ranges
# opc_ok = opc_num[good_rows]
# outpatient_charges_transactions2 = pd.concat([opc_ok, opc_alpha]).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT'])

# =======================================================================

outpatient_charges_transactions = outpatient_charges_transactions1

outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE' ]).AMOUNT.sum().reset_index()
op['ADMISSIONS_DATE'] = op['ORIG_SERVICE_DATE']
op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
op['DAYS_ADMITTED'] = 0

op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']].to_csv(os.path.join(OUT_DATA_PATH, 'outpatient_charges.csv'))



tot = pd.concat([ip, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
tot.to_csv(os.path.join(OUT_DATA_PATH, 'total_charges.csv'), index=False)
84/72:
# def separate_dx_codes(opc):
#     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
#     opc2 = opc[~opc.index.isin(opc1.index)]
#     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
#     dx = dx.droplevel(-1)
#     dx.name = 'DX'
#     del opc1['DX']
#     opc1 = opc1.join(dx)
#     opc1 = opc1[opc2.columns]
#     opc = pd.concat([opc1, opc2]).reset_index()
#     return opc


# # OUTPATIENT
# outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
# outpatient = outpatient[outpatient['AMOUNT']>0]
# outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
# outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
# outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)




# FILTERING BAD PROCEDURES ((422943, 9), (383395, 9), (357001, 10))
# =======================================================================

# Method 0: No filter
# outpatient_charges_transactions0 = outpatient

# Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
# outpatient_charges_transactions1 = outpatient_bad[outpatient_bad['do not include'].isnull()].drop(['do not include'],axis=1)

# Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
# opc_alpha = opc[~opc.index.isin(opc_num.index)]
# opc_num.DX = opc_num.DX.astype(float)
# good_rows = [not x for x in map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_code_ranges]).T)] # False if DX falls in any bad_code ranges
# opc_ok = opc_num[good_rows]
# outpatient_charges_transactions2 = pd.concat([opc_ok, opc_alpha]).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT'])

# =======================================================================

# outpatient_charges_transactions = outpatient_charges_transactions1

# outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
# op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE' ]).AMOUNT.sum().reset_index()
op['ADMISSIONS_DATE'] = op['ORIG_SERVICE_DATE']
# op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
# op['DAYS_ADMITTED'] = 0

op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']].to_csv(os.path.join(OUT_DATA_PATH, 'outpatient_charges.csv'))



tot = pd.concat([inpatient_charges, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
tot.to_csv(os.path.join(OUT_DATA_PATH, 'total_charges.csv'), index=False)
84/73:
# def separate_dx_codes(opc):
#     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
#     opc2 = opc[~opc.index.isin(opc1.index)]
#     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
#     dx = dx.droplevel(-1)
#     dx.name = 'DX'
#     del opc1['DX']
#     opc1 = opc1.join(dx)
#     opc1 = opc1[opc2.columns]
#     opc = pd.concat([opc1, opc2]).reset_index()
#     return opc


# # OUTPATIENT
# outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
# outpatient = outpatient[outpatient['AMOUNT']>0]
# outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
# outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
# outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)




# FILTERING BAD PROCEDURES ((422943, 9), (383395, 9), (357001, 10))
# =======================================================================

# Method 0: No filter
# outpatient_charges_transactions0 = outpatient

# Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
# outpatient_charges_transactions1 = outpatient_bad[outpatient_bad['do not include'].isnull()].drop(['do not include'],axis=1)

# Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
# opc_alpha = opc[~opc.index.isin(opc_num.index)]
# opc_num.DX = opc_num.DX.astype(float)
# good_rows = [not x for x in map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_code_ranges]).T)] # False if DX falls in any bad_code ranges
# opc_ok = opc_num[good_rows]
# outpatient_charges_transactions2 = pd.concat([opc_ok, opc_alpha]).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT'])

# =======================================================================

# outpatient_charges_transactions = outpatient_charges_transactions1

# outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
# op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE' ]).AMOUNT.sum().reset_index()
op['ADMISSION_DATE'] = op['ORIG_SERVICE_DATE']
# op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
# op['DAYS_ADMITTED'] = 0

op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']].to_csv(os.path.join(OUT_DATA_PATH, 'outpatient_charges.csv'))



tot = pd.concat([inpatient_charges, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
tot.to_csv(os.path.join(OUT_DATA_PATH, 'total_charges.csv'), index=False)
84/74:
# def separate_dx_codes(opc):
#     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
#     opc2 = opc[~opc.index.isin(opc1.index)]
#     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
#     dx = dx.droplevel(-1)
#     dx.name = 'DX'
#     del opc1['DX']
#     opc1 = opc1.join(dx)
#     opc1 = opc1[opc2.columns]
#     opc = pd.concat([opc1, opc2]).reset_index()
#     return opc


# # OUTPATIENT
# outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
# outpatient = outpatient[outpatient['AMOUNT']>0]
# outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
# outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
# outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)




# FILTERING BAD PROCEDURES ((422943, 9), (383395, 9), (357001, 10))
# =======================================================================

# Method 0: No filter
# outpatient_charges_transactions0 = outpatient

# Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
# outpatient_charges_transactions1 = outpatient_bad[outpatient_bad['do not include'].isnull()].drop(['do not include'],axis=1)

# Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
# opc_alpha = opc[~opc.index.isin(opc_num.index)]
# opc_num.DX = opc_num.DX.astype(float)
# good_rows = [not x for x in map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_code_ranges]).T)] # False if DX falls in any bad_code ranges
# opc_ok = opc_num[good_rows]
# outpatient_charges_transactions2 = pd.concat([opc_ok, opc_alpha]).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT'])

# =======================================================================

# outpatient_charges_transactions = outpatient_charges_transactions1

# outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
# op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE' ]).AMOUNT.sum().reset_index()
op['ADMISSION_DATE'] = op['ORIG_SERVICE_DATE']
# op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
# op['DAYS_ADMITTED'] = 0

op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']].to_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges.csv'))



tot = pd.concat([inpatient_charges, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)
84/75:
inflation_coeff = get_inflation_data()
# unrelated_ip CHARGES
unrelated_ip = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'ibd_related_ip_charges.csv'), parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE'])
unrelated_ip['PAT_MRN_ID'] = unrelated_ip['PAT_MRN_ID'].astype(int)
unrelated_ip = unrelated_ip.merge(mrn_auto_map[['AUTO_ID', 'PAT_MRN_ID']], on='PAT_MRN_ID')


#INPATIENT
inpatient_charges_txn = pd.read_csv(os.path.join(IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
#adjust for inflation
inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]
inpatient_charges.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']

unrelated_ip = unrelated_ip[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'related']]
inpatient_charges = inpatient_charges.merge(unrelated_ip, how='left',  on=['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE'])
# inpatient_charges[['']] = x[~x.related.isin([0,2])]
# inpatient_charges.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'))
84/76:
inflation_coeff = get_inflation_data()
# unrelated_ip CHARGES
unrelated_ip = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'ibd_related_ip_charges.csv'), parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE'])
unrelated_ip['PAT_MRN_ID'] = unrelated_ip['PAT_MRN_ID'].astype(int)
unrelated_ip = unrelated_ip.merge(mrn_auto_map[['AUTO_ID', 'PAT_MRN_ID']], on='PAT_MRN_ID')


#INPATIENT
inpatient_charges_txn = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
#adjust for inflation
inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]
inpatient_charges.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']

unrelated_ip = unrelated_ip[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'related']]
inpatient_charges = inpatient_charges.merge(unrelated_ip, how='left',  on=['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE'])
# inpatient_charges[['']] = x[~x.related.isin([0,2])]
# inpatient_charges.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'))
84/77: inpatient_charges.groupby('related').unstack()
84/78: inpatient_charges['related'].unstack()
84/79: pd.get_dummies(inpatient_charges['related'].unstack())
84/80: pd.get_dummies(inpatient_charges['related'])
84/81: pd.get_dummies(inpatient_charges['related']).rename_columns(['a','b','c'])
84/82: inpatient_charges.columns
84/83: inpatient_charges.drop(['related'], axis=1)
84/84:
inflation_coeff = get_inflation_data()
# unrelated_ip CHARGES
unrelated_ip = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'ibd_related_ip_charges.csv'), parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE'])
unrelated_ip['PAT_MRN_ID'] = unrelated_ip['PAT_MRN_ID'].astype(int)
unrelated_ip = unrelated_ip.merge(mrn_auto_map[['AUTO_ID', 'PAT_MRN_ID']], on='PAT_MRN_ID')


#INPATIENT
inpatient_charges_txn = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
#adjust for inflation
inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]
inpatient_charges.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']

unrelated_ip = unrelated_ip[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'related']]
inpatient_charges = inpatient_charges.merge(unrelated_ip, how='left',  on=['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE'])
inpatient_charges[['related', 'unrelated', 'unknown']] = pd.get_dummies(inpatient_charges['related'])
inpatient_charges.drop(['related'], inplace=True, axis=1)
# inpatient_charges.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'))
84/85: inpatient_charges
84/86:
# # OUTPATIENT
outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
outpatient = outpatient[outpatient['AMOUNT']>0]
outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)




# FILTERING BAD PROCEDURES ((422943, 9), (383395, 9), (357001, 10))
# =======================================================================

# Method 0: No filter
# outpatient_charges_transactions0 = outpatient

# Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
outpatient_bad = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
outpatient_charges_transactions1 = outpatient_bad[outpatient_bad['do not include'].isnull()].drop(['do not include'],axis=1)

# Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
# def separate_dx_codes(opc):
#     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
#     opc2 = opc[~opc.index.isin(opc1.index)]
#     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
#     dx = dx.droplevel(-1)
#     dx.name = 'DX'
#     del opc1['DX']
#     opc1 = opc1.join(dx)
#     opc1 = opc1[opc2.columns]
#     opc = pd.concat([opc1, opc2]).reset_index()
#     return opc
# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]
# opc_alpha = opc[~opc.index.isin(opc_num.index)]
# opc_num.DX = opc_num.DX.astype(float)
# good_rows = [not x for x in map(any, np.array([opc_num.DX.between(*tup).values for tup in bad_code_ranges]).T)] # False if DX falls in any bad_code ranges
# opc_ok = opc_num[good_rows]
# outpatient_charges_transactions2 = pd.concat([opc_ok, opc_alpha]).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE', 'AMOUNT'])

# =======================================================================

outpatient_charges_transactions = outpatient_charges_transactions1
outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE' ]).AMOUNT.sum().reset_index()
op['ADMISSION_DATE'] = op['ORIG_SERVICE_DATE']
op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
op['DAYS_ADMITTED'] = 0
op[['related', 'unrelated', 'unknown']] = pd.get_dummies(op['unrelated'])
op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT', 'unrelated', 'unknown']].to_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges.csv'))



tot = pd.concat([inpatient_charges, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)
84/87:
# # OUTPATIENT
outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
outpatient = outpatient[outpatient['AMOUNT']>0]
outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)




# FILTERING BAD PROCEDURES ((422943, 9), (383395, 9), (357001, 10))
# =======================================================================

# Method 0: No filter
# outpatient_charges_transactions0 = outpatient

# Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
outpatient_charges_transactions1 = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
 

# Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
# def separate_dx_codes(opc):
#     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
#     opc2 = opc[~opc.index.isin(opc1.index)]
#     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
#     dx = dx.droplevel(-1)
#     dx.name = 'DX'
#     del opc1['DX']
#     opc1 = opc1.join(dx)
#     opc1 = opc1[opc2.columns]
#     opc = pd.concat([opc1, opc2]).reset_index()
#     return opc
# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]

# =======================================================================

outpatient_charges_transactions = outpatient_charges_transactions1
outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE' ]).AMOUNT.sum().reset_index()
op['ADMISSION_DATE'] = op['ORIG_SERVICE_DATE']
op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
op['DAYS_ADMITTED'] = 0
op[['related', 'unrelated', 'unknown']] = pd.get_dummies(op['unrelated'])
op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT', 'unrelated', 'unknown']].to_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges.csv'))



tot = pd.concat([inpatient_charges, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)
84/88: bad_proc
84/89:
# # OUTPATIENT
outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
outpatient = outpatient[outpatient['AMOUNT']>0]
outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)




# FILTERING BAD PROCEDURES ((422943, 9), (383395, 9), (357001, 10))
# =======================================================================

# Method 0: No filter
# outpatient_charges_transactions0 = outpatient

# Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
outpatient_charges_transactions1 = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
 

# Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
# def separate_dx_codes(opc):
#     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
#     opc2 = opc[~opc.index.isin(opc1.index)]
#     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
#     dx = dx.droplevel(-1)
#     dx.name = 'DX'
#     del opc1['DX']
#     opc1 = opc1.join(dx)
#     opc1 = opc1[opc2.columns]
#     opc = pd.concat([opc1, opc2]).reset_index()
#     return opc
# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]

# =======================================================================

outpatient_charges_transactions = outpatient_charges_transactions1
outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE' ]).AMOUNT.sum().reset_index()
op['ADMISSION_DATE'] = op['ORIG_SERVICE_DATE']
op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
op['DAYS_ADMITTED'] = 0
op[['related', 'unrelated']] = pd.get_dummies(op['unrelated'].fillna(0))
op['unknown'] = 0
op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT', 'unrelated', 'unknown']].to_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges.csv'))



tot = pd.concat([inpatient_charges, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)
84/90: op
84/91:
# # OUTPATIENT
outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
outpatient = outpatient[outpatient['AMOUNT']>0]
outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)




# FILTERING BAD PROCEDURES ((422943, 9), (383395, 9), (357001, 10))
# =======================================================================

# Method 0: No filter
# outpatient_charges_transactions0 = outpatient

# Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
outpatient_charges_transactions1 = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
 

# Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
# def separate_dx_codes(opc):
#     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
#     opc2 = opc[~opc.index.isin(opc1.index)]
#     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
#     dx = dx.droplevel(-1)
#     dx.name = 'DX'
#     del opc1['DX']
#     opc1 = opc1.join(dx)
#     opc1 = opc1[opc2.columns]
#     opc = pd.concat([opc1, opc2]).reset_index()
#     return opc
# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]

# =======================================================================

outpatient_charges_transactions = outpatient_charges_transactions1
outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE', 'unrelated']).AMOUNT.sum().reset_index()
op['ADMISSION_DATE'] = op['ORIG_SERVICE_DATE']
op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
op['DAYS_ADMITTED'] = 0
op[['related', 'unrelated']] = pd.get_dummies(op['unrelated'].fillna(0))
op['unknown'] = 0
op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT', 'unrelated', 'unknown']].to_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges.csv'))



tot = pd.concat([inpatient_charges, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)
84/92: pd.get_dummies(op['unrelated'].fillna(0))
84/93:
# pd.get_dummies(op['unrelated'].fillna(0))
op['unrelated']
84/94:
# pd.get_dummies(op['unrelated'].fillna(0))
op['unrelated'].value_counts()
84/95:
# pd.get_dummies(op['unrelated'].fillna(0))
outpatient_charges_transactions['unrelated'].value_counts()
84/96:
# pd.get_dummies(op['unrelated'].fillna(0))
outpatient_charges_transactions['unrelated'].fillna(0).value_counts()
84/97:
# # OUTPATIENT
outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
outpatient = outpatient[outpatient['AMOUNT']>0]
outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)




# FILTERING BAD PROCEDURES ((422943, 9), (383395, 9), (357001, 10))
# =======================================================================

# Method 0: No filter
# outpatient_charges_transactions0 = outpatient

# Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
outpatient_charges_transactions1 = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
 

# Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
# def separate_dx_codes(opc):
#     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
#     opc2 = opc[~opc.index.isin(opc1.index)]
#     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
#     dx = dx.droplevel(-1)
#     dx.name = 'DX'
#     del opc1['DX']
#     opc1 = opc1.join(dx)
#     opc1 = opc1[opc2.columns]
#     opc = pd.concat([opc1, opc2]).reset_index()
#     return opc
# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]

# =======================================================================

outpatient_charges_transactions = outpatient_charges_transactions1
outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE', 'unrelated']).AMOUNT.sum().reset_index()
op['ADMISSION_DATE'] = op['ORIG_SERVICE_DATE']
op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
op['DAYS_ADMITTED'] = 0
op['unrelated'] = op['unrelated'].fillna(0)
op['unknown'] = 0
op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT', 'unrelated', 'unknown']].to_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges.csv'))



tot = pd.concat([inpatient_charges, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)
84/98: tot
84/99:
surgeries_df = outpatient_charges_transactions[outpatient_charges_transactions["TYPE_OF_SERVICE"]=='20-SURGERY MA'] 
surgeries_df.drop_duplicates(subset=["AUTO_ID","ORIG_SERVICE_DATE"],inplace=True)

# surgeries_df.drop(columns=['Unnamed: 0', 'AMOUNT', 'DX','PROC_CODE', 'PROC_NAME', 'PROC_GROUP_NAME'],inplace=True).drop(surgeries_df[surgeries_df["AUTO_ID"]=="NO_MATCH"].index,inplace=True)
# surgeries_df.AUTO_ID=surgeries_df.AUTO_ID.astype(int)
# surgeries_df_agg = surgeries_df.groupby('AUTO_ID').agg({'AUTO_ID':'count', 'ORIG_SERVICE_DATE':'min'})
# surgeries_df_agg['timedelta_yrs'] = surgeries_df_agg.apply(lambda row: max((NOW-row.ORIG_SERVICE_DATE).days/365, 1), axis=1)
# surgeries_df_agg['SURGERIES_PER_YEAR']=surgeries_df_agg.AUTO_ID/surgeries_df_agg.timedelta_yrs
# surgeries_df_agg.drop(columns=['timedelta_yrs'],inplace=True)
# surgeries_df_agg.columns= ['SURGERY_COUNT', 'FIRST_SURGERY_DT', 'SURGERIES_PER_YEAR']
84/100: surgeries_df
84/101:
# # OUTPATIENT
outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
outpatient = outpatient[outpatient['AMOUNT']>0]
outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)




# FILTERING BAD PROCEDURES ((422943, 9), (383395, 9), (357001, 10))
# =======================================================================

# Method 0: No filter
# outpatient_charges_transactions0 = outpatient

# Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
outpatient_charges_transactions1 = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
 

# Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
# def separate_dx_codes(opc):
#     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
#     opc2 = opc[~opc.index.isin(opc1.index)]
#     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
#     dx = dx.droplevel(-1)
#     dx.name = 'DX'
#     del opc1['DX']
#     opc1 = opc1.join(dx)
#     opc1 = opc1[opc2.columns]
#     opc = pd.concat([opc1, opc2]).reset_index()
#     return opc
# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]

# =======================================================================

outpatient_charges_transactions = outpatient_charges_transactions1
outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
op['unrelated'] = op['unrelated'].fillna(0)
op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE', 'unrelated']).AMOUNT.sum().reset_index()
op['ADMISSION_DATE'] = op['ORIG_SERVICE_DATE']
op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
op['DAYS_ADMITTED'] = 0

op['unknown'] = 0
op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT', 'unrelated', 'unknown']].to_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges.csv'))



tot = pd.concat([inpatient_charges, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)
84/102:
surgeries_df = outpatient_charges_transactions[outpatient_charges_transactions["TYPE_OF_SERVICE"]=='20-SURGERY MA'] 
surgeries_df.drop_duplicates(subset=["AUTO_ID","ORIG_SERVICE_DATE"],inplace=True)

# surgeries_df.drop(columns=['Unnamed: 0', 'AMOUNT', 'DX','PROC_CODE', 'PROC_NAME', 'PROC_GROUP_NAME'],inplace=True).drop(surgeries_df[surgeries_df["AUTO_ID"]=="NO_MATCH"].index,inplace=True)
# surgeries_df.AUTO_ID=surgeries_df.AUTO_ID.astype(int)
# surgeries_df_agg = surgeries_df.groupby('AUTO_ID').agg({'AUTO_ID':'count', 'ORIG_SERVICE_DATE':'min'})
# surgeries_df_agg['timedelta_yrs'] = surgeries_df_agg.apply(lambda row: max((NOW-row.ORIG_SERVICE_DATE).days/365, 1), axis=1)
# surgeries_df_agg['SURGERIES_PER_YEAR']=surgeries_df_agg.AUTO_ID/surgeries_df_agg.timedelta_yrs
# surgeries_df_agg.drop(columns=['timedelta_yrs'],inplace=True)
# surgeries_df_agg.columns= ['SURGERY_COUNT', 'FIRST_SURGERY_DT', 'SURGERIES_PER_YEAR']
84/103: surgeries_df
84/104:
# # OUTPATIENT
# outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
# outpatient = outpatient[outpatient['AMOUNT']>0]
# outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
# outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
# outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)




# FILTERING BAD PROCEDURES ((422943, 9), (383395, 9), (357001, 10))
# =======================================================================

# Method 0: No filter
# outpatient_charges_transactions0 = outpatient

# Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_charges_transactions1 = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
 

# Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
# def separate_dx_codes(opc):
#     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
#     opc2 = opc[~opc.index.isin(opc1.index)]
#     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
#     dx = dx.droplevel(-1)
#     dx.name = 'DX'
#     del opc1['DX']
#     opc1 = opc1.join(dx)
#     opc1 = opc1[opc2.columns]
#     opc = pd.concat([opc1, opc2]).reset_index()
#     return opc
# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]

# =======================================================================

outpatient_charges_transactions = outpatient_charges_transactions1
outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
outpatient_charges_transactions['unrelated'] = outpatient_charges_transactions['unrelated'].fillna(0)
op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE', 'unrelated']).AMOUNT.sum().reset_index()
op['ADMISSION_DATE'] = op['ORIG_SERVICE_DATE']
op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
op['DAYS_ADMITTED'] = 0

op['unknown'] = 0
op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT', 'unrelated', 'unknown']].to_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges.csv'))



tot = pd.concat([inpatient_charges, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)
84/105:
# # OUTPATIENT
# outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
# outpatient = outpatient[outpatient['AMOUNT']>0]
# outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
# outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
# outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)




# FILTERING BAD PROCEDURES ((422943, 9), (383395, 9), (357001, 10))
# =======================================================================

# Method 0: No filter
# outpatient_charges_transactions0 = outpatient

# Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_charges_transactions1 = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
 

# Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
# def separate_dx_codes(opc):
#     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
#     opc2 = opc[~opc.index.isin(opc1.index)]
#     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
#     dx = dx.droplevel(-1)
#     dx.name = 'DX'
#     del opc1['DX']
#     opc1 = opc1.join(dx)
#     opc1 = opc1[opc2.columns]
#     opc = pd.concat([opc1, opc2]).reset_index()
#     return opc
# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]

# =======================================================================

outpatient_charges_transactions = outpatient_charges_transactions1
outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
outpatient_charges_transactions['unrelated'] = outpatient_charges_transactions['unrelated'].fillna(0)
op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE', 'unrelated']).AMOUNT.sum().reset_index()
op['ADMISSION_DATE'] = op['ORIG_SERVICE_DATE']
op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
op['DAYS_ADMITTED'] = 0

op['unknown'] = 0
op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT', 'unrelated', 'unknown']].to_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges.csv'))



tot = pd.concat([inpatient_charges, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)
84/106:
# # OUTPATIENT
# outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
# outpatient = outpatient[outpatient['AMOUNT']>0]
# outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
# outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
# outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)




# FILTERING BAD PROCEDURES ((422943, 9), (383395, 9), (357001, 10))
# =======================================================================

# Method 0: No filter
# outpatient_charges_transactions0 = outpatient

# Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_charges_transactions1 = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
 

# Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
# def separate_dx_codes(opc):
#     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
#     opc2 = opc[~opc.index.isin(opc1.index)]
#     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
#     dx = dx.droplevel(-1)
#     dx.name = 'DX'
#     del opc1['DX']
#     opc1 = opc1.join(dx)
#     opc1 = opc1[opc2.columns]
#     opc = pd.concat([opc1, opc2]).reset_index()
#     return opc
# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]

# =======================================================================

outpatient_charges_transactions = outpatient_charges_transactions1
outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
outpatient_charges_transactions['unrelated'] = outpatient_charges_transactions['unrelated'].fillna(0)
op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE', 'unrelated']).AMOUNT.sum().reset_index()
op['ADMISSION_DATE'] = op['ORIG_SERVICE_DATE']
op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
op['DAYS_ADMITTED'] = 0

op['unknown'] = 0
op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT', 'unrelated', 'unknown']].to_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges.csv'))



tot = pd.concat([inpatient_charges, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)
84/107:
surgeries_df = outpatient_charges_transactions[outpatient_charges_transactions["TYPE_OF_SERVICE"]=='20-SURGERY MA'] 
surgeries_df.drop_duplicates(subset=["AUTO_ID","ORIG_SERVICE_DATE"],inplace=True)

# surgeries_df.drop(columns=['Unnamed: 0', 'AMOUNT', 'DX','PROC_CODE', 'PROC_NAME', 'PROC_GROUP_NAME'],inplace=True).drop(surgeries_df[surgeries_df["AUTO_ID"]=="NO_MATCH"].index,inplace=True)
# surgeries_df.AUTO_ID=surgeries_df.AUTO_ID.astype(int)
# surgeries_df_agg = surgeries_df.groupby('AUTO_ID').agg({'AUTO_ID':'count', 'ORIG_SERVICE_DATE':'min'})
# surgeries_df_agg['timedelta_yrs'] = surgeries_df_agg.apply(lambda row: max((NOW-row.ORIG_SERVICE_DATE).days/365, 1), axis=1)
# surgeries_df_agg['SURGERIES_PER_YEAR']=surgeries_df_agg.AUTO_ID/surgeries_df_agg.timedelta_yrs
# surgeries_df_agg.drop(columns=['timedelta_yrs'],inplace=True)
# surgeries_df_agg.columns= ['SURGERY_COUNT', 'FIRST_SURGERY_DT', 'SURGERIES_PER_YEAR']
84/108: surgeries_df
84/109: surgeries_df[surgeries_df.unrelated>0]
84/110: surgeries_df[surgeries_df.unrelated>0].AMOUNT.sum()
84/111: surgeries_df[surgeries_df.unrelated==0].AMOUNT.sum()
84/112: surgeries_df[surgeries_df.unrelated==0].sample(50)
84/113: surgeries_df[surgeries_df.PROC_GROUP_NAME=='EAR, NOSE & THROAT']
84/114: surgeries_df[surgeries_df.PROC_GROUP_NAME=='EAR, NOSE & THROAT'].sample(50)
84/115:
# surgeries_df = outpatient_charges_transactions[outpatient_charges_transactions["TYPE_OF_SERVICE"]=='20-SURGERY MA'] 
# surgeries_df.drop_duplicates(subset=["AUTO_ID","ORIG_SERVICE_DATE"],inplace=True)

####
surgeries_df.PROC_GROUP_NAME.values_counts().iloc[:100]
84/116:
# surgeries_df = outpatient_charges_transactions[outpatient_charges_transactions["TYPE_OF_SERVICE"]=='20-SURGERY MA'] 
# surgeries_df.drop_duplicates(subset=["AUTO_ID","ORIG_SERVICE_DATE"],inplace=True)

####
surgeries_df.PROC_GROUP_NAME.value_counts().iloc[:100]
84/117:
# surgeries_df = outpatient_charges_transactions[outpatient_charges_transactions["TYPE_OF_SERVICE"]=='20-SURGERY MA'] 
# surgeries_df.drop_duplicates(subset=["AUTO_ID","ORIG_SERVICE_DATE"],inplace=True)

####
FEMALE GENITAL & REPRODUCTIVE

surgeries_df[surgeries_df.unrelated==0].PROC_GROUP_NAME.value_counts().iloc[:100]
84/118:
# surgeries_df = outpatient_charges_transactions[outpatient_charges_transactions["TYPE_OF_SERVICE"]=='20-SURGERY MA'] 
# surgeries_df.drop_duplicates(subset=["AUTO_ID","ORIG_SERVICE_DATE"],inplace=True)

####
# FEMALE GENITAL & REPRODUCTIVE

surgeries_df[surgeries_df.unrelated==0].PROC_GROUP_NAME.value_counts().iloc[:100]
84/119: surgeries_df[surgeries_df.PROC_GROUP_NAME=='INTEGUMENTARY/DERMATOLOGY '].sample(50)
84/120: surgeries_df[surgeries_df.PROC_GROUP_NAME=='INTEGUMENTARY/DERMATOLOGY'].sample(50)
84/121: surgeries_df[surgeries_df.PROC_GROUP_NAME=='CARDIOVASCULAR/THORACIC'].sample(50)
84/122: surgeries_df[surgeries_df.PROC_GROUP_NAME=='CARDIOVASCULAR/THORACIC'].sample(50).AMOUNT.sum()
84/123: surgeries_df[surgeries_df.PROC_GROUP_NAME=='MUSCLE/SKELETAL SYSTEM'].sample(50).AMOUNT.sum()
84/124: surgeries_df[surgeries_df.PROC_GROUP_NAME=='MUSCLE/SKELETAL SYSTEM'].sample(50)#.AMOUNT.sum()
84/125: surgeries_df[surgeries_df.PROC_GROUP_NAME=='NERVOUS SYSTEM & NEURO STUDIES'].sample(50)#.AMOUNT.sum()
84/126:
mrn_auto_map = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'MRN_AUTO_mapping.csv'))
mrn_auto_map['PAT_MRN_ID'] = mrn_auto_map['PAT_MRN_ID'].astype(int)

exclude_patients = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'cancer_transplant.csv'))
exclude_patients['PAT_MRN_ID'] = exclude_patients['PAT_MRN_ID'].astype(int)
exclude_patients = exclude_patients.merge(mrn_auto_map, how='left', on='PAT_MRN_ID')
84/127:
mrn_auto_map = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'MRN_AUTO_mapping.csv'))
mrn_auto_map['PAT_MRN_ID'] = mrn_auto_map['PAT_MRN_ID'].astype(int)

exclude_patients = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'cancer_transplants.csv'))
exclude_patients['PAT_MRN_ID'] = exclude_patients['PAT_MRN_ID'].astype(int)
exclude_patients = exclude_patients.merge(mrn_auto_map, how='left', on='PAT_MRN_ID')
84/128: exclude_patients
84/129:
x=exclude_patients[(exclude_patients['Transplant_ kidney_liver_intest_cholangitis']==0) & (exclude_patients['Cancer']==0)]
x
84/130: exclude_patients
84/131: exclude_patients.columns
84/132:
x=exclude_patients.fillna(0)[(exclude_patients['Transplant_ kidney_liver_intest_cholangitis']==0) & (exclude_patients[' Cancer']==0)]
x
84/133:
x=exclude_patients.fillna(0)[(exclude_patients['Transplant_ kidney_liver_intest_cholangitis']=='0') & (exclude_patients[' Cancer']=='0')]
x
84/134: exclude_patients.fillna(0)
84/135:
mrn_auto_map = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'MRN_AUTO_mapping.csv'))
mrn_auto_map['PAT_MRN_ID'] = mrn_auto_map['PAT_MRN_ID'].astype(int)

exclude_patients = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'cancer_transplants.csv'))
exclude_patients['PAT_MRN_ID'] = exclude_patients['PAT_MRN_ID'].astype(int)
exclude_patients = exclude_patients.merge(mrn_auto_map, how='left', on='PAT_MRN_ID')
84/136: exclude_patients.fillna(0)
84/137:
mrn_auto_map = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'MRN_AUTO_mapping.csv'))
mrn_auto_map['PAT_MRN_ID'] = mrn_auto_map['PAT_MRN_ID'].astype(int)

exclude_patients = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'cancer_transplants.csv'))
exclude_patients['PAT_MRN_ID'] = exclude_patients['PAT_MRN_ID'].astype(int)
EXCLUDE = exclude_patients.merge(mrn_auto_map, how='left', on='PAT_MRN_ID')['AUTO_ID']
84/138: EXCLUDE
84/139:
# surgeries_df = outpatient_charges_transactions[outpatient_charges_transactions["TYPE_OF_SERVICE"]=='20-SURGERY MA'] 
# surgeries_df.drop_duplicates(subset=["AUTO_ID","ORIG_SERVICE_DATE"],inplace=True)

####
# FEMALE GENITAL & REPRODUCTIVE, INTEGUMENTARY/DERMATOLOGY, MUSCLE/SKELETAL SYSTEM

surgeries_df[surgeries_df.unrelated==1].PROC_GROUP_NAME.value_counts()
84/140:
# surgeries_df = outpatient_charges_transactions[outpatient_charges_transactions["TYPE_OF_SERVICE"]=='20-SURGERY MA'] 
# surgeries_df.drop_duplicates(subset=["AUTO_ID","ORIG_SERVICE_DATE"],inplace=True)

####
# FEMALE GENITAL & REPRODUCTIVE, INTEGUMENTARY/DERMATOLOGY, MUSCLE/SKELETAL SYSTEM

surgeries_df[surgeries_df.unrelated==0].PROC_GROUP_NAME.value_counts()
84/141:
# surgeries_df = outpatient_charges_transactions[outpatient_charges_transactions["TYPE_OF_SERVICE"]=='20-SURGERY MA'] 
# surgeries_df.drop_duplicates(subset=["AUTO_ID","ORIG_SERVICE_DATE"],inplace=True)

####
# FEMALE GENITAL & REPRODUCTIVE, INTEGUMENTARY/DERMATOLOGY, MUSCLE/SKELETAL SYSTEM

surgeries_df[~surgeries_df.AUTO_ID.isin(EXCLUDE)][surgeries_df.unrelated==0].PROC_GROUP_NAME.value_counts()
84/142: surgeries_df[surgeries_df.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY'].sample(50)#.AMOUNT.sum()
84/143: surgeries_df[surgeries_df.PROC_GROUP_NAME=='GENERAL SURGERY'].sample(50)#.AMOUNT.sum()
84/144: surgeries_df[surgeries_df.PROC_GROUP_NAME=='MALE GENITAL'].sample(50)#.AMOUNT.sum()
84/145: surgeries_df[surgeries_df.PROC_GROUP_NAME=='MALE GENITAL']#.sample(50)#.AMOUNT.sum()
84/146:
# surgeries_df = outpatient_charges_transactions[outpatient_charges_transactions["TYPE_OF_SERVICE"]=='20-SURGERY MA'] 
# surgeries_df.drop_duplicates(subset=["AUTO_ID","ORIG_SERVICE_DATE"],inplace=True)

####
unrelated_proc = ['FEMALE GENITAL & REPRODUCTIVE', 'INTEGUMENTARY/DERMATOLOGY', 'MUSCLE/SKELETAL SYSTEM', 'NERVOUS SYSTEM & NEURO STUDIES', 'MATERNITY CARE & DELIVERY', 'EAR, NOSE & THROAT']
surgeries_df[surgeries_df.PROC_GROUP_NAME.isin(unrelated_proc)]['unrelated']=1
84/147: surgeries_df.unrelated.value_counts()
84/148:
enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'CONSULT'})

# # related/unrelated
# enc_df['unrelated'] = 0

# enc_df[enc_df.DEPT_NAME]


# ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE']]
# pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC').to_csv(os.path.join(OUT_DATA_PATH, 'encounters.csv'))
85/1:
%matplotlib inline
import os
import pandas as pd
import numpy as np
import config as cfg
86/1:
%matplotlib inline
import os
import pandas as pd
import numpy as np
import config as cfg
86/2:
enc_df = pd.read_csv(os.path.join(IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'CONSULT'})

# related/unrelated
# enc_df['unrelated'] = 0

# enc_df[enc_df.DEPT_NAME]


# ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE']]
# pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC').to_csv(os.path.join(OUT_DATA_PATH, 'encounters.csv'))
86/3:
enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'CONSULT'})

# related/unrelated
# enc_df['unrelated'] = 0

# enc_df[enc_df.DEPT_NAME]


# ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE']]
# pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC').to_csv(os.path.join(OUT_DATA_PATH, 'encounters.csv'))
86/4: enc_df.DEPT_NAME.value_counts()
86/5: enc_df.DEPT_NAME.value_counts().sort_values(ascending=False).iloc[:100]
86/6: enc_df.DEPT_NAME.value_counts().sort_values(ascending=True).iloc[:100]
86/7: enc_df.DEPT_NAME.value_counts().sort_values(ascending=True).iloc[100:200]
86/8: enc_df.DEPT_NAME.value_counts().sort_values(ascending=True).iloc[:500]
86/9: enc_df.DEPT_NAME.value_counts().sort_values(ascending=True).iloc[:1000]
86/10:
s=enc_df.DEPT_NAME.value_counts()#.sort_values(ascending=True).iloc[:1000]
s[s<=10].shape
86/11:
s=enc_df.DEPT_NAME.value_counts().iloc[:100]
s[s<=10].shape
86/12: enc_df.DEPT_NAME.value_counts().iloc[:100]
86/13: enc_df.DEPT_NAME.value_counts().iloc[:100].index
86/14:
import re
print(re.match('DERM ', 'DERM SHADYSIDE OFFICE'))
86/15:
import re
print(re.match('DERM ', 'M SHADYSIDE OFFICE'))
86/16:
import re
p = re.compile(r'DERM |ORTHO ')
print(re.match('DERM ', 'DERM SHADYSIDE OFFICE'))
86/17:
import re
p = re.compile(r'DERM |ORTHO ')
print(p.match('DERM SHADYSIDE OFFICE'))
86/18:
import re
p = re.compile(r'DERM |ORTHO ')
print(p.match('DERM SHADYSIDE OFFICE'))
86/19:
import re
p = re.compile(r'DERM |ORTHO ')
print(p.match('M SHADYSIDE OFFICE'))
86/20:
import re
p = re.compile(r'DERM |ORTHO ')
print(p.match('ORTHO SHADYSIDE OFFICE'))
86/21:
enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'CONSULT'})

# related/unrelated
enc_df['unrelated'] = 0
bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY ')
enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)


# ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE']]
# pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC').to_csv(os.path.join(OUT_DATA_PATH, 'encounters.csv'))
86/22: enc_df.head().DEPT_NAME.str.match(bad_dept)
86/23: enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept)
86/24: enc_df['unrelated'].value_counts()
86/25: enc_df['unrelated'].isnull().sum()
86/26: enc_df[enc_df['unrelated'].isnull()].DEPT_NAME
86/27:
# enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
# enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'CONSULT'})
enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]
# related/unrelated
enc_df['unrelated'] = 0
bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY ')
enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)


# ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE']]
# pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC').to_csv(os.path.join(OUT_DATA_PATH, 'encounters.csv'))
86/28:
# enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
# enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'CONSULT'})
enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]

# related/unrelated
enc_df['unrelated'] = 0
bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY ')
enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)


# ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE']]
# pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC').to_csv(os.path.join(OUT_DATA_PATH, 'encounters.csv'))
86/29: enc_df['unrelated'].value_counts()
86/30: enc_df.head()
86/31: enc_df[enc_df.ENC_TYPE_NAME='PROC']
86/32: enc_df[enc_df.ENC_TYPE_NAME=='PROC']
86/33:
# enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
# enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'CONSULT'})
# enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]

# related/unrelated
enc_df['unrelated'] = 0
bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY |ENT MERCY')
enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)


# ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE']]
# pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC').to_csv(os.path.join(OUT_DATA_PATH, 'encounters.csv'))
86/34: enc_df[enc_df.unrelated==0 & enc_df.ENC_TYPE_NAME=='PROC']
86/35: enc_df[(enc_df.unrelated==0) & (enc_df.ENC_TYPE_NAME=='PROC')]
86/36:
diagnostic = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Rad_Diagnostic_Tests_2018-07-05-12-19-00.csv'))
diagnostic = diagnostic[diagnostic.ORDER_TYPE=='GI PROCEDURES']
diagnostic.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'gi_diagnostics.csv'))
86/37: diagnostic.columns
86/38:
diagnostic = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Rad_Diagnostic_Tests_2018-07-05-12-19-00.csv'))
diagnostic = diagnostic[diagnostic.ORDER_TYPE=='GI PROCEDURES']
diagnostic[['AUTO_ID', 'VISIT_DATE', 'DESCRIPTION']].to_csv(os.path.join(cfg.OUT_DATA_PATH, 'gi_diagnostics.csv'))
86/39: enc_df[(enc_df.unrelated==0) & (enc_df.ENC_TYPE_NAME=='PROC')].DEPT_NAME.value_counts()
86/40: enc_df[(enc_df.unrelated==1) & (enc_df.ENC_TYPE_NAME=='PROC')].DEPT_NAME.value_counts()
86/41: enc_df[(enc_df.unrelated==0) & (enc_df.ENC_TYPE_NAME=='PROC')].DEPT_NAME.value_counts()
86/42:
# enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
# enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'CONSULT'})
# enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]

# related/unrelated
# enc_df['unrelated'] = 0
# bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY |ENT MERCY')
# enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)

ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE', 'unrelated']]
pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC')#.to_csv(os.path.join(OUT_DATA_PATH, 'encounters.csv'))
86/43:
# enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
# enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'CONSULT'})
# enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]

# related/unrelated
# enc_df['unrelated'] = 0
# bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY |ENT MERCY')
# enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)

ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE', 'unrelated']]
pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC').to_csv(os.path.join(OUT_DATA_PATH, 'encounters.csv'), index=False)
86/44:
# enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
# enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'CONSULT'})
# enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]

# related/unrelated
# enc_df['unrelated'] = 0
# bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY |ENT MERCY')
# enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)

ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE', 'unrelated']]
pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC').to_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'), index=False)
90/1: import pandas as pd
90/2: diag = pd.read_csv('gi_diagnostics.csv')
90/3: diag.head()
90/4: diag.DESCRIPTION.value_counts()
90/5: diag.DESCRIPTION.value_counts()[:20]
90/6: diag.['GROUP']=np.nan
90/7: diag['GROUP']=np.nan
90/8: import numpy as np
90/9: diag.DESCRIPTION.value_counts()[:20]
90/10: diag.DESCRIPTION.value_counts()[:40]
90/11: diag.DESCRIPTION.value_counts()[:15]
90/12: diag[diag.DESCRIPTION.str.match('COLONOSCOP')]
90/13: diag[diag.DESCRIPTION.str.match('COLONOSCOP')].DESCRIPTION.value_counts()
90/14: del diag['GROUP']
90/15: del diag['GROUP']
90/16: diag.columns
90/17: diag.isnull().sum()
90/18: diag[diag.DESCRIPTION.str.match('COLONOSCOP')]['COLONOSCOPY'] = 1
90/19: diag[diag.DESCRIPTION.str.match('ENDOSCOP')]['ENDOSCOPY'] = 1
90/20: diag.DESCRIPTION.value_counts()[:15]
90/21: diag[diag.DESCRIPTION.str.match('SIGMOIDOSCOP')]['SIGMOIDOSCOPY'] = 1
90/22: diag[diag.DESCRIPTION.str.match('ILEOSCOP')]['ILEOSCOPY'] = 1
90/23: diag[diag.DESCRIPTION.str.match('ERCP')]['ERCP'] = 1
90/24: diag[diag.DESCRIPTION.str.match('ESOPHOGASTRODUODENOSCOPY')]['EGD'] = 1
90/25: diag[diag.DESCRIPTION.str.match('UPPER GI ENDOSCOPY')].DESCRIPTION.value_counts()
90/26: diag[diag.DESCRIPTION.str.match('UPPER GI ENDOSCOPY')]['EGD']=1
90/27: diag.DESCRIPTION.value_counts()[15:]
90/28: diag.DESCRIPTION.value_counts()[15:30]
90/29: diag[diag.DESCRIPTION.str.match('UPPER EUS')]['EGD']=1
90/30: diag.DESCRIPTION.value_counts()[30:50]
90/31: diag.head()
87/1:
diagnostic = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Rad_Diagnostic_Tests_2018-07-05-12-19-00.csv'))
diag = diagnostic[diagnostic.ORDER_TYPE=='GI PROCEDURES']

def onehot_flag(col_name, pat):
    diag[col_name] = 0
    diag[diag.DESCRIPTION.str.match(pat), col_name] = 1 

for c,p in [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'),\
            ('ERCP', 'ERCP'), ('ESOPHOGASTRODUODENOSCOPY', 'EGD'), ('UPPER GI ENDO', 'EGD'), ('UPPER EUS', 'EGD')]:
    onehot_flag(c, p)


# diagnostic[['AUTO_ID', 'VISIT_DATE', 'DESCRIPTION']].to_csv(os.path.join(cfg.OUT_DATA_PATH, 'gi_diagnostics.csv'))
87/2:
%matplotlib inline
import os
import pandas as pd
import numpy as np
import config as cfg
87/3:
diagnostic = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Rad_Diagnostic_Tests_2018-07-05-12-19-00.csv'))
diag = diagnostic[diagnostic.ORDER_TYPE=='GI PROCEDURES']

def onehot_flag(col_name, pat):
    diag[col_name] = 0
    diag[diag.DESCRIPTION.str.match(pat), col_name] = 1 

col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'),\
            ('ERCP', 'ERCP'), ('ESOPHOGASTRODUODENOSCOPY', 'EGD'), ('UPPER GI ENDO', 'EGD'), ('UPPER EUS', 'EGD')]
for c,p in col_pat:
    onehot_flag(c, p)



# diagnostic[['AUTO_ID', 'VISIT_DATE', 'DESCRIPTION']].to_csv(os.path.join(cfg.OUT_DATA_PATH, 'gi_diagnostics.csv'))
87/4:
# diagnostic = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Rad_Diagnostic_Tests_2018-07-05-12-19-00.csv'))
# diag = diagnostic[diagnostic.ORDER_TYPE=='GI PROCEDURES']

def onehot_flag(col_name, pat):
    diag[col_name] = 0
    diag[diag.DESCRIPTION.str.match(pat)][col_name] = 1 

col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'),\
            ('ERCP', 'ERCP'), ('ESOPHOGASTRODUODENOSCOPY', 'EGD'), ('UPPER GI ENDO', 'EGD'), ('UPPER EUS', 'EGD')]
for c,p in col_pat:
    onehot_flag(c, p)



# diagnostic[['AUTO_ID', 'VISIT_DATE', 'DESCRIPTION']].to_csv(os.path.join(cfg.OUT_DATA_PATH, 'gi_diagnostics.csv'))
87/5: diag[c for c,_ in col_pat].any()
87/6: diag[[c for c,_ in col_pat]].any()
87/7: diag#[[c for c,_ in col_pat]].any()
87/8:
# diagnostic = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Rad_Diagnostic_Tests_2018-07-05-12-19-00.csv'))
# diag = diagnostic[diagnostic.ORDER_TYPE=='GI PROCEDURES']

def onehot_flag(col_name, pat):
    diag[col_name] = 0
    diag.loc[diag.DESCRIPTION.str.match(pat), col_name] = 1 

col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'),\
            ('ERCP', 'ERCP'), ('ESOPHOGASTRODUODENOSCOPY', 'EGD'), ('UPPER GI ENDO', 'EGD'), ('UPPER EUS', 'EGD')]
for c,p in col_pat:
    onehot_flag(c, p)



# diagnostic[['AUTO_ID', 'VISIT_DATE', 'DESCRIPTION']].to_csv(os.path.join(cfg.OUT_DATA_PATH, 'gi_diagnostics.csv'))
87/9: diag[[c for c,_ in col_pat]].any()
87/10: diag[[c for c,_ in col_pat]].any(axis=1)
87/11: diag#[[c for c,_ in col_pat]].any(axis=1)
87/12:
# diagnostic = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Rad_Diagnostic_Tests_2018-07-05-12-19-00.csv'))
diag = diagnostic[diagnostic.ORDER_TYPE=='GI PROCEDURES']

def onehot_flag(col_name, pat):
    diag[col_name] = 0
    diag.loc[diag.DESCRIPTION.str.match(pat), col_name] = 1 

col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'),\
            ('ERCP', 'ERCP'), ('ESOPHOGASTRODUODENOSCOPY', 'EGD'), ('UPPER GI ENDO', 'EGD'), ('UPPER EUS', 'EGD')]
for c,p in col_pat:
    onehot_flag(c, p)



# diagnostic[['AUTO_ID', 'VISIT_DATE', 'DESCRIPTION']].to_csv(os.path.join(cfg.OUT_DATA_PATH, 'gi_diagnostics.csv'))
87/13: diag#[[c for c,_ in col_pat]].any(axis=1)
87/14:
diag[diag.DESCRIPTION.str.match('ENDOSCOP')]
#[[c for c,_ in col_pat]].any(axis=1)
87/15:
diag[diag.DESCRIPTION.str.match('*ENDOSCOP*')]
#[[c for c,_ in col_pat]].any(axis=1)
87/16:
diag[diag.DESCRIPTION.str.match(r'*ENDOSCOP*')]
#[[c for c,_ in col_pat]].any(axis=1)
87/17:
diag[diag.DESCRIPTION.str.search('ENDOSCOP')]
#[[c for c,_ in col_pat]].any(axis=1)
87/18:
# diagnostic = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Rad_Diagnostic_Tests_2018-07-05-12-19-00.csv'))
diag = diagnostic[diagnostic.ORDER_TYPE=='GI PROCEDURES']

def onehot_flag(col_name, pat):
    diag[col_name] = 0
    diag.loc[diag.DESCRIPTION.str.contains(pat), col_name] = 1 

col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'),\
            ('ERCP', 'ERCP'), ('ESOPHOGASTRODUODENOSCOPY', 'EGD'), ('UPPER GI ENDO', 'EGD'), ('UPPER EUS', 'EGD')]
for c,p in col_pat:
    onehot_flag(c, p)



# diagnostic[['AUTO_ID', 'VISIT_DATE', 'DESCRIPTION']].to_csv(os.path.join(cfg.OUT_DATA_PATH, 'gi_diagnostics.csv'))
87/19:
diag[diag.DESCRIPTION.str.search('ENDOSCOP')]
#[[c for c,_ in col_pat]].any(axis=1)
87/20:
diag#[diag.DESCRIPTION.str.search('ENDOSCOP')]
#[[c for c,_ in col_pat]].any(axis=1)
87/21: diag[[c for c,_ in col_pat]].any(axis=1)
87/22: diag[[c for c,_ in col_pat]].any(axis=1).sum()
87/23: diag[[c for c,_ in col_pat]].any(axis=1)
87/24:
diag['GI_PROCEDURE'] = 0
diag.loc[~diag[[c for c,_ in col_pat]].any(axis=1), 'GI_PROCEDURE'] = 1
87/25:
# diag['GI_PROCEDURE'] = 0
# diag.loc[~diag[[c for c,_ in col_pat]].any(axis=1), 'GI_PROCEDURE'] = 1
diag
87/26:
# diag['GI_PROCEDURE'] = 0
# diag.loc[~diag[[c for c,_ in col_pat]].any(axis=1), 'GI_PROCEDURE'] = 1
diag.GI_PROCEDURE.sum()
87/27:
# diagnostic = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Rad_Diagnostic_Tests_2018-07-05-12-19-00.csv'))
diag = diagnostic[diagnostic.ORDER_TYPE=='GI PROCEDURES']

def onehot_flag(col_name, pat):
    diag[col_name] = 0
    diag.loc[diag.DESCRIPTION.str.contains(pat), col_name] = 1 

col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'),\
            ('ERCP', 'ERCP'), ('ESOPHOGASTRODUODENOSCOPY', 'EGD'), ('UPPER GI ENDO', 'EGD'), ('UPPER EUS', 'EGD')]
for c,p in col_pat:
    onehot_flag(c, p)

diag['GI_PROCEDURE'] = 0
diag.loc[~diag[[c for c,_ in col_pat]].any(axis=1), 'GI_PROCEDURE'] = 1

diagnostic[['AUTO_ID', 'VISIT_DATE', 'DESCRIPTION']+[c for c,_ in col_pat]+['GI_PROCEDURE']].to_csv(os.path.join(cfg.OUT_DATA_PATH, 'gi_diagnostics.csv'))
87/28:
# diagnostic = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Rad_Diagnostic_Tests_2018-07-05-12-19-00.csv'))
diag = diagnostic[diagnostic.ORDER_TYPE=='GI PROCEDURES']

def onehot_flag(col_name, pat):
    diag[col_name] = 0
    diag.loc[diag.DESCRIPTION.str.contains(pat), col_name] = 1 

col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'),\
            ('ERCP', 'ERCP'), ('ESOPHOGASTRODUODENOSCOPY', 'EGD'), ('UPPER GI ENDO', 'EGD'), ('UPPER EUS', 'EGD')]
for c,p in col_pat:
    onehot_flag(c, p)

diag['GI_PROCEDURE'] = 0
diag.loc[~diag[[c for c,_ in col_pat]].any(axis=1), 'GI_PROCEDURE'] = 1

diag[['AUTO_ID', 'VISIT_DATE', 'DESCRIPTION']+[c for c,_ in col_pat]+['GI_PROCEDURE']].to_csv(os.path.join(cfg.OUT_DATA_PATH, 'gi_diagnostics.csv'))
87/29:
# diagnostic = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Rad_Diagnostic_Tests_2018-07-05-12-19-00.csv'))
diag = diagnostic[diagnostic.ORDER_TYPE=='GI PROCEDURES']

def onehot_flag(col_name, pat):
    diag[col_name] = 0
    diag.loc[diag.DESCRIPTION.str.contains(pat), col_name] = 1 

col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'),\
            ('ERCP', 'ERCP'), ('EGD', 'ESOPHOGASTRODUODENOSCOPY'), ('UPPER GI ENDO', 'EGD'), ('UPPER EUS', 'EGD')]
for c,p in col_pat:
    onehot_flag(c, p)

diag['GI_PROCEDURE'] = 0
diag.loc[~diag[[c for c,_ in col_pat]].any(axis=1), 'GI_PROCEDURE'] = 1

diag[['AUTO_ID', 'VISIT_DATE', 'DESCRIPTION']+[c for c,_ in col_pat]+['GI_PROCEDURE']].to_csv(os.path.join(cfg.OUT_DATA_PATH, 'gi_diagnostics.csv'))
87/30:
inflation_coeff = get_inflation_data()

# unrelated_ip CHARGES -- from Claudia
unrelated_ip = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'ibd_related_ip_charges.csv'), parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE'])
unrelated_ip['PAT_MRN_ID'] = unrelated_ip['PAT_MRN_ID'].astype(int)
unrelated_ip = unrelated_ip.merge(mrn_auto_map[['AUTO_ID', 'PAT_MRN_ID']], on='PAT_MRN_ID')


#INPATIENT
inpatient_charges_txn = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
#adjust for inflation
inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]
inpatient_charges.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']

unrelated_ip = unrelated_ip[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'related']]
inpatient_charges = inpatient_charges.merge(unrelated_ip, how='left',  on=['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE'])
inpatient_charges[['related', 'unrelated', 'unknown']] = pd.get_dummies(inpatient_charges['related'])
inpatient_charges.drop(['related'], inplace=True, axis=1)
inpatient_charges.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'))
87/31:
def get_inflation_data():
    #http://www.usinflationcalculator.com/inflation/current-inflation-rates/
    #I took it from the December column as it is explained on the website (last 12 months inflation)
    #year,inflation
    inflationrates={2005:3.4,
                    2006:2.5,
                    2007:4.1,
                    2008:0.1,
                    2009:2.7,
                    2010:1.5,
                    2011:3.0,
                    2012:1.7,
                    2013:1.5,
                    2014:0.8,
                    2015:0.7,
                    2016:2.1,
                    2017:2.1 } #last update: last charges made in 2018, so need to take into account only till 2017
    for (year,rate) in inflationrates.items():
        inflationrates[year]=rate*0.01+1
    
    #calculate coefficients    
    lastyear=np.max(list(inflationrates.keys()))
    inflation_coeff={lastyear+1:1.0}
    for year in range(lastyear, 2005, -1):
        inflation_coeff[year]=inflation_coeff[year+1]*inflationrates[year]
    return inflation_coeff
87/32:
inflation_coeff = get_inflation_data()

# unrelated_ip CHARGES -- from Claudia
unrelated_ip = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'ibd_related_ip_charges.csv'), parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE'])
unrelated_ip['PAT_MRN_ID'] = unrelated_ip['PAT_MRN_ID'].astype(int)
unrelated_ip = unrelated_ip.merge(mrn_auto_map[['AUTO_ID', 'PAT_MRN_ID']], on='PAT_MRN_ID')


#INPATIENT
inpatient_charges_txn = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
#adjust for inflation
inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]
inpatient_charges.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']

unrelated_ip = unrelated_ip[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'related']]
inpatient_charges = inpatient_charges.merge(unrelated_ip, how='left',  on=['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE'])
inpatient_charges[['related', 'unrelated', 'unknown']] = pd.get_dummies(inpatient_charges['related'])
inpatient_charges.drop(['related'], inplace=True, axis=1)
inpatient_charges.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'))
87/33:
mrn_auto_map = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'MRN_AUTO_mapping.csv'))
mrn_auto_map['PAT_MRN_ID'] = mrn_auto_map['PAT_MRN_ID'].astype(int)

exclude_patients = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'cancer_transplants.csv'))
exclude_patients['PAT_MRN_ID'] = exclude_patients['PAT_MRN_ID'].astype(int)
EXCLUDE = exclude_patients.merge(mrn_auto_map, how='left', on='PAT_MRN_ID')['AUTO_ID']
exclude_patients.to_csv(os.path.join(cfg.OUT_DATA_PATH), 'exclude_patients.csv')
87/34:
mrn_auto_map = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'MRN_AUTO_mapping.csv'))
mrn_auto_map['PAT_MRN_ID'] = mrn_auto_map['PAT_MRN_ID'].astype(int)

exclude_patients = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'cancer_transplants.csv'))
exclude_patients['PAT_MRN_ID'] = exclude_patients['PAT_MRN_ID'].astype(int)
EXCLUDE = exclude_patients.merge(mrn_auto_map, how='left', on='PAT_MRN_ID')['AUTO_ID']
exclude_patients.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'exclude_patients.csv')
87/35:
mrn_auto_map = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'MRN_AUTO_mapping.csv'))
mrn_auto_map['PAT_MRN_ID'] = mrn_auto_map['PAT_MRN_ID'].astype(int)

exclude_patients = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'cancer_transplants.csv'))
exclude_patients['PAT_MRN_ID'] = exclude_patients['PAT_MRN_ID'].astype(int)
EXCLUDE = exclude_patients.merge(mrn_auto_map, how='left', on='PAT_MRN_ID')['AUTO_ID']
exclude_patients.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'exclude_patients.csv'))
87/36:
inflation_coeff = get_inflation_data()

# unrelated_ip CHARGES -- from Claudia
unrelated_ip = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'ibd_related_ip_charges.csv'), parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE'])
unrelated_ip['PAT_MRN_ID'] = unrelated_ip['PAT_MRN_ID'].astype(int)
unrelated_ip = unrelated_ip.merge(mrn_auto_map[['AUTO_ID', 'PAT_MRN_ID']], on='PAT_MRN_ID')


#INPATIENT
inpatient_charges_txn = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
#adjust for inflation
inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]
inpatient_charges.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']

unrelated_ip = unrelated_ip[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'related']]
inpatient_charges = inpatient_charges.merge(unrelated_ip, how='left',  on=['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE'])
inpatient_charges[['related', 'unrelated', 'unknown']] = pd.get_dummies(inpatient_charges['related'])
inpatient_charges.drop(['related'], inplace=True, axis=1)
inpatient_charges.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'))
87/37:
# # OUTPATIENT
outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
outpatient = outpatient[outpatient['AMOUNT']>0]
outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)




# FILTERING BAD PROCEDURES ((422943, 9), (383395, 9), (357001, 10))
# =======================================================================

# Method 0: No filter
# outpatient_charges_transactions0 = outpatient

# Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
outpatient_charges_transactions1 = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
 

# Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
# def separate_dx_codes(opc):
#     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
#     opc2 = opc[~opc.index.isin(opc1.index)]
#     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
#     dx = dx.droplevel(-1)
#     dx.name = 'DX'
#     del opc1['DX']
#     opc1 = opc1.join(dx)
#     opc1 = opc1[opc2.columns]
#     opc = pd.concat([opc1, opc2]).reset_index()
#     return opc
# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]

# =======================================================================

outpatient_charges_transactions = outpatient_charges_transactions1
outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
outpatient_charges_transactions['unrelated'] = outpatient_charges_transactions['unrelated'].fillna(0)
op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE', 'unrelated']).AMOUNT.sum().reset_index()
op['ADMISSION_DATE'] = op['ORIG_SERVICE_DATE']
op['DISCHARGE_DATE'] = op['ORIG_SERVICE_DATE']
op['DAYS_ADMITTED'] = 0

op['unknown'] = 0
op[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT', 'unrelated', 'unknown']].to_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges.csv'))



tot = pd.concat([inpatient_charges, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)
87/38: tot.shape
87/39: inpatient_charges.shape
87/40: op.shape
87/41: outpatient_charges_transactions.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges_txn.csv'))
87/42: inpatient_charges.unknown.value_counts()
87/43:
# inflation_coeff = get_inflation_data()

# # unrelated_ip CHARGES -- from Claudia
# unrelated_ip = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'ibd_related_ip_charges.csv'), parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE'])
# unrelated_ip['PAT_MRN_ID'] = unrelated_ip['PAT_MRN_ID'].astype(int)
# unrelated_ip = unrelated_ip.merge(mrn_auto_map[['AUTO_ID', 'PAT_MRN_ID']], on='PAT_MRN_ID')


# #INPATIENT
# inpatient_charges_txn = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
# inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
# inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
# #adjust for inflation
# inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
# inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

# inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]
inpatient_charges.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']

unrelated_ip = unrelated_ip[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'related']]
inpatient_charges = inpatient_charges.merge(unrelated_ip, how='left',  on=['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE'])
inpatient_charges[['related', 'unrelated']] = pd.get_dummies(inpatient_charges['related'].replace({2:1}))
inpatient_charges.drop(['related'], inplace=True, axis=1)
# inpatient_charges.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'))
87/44: inpatient_charges.unknown.value_counts()
87/45: inpatient_charges.unrelated.value_counts()
87/46:
# inflation_coeff = get_inflation_data()

# # unrelated_ip CHARGES -- from Claudia
# unrelated_ip = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'ibd_related_ip_charges.csv'), parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE'])
# unrelated_ip['PAT_MRN_ID'] = unrelated_ip['PAT_MRN_ID'].astype(int)
# unrelated_ip = unrelated_ip.merge(mrn_auto_map[['AUTO_ID', 'PAT_MRN_ID']], on='PAT_MRN_ID')


# #INPATIENT
# inpatient_charges_txn = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
# inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
# inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
# #adjust for inflation
# inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
# inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

# inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]
inpatient_charges.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT']

unrelated_ip = unrelated_ip[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'related']]
inpatient_charges = inpatient_charges.merge(unrelated_ip, how='left',  on=['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE'])
inpatient_charges[['related', 'unrelated']] = pd.get_dummies(inpatient_charges['related'].replace({2:1}))
inpatient_charges.drop(['related'], inplace=True, axis=1)
inpatient_charges.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'))
87/47: inpatient_charges.DAYS_ADMITTED.min()
87/48: inpatient_charges[inpatient_charges.DAYS_ADMITTED==0]
87/49:
# # OUTPATIENT
outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
outpatient = outpatient[outpatient['AMOUNT']>0]
outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)




# FILTERING BAD PROCEDURES ((422943, 9), (383395, 9), (357001, 10))
# =======================================================================

# Method 0: No filter
# outpatient_charges_transactions0 = outpatient

# Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
outpatient_charges_transactions1 = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
 

# Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
# def separate_dx_codes(opc):
#     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
#     opc2 = opc[~opc.index.isin(opc1.index)]
#     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
#     dx = dx.droplevel(-1)
#     dx.name = 'DX'
#     del opc1['DX']
#     opc1 = opc1.join(dx)
#     opc1 = opc1[opc2.columns]
#     opc = pd.concat([opc1, opc2]).reset_index()
#     return opc
# outpatient = separate_dx_codes(outpatient)
# opc = outpatient[outpatient.DX!='']
# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]

# Method 3: Specify unrelated PROC_GROUP_NAME
bad_proc_grp = ['FEMALE GENITAL & REPRODUCTIVE', 'INTEGUMENTARY/DERMATOLOGY', 'MUSCLE/SKELETAL SYSTEM',\
            'NERVOUS SYSTEM & NEURO STUDIES', 'MATERNITY CARE & DELIVERY', 'EAR, NOSE & THROAT', 'DENTAL', 'DME',\
           'EYE & OCULAR', 'MALE GENITAL', 'OTHER', 'PHYSICAL MEDICINE & REHAB', 'PULMONARY', 'SUPPLIES', 'URINARY/RENAL SYSTEM']

# =======================================================================

outpatient_charges_transactions = outpatient_charges_transactions1
outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
outpatient_charges_transactions.loc[outpatient_charges_transactions.PROC_GROUP_NAME.isin(bad_proc_grp), 'unrelated'] = 1
outpatient_charges_transactions['unrelated'] = outpatient_charges_transactions['unrelated'].fillna(0)
outpatient_charges_transactions.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges_txn.csv'))

# op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE', 'unrelated']).AMOUNT.sum().reset_index()
outpatient_charges_transactions['ADMISSION_DATE'] = outpatient_charges_transactions['ORIG_SERVICE_DATE']
outpatient_charges_transactions['DISCHARGE_DATE'] = outpatient_charges_transactions['ORIG_SERVICE_DATE']
outpatient_charges_transactions['DAYS_ADMITTED'] = 0
outpatient_charges_transactions[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT', 'unrelated']].to_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges.csv'))

tot = pd.concat([inpatient_charges, outpatient_charges_transactions], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
# tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
# tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)
87/50:
# # # OUTPATIENT
# outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
# outpatient = outpatient[outpatient['AMOUNT']>0]
# outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
# outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
# outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)




# # FILTERING BAD PROCEDURES ((422943, 9), (383395, 9), (357001, 10))
# # =======================================================================

# # Method 0: No filter
# # outpatient_charges_transactions0 = outpatient

# # Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_charges_transactions1 = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
 

# # Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
# # def separate_dx_codes(opc):
# #     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
# #     opc2 = opc[~opc.index.isin(opc1.index)]
# #     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
# #     dx = dx.droplevel(-1)
# #     dx.name = 'DX'
# #     del opc1['DX']
# #     opc1 = opc1.join(dx)
# #     opc1 = opc1[opc2.columns]
# #     opc = pd.concat([opc1, opc2]).reset_index()
# #     return opc
# # outpatient = separate_dx_codes(outpatient)
# # opc = outpatient[outpatient.DX!='']
# # bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# # opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]

# # Method 3: Specify unrelated PROC_GROUP_NAME
# bad_proc_grp = ['FEMALE GENITAL & REPRODUCTIVE', 'INTEGUMENTARY/DERMATOLOGY', 'MUSCLE/SKELETAL SYSTEM',\
#             'NERVOUS SYSTEM & NEURO STUDIES', 'MATERNITY CARE & DELIVERY', 'EAR, NOSE & THROAT', 'DENTAL', 'DME',\
#            'EYE & OCULAR', 'MALE GENITAL', 'OTHER', 'PHYSICAL MEDICINE & REHAB', 'PULMONARY', 'SUPPLIES', 'URINARY/RENAL SYSTEM']

# # =======================================================================

# outpatient_charges_transactions = outpatient_charges_transactions1
# outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
# outpatient_charges_transactions.loc[outpatient_charges_transactions.PROC_GROUP_NAME.isin(bad_proc_grp), 'unrelated'] = 1
# outpatient_charges_transactions['unrelated'] = outpatient_charges_transactions['unrelated'].fillna(0)
outpatient_charges_transactions.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges_txn.csv'))

# op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE', 'unrelated']).AMOUNT.sum().reset_index()
outpatient_charges_transactions['ADMISSION_DATE'] = outpatient_charges_transactions['ORIG_SERVICE_DATE']
outpatient_charges_transactions['DISCHARGE_DATE'] = outpatient_charges_transactions['ORIG_SERVICE_DATE']
outpatient_charges_transactions['DAYS_ADMITTED'] = 0
outpatient_charges_transactions[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT', 'unrelated']].to_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges.csv'))

tot = pd.concat([inpatient_charges, outpatient_charges_transactions], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
# tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
# tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)
87/51: tot.groupby(['AUTO_ID', 'unrelated']).AMOUNT.sum()
87/52: tot.groupby(['AUTO_ID', 'unrelated']).AMOUNT.cumsum()
87/53: tot['chk'] = tot.groupby(['AUTO_ID', 'unrelated']).AMOUNT.cumsum()
87/54: tot.head(30)
87/55:
# # # OUTPATIENT
# outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
# outpatient = outpatient[outpatient['AMOUNT']>0]
# outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
# outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
# outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)




# # FILTERING BAD PROCEDURES ((422943, 9), (383395, 9), (357001, 10))
# # =======================================================================

# # Method 0: No filter
# # outpatient_charges_transactions0 = outpatient

# # Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_charges_transactions1 = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
 

# # Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
# # def separate_dx_codes(opc):
# #     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
# #     opc2 = opc[~opc.index.isin(opc1.index)]
# #     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
# #     dx = dx.droplevel(-1)
# #     dx.name = 'DX'
# #     del opc1['DX']
# #     opc1 = opc1.join(dx)
# #     opc1 = opc1[opc2.columns]
# #     opc = pd.concat([opc1, opc2]).reset_index()
# #     return opc
# # outpatient = separate_dx_codes(outpatient)
# # opc = outpatient[outpatient.DX!='']
# # bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# # opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]

# # Method 3: Specify unrelated PROC_GROUP_NAME
# bad_proc_grp = ['FEMALE GENITAL & REPRODUCTIVE', 'INTEGUMENTARY/DERMATOLOGY', 'MUSCLE/SKELETAL SYSTEM',\
#             'NERVOUS SYSTEM & NEURO STUDIES', 'MATERNITY CARE & DELIVERY', 'EAR, NOSE & THROAT', 'DENTAL', 'DME',\
#            'EYE & OCULAR', 'MALE GENITAL', 'OTHER', 'PHYSICAL MEDICINE & REHAB', 'PULMONARY', 'SUPPLIES', 'URINARY/RENAL SYSTEM']

# # =======================================================================

# outpatient_charges_transactions = outpatient_charges_transactions1
# outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
# outpatient_charges_transactions.loc[outpatient_charges_transactions.PROC_GROUP_NAME.isin(bad_proc_grp), 'unrelated'] = 1
# outpatient_charges_transactions['unrelated'] = outpatient_charges_transactions['unrelated'].fillna(0)
# outpatient_charges_transactions.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges_txn.csv'))

# # op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE', 'unrelated']).AMOUNT.sum().reset_index()
# outpatient_charges_transactions['ADMISSION_DATE'] = outpatient_charges_transactions['ORIG_SERVICE_DATE']
# outpatient_charges_transactions['DISCHARGE_DATE'] = outpatient_charges_transactions['ORIG_SERVICE_DATE']
# outpatient_charges_transactions['DAYS_ADMITTED'] = 0
# op = outpatient_charges_transactions[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT', 'unrelated']]
# op.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges.csv'))

tot = pd.concat([inpatient_charges, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
# tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
# tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)
87/56:
# # # OUTPATIENT
# outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
# outpatient = outpatient[outpatient['AMOUNT']>0]
# outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
# outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
# outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)




# # FILTERING BAD PROCEDURES ((422943, 9), (383395, 9), (357001, 10))
# # =======================================================================

# # Method 0: No filter
# # outpatient_charges_transactions0 = outpatient

# # Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_charges_transactions1 = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
 

# # Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
# # def separate_dx_codes(opc):
# #     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
# #     opc2 = opc[~opc.index.isin(opc1.index)]
# #     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
# #     dx = dx.droplevel(-1)
# #     dx.name = 'DX'
# #     del opc1['DX']
# #     opc1 = opc1.join(dx)
# #     opc1 = opc1[opc2.columns]
# #     opc = pd.concat([opc1, opc2]).reset_index()
# #     return opc
# # outpatient = separate_dx_codes(outpatient)
# # opc = outpatient[outpatient.DX!='']
# # bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# # opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]

# # Method 3: Specify unrelated PROC_GROUP_NAME
# bad_proc_grp = ['FEMALE GENITAL & REPRODUCTIVE', 'INTEGUMENTARY/DERMATOLOGY', 'MUSCLE/SKELETAL SYSTEM',\
#             'NERVOUS SYSTEM & NEURO STUDIES', 'MATERNITY CARE & DELIVERY', 'EAR, NOSE & THROAT', 'DENTAL', 'DME',\
#            'EYE & OCULAR', 'MALE GENITAL', 'OTHER', 'PHYSICAL MEDICINE & REHAB', 'PULMONARY', 'SUPPLIES', 'URINARY/RENAL SYSTEM']

# # =======================================================================

# outpatient_charges_transactions = outpatient_charges_transactions1
# outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
# outpatient_charges_transactions.loc[outpatient_charges_transactions.PROC_GROUP_NAME.isin(bad_proc_grp), 'unrelated'] = 1
# outpatient_charges_transactions['unrelated'] = outpatient_charges_transactions['unrelated'].fillna(0)
# outpatient_charges_transactions.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges_txn.csv'))

# # op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE', 'unrelated']).AMOUNT.sum().reset_index()
# outpatient_charges_transactions['ADMISSION_DATE'] = outpatient_charges_transactions['ORIG_SERVICE_DATE']
# outpatient_charges_transactions['DISCHARGE_DATE'] = outpatient_charges_transactions['ORIG_SERVICE_DATE']
# outpatient_charges_transactions['DAYS_ADMITTED'] = 0
op = outpatient_charges_transactions[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT', 'unrelated']]
# op.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges.csv'))

tot = pd.concat([inpatient_charges, op], ignore_index=True).sort_values(['AUTO_ID', 'ADMISSION_DATE'])
# tot['CUMULATIVE'] = tot.groupby('AUTO_ID').AMOUNT.cumsum()
# tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)
87/57: tot.head(30)
87/58: tot.head(30)
87/59: tot['chk'] = tot.groupby(['AUTO_ID', 'unrelated']).AMOUNT.cumsum()
87/60: tot.head(30)
87/61: del tot.chk
87/62: del tot['chk']
87/63:
tot['CUMSUM'] = tot.groupby(['AUTO_ID', 'unrelated']).AMOUNT.cumsum()
tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)
87/64:
# diagnostic = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Rad_Diagnostic_Tests_2018-07-05-12-19-00.csv'))
# diag = diagnostic[diagnostic.ORDER_TYPE=='GI PROCEDURES']

# def onehot_flag(col_name, pat):
#     diag[col_name] = 0
#     diag.loc[diag.DESCRIPTION.str.contains(pat), col_name] = 1 

# col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'),\
#             ('ERCP', 'ERCP'), ('EGD', 'ESOPHOGASTRODUODENOSCOPY'), ('UPPER GI ENDO', 'EGD'), ('UPPER EUS', 'EGD')]
# for c,p in col_pat:
#     onehot_flag(c, p)

# diag['GI_PROCEDURE'] = 0
# diag.loc[~diag[[c for c,_ in col_pat]].any(axis=1), 'GI_PROCEDURE'] = 1

diag[['AUTO_ID', 'VISIT_DATE', 'DESCRIPTION']+[c for c,_ in col_pat]+['GI_PROCEDURE']].drop_duplicates().to_csv(os.path.join(cfg.OUT_DATA_PATH, 'gi_diagnostics.csv'))
87/65:
enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'OFF'})
enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]

# == should we drop these? not sure if complete subset of diag.csv. In any case, it is a one-hot column, so keeping for now,
# enc_df = enc_df[enc_df.ENC_TYPE_NAME!='PROC'] # Data captured in GI_Diagnostics.csv

# related/unrelated
enc_df['unrelated'] = 0
bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY |ENT MERCY')
enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)

ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE', 'unrelated']]
pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC').to_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'), index=False)
87/66:
import re

enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'OFF'})
enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]

# == should we drop these? not sure if complete subset of diag.csv. In any case, it is a one-hot column, so keeping for now,
# enc_df = enc_df[enc_df.ENC_TYPE_NAME!='PROC'] # Data captured in GI_Diagnostics.csv

# related/unrelated
enc_df['unrelated'] = 0
bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY |ENT MERCY')
enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)

ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE', 'unrelated']]
pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC').to_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'), index=False)
87/67:
labs = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'labs.csv'))
labs.RESULT_DATE.dt.year
87/68:
labs = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'labs.csv'), parse_dates=['RESULT_DATE'])
labs.RESULT_DATE.dt.year
87/69:
# labs = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'labs.csv'), parse_dates=['RESULT_DATE'])
labs.RESULT_DATE.dt.month
87/70:
labs['RESULT_YEAR'] = labs.RESULT_DATE.dt.year
labs['RESULT_MONTH'] = labs.RESULT_DATE.dt.month
labs = labs[['AUTO_ID', 'RESULT_DATE', 'RESULT_FLAG', 'GROUP', 'NEW_LOW', 'NEW_HIGH', 'ORD_NUM_VALUE', 'RESULT_YEAR', 'RESULT_MONTH']].drop_duplicates(['AUTO_ID', 'RESULT_DATE', 'RESULT_FLAG', 'GROUP'])
labs.to_csv(os.path.join(OUT_DATA_PATH, 'labs.csv'), index=False)
87/71:
labs['RESULT_YEAR'] = labs.RESULT_DATE.dt.year
labs['RESULT_MONTH'] = labs.RESULT_DATE.dt.month
labs = labs[['AUTO_ID', 'RESULT_DATE', 'RESULT_FLAG', 'GROUP', 'NEW_LOW', 'NEW_HIGH', 'ORD_NUM_VALUE', 'RESULT_YEAR', 'RESULT_MONTH']].drop_duplicates(['AUTO_ID', 'RESULT_DATE', 'RESULT_FLAG', 'GROUP'])
labs.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'labs.csv'), index=False)
87/72:
diag['RESULT_YEAR'] = diag.VISIT_DATE.dt.year
diag['RESULT_MONTH'] = diag.VISIT_DATE.dt.month
87/73:
diag['VISIT_DATE'] = pd.to_Datetime(diag.VISIT_DATE)
diag['RESULT_YEAR'] = diag.VISIT_DATE.dt.year
diag['RESULT_MONTH'] = diag.VISIT_DATE.dt.month
87/74:
diag['VISIT_DATE'] = pd.to_datetime(diag.VISIT_DATE)
diag['RESULT_YEAR'] = diag.VISIT_DATE.dt.year
diag['RESULT_MONTH'] = diag.VISIT_DATE.dt.month
87/75:
# diag['VISIT_DATE'] = pd.to_datetime(diag.VISIT_DATE)
# diag['RESULT_YEAR'] = diag.VISIT_DATE.dt.year
# diag['RESULT_MONTH'] = diag.VISIT_DATE.dt.month
diag.head()
87/76:
# diagnostic = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Rad_Diagnostic_Tests_2018-07-05-12-19-00.csv'), parse_dates=['VISIT_DATE'])
# diag = diagnostic[diagnostic.ORDER_TYPE=='GI PROCEDURES']

# def onehot_flag(col_name, pat):
#     diag[col_name] = 0
#     diag.loc[diag.DESCRIPTION.str.contains(pat), col_name] = 1 

# col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'),\
#             ('ERCP', 'ERCP'), ('EGD', 'ESOPHOGASTRODUODENOSCOPY'), ('UPPER GI ENDO', 'EGD'), ('UPPER EUS', 'EGD')]
# for c,p in col_pat:
#     onehot_flag(c, p)

# diag['GI_PROCEDURE'] = 0
# diag.loc[~diag[[c for c,_ in col_pat]].any(axis=1), 'GI_PROCEDURE'] = 1

# diag['RESULT_YEAR'] = diag.VISIT_DATE.dt.year
# diag['RESULT_MONTH'] = diag.VISIT_DATE.dt.month
diag[['AUTO_ID', 'VISIT_DATE', 'DESCRIPTION']+[c for c,_ in col_pat]+['GI_PROCEDURE', 'RESULT_YEAR', 'RESULT_MONTH']].drop_duplicates().to_csv(os.path.join(cfg.OUT_DATA_PATH, 'gi_diagnostics.csv'))
87/77:
# import re

# enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
# enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'OFF'})
# enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]

# # == should we drop these? not sure if complete subset of diag.csv. In any case, it is a one-hot column, so keeping for now,
# # enc_df = enc_df[enc_df.ENC_TYPE_NAME!='PROC'] # Data captured in GI_Diagnostics.csv

# # related/unrelated
# enc_df['unrelated'] = 0
# bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY |ENT MERCY')
# enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)

ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE', 'unrelated']]
ENC = pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC')
ENC['RESULT_YEAR'] = ENC.CONTACT_DATE.dt.year
ENC['RESULT_MONTH'] = ENC.CONTACT_DATE.dt.month
ENC.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'), index=False)
87/78: ENC.head()
87/79:
# # # OUTPATIENT
# outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_OP_charges_2018.csv"), parse_dates=['ORIG_SERVICE_DATE'])
# outpatient = outpatient[outpatient['AMOUNT']>0]
# outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)
# outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']
# outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)




# # FILTERING BAD PROCEDURES ((422943, 9), (383395, 9), (357001, 10))
# # =======================================================================

# # Method 0: No filter
# # outpatient_charges_transactions0 = outpatient

# # Method 1: USe Claudia-provided procedures_to_exclude.csv as mask
# bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))
# outpatient_charges_transactions1 = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()
 

# # Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges
# # def separate_dx_codes(opc):
# #     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]
# #     opc2 = opc[~opc.index.isin(opc1.index)]
# #     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()
# #     dx = dx.droplevel(-1)
# #     dx.name = 'DX'
# #     del opc1['DX']
# #     opc1 = opc1.join(dx)
# #     opc1 = opc1[opc2.columns]
# #     opc = pd.concat([opc1, opc2]).reset_index()
# #     return opc
# # outpatient = separate_dx_codes(outpatient)
# # opc = outpatient[outpatient.DX!='']
# # bad_code_ranges = [(630, 780), (460, 520), (280, 290)]
# # opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]

# # Method 3: Specify unrelated PROC_GROUP_NAME
# bad_proc_grp = ['FEMALE GENITAL & REPRODUCTIVE', 'INTEGUMENTARY/DERMATOLOGY', 'MUSCLE/SKELETAL SYSTEM',\
#             'NERVOUS SYSTEM & NEURO STUDIES', 'MATERNITY CARE & DELIVERY', 'EAR, NOSE & THROAT', 'DENTAL', 'DME',\
#            'EYE & OCULAR', 'MALE GENITAL', 'OTHER', 'PHYSICAL MEDICINE & REHAB', 'PULMONARY', 'SUPPLIES', 'URINARY/RENAL SYSTEM']

# # =======================================================================

# outpatient_charges_transactions = outpatient_charges_transactions1
# outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation
# outpatient_charges_transactions.loc[outpatient_charges_transactions.PROC_GROUP_NAME.isin(bad_proc_grp), 'unrelated'] = 1
# outpatient_charges_transactions['unrelated'] = outpatient_charges_transactions['unrelated'].fillna(0)
# outpatient_charges_transactions.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges_txn.csv'))

# # op = outpatient_charges_transactions.groupby(['AUTO_ID', 'ORIG_SERVICE_DATE', 'unrelated']).AMOUNT.sum().reset_index()
# outpatient_charges_transactions['ADMISSION_DATE'] = outpatient_charges_transactions['ORIG_SERVICE_DATE']
# outpatient_charges_transactions['DISCHARGE_DATE'] = outpatient_charges_transactions['ORIG_SERVICE_DATE']
# outpatient_charges_transactions['DAYS_ADMITTED'] = 0
# op = outpatient_charges_transactions[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT', 'unrelated']]
# op.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges.csv'))

tot = pd.concat([inpatient_charges, op], ignore_index=True).sort_values(['AUTO_ID', 'DISCHARGE_DATE'])
tot['RESULT_YEAR'] = tot.DISCHARGE_DATE.dt.year
tot['RESULT_MONTH'] = tot.DISCHARGE_DATE.dt.month
tot['CUMSUM'] = tot.groupby(['AUTO_ID', 'unrelated']).AMOUNT.cumsum()
tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)
87/80:
# diagnostic = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Rad_Diagnostic_Tests_2018-07-05-12-19-00.csv'), parse_dates=['VISIT_DATE'])
# diag = diagnostic[diagnostic.ORDER_TYPE=='GI PROCEDURES']

# def onehot_flag(col_name, pat):
#     diag[col_name] = 0
#     diag.loc[diag.DESCRIPTION.str.contains(pat), col_name] = 1 

# col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'),\
#             ('ERCP', 'ERCP'), ('EGD', 'ESOPHOGASTRODUODENOSCOPY'), ('UPPER GI ENDO', 'EGD'), ('UPPER EUS', 'EGD')]
# for c,p in col_pat:
#     onehot_flag(c, p)

# diag['GI_PROCEDURE'] = 0
# diag.loc[~diag[[c for c,_ in col_pat]].any(axis=1), 'GI_PROCEDURE'] = 1

# diag['RESULT_YEAR'] = diag.VISIT_DATE.dt.year
# diag['RESULT_MONTH'] = diag.VISIT_DATE.dt.month
diag[['AUTO_ID', 'VISIT_DATE', 'DESCRIPTION']+[c for c,_ in col_pat]+['GI_PROCEDURE', 'RESULT_YEAR', 'RESULT_MONTH']].drop_duplicates().to_csv(os.path.join(cfg.OUT_DATA_PATH, 'gi_diagnostics.csv'), index=False)
91/1:
import pandas as pd
import config as cfg
91/2:
charges = pd.read_csv('total_charges.csv')
encounters = pd.read_csv('encounters.csv')
91/3:
import pandas as pd
import config as cfg
import os
91/4:
import pandas as pd
import config as cfg
import os

opj = os.path.join
91/5:
charges = pd.read_csv(opj(cfg.OUT_DATA_PATH, 'total_charges.csv'))
encounters = pd.read_csv(opj(cfg.OUT_DATA_PATH'encounters.csv'))
91/6:
charges = pd.read_csv(opj(cfg.OUT_DATA_PATH, 'total_charges.csv'))
encounters = pd.read_csv(opj(cfg.OUT_DATA_PATH,'encounters.csv'))
91/7: encounters
91/8: charges
92/1: unrelated_ip
92/2:
%matplotlib inline
import os
import pandas as pd
import numpy as np
import config as cfg
92/3:
%matplotlib inline
import os
import pandas as pd
import numpy as np
import config as cfg
92/4:
inflation_coeff = get_inflation_data()

# unrelated_ip CHARGES -- from Claudia
unrelated_ip = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'ibd_related_ip_charges.csv'), parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE'])
unrelated_ip['PAT_MRN_ID'] = unrelated_ip['PAT_MRN_ID'].astype(int)
unrelated_ip = unrelated_ip.merge(mrn_auto_map[['AUTO_ID', 'PAT_MRN_ID']], on='PAT_MRN_ID')


# #INPATIENT
# inpatient_charges_txn = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
# inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
# inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
# #adjust for inflation
# inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
# inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

# # inpatients must have >0 DAYS_ADMITTED
# inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
# # inpatient_charges_txn.loc[inpatient_charges.DAYS_ADMITTED==0, 'DAYS_ADMITTED'] = 1


# inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]
# inpatient_charges['INPATIENT'] = 1
# inpatient_charges.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT', 'INPATIENT']

# unrelated_ip = unrelated_ip[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'related']]
# inpatient_charges = inpatient_charges.merge(unrelated_ip, how='left',  on=['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE'])
# inpatient_charges[['related', 'unrelated']] = pd.get_dummies(inpatient_charges['related'].replace({2:1}))
# inpatient_charges.drop(['related'], inplace=True, axis=1)
# inpatient_charges.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'))
92/5:
def get_inflation_data():
    #http://www.usinflationcalculator.com/inflation/current-inflation-rates/
    #I took it from the December column as it is explained on the website (last 12 months inflation)
    #year,inflation
    inflationrates={2005:3.4,
                    2006:2.5,
                    2007:4.1,
                    2008:0.1,
                    2009:2.7,
                    2010:1.5,
                    2011:3.0,
                    2012:1.7,
                    2013:1.5,
                    2014:0.8,
                    2015:0.7,
                    2016:2.1,
                    2017:2.1 } #last update: last charges made in 2018, so need to take into account only till 2017
    for (year,rate) in inflationrates.items():
        inflationrates[year]=rate*0.01+1
    
    #calculate coefficients    
    lastyear=np.max(list(inflationrates.keys()))
    inflation_coeff={lastyear+1:1.0}
    for year in range(lastyear, 2005, -1):
        inflation_coeff[year]=inflation_coeff[year+1]*inflationrates[year]
    return inflation_coeff
92/6:
inflation_coeff = get_inflation_data()

# unrelated_ip CHARGES -- from Claudia
unrelated_ip = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'ibd_related_ip_charges.csv'), parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE'])
unrelated_ip['PAT_MRN_ID'] = unrelated_ip['PAT_MRN_ID'].astype(int)
unrelated_ip = unrelated_ip.merge(mrn_auto_map[['AUTO_ID', 'PAT_MRN_ID']], on='PAT_MRN_ID')


# #INPATIENT
# inpatient_charges_txn = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
# inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
# inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
# #adjust for inflation
# inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
# inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

# # inpatients must have >0 DAYS_ADMITTED
# inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
# # inpatient_charges_txn.loc[inpatient_charges.DAYS_ADMITTED==0, 'DAYS_ADMITTED'] = 1


# inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]
# inpatient_charges['INPATIENT'] = 1
# inpatient_charges.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT', 'INPATIENT']

# unrelated_ip = unrelated_ip[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'related']]
# inpatient_charges = inpatient_charges.merge(unrelated_ip, how='left',  on=['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE'])
# inpatient_charges[['related', 'unrelated']] = pd.get_dummies(inpatient_charges['related'].replace({2:1}))
# inpatient_charges.drop(['related'], inplace=True, axis=1)
# inpatient_charges.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'))
92/7:
mrn_auto_map = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'MRN_AUTO_mapping.csv'))
mrn_auto_map['PAT_MRN_ID'] = mrn_auto_map['PAT_MRN_ID'].astype(int)

exclude_patients = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'cancer_transplants.csv'))
exclude_patients['PAT_MRN_ID'] = exclude_patients['PAT_MRN_ID'].astype(int)
EXCLUDE = exclude_patients.merge(mrn_auto_map, how='left', on='PAT_MRN_ID')['AUTO_ID']
exclude_patients.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'exclude_patients.csv'))
92/8:
inflation_coeff = get_inflation_data()

# unrelated_ip CHARGES -- from Claudia
unrelated_ip = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'ibd_related_ip_charges.csv'), parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE'])
unrelated_ip['PAT_MRN_ID'] = unrelated_ip['PAT_MRN_ID'].astype(int)
unrelated_ip = unrelated_ip.merge(mrn_auto_map[['AUTO_ID', 'PAT_MRN_ID']], on='PAT_MRN_ID')


# #INPATIENT
# inpatient_charges_txn = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
# inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
# inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
# #adjust for inflation
# inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
# inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

# # inpatients must have >0 DAYS_ADMITTED
# inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
# # inpatient_charges_txn.loc[inpatient_charges.DAYS_ADMITTED==0, 'DAYS_ADMITTED'] = 1


# inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]
# inpatient_charges['INPATIENT'] = 1
# inpatient_charges.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT', 'INPATIENT']

# unrelated_ip = unrelated_ip[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'related']]
# inpatient_charges = inpatient_charges.merge(unrelated_ip, how='left',  on=['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE'])
# inpatient_charges[['related', 'unrelated']] = pd.get_dummies(inpatient_charges['related'].replace({2:1}))
# inpatient_charges.drop(['related'], inplace=True, axis=1)
# inpatient_charges.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'))
92/9: unrelated_ip
92/10: inpatient_charges
92/11:
inflation_coeff = get_inflation_data()

# unrelated_ip CHARGES -- from Claudia
unrelated_ip = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'ibd_related_ip_charges.csv'), parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE'])
unrelated_ip['PAT_MRN_ID'] = unrelated_ip['PAT_MRN_ID'].astype(int)
unrelated_ip['related'] = unrelated_ip['related'].replace({2:1})
unrelated_ip = unrelated_ip.merge(mrn_auto_map[['AUTO_ID', 'PAT_MRN_ID']], on='PAT_MRN_ID')


#INPATIENT
inpatient_charges_txn = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
#adjust for inflation
inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

# inpatients must have >0 DAYS_ADMITTED
inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
# inpatient_charges_txn.loc[inpatient_charges.DAYS_ADMITTED==0, 'DAYS_ADMITTED'] = 1


inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]
inpatient_charges['INPATIENT'] = 1
inpatient_charges.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT', 'INPATIENT']

unrelated_ip = unrelated_ip[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'related']]
inpatient_charges = inpatient_charges.merge(unrelated_ip, how='left',  on=['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE'])
inpatient_charges = pd.get_dummies(inpatient_charges['related'].replace({2:1}))
inpatient_charges.drop(['related'], inplace=True, axis=1)
inpatient_charges.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'))
92/12: (inpatient_charges['related'].replace({2:1}) - 1).abs()
92/13: (inpatient_charges['related'].astype(int).replace({2:1}) - 1).abs()
92/14:
inpatient_charges['related']
# (inpatient_charges['related'].astype(int).replace({2:1}) - 1).abs()
92/15:
inpatient_charges
# (inpatient_charges['related'].astype(int).replace({2:1}) - 1).abs()
92/16:
# inflation_coeff = get_inflation_data()

# # unrelated_ip CHARGES -- from Claudia
# unrelated_ip = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'ibd_related_ip_charges.csv'), parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE'])
# unrelated_ip['PAT_MRN_ID'] = unrelated_ip['PAT_MRN_ID'].astype(int)
# unrelated_ip['related'] = unrelated_ip['related'].replace({2:1})
# unrelated_ip = unrelated_ip.merge(mrn_auto_map[['AUTO_ID', 'PAT_MRN_ID']], on='PAT_MRN_ID')


# #INPATIENT
# inpatient_charges_txn = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
# inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
# inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
# #adjust for inflation
# inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
# inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

# # inpatients must have >0 DAYS_ADMITTED
# inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
# inpatient_charges_txn.loc[inpatient_charges.DAYS_ADMITTED==0, 'DAYS_ADMITTED'] = 1


inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]
inpatient_charges['INPATIENT'] = 1
inpatient_charges.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT', 'INPATIENT']

unrelated_ip = unrelated_ip[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'related']]
inpatient_charges = inpatient_charges.merge(unrelated_ip, how='left',  on=['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE'])
inpatient_charges['unrelated'] = (inpatient_charges['related'].replace({2:1}) - 1).abs()
inpatient_charges.drop(['related'], inplace=True, axis=1)
inpatient_charges.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'))
92/17:
inpatient_charges
# (inpatient_charges['related'].astype(int).replace({2:1}) - 1).abs()
92/18:
inpatient_charges.unrelated.value_counts()
# (inpatient_charges['related'].astype(int).replace({2:1}) - 1).abs()
92/19:
unrelated_ip.related.value_counts()
# (inpatient_charges['related'].astype(int).replace({2:1}) - 1).abs()
92/20:
inpatient_charges.unrelated.value_counts()
# (inpatient_charges['related'].astype(int).replace({2:1}) - 1).abs()
92/21: inpatient_charges
92/22: inpatient_charges.unrelated.value_counts()
92/23: inpatient_charges.unrelated.isnull().sum()
92/24:
# inflation_coeff = get_inflation_data()

# # unrelated_ip CHARGES -- from Claudia
# unrelated_ip = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'ibd_related_ip_charges.csv'), parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE'])
# unrelated_ip['PAT_MRN_ID'] = unrelated_ip['PAT_MRN_ID'].astype(int)
# unrelated_ip['related'] = unrelated_ip['related'].replace({2:1})
# unrelated_ip = unrelated_ip.merge(mrn_auto_map[['AUTO_ID', 'PAT_MRN_ID']], on='PAT_MRN_ID')


# #INPATIENT
# inpatient_charges_txn = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, "deid_IP_Charges_Aug_2018_12_6_18.csv"),thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)
# inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
# inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)
# #adjust for inflation
# inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)
# inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)

# # inpatients must have >0 DAYS_ADMITTED
# inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days
# inpatient_charges_txn.loc[inpatient_charges.DAYS_ADMITTED==0, 'DAYS_ADMITTED'] = 1


inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]
inpatient_charges['INPATIENT'] = 1
inpatient_charges.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT', 'INPATIENT']

unrelated_ip = unrelated_ip[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'related']]
inpatient_charges = inpatient_charges.merge(unrelated_ip, how='left',  on=['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE'])
inpatient_charges['unrelated'] = (inpatient_charges['related'].replace({2:1}) - 1).abs().fillna(0)
inpatient_charges.drop(['related'], inplace=True, axis=1)
inpatient_charges.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'inpatient_charges.csv'))
92/25: inpatient_charges.unrelated.value_counts()
92/26:
tot = pd.concat([inpatient_charges, op], ignore_index=True).sort_values(['AUTO_ID', 'DISCHARGE_DATE'])
tot['RESULT_YEAR'] = tot.DISCHARGE_DATE.dt.year
tot['RESULT_MONTH'] = tot.DISCHARGE_DATE.dt.month
tot['CUMSUM'] = tot.groupby(['AUTO_ID', 'unrelated']).AMOUNT.cumsum()
tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)
92/27: op = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges.csv'), parse_dates=['DISCHARGE_DATE'])
92/28:
tot = pd.concat([inpatient_charges, op], ignore_index=True).sort_values(['AUTO_ID', 'DISCHARGE_DATE'])
tot['RESULT_YEAR'] = tot.DISCHARGE_DATE.dt.year
tot['RESULT_MONTH'] = tot.DISCHARGE_DATE.dt.month
tot['CUMSUM'] = tot.groupby(['AUTO_ID', 'unrelated']).AMOUNT.cumsum()
tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)
92/29: tot = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'))
92/30:
# tot = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'))
aggregates = tot.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'INPATIENT', 'unrelated']).AMOUNT.sum()
92/31:
# tot = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'))
# aggregates = tot.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'INPATIENT', 'unrelated']).AMOUNT.sum()
aggregates.unstack(-2)
92/32:
# tot = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'))
# aggregates = tot.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'INPATIENT', 'unrelated']).AMOUNT.sum()
aggregates.unstack([-1, -2])
92/33:
tot = pd.concat([inpatient_charges, op], ignore_index=True).sort_values(['AUTO_ID', 'DISCHARGE_DATE'])
tot['RESULT_YEAR'] = tot.DISCHARGE_DATE.dt.year
tot['RESULT_MONTH'] = tot.DISCHARGE_DATE.dt.month
tot['CUMSUM'] = tot.groupby(['AUTO_ID', 'unrelated']).AMOUNT.cumsum()
tot['unrelated'] = tot['unrelated'].replace({0:'RELATED', 1:'UNRELATED'})
tot['INPATIENT'] = tot['INPATIENT'].replace({0:'OP', 1:'IP'})
tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)
92/34:
aggregates = tot.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'INPATIENT', 'unrelated']).AMOUNT.sum()
aggregates.unstack([-1, -2])
92/35:
aggregates = tot.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'INPATIENT', 'unrelated']).AMOUNT.sum()
aggregates.unstack([-1, -2]).reset_index()
92/36:
aggregates = tot.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'INPATIENT', 'unrelated']).AMOUNT.sum()
aggregates.unstack([-1, -2])#.reset_index()
92/37: tot.INPATIENT
92/38: tot.INPATIENT.value_counts()
92/39:
tot = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'))
# aggregates = tot.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'INPATIENT', 'unrelated']).AMOUNT.sum()
# aggregates.unstack([-1, -2])
92/40: tot.INPATIENT.value_counts()
92/41: tot.INPATIENT.value_counts()
92/42:
tot = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'))
# aggregates = tot.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'INPATIENT', 'unrelated']).AMOUNT.sum()
# aggregates.unstack([-1, -2])
92/43: tot.INPATIENT.value_counts()
92/44:
# tot = pd.concat([inpatient_charges, op], ignore_index=True).sort_values(['AUTO_ID', 'DISCHARGE_DATE'])
# tot['RESULT_YEAR'] = tot.DISCHARGE_DATE.dt.year
# tot['RESULT_MONTH'] = tot.DISCHARGE_DATE.dt.month
# tot['CUMSUM'] = tot.groupby(['AUTO_ID', 'unrelated']).AMOUNT.cumsum()
# tot['unrelated'] = tot['unrelated'].replace({0:'RELATED', 1:'UNRELATED'})
# tot['INPATIENT'] = tot['INPATIENT'].replace({0:'OP', 1:'IP'})
# 
tot['INPATIENT'] = tot['INPATIENT'].fillna(0)
tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)
92/45:
# tot = pd.concat([inpatient_charges, op], ignore_index=True).sort_values(['AUTO_ID', 'DISCHARGE_DATE'])
# tot['RESULT_YEAR'] = tot.DISCHARGE_DATE.dt.year
# tot['RESULT_MONTH'] = tot.DISCHARGE_DATE.dt.month
# tot['CUMSUM'] = tot.groupby(['AUTO_ID', 'unrelated']).AMOUNT.cumsum()
# tot['unrelated'] = tot['unrelated'].replace({0:'RELATED', 1:'UNRELATED'})
tot['INPATIENT'] = tot['INPATIENT'].replace({0:'OP', 1:'IP'})
tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)
92/46: tot.INPATIENT.value_counts()
92/47:
aggregates = tot.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'INPATIENT', 'unrelated']).AMOUNT.sum()
aggregates.unstack([-1, -2])
92/48:
# aggregates = tot.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'INPATIENT', 'unrelated']).AMOUNT.sum()
aggregates.unstack([-1, -2]).reset_index().rename_axis(None, axis=1)
92/49:
# aggregates = tot.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'INPATIENT', 'unrelated']).AMOUNT.sum()
aggregates.unstack([-1, -2]).reset_index()#.rename_axis(None, axis=1)
92/50:
# aggregates = tot.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'INPATIENT', 'unrelated']).AMOUNT.sum()
aggregates.unstack([-1, -2]).reset_index().columns#.rename_axis(None, axis=1)
92/51:
# aggregates = tot.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'INPATIENT', 'unrelated']).AMOUNT.sum()
# aggregates.columns = aggregates.columns.map('_'.join).str.strip('_')
aggregates.columns.map('_'.join).str.strip('_')
92/52:
# aggregates = tot.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'INPATIENT', 'unrelated']).AMOUNT.sum()
agg = aggregates.unstack([-1, -2])
agg.columns.map('_'.join).str.strip('_')
92/53:
# aggregates = tot.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'INPATIENT', 'unrelated']).AMOUNT.sum()
agg = aggregates.unstack([-1, -2])
agg
# agg.columns.map('_'.join).str.strip('_')
92/54:
# aggregates = tot.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'INPATIENT', 'unrelated']).AMOUNT.sum()
agg = aggregates.unstack([-1, -2])
agg.reset_index()
# agg.columns.map('_'.join).str.strip('_')
92/55:
# aggregates = tot.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'INPATIENT', 'unrelated']).AMOUNT.sum()
agg = aggregates.unstack([-1, -2]).reset_index()
agg.columns.map('_'.join).str.strip('_')
92/56:
# aggregates = tot.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'INPATIENT', 'unrelated']).AMOUNT.sum()
agg = aggregates.unstack([-1, -2]).reset_index()
agg.columns = agg.columns.map('_'.join).str.strip('_')
agg
92/57:
# aggregates = tot.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'INPATIENT', 'unrelated']).AMOUNT.sum()
agg = aggregates.unstack([-1, -2]).reset_index()
agg.columns = agg.columns.map('_'.join).str.strip('_')
# agg.to_csv(os.path.join(cfg.LONG_IN, 'charges.csv'), index=False)
agg.to_csv(os.path.join('.\\datasets\\long_in', 'charges.csv'), index=False)
92/58:
# aggregates = tot.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'INPATIENT', 'unrelated']).AMOUNT.sum()
agg = aggregates.unstack([-1, -2]).reset_index().fillna(0)
agg.columns = agg.columns.map('_'.join).str.strip('_')
# agg.to_csv(os.path.join(cfg.LONG_IN, 'charges.csv'), index=False)
agg.to_csv(os.path.join('.\\datasets\\long_in', 'charges.csv'), index=False)
92/59: agg
92/60: agg
92/61: ENC
92/62:
# import re

# enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
# enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'OFF'})
# enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]

# # == should we drop these? not sure if complete subset of diag.csv. In any case, it is a one-hot column, so keeping for now,
# # enc_df = enc_df[enc_df.ENC_TYPE_NAME!='PROC'] # Data captured in GI_Diagnostics.csv

# # related/unrelated
# enc_df['unrelated'] = 0
# bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY |ENT MERCY')
# enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)

ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE', 'unrelated']]
ENC = pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC')
ENC['RESULT_YEAR'] = ENC.CONTACT_DATE.dt.year
ENC['RESULT_MONTH'] = ENC.CONTACT_DATE.dt.month
ENC.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'), index=False)
92/63:
ENC = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'))
identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
ENC = ENC[identifier_cols+[x for x in ENC.columns if x not in identifier_cols]]
ENC
92/64:
import re

enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'OFF'})
enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]

# == should we drop these? not sure if complete subset of diag.csv. In any case, it is a one-hot column, so keeping for now,
# enc_df = enc_df[enc_df.ENC_TYPE_NAME!='PROC'] # Data captured in GI_Diagnostics.csv

# related/unrelated
enc_df['unrelated'] = 0
bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY |ENT MERCY')
enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)

ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE', 'unrelated']]
ENC = pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC')
ENC['RESULT_YEAR'] = ENC.CONTACT_DATE.dt.year
ENC['RESULT_MONTH'] = ENC.CONTACT_DATE.dt.month

# ENC.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'), index=False)
# ENC = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'))

# identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
# ENC = ENC[identifier_cols+[x for x in ENC.columns if x not in identifier_cols]]
g = ENC.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'unrelated']).sum()
g.unstack().reset_index()
g.columns = g.columns.apply('_'.join).str.trim('|')
92/65:
import re

enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'OFF'})
enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]

# == should we drop these? not sure if complete subset of diag.csv. In any case, it is a one-hot column, so keeping for now,
# enc_df = enc_df[enc_df.ENC_TYPE_NAME!='PROC'] # Data captured in GI_Diagnostics.csv

# related/unrelated
enc_df['unrelated'] = 0
bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY |ENT MERCY')
enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)

ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE', 'unrelated']]
ENC = pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC')
ENC['RESULT_YEAR'] = ENC.CONTACT_DATE.dt.year
ENC['RESULT_MONTH'] = ENC.CONTACT_DATE.dt.month

# ENC.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'), index=False)
# ENC = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'))

# identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
# ENC = ENC[identifier_cols+[x for x in ENC.columns if x not in identifier_cols]]
# g = ENC.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'unrelated']).sum()
# g.unstack().reset_index()
g.columns = g.columns.map('_'.join).str.trim('|')
92/66:
import re

enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'OFF'})
enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]

# == should we drop these? not sure if complete subset of diag.csv. In any case, it is a one-hot column, so keeping for now,
# enc_df = enc_df[enc_df.ENC_TYPE_NAME!='PROC'] # Data captured in GI_Diagnostics.csv

# related/unrelated
enc_df['unrelated'] = 0
bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY |ENT MERCY')
enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)

ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE', 'unrelated']]
ENC = pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC')
ENC['RESULT_YEAR'] = ENC.CONTACT_DATE.dt.year
ENC['RESULT_MONTH'] = ENC.CONTACT_DATE.dt.month

# ENC.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'), index=False)
# ENC = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'))

# identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
# ENC = ENC[identifier_cols+[x for x in ENC.columns if x not in identifier_cols]]
# g = ENC.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'unrelated']).sum()
# g.unstack().reset_index()
g.columns = g.columns.map('_'.join).str.strip('|')
92/67:
import re

# enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
# enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'OFF'})
# enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]

# # == should we drop these? not sure if complete subset of diag.csv. In any case, it is a one-hot column, so keeping for now,
# # enc_df = enc_df[enc_df.ENC_TYPE_NAME!='PROC'] # Data captured in GI_Diagnostics.csv

# # related/unrelated
# enc_df['unrelated'] = 0
# bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY |ENT MERCY')
# enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)

# ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE', 'unrelated']]
# ENC = pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC')
# ENC['RESULT_YEAR'] = ENC.CONTACT_DATE.dt.year
# ENC['RESULT_MONTH'] = ENC.CONTACT_DATE.dt.month

# ENC.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'), index=False)
# ENC = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'))

# identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
# ENC = ENC[identifier_cols+[x for x in ENC.columns if x not in identifier_cols]]
# g = ENC.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'unrelated']).sum()
# g.unstack().reset_index()
g.columns = g.columns.map('_'.join).str.strip('|')
92/68: g
92/69:
import re

# enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
# enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'OFF'})
# enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]

# # == should we drop these? not sure if complete subset of diag.csv. In any case, it is a one-hot column, so keeping for now,
# # enc_df = enc_df[enc_df.ENC_TYPE_NAME!='PROC'] # Data captured in GI_Diagnostics.csv

# # related/unrelated
# enc_df['unrelated'] = 0
# bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY |ENT MERCY')
# enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)

# ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE', 'unrelated']]
# ENC = pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC')
# ENC['RESULT_YEAR'] = ENC.CONTACT_DATE.dt.year
# ENC['RESULT_MONTH'] = ENC.CONTACT_DATE.dt.month

# ENC.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'), index=False)
# ENC = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'))

# identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
# ENC = ENC[identifier_cols+[x for x in ENC.columns if x not in identifier_cols]]
g = ENC.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'unrelated']).sum()
g.unstack().reset_index()
g.columns = g.columns.map('_'.join).str.strip('|')
92/70: g
92/71:
import re

# enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
# enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'OFF'})
# enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]

# # == should we drop these? not sure if complete subset of diag.csv. In any case, it is a one-hot column, so keeping for now,
# # enc_df = enc_df[enc_df.ENC_TYPE_NAME!='PROC'] # Data captured in GI_Diagnostics.csv

# # related/unrelated
# enc_df['unrelated'] = 0
# bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY |ENT MERCY')
# enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)

# ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE', 'unrelated']]
# ENC = pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC')
# ENC['RESULT_YEAR'] = ENC.CONTACT_DATE.dt.year
# ENC['RESULT_MONTH'] = ENC.CONTACT_DATE.dt.month

# ENC.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'), index=False)
# ENC = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'))

# identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
# ENC = ENC[identifier_cols+[x for x in ENC.columns if x not in identifier_cols]]
g = ENC.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'unrelated']).sum()
g = g.unstack().reset_index()
g.columns = g.columns.map('_'.join).str.strip('|')
92/72:
import re

# enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
# enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'OFF'})
# enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]

# # == should we drop these? not sure if complete subset of diag.csv. In any case, it is a one-hot column, so keeping for now,
# # enc_df = enc_df[enc_df.ENC_TYPE_NAME!='PROC'] # Data captured in GI_Diagnostics.csv

# # related/unrelated
# enc_df['unrelated'] = 0
# bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY |ENT MERCY')
# enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)

# ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE', 'unrelated']]
# ENC = pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC')
# ENC['RESULT_YEAR'] = ENC.CONTACT_DATE.dt.year
# ENC['RESULT_MONTH'] = ENC.CONTACT_DATE.dt.month

# ENC.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'), index=False)
# ENC = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'))

# identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
# ENC = ENC[identifier_cols+[x for x in ENC.columns if x not in identifier_cols]]
g = ENC.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'unrelated']).sum()
# g = g.unstack().reset_index()
# g.columns = g.columns.map('_'.join).str.strip('|')
92/73: g
92/74:
import re

# enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
# enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'OFF'})
# enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]

# # == should we drop these? not sure if complete subset of diag.csv. In any case, it is a one-hot column, so keeping for now,
# # enc_df = enc_df[enc_df.ENC_TYPE_NAME!='PROC'] # Data captured in GI_Diagnostics.csv

# # related/unrelated
# enc_df['unrelated'] = 0
# bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY |ENT MERCY')
# enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)

# ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE', 'unrelated']]
# ENC = pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC')
# ENC['RESULT_YEAR'] = ENC.CONTACT_DATE.dt.year
# ENC['RESULT_MONTH'] = ENC.CONTACT_DATE.dt.month

# ENC.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'), index=False)
# ENC = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'))

# identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
# ENC = ENC[identifier_cols+[x for x in ENC.columns if x not in identifier_cols]]
# g = ENC.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'unrelated']).sum()
g = g.unstack(-1).reset_index()
# g.columns = g.columns.map('_'.join).str.strip('|')
92/75: g
92/76:
import re

# enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
# enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'OFF'})
# enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]

# # == should we drop these? not sure if complete subset of diag.csv. In any case, it is a one-hot column, so keeping for now,
# # enc_df = enc_df[enc_df.ENC_TYPE_NAME!='PROC'] # Data captured in GI_Diagnostics.csv

# # related/unrelated
# enc_df['unrelated'] = 0
# bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY |ENT MERCY')
# enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)

# ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE', 'unrelated']]
# ENC = pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC')
# ENC['RESULT_YEAR'] = ENC.CONTACT_DATE.dt.year
# ENC['RESULT_MONTH'] = ENC.CONTACT_DATE.dt.month

# ENC.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'), index=False)
# ENC = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'))

# identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
# ENC = ENC[identifier_cols+[x for x in ENC.columns if x not in identifier_cols]]
ENC['unrelated'] = ENC['unrelated'].replace({0:'Related', 1:'Unrelated'})
g = ENC.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'unrelated']).sum()
g = g.unstack(-1).reset_index()
# g.columns = g.columns.map('_'.join).str.strip('|')
92/77: g
92/78: g.columns.map('_'.join).str.strip('|')
92/79:
import re

# enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
# enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'OFF'})
# enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]

# # == should we drop these? not sure if complete subset of diag.csv. In any case, it is a one-hot column, so keeping for now,
# # enc_df = enc_df[enc_df.ENC_TYPE_NAME!='PROC'] # Data captured in GI_Diagnostics.csv

# # related/unrelated
# enc_df['unrelated'] = 0
# bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY |ENT MERCY')
# enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)

# ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE', 'unrelated']]
# ENC = pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC')
# ENC['RESULT_YEAR'] = ENC.CONTACT_DATE.dt.year
# ENC['RESULT_MONTH'] = ENC.CONTACT_DATE.dt.month

# ENC.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'), index=False)
# ENC = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'))

# identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
# ENC = ENC[identifier_cols+[x for x in ENC.columns if x not in identifier_cols]]
# ENC['unrelated'] = ENC['unrelated'].replace({0:'Related', 1:'Unrelated'})
# g = ENC.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'unrelated']).sum().fillna(0)
# g = g.unstack(-1).reset_index()
g.columns = g.columns.map('_'.join).str.strip('|')
92/80: g
92/81:
import re

# enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
# enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'OFF'})
# enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]

# # == should we drop these? not sure if complete subset of diag.csv. In any case, it is a one-hot column, so keeping for now,
# # enc_df = enc_df[enc_df.ENC_TYPE_NAME!='PROC'] # Data captured in GI_Diagnostics.csv

# # related/unrelated
# enc_df['unrelated'] = 0
# bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY |ENT MERCY')
# enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)

# ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE', 'unrelated']]
# ENC = pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC')
# ENC['RESULT_YEAR'] = ENC.CONTACT_DATE.dt.year
# ENC['RESULT_MONTH'] = ENC.CONTACT_DATE.dt.month

# ENC.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'), index=False)
# ENC = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'))

# identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
# ENC = ENC[identifier_cols+[x for x in ENC.columns if x not in identifier_cols]]
ENC['unrelated'] = ENC['unrelated'].replace({0:'Related', 1:'Unrelated'})
ENC = ENC.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'unrelated']).sum()
ENC = ENC.unstack(-1).reset_index().fillna(0)
ENC.columns = ENC.columns.map('_'.join).str.strip('|')
ENC.to_csv(os.path.join(cfg.LONG_IN, 'encounters.csv'))
92/82: ENC.to_csv(os.path.join('datasets\\long_in','encounters.csv'))
92/83: ENC
92/84: ENC
92/85: diag = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'gi_diagnostics.csv'))
92/86:
diag = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'gi_diagnostics.csv'))
diag
92/87: identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
92/88:
def identifier_prefix(cols):
    identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
    return identifier_cols+[x for x in cols if x not in identifier_cols]
92/89:
import importlib
importlib.reload(cfg)
92/90:
diag=diag.drop(['VISIT_DATE', 'DESCRIPTION'], axis=1).groupby(identifier_cols).sum()
diag.to_csv(opj(cfg.LONG_IN, 'gi_diagnostics.csv')
92/91:
diag=diag.drop(['VISIT_DATE', 'DESCRIPTION'], axis=1).groupby(identifier_cols).sum()
diag.to_csv(opj(cfg.LONG_IN, 'gi_diagnostics.csv'))
92/92:
def identifier_prefix(cols):
    identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
    return identifier_cols+[x for x in cols if x not in identifier_cols]

opj = os.path.join
92/93:
# diag=diag.drop(['VISIT_DATE', 'DESCRIPTION'], axis=1).groupby(identifier_cols).sum()
diag.to_csv(opj(cfg.LONG_IN, 'gi_diagnostics.csv'))
92/94: diag
92/95: diag.reset_index()
92/96: diag.reset_index().to_csv(opj(cfg.LONG_IN, 'gi_diagnostics.csv')
92/97: diag.reset_index().to_csv(opj(cfg.LONG_IN, 'gi_diagnostics.csv'))
92/98:
labs = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'labs.csv'))
labs.groupby(['AUTO_ID','RESULT_YEAR','RESULT_MONTH', 'GROUP']).RESULT_FLAG.count()
92/99:
labs = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'labs.csv'))
labs.groupby(['AUTO_ID','RESULT_YEAR','RESULT_MONTH', 'GROUP']).RESULT_FLAG.count().describe()
92/100: labs.RESULT_FLAG.get_dummies()
92/101: pd.get_dummies(labs.RESULT_FLAG)
92/102: labs.groupby(['AUTO_ID','RESULT_YEAR','RESULT_MONTH', 'GROUP']).RESULT_FLAG.apply(pd.get_dummies)
92/103: labs.head(1000).groupby(['AUTO_ID','RESULT_YEAR','RESULT_MONTH', 'GROUP']).RESULT_FLAG.apply(pd.get_dummies)
92/104:
# labs.head(1000).groupby(['AUTO_ID','RESULT_YEAR','RESULT_MONTH', 'GROUP']).RESULT_FLAG.apply(pd.get_dummies)
labs
92/105:
# labs.head(1000).groupby(['AUTO_ID','RESULT_YEAR','RESULT_MONTH', 'GROUP'])
labs['newcol'] = labs['GROUP']+'_'+ labs['RESULT_FLAG']
pd.concat([labs[identifier_cols], pd.get_dummies(labs.RESULT_FLAG)])
92/106:
# labs.head(1000).groupby(['AUTO_ID','RESULT_YEAR','RESULT_MONTH', 'GROUP'])
labs['newcol'] = labs['GROUP']+'_'+ labs['RESULT_FLAG']
pd.concat([labs[identifier_cols], pd.get_dummies(labs.newcol)])
92/107:
# labs.head(1000).groupby(['AUTO_ID','RESULT_YEAR','RESULT_MONTH', 'GROUP'])
labs['newcol'] = labs['GROUP']+'_'+ labs['RESULT_FLAG']
pd.concat([labs[identifier_cols], pd.get_dummies(labs.newcol)], axis=1)
92/108:
# labs.head(1000).groupby(['AUTO_ID','RESULT_YEAR','RESULT_MONTH', 'GROUP'])
labs['newcol'] = labs['GROUP']+'_'+ labs['RESULT_FLAG']
pd.concat([labs[identifier_cols], pd.get_dummies(labs.newcol)], axis=1).groupby(identifier_cols).sum().reset_index()
92/109: diag.to_csv(opj(cfg.LONG_IN, 'gi_diagnostics.csv'), index=False)
92/110: ENC.to_csv(os.path.join(cfg.LONG_IN, 'encounters.csv'), index=False)
92/111:
# labs.head(1000).groupby(['AUTO_ID','RESULT_YEAR','RESULT_MONTH', 'GROUP'])
labs['newcol'] = labs['GROUP']+'_'+ labs['RESULT_FLAG']
labs = pd.concat([labs[identifier_cols], pd.get_dummies(labs.newcol)], axis=1).groupby(identifier_cols).sum().reset_index()
labs.to_csv(opj(cfg.LONG_IN, 'labs.csv'), index=False)
93/1:
import pandas as pd
from config import *
import os
93/2:
import pandas as pd
from config import *
import os
93/3:
import pandas as pd
from config import *
import os
93/4:
x = charges.merge(encounters, how='outer', on=IDENTIFIER_COLS)
x
93/5:
charges = pd.read_csv(opj(LONG_IN, 'charges.csv'))
encounters = pd.read_csv(opj(LONG_IN,'encounters.csv'))
diag = pd.read_csv(opj(LONG_IN,'gi_diagnostics.csv'))
labs = pd.read_csv(opj(LONG_IN, 'labs.csv'))
93/6:
x = charges.merge(encounters, how='outer', on=IDENTIFIER_COLS)
x
93/7: charges
93/8: encounters
92/112:
# import re

# enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
# enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'OFF'})
# enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]

# # # == should we drop these? not sure if complete subset of diag.csv. In any case, it is a one-hot column, so keeping for now,
# # # enc_df = enc_df[enc_df.ENC_TYPE_NAME!='PROC'] # Data captured in GI_Diagnostics.csv

# # related/unrelated
# enc_df['unrelated'] = 0
# bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY |ENT MERCY')
# enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)

# ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE', 'unrelated']]
# ENC = pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC')
# ENC['RESULT_YEAR'] = ENC.CONTACT_DATE.dt.year
# ENC['RESULT_MONTH'] = ENC.CONTACT_DATE.dt.month

# ENC.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'), index=False)
ENC = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'))


ENC = ENC[identifier_cols+[x for x in ENC.columns if x not in identifier_cols]]
ENC['unrelated'] = ENC['unrelated'].replace({0:'Related', 1:'Unrelated'})
ENC = ENC.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'unrelated']).sum()
ENC = ENC.unstack(-1).reset_index().fillna(0)
ENC.columns = ENC.columns.map('_'.join).str.strip('|')
ENC.to_csv(os.path.join(cfg.LONG_IN, 'encounters.csv'))
93/9:
charges = pd.read_csv(opj(LONG_IN, 'charges.csv'))
encounters = pd.read_csv(opj(LONG_IN,'encounters.csv'))
diag = pd.read_csv(opj(LONG_IN,'gi_diagnostics.csv'))
labs = pd.read_csv(opj(LONG_IN, 'labs.csv'))
93/10: encounters
92/113:
# import re

# enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
# enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'OFF'})
# enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]

# # # == should we drop these? not sure if complete subset of diag.csv. In any case, it is a one-hot column, so keeping for now,
# # # enc_df = enc_df[enc_df.ENC_TYPE_NAME!='PROC'] # Data captured in GI_Diagnostics.csv

# # related/unrelated
# enc_df['unrelated'] = 0
# bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY |ENT MERCY')
# enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)

# ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE', 'unrelated']]
# ENC = pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC')
# ENC['RESULT_YEAR'] = ENC.CONTACT_DATE.dt.year
# ENC['RESULT_MONTH'] = ENC.CONTACT_DATE.dt.month

# ENC.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'), index=False)
ENC = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'))


ENC = ENC[identifier_cols+[x for x in ENC.columns if x not in identifier_cols]]
ENC['unrelated'] = ENC['unrelated'].replace({0:'Related', 1:'Unrelated'})
ENC = ENC.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'unrelated']).sum()
ENC = ENC.unstack(-1).reset_index().fillna(0)
ENC.columns = ENC.columns.map('_'.join).str.strip('_')
ENC.to_csv(os.path.join(cfg.LONG_IN, 'encounters.csv'), index=False)
92/114:
# import re

# enc_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv"), parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)
# enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'OFF'})
# enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]

# # # == should we drop these? not sure if complete subset of diag.csv. In any case, it is a one-hot column, so keeping for now,
# # # enc_df = enc_df[enc_df.ENC_TYPE_NAME!='PROC'] # Data captured in GI_Diagnostics.csv

# # related/unrelated
# enc_df['unrelated'] = 0
# bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY |ENT MERCY')
# enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)

# ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE', 'unrelated']]
# ENC = pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC')
# ENC['RESULT_YEAR'] = ENC.CONTACT_DATE.dt.year
# ENC['RESULT_MONTH'] = ENC.CONTACT_DATE.dt.month

# ENC.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'), index=False)
ENC = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'encounters.csv'))


ENC = ENC[identifier_cols+[x for x in ENC.columns if x not in identifier_cols]]
ENC['unrelated'] = ENC['unrelated'].replace({0:'Related', 1:'Unrelated'})
ENC = ENC.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'unrelated']).sum()
ENC = ENC.unstack(-1).reset_index().fillna(0)
ENC.columns = ENC.columns.map('_'.join).str.strip('_')
ENC.to_csv(os.path.join(cfg.LONG_IN, 'encounters.csv'), index=False)
93/11:
charges = pd.read_csv(opj(LONG_IN, 'charges.csv'))
encounters = pd.read_csv(opj(LONG_IN,'encounters.csv'))
diag = pd.read_csv(opj(LONG_IN,'gi_diagnostics.csv'))
labs = pd.read_csv(opj(LONG_IN, 'labs.csv'))
93/12: encounters
93/13:
x = charges.merge(encounters, how='outer', on=IDENTIFIER_COLS)
x
93/14:
x = charges.merge(encounters, how='outer', on=IDENTIFIER_COLS)
charges.AUTO_ID
93/15:
x = charges.merge(encounters, how='outer', on=IDENTIFIER_COLS)
encounters.AUTO_ID
93/16:
x = charges.merge(encounters, how='outer', on=IDENTIFIER_COLS)
encounters.RESULT_YEAR
93/17:
x = charges.merge(encounters, how='outer', on=IDENTIFIER_COLS)
charges.RESULT_YEAR
93/18:
x = charges.merge(encounters, how='inner', on=IDENTIFIER_COLS)
x
93/19:
x = charges.merge(encounters, how='outer', on=IDENTIFIER_COLS)
x
93/20:
x = charges.merge(encounters, how='outer', on=IDENTIFIER_COLS)
x[:15]
93/21:
ce = charges.merge(encounters, how='outer', on=IDENTIFIER_COLS)
ced = ce.merge(diag, how='outer', on=IDENTIFIER_COLS)
ced[:15]
92/115: diag
92/116: diag.reset_index()
92/117: diag.reset_index().to_csv(opj(cfg.LONG_IN, 'gi_diagnostics.csv'), index=False)
93/22:
charges = pd.read_csv(opj(LONG_IN, 'charges.csv'))
encounters = pd.read_csv(opj(LONG_IN,'encounters.csv'))
diag = pd.read_csv(opj(LONG_IN,'gi_diagnostics.csv'))
labs = pd.read_csv(opj(LONG_IN, 'labs.csv'))
93/23:
ce = charges.merge(encounters, how='outer', on=IDENTIFIER_COLS)
ced = ce.merge(diag, how='outer', on=IDENTIFIER_COLS)
ced[:15]
92/118: labs
93/24:
charges = pd.read_csv(opj(LONG_IN, 'charges.csv'))
encounters = pd.read_csv(opj(LONG_IN,'encounters.csv'))
diag = pd.read_csv(opj(LONG_IN,'gi_diagnostics.csv'))
diag.columns = [f'DIAG_{x}' for x in diag.columns if x not in IDENTIFIER_COLS]
labs = pd.read_csv(opj(LONG_IN, 'labs.csv'))
labs.columns = [f'LAB_{x}' for x in diag.columns if x not in IDENTIFIER_COLS]
93/25:
ce = charges.merge(encounters, how='outer', on=IDENTIFIER_COLS)
ced = ce.merge(diag, how='outer', on=IDENTIFIER_COLS)
ced[ced[[x for x in ced.columns if x{:4}=='DIAG']].notnull()]
93/26:
ce = charges.merge(encounters, how='outer', on=IDENTIFIER_COLS)
ced = ce.merge(diag, how='outer', on=IDENTIFIER_COLS)
ced[ced[[x for x in ced.columns if x[:4]=='DIAG']].notnull()]
93/27:
charges = pd.read_csv(opj(LONG_IN, 'charges.csv'))
encounters = pd.read_csv(opj(LONG_IN,'encounters.csv'))
diag = pd.read_csv(opj(LONG_IN,'gi_diagnostics.csv'))
diag.columns = [f'DIAG_{x}' if x not in IDENTIFIER_COLS else x for x in diag.columns ]
labs = pd.read_csv(opj(LONG_IN, 'labs.csv'))
labs.columns = [f'LAB_{x}' if x not in IDENTIFIER_COLS else x for x in diag.columns]
93/28:
charges = pd.read_csv(opj(LONG_IN, 'charges.csv'))
encounters = pd.read_csv(opj(LONG_IN,'encounters.csv'))
diag = pd.read_csv(opj(LONG_IN,'gi_diagnostics.csv'))
diag.columns = [f'DIAG_{x}' if x not in IDENTIFIER_COLS else x for x in diag.columns ]
labs = pd.read_csv(opj(LONG_IN, 'labs.csv'))
labs.columns = [f'LAB_{x}' if x not in IDENTIFIER_COLS else x for x in labs.columns]
93/29:
# ce = charges.merge(encounters, how='outer', on=IDENTIFIER_COLS)
# ced = ce.merge(diag, how='outer', on=IDENTIFIER_COLS)
ced[ced[[x for x in ced.columns if x[:4]=='DIAG']].notnull()]
93/30: ced[[x for x in ced.columns if x[:4]=='DIAG']]
93/31:
# ced[[x for x in ced.columns if x[:4]=='DIAG']]
[x[:4] for x in ced.columns']
93/32:
# ced[[x for x in ced.columns if x[:4]=='DIAG']]
[x[:4] for x in ced.columns]
93/33:
ce = charges.merge(encounters, how='outer', on=IDENTIFIER_COLS)
ced = ce.merge(diag, how='outer', on=IDENTIFIER_COLS)
ced[ced[[x for x in ced.columns if x[:4]=='DIAG']].notnull()]
93/34:
# ced[[x for x in ced.columns if x[:4]=='DIAG']]
[x[:4] for x in ced.columns]
93/35: ced[[x for x in ced.columns if x[:4]=='DIAG']]
93/36:
ce = charges.merge(encounters, how='outer', on=IDENTIFIER_COLS)
ced = ce.merge(diag, how='inner', on=IDENTIFIER_COLS)
# ced[ced[[x for x in ced.columns if x[:4]=='DIAG']].notnull()]
ced
93/37: diag.shape
93/38:
ce = charges.merge(encounters, how='outer', on=IDENTIFIER_COLS)
ced = ce.merge(diag, how='outer', on=IDENTIFIER_COLS)
cedl = ced.merge(labs, how='inner', on=IDENTIFIER_COLS)
cedl
93/39: labs.shape
93/40:
ce = charges.merge(encounters, how='outer', on=IDENTIFIER_COLS)
ced = ce.merge(diag, how='outer', on=IDENTIFIER_COLS)
cedl = ced.merge(labs, how='outer', on=IDENTIFIER_COLS)
cedl.shape
93/41:
ce = charges.merge(encounters, how='outer', on=IDENTIFIER_COLS)
ced = ce.merge(diag, how='outer', on=IDENTIFIER_COLS)
cedl = ced.merge(labs, how='outer', on=IDENTIFIER_COLS)
cedl
93/42:
exclude_patients = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'exclude_patients.csv'))
exclude_patients
93/43:
exclude_patients = pd.read_csv(os.path.join(OUT_DATA_PATH, 'exclude_patients.csv'))
exclude_patients
92/119:
# mrn_auto_map = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'MRN_AUTO_mapping.csv'))
# mrn_auto_map['PAT_MRN_ID'] = mrn_auto_map['PAT_MRN_ID'].astype(int)

# exclude_patients = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'cancer_transplants.csv'))
# exclude_patients['PAT_MRN_ID'] = exclude_patients['PAT_MRN_ID'].astype(int)
# EXCLUDE = exclude_patients.merge(mrn_auto_map, how='left', on='PAT_MRN_ID')['AUTO_ID']
EXCLUDE.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'exclude_patients.csv'))
93/44:
exclude_patients = pd.read_csv(os.path.join(OUT_DATA_PATH, 'exclude_patients.csv'))
exclude_patients
92/120:
mrn_auto_map = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'MRN_AUTO_mapping.csv'))
mrn_auto_map['PAT_MRN_ID'] = mrn_auto_map['PAT_MRN_ID'].astype(int)

exclude_patients = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'cancer_transplants.csv'))
exclude_patients['PAT_MRN_ID'] = exclude_patients['PAT_MRN_ID'].astype(int)
EXCLUDE = exclude_patients.merge(mrn_auto_map, how='left', on='PAT_MRN_ID')
EXCLUDE.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'exclude_patients.csv'))
93/45:
exclude_patients = pd.read_csv(os.path.join(OUT_DATA_PATH, 'exclude_patients.csv'))
exclude_patients
93/46:
exclude_patients = pd.read_csv(os.path.join(OUT_DATA_PATH, 'exclude_patients.csv'))

ct_aid = exclude_patients.AUTO_ID
CT_filtered = cedl[~cedl.AUTO_ID.isin(ct_aid)]
CT_filtered
93/47: ct_aid.shape
93/48:
import pandas as pd
from config import *
import os
93/49:
cedl.to_csv(opj(LONG_OUT, 'all.csv'), index=False)
CT_filtered.to_csv(opj(LONG_OUT, 'noCT.csv'), index=False)
93/50:
import importlib
importlib.reload(re)
93/51:
import importlib
importlib.reload(cfg)
93/52:
import importlib
importlib.reload(config)
93/53:
import pandas as pd
from config import *
import os
93/54:
import pandas as pd
from config import *
import os
93/55:
import pandas as pd
from config import *
import os
93/56:
import pandas as pd
from config import *
import os
93/57:
import pandas as pd
from config import *
import os
93/58:
cedl.to_csv(opj(LONG_OUT, 'all.csv'), index=False)
CT_filtered.to_csv(opj(LONG_OUT, 'noCT.csv'), index=False)
94/1:
import pandas as pd
from config import *
import os
94/2:
charges = pd.read_csv(opj(LONG_IN, 'charges.csv'))
encounters = pd.read_csv(opj(LONG_IN,'encounters.csv'))
diag = pd.read_csv(opj(LONG_IN,'gi_diagnostics.csv'))
diag.columns = [f'DIAG_{x}' if x not in IDENTIFIER_COLS else x for x in diag.columns ]
labs = pd.read_csv(opj(LONG_IN, 'labs.csv'))
labs.columns = [f'LAB_{x}' if x not in IDENTIFIER_COLS else x for x in labs.columns]
94/3:
ce = charges.merge(encounters, how='outer', on=IDENTIFIER_COLS)
ced = ce.merge(diag, how='outer', on=IDENTIFIER_COLS)
cedl = ced.merge(labs, how='outer', on=IDENTIFIER_COLS)
94/4:
exclude_patients = pd.read_csv(os.path.join(OUT_DATA_PATH, 'exclude_patients.csv'))
ct_aid = exclude_patients.AUTO_ID
CT_filtered = cedl[~cedl.AUTO_ID.isin(ct_aid)]
94/5:
cedl.to_csv(opj(LONG_OUT, 'all.csv'), index=False)
CT_filtered.to_csv(opj(LONG_OUT, 'noCT.csv'), index=False)
94/6:
cedl.to_csv(opj(LONG_OUT, 'all.csv'), index=False)
CT_filtered.to_csv(opj(LONG_OUT, 'noCT.csv'), index=False)
95/1: import pandas as pd
95/2:
import pandas as pd
import config as cfg
95/3:
def load_irregular_sequence(exclude_ct=True):
    if exclude_ct:
        return pd.read_csv(cfg.opj(cfg.LONG_OUT, 'noCT.csv'))
    return longit_raw = pd.read_csv(cfg.opj(cfg.LONG_OUT, 'all.csv'))
95/4:
def load_irregular_sequence(exclude_ct=True):
    if exclude_ct:
        return pd.read_csv(cfg.opj(cfg.LONG_OUT, 'noCT.csv'))
    return pd.read_csv(cfg.opj(cfg.LONG_OUT, 'all.csv'))
95/5:
raw_df = load_irregular_sequence()
raw_df.sample(10)
95/6:
raw_df = raw_df[raw_df.AUTO_ID==0]
[miny, minm] = raw_df[(raw_df.RESULT_YEAR+raw_df.RESULT_MONTH).min()][['RESULT_YEAR','RESULT_MONTH']].values
# raw_df['mo_ts'] = 
[miny, minm]
95/7:
raw_df = raw_df[raw_df.AUTO_ID==0]
[miny, minm] = raw_df[(raw_df.RESULT_YEAR+raw_df.RESULT_MONTH).min().index][['RESULT_YEAR','RESULT_MONTH']].values
# raw_df['mo_ts'] = 
[miny, minm]
95/8:
raw_df = raw_df[raw_df.AUTO_ID==0]
[miny, minm] = raw_df[(raw_df.RESULT_YEAR+raw_df.RESULT_MONTH).idxmin()][['RESULT_YEAR','RESULT_MONTH']].values
# raw_df['mo_ts'] = 
[miny, minm]
95/9: raw_df[(raw_df.RESULT_YEAR+raw_df.RESULT_MONTH).idxmin()]
95/10:
# raw_df[(raw_df.RESULT_YEAR+raw_df.RESULT_MONTH).idxmin()]
(raw_df.RESULT_YEAR+raw_df.RESULT_MONTH)#.idxmin()
95/11:
# raw_df[(raw_df.RESULT_YEAR+raw_df.RESULT_MONTH).idxmin()]
(raw_df.RESULT_YEAR.astype(str)+raw_df.RESULT_MONTH.astype(str))#.idxmin()
95/12:
# raw_df[(raw_df.RESULT_YEAR+raw_df.RESULT_MONTH).idxmin()]
(raw_df.RESULT_YEAR.astype(str)+raw_df.RESULT_MONTH.zfill(0).astype(str))#.idxmin()
95/13:
# raw_df[(raw_df.RESULT_YEAR+raw_df.RESULT_MONTH).idxmin()]
(raw_df.RESULT_YEAR.astype(str)+raw_df.RESULT_MONTH.astype(str).str.zfill(2))#.idxmin()
95/14:
# raw_df[(raw_df.RESULT_YEAR+raw_df.RESULT_MONTH).idxmin()]
(raw_df.RESULT_YEAR.astype(str)+raw_df.RESULT_MONTH.astype(str).str.zfill(2)).idxmin()
95/15:
# raw_df[(raw_df.RESULT_YEAR+raw_df.RESULT_MONTH).idxmin()]
(raw_df.RESULT_YEAR.astype(str)+raw_df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int).idxmin()
95/16:
raw_df = raw_df[raw_df.AUTO_ID==0]
first_datapoint_ix = (raw_df.RESULT_YEAR.astype(str)+raw_df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int).idxmin()
[miny, minm] = raw_df[first_datapoint][['RESULT_YEAR','RESULT_MONTH']].values
# raw_df['mo_ts'] = 
[miny, minm]
95/17:
raw_df = raw_df[raw_df.AUTO_ID==0]
first_datapoint_ix = (raw_df.RESULT_YEAR.astype(str)+raw_df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int).idxmin()
[miny, minm] = raw_df[first_datapoint_ix][['RESULT_YEAR','RESULT_MONTH']].values
# raw_df['mo_ts'] = 
[miny, minm]
95/18:
raw_df = raw_df[raw_df.AUTO_ID==0]
first_datapoint_ix = (raw_df.RESULT_YEAR.astype(str)+raw_df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int).idxmin()
[miny, minm] = raw_df.loc[first_datapoint_ix][['RESULT_YEAR','RESULT_MONTH']].values
# raw_df['mo_ts'] = 
[miny, minm]
95/19:
raw_df = raw_df[raw_df.AUTO_ID==0]
first_datapoint_ix = (raw_df.RESULT_YEAR.astype(str)+raw_df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int).idxmin()
[miny, minm] = raw_df.loc[first_datapoint_ix][['RESULT_YEAR','RESULT_MONTH']].values
raw_df['mo_ts'] = (raw_df['RESULT_YEAR']-miny)*12+raw_df['RESULT_MONTH']-minm
raw_df
95/20:
raw_df = raw_df[raw_df.AUTO_ID==0]
first_datapoint_ix = (raw_df.RESULT_YEAR.astype(str)+raw_df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int).idxmin()
[miny, minm] = raw_df.loc[first_datapoint_ix][['RESULT_YEAR','RESULT_MONTH']].values
raw_df['mo_ts'] = (raw_df['RESULT_YEAR']-miny)*12+raw_df['RESULT_MONTH']-minm
raw_df.sorted_calues('mo_ts')
95/21:
raw_df = raw_df[raw_df.AUTO_ID==0]
first_datapoint_ix = (raw_df.RESULT_YEAR.astype(str)+raw_df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int).idxmin()
[miny, minm] = raw_df.loc[first_datapoint_ix][['RESULT_YEAR','RESULT_MONTH']].values
raw_df['mo_ts'] = (raw_df['RESULT_YEAR']-miny)*12+raw_df['RESULT_MONTH']-minm
raw_df.sorted_values('mo_ts')
95/22:
raw_df = raw_df[raw_df.AUTO_ID==0]
first_datapoint_ix = (raw_df.RESULT_YEAR.astype(str)+raw_df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int).idxmin()
[miny, minm] = raw_df.loc[first_datapoint_ix][['RESULT_YEAR','RESULT_MONTH']].values
raw_df['mo_ts'] = (raw_df['RESULT_YEAR']-miny)*12+raw_df['RESULT_MONTH']-minm
raw_df.sort_values('mo_ts')
95/23: raw_df = load_irregular_sequence()
95/24:
# raw_df = raw_df[raw_df.AUTO_ID==0]

# def monthly_timestamps(df):
#     groups = df.AUTO_ID
    
raw_df = df.groupby(AUTO_ID)
# first_datapoint_ix = (raw_df.RESULT_YEAR.astype(str)+raw_df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int).idxmin()
# [miny, minm] = raw_df.loc[first_datapoint_ix][['RESULT_YEAR','RESULT_MONTH']].values
# raw_df['MONTH_TS'] = (raw_df['RESULT_YEAR']-miny)*12+raw_df['RESULT_MONTH']-minm
(raw_df.RESULT_YEAR.astype(str)+raw_df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int).idxmin()
95/25: df = load_irregular_sequence()
95/26:
# raw_df = raw_df[raw_df.AUTO_ID==0]

# def monthly_timestamps(df):
#     groups = df.AUTO_ID
    
raw_df = df.groupby(AUTO_ID)
# first_datapoint_ix = (raw_df.RESULT_YEAR.astype(str)+raw_df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int).idxmin()
# [miny, minm] = raw_df.loc[first_datapoint_ix][['RESULT_YEAR','RESULT_MONTH']].values
# raw_df['MONTH_TS'] = (raw_df['RESULT_YEAR']-miny)*12+raw_df['RESULT_MONTH']-minm
(raw_df.RESULT_YEAR.astype(str)+raw_df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int).idxmin()
95/27:
# raw_df = raw_df[raw_df.AUTO_ID==0]

# def monthly_timestamps(df):
#     groups = df.AUTO_ID
    
raw_df = df.groupby('AUTO_ID')
# first_datapoint_ix = (raw_df.RESULT_YEAR.astype(str)+raw_df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int).idxmin()
# [miny, minm] = raw_df.loc[first_datapoint_ix][['RESULT_YEAR','RESULT_MONTH']].values
# raw_df['MONTH_TS'] = (raw_df['RESULT_YEAR']-miny)*12+raw_df['RESULT_MONTH']-minm
(raw_df.RESULT_YEAR.astype(str)+raw_df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int).idxmin()
95/28:
# raw_df = raw_df[raw_df.AUTO_ID==0]

# def monthly_timestamps(df):
#     groups = df.AUTO_ID

df['tmp_ts'] = (df.RESULT_YEAR.astype(str)+df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int)
raw_df = df.groupby('AUTO_ID').tmp_ts.idxmin()




# first_datapoint_ix = (raw_df.RESULT_YEAR.astype(str)+raw_df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int).idxmin()
# [miny, minm] = raw_df.loc[first_datapoint_ix][['RESULT_YEAR','RESULT_MONTH']].values
# raw_df['MONTH_TS'] = (raw_df['RESULT_YEAR']-miny)*12+raw_df['RESULT_MONTH']-minm
95/29: raw_df
95/30:
# raw_df = raw_df[raw_df.AUTO_ID==0]

# def monthly_timestamps(df):
#     groups = df.AUTO_ID

df['tmp_ts'] = (df.RESULT_YEAR.astype(str)+df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int)
t0_indices = df.groupby('AUTO_ID').tmp_ts.idxmin()
t0 = t0_indices.map(lambda x:df.loc[x][['RESULT_YEAR','RESULT_MONTH']])


# first_datapoint_ix = (raw_df.RESULT_YEAR.astype(str)+raw_df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int).idxmin()
# [miny, minm] = raw_df.loc[first_datapoint_ix][['RESULT_YEAR','RESULT_MONTH']].values
# raw_df['MONTH_TS'] = (raw_df['RESULT_YEAR']-miny)*12+raw_df['RESULT_MONTH']-minm
95/31: t0
95/32: t0.loc[0]
95/33: t0.shape
95/34: type(t0)
95/35: t0.to_dict()
95/36: t0
95/37: t0.values
95/38:
# raw_df = raw_df[raw_df.AUTO_ID==0]

# def monthly_timestamps(df):
#     groups = df.AUTO_ID

df['tmp_ts'] = (df.RESULT_YEAR.astype(str)+df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int)
t0_indices = df.groupby('AUTO_ID').tmp_ts.idxmin()
t0 = t0_indices.applymap(lambda x:df.loc[x][['RESULT_YEAR','RESULT_MONTH']])


# first_datapoint_ix = (raw_df.RESULT_YEAR.astype(str)+raw_df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int).idxmin()
# [miny, minm] = raw_df.loc[first_datapoint_ix][['RESULT_YEAR','RESULT_MONTH']].values
# raw_df['MONTH_TS'] = (raw_df['RESULT_YEAR']-miny)*12+raw_df['RESULT_MONTH']-minm
95/39: t0_indices
95/40: t0_indices.to_dict()
95/41: t0_indices
95/42: t0_indices.to_dataframe()
95/43: t0_indices.to_frame()
95/44: t0_indices.to_frame().reset_index()
95/45: t0_indices
95/46: df.loc[t0_indices]
95/47: df.loc[t0_indices][['AUTO_ID','RESULT_YEAR','RESULT_MONTH']]
95/48:
# raw_df = raw_df[raw_df.AUTO_ID==0]

# def monthly_timestamps(df):
#     groups = df.AUTO_ID

df['tmp_ts'] = (df.RESULT_YEAR.astype(str)+df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int)
t0_indices = df.groupby('AUTO_ID').tmp_ts.idxmin()
t0_df = df.loc[t0_indices][['AUTO_ID','RESULT_YEAR','RESULT_MONTH']]
t0_df.columns = ['AUTO_ID', 'tmp_minY', 'tmp_minMo']

timestamped_df = df.merge(t0_df, how='left', on='AUTO_ID')
timestamped_df['MONTH_TS'] = (timestamped_df['RESULT_YEAR']-timestamped_df['tmp_minY'])*12 + timestamped_df['RESULT_MONTH']-timestamped_df['tmp_minMo']
timestamped_df = timestamped_df[[x if x[:3]!='tmp' for x in timestamped_df.columns]]

timestamped_df

# first_datapoint_ix = (raw_df.RESULT_YEAR.astype(str)+raw_df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int).idxmin()
# [miny, minm] = raw_df.loc[first_datapoint_ix][['RESULT_YEAR','RESULT_MONTH']].values
# raw_df['MONTH_TS'] = (raw_df['RESULT_YEAR']-miny)*12+raw_df['RESULT_MONTH']-minm
95/49:
# raw_df = raw_df[raw_df.AUTO_ID==0]

# def monthly_timestamps(df):
#     groups = df.AUTO_ID

df['tmp_ts'] = (df.RESULT_YEAR.astype(str)+df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int)
t0_indices = df.groupby('AUTO_ID').tmp_ts.idxmin()
t0_df = df.loc[t0_indices][['AUTO_ID','RESULT_YEAR','RESULT_MONTH']]
t0_df.columns = ['AUTO_ID', 'tmp_minY', 'tmp_minMo']

timestamped_df = df.merge(t0_df, how='left', on='AUTO_ID')
timestamped_df['MONTH_TS'] = (timestamped_df['RESULT_YEAR']-timestamped_df['tmp_minY'])*12 + timestamped_df['RESULT_MONTH']-timestamped_df['tmp_minMo']
timestamped_df = timestamped_df[[x for x in timestamped_df.columns if x[:3]!='tmp']]

timestamped_df

# first_datapoint_ix = (raw_df.RESULT_YEAR.astype(str)+raw_df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int).idxmin()
# [miny, minm] = raw_df.loc[first_datapoint_ix][['RESULT_YEAR','RESULT_MONTH']].values
# raw_df['MONTH_TS'] = (raw_df['RESULT_YEAR']-miny)*12+raw_df['RESULT_MONTH']-minm
95/50: timestamped_df[timestamped_df.AUTO_ID==0]
95/51:
%matplotlib inline
import pandas as pd
import config as cfg
import matplotlib.pyplot as plt
95/52:
gg = timestamped_df.groupby('AUTO_ID').MONTH_TS
data_quality = gg.count()/gg.max()
data_quality.plot.hist(bins=100)
95/53:
gg = timestamped_df.groupby('AUTO_ID').MONTH_TS
data_quality = gg.count()/gg.max()
data_quality.describe()
# data_quality.plot.hist(bins=100)
95/54:
gg = timestamped_df.groupby('AUTO_ID').MONTH_TS
data_quality = (gg.count()/gg.max())+0.0001
# data_quality.describe()
data_quality.plot.hist(bins=100)
95/55:
gg = timestamped_df.groupby('AUTO_ID').MONTH_TS
data_quality = (gg.count()/max(1, gg.max()))
# data_quality.describe()
data_quality.plot.hist(bins=100)
95/56:
gg = timestamped_df.groupby('AUTO_ID').MONTH_TS
# data_quality = gg.count()/max(1, gg.max())
gg.count().describe()
# data_quality.describe()
# data_quality.plot.hist(bins=100)
95/57: gg = timestamped_df.groupby('AUTO_ID').MONTH_TS
95/58:
%matplotlib inline
import pandas as pd
import config as cfg
import matplotlib.pyplot as plt
imoprt seaborn as sns
95/59: sns.distplot(gg.count())
95/60:
%matplotlib inline
import pandas as pd
import config as cfg
import matplotlib.pyplot as plt
import seaborn as sns
95/61: sns.distplot(gg.count())
95/62: gg.count().quantile(50)
95/63: gg.count().quantile(0.5)
95/64: gg.count().quantile(0.1)
95/65: gg.count().quantile(0.2)
95/66: (gg.count()<10).sum()
95/67: (gg.count()<10).sum()/gg.count().shape[0]
95/68: print(f'{(gg.count()<10).sum()*100/gg.count().shape[0]}%')
95/69: print(f'{round((gg.count()<10).sum()*100/gg.count().shape[0], 2)}%')
95/70:
c = gg.count()
print(f'{round((c<10).sum()*100/c.shape[0], 2)}%')
c[c<10].describe()
95/71:
sns.distplot(gg.count())
gg.count().describe()
95/72:
c = gg.count()
m = gg.max()
c2 = c[c>1]
m2 = m[c2.index]
(c2/m2).describe()
sns.distplot(c2/m2)
95/73:
c = gg.count()
m = gg.max()
c2 = c[c>1]
m2 = m[c2.index]
print((c2/m2).describe())
sns.distplot(c2/m2)
95/74:
c = gg.count()
m = gg.max()
c2 = c[c>1]
m2 = m[c2.index]
pq = c2/m2
print(pq.describe())
sns.distplot(pq)
95/75:
c = gg.count()
m = gg.max()
c2 = c[c>2]
m2 = m[c2.index]
pq = c2/m2
print(pq.describe())
sns.distplot(pq)
95/76:
c = gg.count()
m = gg.max()
c2 = c[c>3]
m2 = m[c2.index]
pq = c2/m2
print(pq.describe())
sns.distplot(pq)
95/77:
c = gg.count()
m = gg.max()
c2 = c[c>5]
m2 = m[c2.index]
pq = c2/m2
print(pq.describe())
sns.distplot(pq)
95/78:
c = gg.count()
m = gg.max()
c2 = c[c>5]
m2 = m[c2.index]
pq = 1-(c2/m2)
print(pq.describe())
sns.distplot(pq)
95/79:
g = timestamped_df.groupby('AUTO_ID')
full_batch = pd.Series(range(0,g.MONTH_TS.max()+1))
full_batch
95/80:
g = timestamped_df.groupby('AUTO_ID')
full_batch = pd.Series(list(range(0,g.MONTH_TS.max()+1)))
full_batch
95/81:
g = timestamped_df.groupby('AUTO_ID')
g.MONTH_TS.max()+1
# full_batch = pd.Series(list(range(0,)))
# full_batch
95/82:
g = timestamped_df.groupby('AUTO_ID')
(g.MONTH_TS.max()+1).to_frame()
# full_batch = pd.Series(list(range(0,)))
# full_batch
95/83:
g = timestamped_df.groupby('AUTO_ID')
(g.MONTH_TS.max()+1).to_frame().reset_index()
# full_batch = pd.Series(list(range(0,)))
# full_batch
95/84:
g = timestamped_df.groupby('AUTO_ID')
(g.MONTH_TS.max()+1).to_frame()
# full_batch = pd.Series(list(range(0,)))
# full_batch
95/85:
tmp = timestamped_df[timestamped_df.AUTO_ID.isin([0,1])]
g = tmp.groupby('AUTO_ID')
g.set_index('MONTH_TS')
95/86:
tmp = timestamped_df[timestamped_df.AUTO_ID.isin([0,1])]
g = tmp.groupby('AUTO_ID')
# Reindex each patient's data with missing months
tmp2 = g.apply(lambda df: df.set_index('MONTH_TS').reindex(pd.Index(range(0, df.MONTH_TS.max()+1), name="MONTH_TS")))
95/87:
tmp = timestamped_df[timestamped_df.AUTO_ID.isin([0,1])]
g = tmp.groupby('AUTO_ID')
# Reindex each patient's data with missing months
tmp2 = g.apply(lambda df: df.set_index('MONTH_TS').reindex(pd.Index(range(0, df.MONTH_TS.max()+1), name="MONTH_TS"))) 
tmp2
95/88:
tmp = timestamped_df[timestamped_df.AUTO_ID.isin([0,1])]
g = tmp.groupby('AUTO_ID')
# Reindex each patient's data with missing months
tmp2 = g.apply(lambda df: df.set_index('MONTH_TS').reindex(pd.Index(range(0, df.MONTH_TS.max()+1), name="MONTH_TS")).reset_index()) 
tmp2
95/89:
tmp = timestamped_df[timestamped_df.AUTO_ID.isin([0,1])]
g = tmp.groupby('AUTO_ID')
# Reindex each patient's data with missing months
tmp2 = g.apply(lambda df: df.set_index('MONTH_TS').reindex(pd.Index(range(0, df.MONTH_TS.max()+1), name="MONTH_TS")).reset_index()) 
tmp2.reset_index()
95/90:
tmp = timestamped_df[timestamped_df.AUTO_ID.isin([0,1])]
g = tmp.groupby('AUTO_ID')
# Reindex each patient's data with missing months
tmp2 = g.apply(lambda df: df.set_index('MONTH_TS').reindex(pd.Index(range(0, df.MONTH_TS.max()+1), name="MONTH_TS"))) 
tmp2
95/91:
tmp = timestamped_df[timestamped_df.AUTO_ID.isin([0,1])]
g = tmp.groupby('AUTO_ID')
# Reindex each patient's data with missing months
tmp2 = g.apply(lambda df: df.set_index('MONTH_TS').reindex(pd.Index(range(0, df.MONTH_TS.max()+1), name="MONTH_TS"))) 
tmp2.reset_index()
95/92:
tmp = timestamped_df[timestamped_df.AUTO_ID.isin([0,1])]
g = tmp.groupby('AUTO_ID')
# Reindex each patient's data with missing months
tmp2 = g.apply(lambda df: df.set_index('MONTH_TS').reindex(pd.Index(range(0, df.MONTH_TS.max()+1), name="MONTH_TS"))) 
tmp2#.reset_index()
95/93:
tmp = timestamped_df[timestamped_df.AUTO_ID.isin([0,1])]
g = tmp.groupby('AUTO_ID')
# Reindex each patient's data with missing months
tmp2 = g.apply(lambda df: df.set_index('MONTH_TS').reindex(pd.Index(range(0, df.MONTH_TS.max()+1), name="MONTH_TS")).reset_index()) 
tmp2#.reset_index()
95/94: tmp2.index
95/95:
tmp = timestamped_df[timestamped_df.AUTO_ID.isin([0,1])]
g = tmp.groupby('AUTO_ID')
# Reindex each patient's data with missing months
tmp2 = g.apply(lambda df: df.set_index('MONTH_TS').reindex(pd.Index(range(0, df.MONTH_TS.max()+1), name="MONTH_TS"))) 
tmp2#.reset_index()
95/96: tmp2.index
95/97: tmp2.index = range(0,tmp2.shape[0])
95/98:
# tmp = timestamped_df[timestamped_df.AUTO_ID.isin([0,1])]
# g = tmp.groupby('AUTO_ID')
# # Reindex each patient's data with missing months
# tmp2 = g.apply(lambda df: df.set_index('MONTH_TS').reindex(pd.Index(range(0, df.MONTH_TS.max()+1), name="MONTH_TS"))) 
tmp2#.reset_index()
95/99:
# tmp = timestamped_df[timestamped_df.AUTO_ID.isin([0,1])]
# g = tmp.groupby('AUTO_ID')
# # Reindex each patient's data with missing months
tmp2 = g.apply(lambda df: df.set_index('MONTH_TS').reindex(pd.Index(range(0, df.MONTH_TS.max()+1), name="MONTH_TS")).reset_index()) 
tmp2.index = range(0,tmp2.shape[0])
tmp2
95/100: tmp2.AUTO_ID.head(1)
95/101: tmp2.AUTO_ID.values[0]
95/102:
# tmp = timestamped_df[timestamped_df.AUTO_ID.isin([0,1])]
# g = tmp.groupby('AUTO_ID')
# # Reindex each patient's data with missing months
tmp2 = g.apply(lambda df: df.set_index(['AUTO_ID','MONTH_TS']).reindex(pd.MultiIndex.from_product([(df.AUTO_ID.values[0]), range(0, df.MONTH_TS.max()+1)], name="MONTH_TS")).reset_index()) 
# tmp2.index = range(0,tmp2.shape[0])
tmp2
95/103:
# tmp = timestamped_df[timestamped_df.AUTO_ID.isin([0,1])]
# g = tmp.groupby('AUTO_ID')
# # Reindex each patient's data with missing months
tmp2 = g.apply(lambda df: df.set_index(['AUTO_ID','MONTH_TS']).reindex(pd.MultiIndex.from_product([(df.AUTO_ID.values[0]), range(0, df.MONTH_TS.max()+1)], names=['AUTO_ID', 'MONTH_TS'])).reset_index()) 
# tmp2.index = range(0,tmp2.shape[0])
tmp2
95/104:
# tmp = timestamped_df[timestamped_df.AUTO_ID.isin([0,1])]
g = tmp.groupby('AUTO_ID')
# # Reindex each patient's data with missing months
tmp2 = g.apply(lambda df: df.reset_index().set_index(['AUTO_ID','MONTH_TS']).reindex(pd.MultiIndex.from_product([(df.AUTO_ID.values[0]), range(0, df.MONTH_TS.max()+1)], names=['AUTO_ID', 'MONTH_TS'])).reset_index()) 
# tmp2.index = range(0,tmp2.shape[0])
tmp2
95/105:
df = tmp[tmp.AUTO_ID==0]
pd.MultiIndex.from_product([(df.AUTO_ID.values[0]), range(0, df.MONTH_TS.max()+1)], names=['AUTO_ID', 'MONTH_TS'])
95/106:
df = tmp[tmp.AUTO_ID==0]
pd.MultiIndex.from_product([[df.AUTO_ID.values[0]], list(range(0, df.MONTH_TS.max()+1))], names=['AUTO_ID', 'MONTH_TS'])
95/107:
df = tmp[tmp.AUTO_ID==0]

def func(df):
    df = df.reset_index()
    new_index = pd.MultiIndex.from_product([[df.AUTO_ID.values[0]], list(range(0, df.MONTH_TS.max()+1))], names=['AUTO_ID', 'MONTH_TS'])
    df = df.set_index(new_index)
    return df


func(df)
95/108:
df = tmp[tmp.AUTO_ID==0]

def func(df):
    df = df.reset_index()
    new_index = pd.MultiIndex.from_product([[df.AUTO_ID.values[0]], list(range(0, df.MONTH_TS.max()+1))], names=['AUTO_ID', 'MONTH_TS'])
    df = df.reindex(new_index).reset_index()
    return df


func(df)
95/109:
df = tmp[tmp.AUTO_ID==0]

def func(df):
    df = df.reset_index()
    new_index = pd.MultiIndex.from_product([[df.AUTO_ID.values[0]], list(range(0, df.MONTH_TS.max()+1))], names=['AUTO_ID', 'MONTH_TS'])
    df = df.set_index(['AUTO_ID', 'MONTH_TS']).reindex(new_index).reset_index()
    return df


func(df)
95/110:
def pad_missing_months(df):
    df = df.reset_index()
    new_index = pd.MultiIndex.from_product([[df.AUTO_ID.values[0]], list(range(0, df.MONTH_TS.max()+1))], names=['AUTO_ID', 'MONTH_TS'])
    df = df.set_index(['AUTO_ID', 'MONTH_TS']).reindex(new_index).reset_index()
    return df

padded = timestamped_df.groupby('AUTO_ID').apply(pad_missing_months)
padded
95/111:
def pad_missing_months(df):
    df = df.reset_index()
    new_index = pd.MultiIndex.from_product([[df.AUTO_ID.values[0]], list(range(0, df.MONTH_TS.max()+1))], names=['AUTO_ID', 'MONTH_TS'])
    df = df.set_index(['AUTO_ID', 'MONTH_TS']).reindex(new_index).reset_index()
    return df

# padded = timestamped_df.groupby('AUTO_ID').apply(pad_missing_months)
padded.reset_index()
95/112:
def pad_missing_months(df):
    df = df.reset_index()
    new_index = pd.MultiIndex.from_product([[df.AUTO_ID.values[0]], list(range(0, df.MONTH_TS.max()+1))], names=['AUTO_ID', 'MONTH_TS'])
    df = df.set_index(['AUTO_ID', 'MONTH_TS']).reindex(new_index).reset_index()
    return df

# padded = timestamped_df.groupby('AUTO_ID').apply(pad_missing_months)
padded.index = range(0, padded.shape[0])
95/113:
def pad_missing_months(df):
    df = df.reset_index()
    new_index = pd.MultiIndex.from_product([[df.AUTO_ID.values[0]], list(range(0, df.MONTH_TS.max()+1))], names=['AUTO_ID', 'MONTH_TS'])
    df = df.set_index(['AUTO_ID', 'MONTH_TS']).reindex(new_index).reset_index()
    return df

# padded = timestamped_df.groupby('AUTO_ID').apply(pad_missing_months)
# padded.index = range(0, padded.shape[0])
padded
95/114:
tmp = timestamped_df.head(1000)
pad_missing_months(tmp)
95/115:
tmp = timestamped_df.head(1000)
tmp.set_index(['AUTO_ID', 'MONTH_TS'])
95/116:
df = timestamped_df.head(1000)
new_index = pd.MultiIndex.from_product([[df.AUTO_ID.values[0]], list(range(0, df.MONTH_TS.max()+1))], names=['AUTO_ID', 'MONTH_TS'])
df.set_index(['AUTO_ID', 'MONTH_TS']).reindex(new_index)
95/117:
df = timestamped_df.head(1000)
new_index = pd.MultiIndex.from_product([[df.AUTO_ID.values[0]], list(range(0, df.MONTH_TS.max()+1))], names=['AUTO_ID', 'MONTH_TS'])
df.set_index(['AUTO_ID', 'MONTH_TS']).reindex(new_index).reset_index()
95/118: padded.drop('Index')
95/119: padded.drop('Index', axis=1)
95/120: padded.columns #drop('Index', axis=1)
95/121: padded.drop('index', axis=1)
95/122: padded.to_pickle(opj(cfg.LONG_OUT, 'padded'))
95/123: padded.to_pickle(cfg.opj(cfg.LONG_OUT, 'padded'))
95/124: padded.columns
95/125:
def pad_missing_months(df):
    df = df.reset_index()
    new_index = pd.MultiIndex.from_product([[df.AUTO_ID.values[0]], list(range(0, df.MONTH_TS.max()+1))], names=['AUTO_ID', 'MONTH_TS'])
    df = df.set_index(['AUTO_ID', 'MONTH_TS']).reindex(new_index).reset_index()
    return df

# padded = timestamped_df.groupby('AUTO_ID').apply(pad_missing_months).drop(['index', 'RESULT_YEAR', 'RESULT_MONTH'], inplace=True)
# padded.index = range(0, padded.shape[0])
padded.drop(['index', 'RESULT_YEAR', 'RESULT_MONTH'], inplace=True)
95/126:
def pad_missing_months(df):
    df = df.reset_index()
    new_index = pd.MultiIndex.from_product([[df.AUTO_ID.values[0]], list(range(0, df.MONTH_TS.max()+1))], names=['AUTO_ID', 'MONTH_TS'])
    df = df.set_index(['AUTO_ID', 'MONTH_TS']).reindex(new_index).reset_index()
    return df

# padded = timestamped_df.groupby('AUTO_ID').apply(pad_missing_months).drop(['index', 'RESULT_YEAR', 'RESULT_MONTH'], inplace=True, axis=1)
# padded.index = range(0, padded.shape[0])
padded.drop(['index', 'RESULT_YEAR', 'RESULT_MONTH'], inplace=True, axis=1)
95/127: padded.columns
95/128: padded
95/129:
def pad_missing_months(df):
    df = df.reset_index()
    new_index = pd.MultiIndex.from_product([[df.AUTO_ID.values[0]], list(range(0, df.MONTH_TS.max()+1))], names=['AUTO_ID', 'MONTH_TS'])
    df = df.set_index(['AUTO_ID', 'MONTH_TS']).reindex(new_index).reset_index()
    return df

padded = timestamped_df.groupby('AUTO_ID').apply(pad_missing_months).drop(['RESULT_YEAR', 'RESULT_MONTH'], inplace=True, axis=1)
padded.index = range(0, padded.shape[0])
95/130:
def pad_missing_months(df):
    df = df.reset_index()
    new_index = pd.MultiIndex.from_product([[df.AUTO_ID.values[0]], list(range(0, df.MONTH_TS.max()+1))], names=['AUTO_ID', 'MONTH_TS'])
    df = df.set_index(['AUTO_ID', 'MONTH_TS']).reindex(new_index).reset_index()
    return df

padded = timestamped_df.groupby('AUTO_ID').apply(pad_missing_months).drop(['RESULT_YEAR', 'RESULT_MONTH'], axis=1)
padded.index = range(0, padded.shape[0])
95/131: padded
95/132: padded.index.map(lambda x: 1 if x else 0)
95/133: padded['PADDED'] = padded.index.map(lambda x: 1 if x else 0)
95/134:
# padded['PADDED'] = padded.index.map(lambda x: 1 if x else 0)
padded
95/135:
padded['PADDED'] = padded.index.apply(lambda x: 1 if x else 0)
padded
95/136:
def pad_missing_months(df):
    df = df.reset_index()
    new_index = pd.MultiIndex.from_product([[df.AUTO_ID.values[0]], list(range(0, df.MONTH_TS.max()+1))], names=['AUTO_ID', 'MONTH_TS'])
    df = df.set_index(['AUTO_ID', 'MONTH_TS']).reindex(new_index).reset_index()
    return df

padded = timestamped_df.groupby('AUTO_ID').apply(pad_missing_months)
padded.index = range(0, padded.shape[0])
# .drop(['RESULT_YEAR', 'RESULT_MONTH'], axis=1)
95/137:
# padded['PADDED'] = padded.index.apply(lambda x: 1 if x else 0)
padded
95/138:
padded['PADDED'] = padded.RESULT_YEAR.map(lambda x: 1 if x else 0)
padded
95/139:
padded['PADDED'] = padded.RESULT_YEAR.isnull().astype(int)
padded
95/140: padded.columns
95/141:
def pad_missing_months(df):
    df = df.reset_index()
    new_index = pd.MultiIndex.from_product([[df.AUTO_ID.values[0]], list(range(0, df.MONTH_TS.max()+1))], names=['AUTO_ID', 'MONTH_TS'])
    df = df.set_index(['AUTO_ID', 'MONTH_TS']).reindex(new_index).reset_index()
    return df

# padded = timestamped_df.groupby('AUTO_ID').apply(pad_missing_months)
# padded.index = range(0, padded.shape[0])
# padded['PADDED'] = padded.RESULT_YEAR.isnull().astype(int)
padded.drop(['index', 'RESULT_YEAR', 'RESULT_MONTH'], axis=1, inplace=True)

padded.to_pickle(cfg.opj(cfg.LONG_OUT, 'padded'))
95/142: padded.columns
95/143:
cols_to_impute = ['ENC_OFF_Related', 'ENC_OFF_Unrelated',\
                'ENC_PROC_Related', 'ENC_PROC_Unrelated', 'ENC_TEL_Related',\
                'ENC_TEL_Unrelated', 'DIAG_COLONOSCOPY', 'DIAG_ENDOSCOPY',\
                'DIAG_SIGMOIDOSCOPY', 'DIAG_ILEOSCOPY', 'DIAG_ERCP', 'DIAG_EGD',\
                'DIAG_UPPER GI ENDO', 'DIAG_UPPER EUS', 'DIAG_GI_PROCEDURE',\
                'LAB_albumin_High', 'LAB_albumin_Low', 'LAB_albumin_Normal',\
                'LAB_crp_High', 'LAB_crp_Low', 'LAB_crp_Normal', 'LAB_eos_High',\
                'LAB_eos_Low', 'LAB_eos_Normal', 'LAB_esr_High', 'LAB_esr_Normal',\
                'LAB_hemoglobin_High', 'LAB_hemoglobin_Low', 'LAB_hemoglobin_Normal',\
                'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_monocytes_Normal',\
                'LAB_vitamin_d_High', 'LAB_vitamin_d_Low', 'LAB_vitamin_d_Normal']

def mean_imputer(df):
    obs = df.loc[df.PADDED==0, cols_to_impute].mean()
95/144:
obs = padded.loc[padded.PADDED==0, cols_to_impute].mean()
padded.head().fillna(obs)
95/145: padded.head()
95/146:
x=padded[padded.AUTO_ID==0]
x.fillna(None, 'ffill')
95/147:
x=padded[padded.AUTO_ID==0]
x[cols_to_impute].fillna(None, 'ffill')
95/148:
cols_to_impute = ['ENC_OFF_Related', 'ENC_OFF_Unrelated',\
                'ENC_PROC_Related', 'ENC_PROC_Unrelated', 'ENC_TEL_Related',\
                'ENC_TEL_Unrelated', 'DIAG_COLONOSCOPY', 'DIAG_ENDOSCOPY',\
                'DIAG_SIGMOIDOSCOPY', 'DIAG_ILEOSCOPY', 'DIAG_ERCP', 'DIAG_EGD',\
                'DIAG_UPPER GI ENDO', 'DIAG_UPPER EUS', 'DIAG_GI_PROCEDURE',\
                'LAB_albumin_High', 'LAB_albumin_Low', 'LAB_albumin_Normal',\
                'LAB_crp_High', 'LAB_crp_Low', 'LAB_crp_Normal', 'LAB_eos_High',\
                'LAB_eos_Low', 'LAB_eos_Normal', 'LAB_esr_High', 'LAB_esr_Normal',\
                'LAB_hemoglobin_High', 'LAB_hemoglobin_Low', 'LAB_hemoglobin_Normal',\
                'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_monocytes_Normal',\
                'LAB_vitamin_d_High', 'LAB_vitamin_d_Low', 'LAB_vitamin_d_Normal']

def mean_imputer(df):
    obs = df.loc[df.PADDED==0, cols_to_impute].mean()
    return df.fillna(obs)
    
def fwd_imputer(df):
    d = df.groupby('AUTO_ID').fillna(method='ffill')
    return d
95/149: mean_imputer(df)[:20]
95/150: mean_imputer(padded)[:20]
95/151:
x=padded[padded.AUTO_ID==0]
x.loc[:,cols_to_impute].fillna(None, 'ffill')
95/152:
x=padded[padded.AUTO_ID==0]
x.loc[:,cols_to_impute].fillna(None, 'ffill', inplace=True)
x
95/153:
x=padded[padded.AUTO_ID==0]
x.loc[:,cols_to_impute] = x.loc[:,cols_to_impute].fillna(None, 'ffill')
x
95/154:
cols_to_impute = ['ENC_OFF_Related', 'ENC_OFF_Unrelated',\
                'ENC_PROC_Related', 'ENC_PROC_Unrelated', 'ENC_TEL_Related',\
                'ENC_TEL_Unrelated', 'DIAG_COLONOSCOPY', 'DIAG_ENDOSCOPY',\
                'DIAG_SIGMOIDOSCOPY', 'DIAG_ILEOSCOPY', 'DIAG_ERCP', 'DIAG_EGD',\
                'DIAG_UPPER GI ENDO', 'DIAG_UPPER EUS', 'DIAG_GI_PROCEDURE',\
                'LAB_albumin_High', 'LAB_albumin_Low', 'LAB_albumin_Normal',\
                'LAB_crp_High', 'LAB_crp_Low', 'LAB_crp_Normal', 'LAB_eos_High',\
                'LAB_eos_Low', 'LAB_eos_Normal', 'LAB_esr_High', 'LAB_esr_Normal',\
                'LAB_hemoglobin_High', 'LAB_hemoglobin_Low', 'LAB_hemoglobin_Normal',\
                'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_monocytes_Normal',\
                'LAB_vitamin_d_High', 'LAB_vitamin_d_Low', 'LAB_vitamin_d_Normal']

def commonsense_imputer(df):
    """
    - 0 for encounters (if it wasn't recorded, it probably didn't happen)
    - 0 for diagnoses (-- " --)
    """
    enc_diag_cols = [x for x in cols_to_impute if x[:3]=='ENC' or x[:4]=='DIAG']
    df.loc[:, enc_diag_cols] = df.loc[:, enc_diag_cols].fillna(0)
    return df
        

def mean_imputer(df):
    obs_mean = df.loc[df.PADDED==0, cols_to_impute].mean()
    return df.fillna(obs)
   
    
def fwd_imputer(df):
    # fillna of 0th row with mean values
    obs_mean = df.loc[df.PADDED==0, cols_to_impute].mean()
    df.loc[df.MONTH_TS==0, cols_to_impute] = df.loc[df.MONTH_TS==0, cols_to_impute].fillna(obs_mean)
    def fwd(x):
        x.loc[:,cols_to_impute] = x.loc[:,cols_to_impute].fillna(None, 'ffill')
    return df.groupby('AUTO_ID').apply(fwd)
95/155: padded
95/156: padded[padded.PADDED==0]
95/157:
x=padded[padded.PADDED==0]
for grp in ['albumin', 'eos', 'esr', 'hemoglobin', 'monocytes', 'vitamin_d']:
    x1 = x[[f'LAB_{grp}_{l}' for l in ['High','Normal','Low']]].isnull().sum(axis=1)
    chk = x1.isin([1,2]).sum()
    print(f"grp: {grp}, count: {chk}")
95/158:
x=padded[padded.PADDED==0]
for grp in ['albumin', 'eos', 'hemoglobin', 'monocytes', 'vitamin_d']:
    x1 = x[[f'LAB_{grp}_{l}' for l in ['High','Normal','Low']]].isnull().sum(axis=1)
    chk = x1.isin([1,2]).sum()
    print(f"grp: {grp}, count: {chk}")
95/159:
cols_to_impute = ['ENC_OFF_Related', 'ENC_OFF_Unrelated',\
                'ENC_PROC_Related', 'ENC_PROC_Unrelated', 'ENC_TEL_Related',\
                'ENC_TEL_Unrelated', 'DIAG_COLONOSCOPY', 'DIAG_ENDOSCOPY',\
                'DIAG_SIGMOIDOSCOPY', 'DIAG_ILEOSCOPY', 'DIAG_ERCP', 'DIAG_EGD',\
                'DIAG_UPPER GI ENDO', 'DIAG_UPPER EUS', 'DIAG_GI_PROCEDURE',\
                'LAB_albumin_High', 'LAB_albumin_Low', 'LAB_albumin_Normal',\
                'LAB_crp_High', 'LAB_crp_Low', 'LAB_crp_Normal', 'LAB_eos_High',\
                'LAB_eos_Low', 'LAB_eos_Normal', 'LAB_esr_High', 'LAB_esr_Normal',\
                'LAB_hemoglobin_High', 'LAB_hemoglobin_Low', 'LAB_hemoglobin_Normal',\
                'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_monocytes_Normal',\
                'LAB_vitamin_d_High', 'LAB_vitamin_d_Low', 'LAB_vitamin_d_Normal']

def commonsense_imputer(df):
    """
    - 0 for encounters (if it wasn't recorded, it probably didn't happen)
    - 0 for diagnoses (-- " --)
    - 1 for LAB_*_Normal (if it wasn't prescribed, it's probably normal)
    """
    enc_diag_cols = [x for x in cols_to_impute if x[:3]=='ENC' or x[:4]=='DIAG']
    df.loc[:, enc_diag_cols] = df.loc[:, enc_diag_cols].fillna(0)
    
    labgrp = ['albumin', 'eos', 'hemoglobin', 'monocytes', 'vitamin_d']
    lablvl = ['High', 'Low', 'Normal']
    lab_fillna = {f'LAB_{g}_{l}':0 for g in labgrp for l in lablvl if l!='Normal' else f'LAB_{g}_{l}':1}
    df = df.fillna(lab_fillna)
    return df
        

def mean_imputer(df):
    obs_mean = df.loc[df.PADDED==0, cols_to_impute].mean()
    return df.fillna(obs)
   
    
def fwd_imputer(df):
    # fillna of 0th row with mean values
    obs_mean = df.loc[df.PADDED==0, cols_to_impute].mean()
    df.loc[df.MONTH_TS==0, cols_to_impute] = df.loc[df.MONTH_TS==0, cols_to_impute].fillna(obs_mean)
    def fwd(x):
        x.loc[:,cols_to_impute] = x.loc[:,cols_to_impute].fillna(None, 'ffill')
    return df.groupby('AUTO_ID').apply(fwd)
95/160: x=padded.copy()
95/161:
x=padded.copy()
commonsense_imputer(x)
95/162:
# x=padded.copy()
# commonsense_imputer(x)
labgrp = ['albumin', 'eos', 'hemoglobin', 'monocytes', 'vitamin_d']
lablvl = ['High', 'Low', 'Normal']
lab_fillna = {f'LAB_{g}_{l}':0 for g in labgrp for l in lablvl if l!='Normal' else f'LAB_{g}_{l}':1}
lab_fillna
95/163:
# x=padded.copy()
# commonsense_imputer(x)
labgrp = ['albumin', 'eos', 'hemoglobin', 'monocytes', 'vitamin_d']
lablvl = ['High', 'Low', 'Normal']
lab_fillna = {f'LAB_{g}_{l}':(1 if l=='Normal' else 0) for g in labgrp for l in lablvl}
lab_fillna
95/164:
cols_to_impute = ['ENC_OFF_Related', 'ENC_OFF_Unrelated',\
                'ENC_PROC_Related', 'ENC_PROC_Unrelated', 'ENC_TEL_Related',\
                'ENC_TEL_Unrelated', 'DIAG_COLONOSCOPY', 'DIAG_ENDOSCOPY',\
                'DIAG_SIGMOIDOSCOPY', 'DIAG_ILEOSCOPY', 'DIAG_ERCP', 'DIAG_EGD',\
                'DIAG_UPPER GI ENDO', 'DIAG_UPPER EUS', 'DIAG_GI_PROCEDURE',\
                'LAB_albumin_High', 'LAB_albumin_Low', 'LAB_albumin_Normal',\
                'LAB_crp_High', 'LAB_crp_Low', 'LAB_crp_Normal', 'LAB_eos_High',\
                'LAB_eos_Low', 'LAB_eos_Normal', 'LAB_esr_High', 'LAB_esr_Normal',\
                'LAB_hemoglobin_High', 'LAB_hemoglobin_Low', 'LAB_hemoglobin_Normal',\
                'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_monocytes_Normal',\
                'LAB_vitamin_d_High', 'LAB_vitamin_d_Low', 'LAB_vitamin_d_Normal']

def commonsense_imputer(df):
    """
    - 0 for encounters (if it wasn't recorded, it probably didn't happen)
    - 0 for diagnoses (-- " --)
    - 1 for LAB_*_Normal (if it wasn't prescribed, it's probably normal)
    """
    enc_diag_cols = [x for x in cols_to_impute if x[:3]=='ENC' or x[:4]=='DIAG']
    df.loc[:, enc_diag_cols] = df.loc[:, enc_diag_cols].fillna(0)
    
    labgrp = ['albumin', 'eos', 'hemoglobin', 'monocytes', 'vitamin_d']
    lablvl = ['High', 'Low', 'Normal']
    lab_fillna = {f'LAB_{g}_{l}':(1 if l=='Normal' else 0) for g in labgrp for l in lablvl}
    df = df.fillna(lab_fillna)
    return df
        

def mean_imputer(df):
    obs_mean = df.loc[df.PADDED==0, cols_to_impute].mean()
    return df.fillna(obs)
   
    
def fwd_imputer(df):
    # fillna of 0th row with mean values
    obs_mean = df.loc[df.PADDED==0, cols_to_impute].mean()
    df.loc[df.MONTH_TS==0, cols_to_impute] = df.loc[df.MONTH_TS==0, cols_to_impute].fillna(obs_mean)
    def fwd(x):
        x.loc[:,cols_to_impute] = x.loc[:,cols_to_impute].fillna(None, 'ffill')
    return df.groupby('AUTO_ID').apply(fwd)
95/165:
x=padded.copy()
commonsense_imputer(x)
95/166:
cols_to_impute = ['ENC_OFF_Related', 'ENC_OFF_Unrelated',\
                'ENC_PROC_Related', 'ENC_PROC_Unrelated', 'ENC_TEL_Related',\
                'ENC_TEL_Unrelated', 'DIAG_COLONOSCOPY', 'DIAG_ENDOSCOPY',\
                'DIAG_SIGMOIDOSCOPY', 'DIAG_ILEOSCOPY', 'DIAG_ERCP', 'DIAG_EGD',\
                'DIAG_UPPER GI ENDO', 'DIAG_UPPER EUS', 'DIAG_GI_PROCEDURE',\
                'LAB_albumin_High', 'LAB_albumin_Low', 'LAB_albumin_Normal',\
                'LAB_crp_High', 'LAB_crp_Low', 'LAB_crp_Normal', 'LAB_eos_High',\
                'LAB_eos_Low', 'LAB_eos_Normal', 'LAB_esr_High', 'LAB_esr_Normal',\
                'LAB_hemoglobin_High', 'LAB_hemoglobin_Low', 'LAB_hemoglobin_Normal',\
                'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_monocytes_Normal',\
                'LAB_vitamin_d_High', 'LAB_vitamin_d_Low', 'LAB_vitamin_d_Normal']

def commonsense_imputer(df):
    """
    - 0 for encounters (if it wasn't recorded, it probably didn't happen)
    - 0 for diagnoses (-- " --)
    - 1 for LAB_*_Normal (if it wasn't prescribed, it's probably normal)
    """
    enc_diag_cols = [x for x in cols_to_impute if x[:3]=='ENC' or x[:4]=='DIAG']
    df.loc[:, enc_diag_cols] = df.loc[:, enc_diag_cols].fillna(0)
    
    # One-hot encoding of normal labs
    labgrp = ['albumin', 'eos', 'hemoglobin', 'monocytes', 'vitamin_d']
    lablvl = ['High', 'Low', 'Normal']
    lab_fillna = {f'LAB_{g}_{l}':(1 if l=='Normal' else 0) for g in labgrp for l in lablvl}
    df = df.fillna(lab_fillna)
    return df
        

def mean_imputer(df):
    obs_mean = df.loc[df.PADDED==0, cols_to_impute].mean()
    return df.fillna(obs)
   
    
def fwd_imputer(df):
    # fillna of 0th row with commonsense values    
    df.loc[df.MONTH_TS==0, cols_to_impute] = commonsense_imputer(df.loc[df.MONTH_TS==0, cols_to_impute])
    # fwd-fill 
    def fwd(x):
        x.loc[:,cols_to_impute] = x.loc[:,cols_to_impute].fillna(None, 'ffill')
    return df.groupby('AUTO_ID').apply(fwd)
95/167: padded.isnull()
95/168: padded[cols_to_impute].isnull()
95/169: padded
95/170: padded[padded.AUTO_ID==0]
95/171: padded[padded.AUTO_ID==0].to_csv('tmp.csv')
95/172:
med_rx = pd.read_csv(opj(cfg.LONG_IN, 'rx_intervals.csv'))
med_rx
95/173:
med_rx = pd.read_csv(cfg.opj(cfg.LONG_IN, 'rx_intervals.csv'))
med_rx
95/174:
def pad_missing_months(df):
    df = df.reset_index()
    new_index = pd.MultiIndex.from_product([[df.AUTO_ID.values[0]], list(range(0, df.MONTH_TS.max()+1))], names=['AUTO_ID', 'MONTH_TS'])
    df = df.set_index(['AUTO_ID', 'MONTH_TS']).reindex(new_index).reset_index()
    return df

padded = timestamped_df.groupby('AUTO_ID').apply(pad_missing_months)
padded.index = range(0, padded.shape[0])
padded['PADDED'] = padded.RESULT_YEAR.isnull().astype(int)
95/175:
# med_rx = pd.read_csv(cfg.opj(cfg.LONG_IN, 'rx_intervals.csv'))
med_rx.groupby(['AUTO_ID', 'DRUG_CLASS']).unstack()
95/176:
# med_rx = pd.read_csv(cfg.opj(cfg.LONG_IN, 'rx_intervals.csv'))
med_rx.groupby(['AUTO_ID', 'DRUG_CLASS']).ACTIVE_DURING.agg('first').unstack()
95/177:
# med_rx = pd.read_csv(cfg.opj(cfg.LONG_IN, 'rx_intervals.csv'))
med_rx.groupby(['AUTO_ID', 'DRUG_CLASS']).ACTIVE_DURING.agg('first').unstack().reset_index()
95/178:
def pad_missing_months(df):
    df = df.reset_index()
    new_index = pd.MultiIndex.from_product([[df.AUTO_ID.values[0]], list(range(0, df.MONTH_TS.max()+1))], names=['AUTO_ID', 'MONTH_TS'])
    df = df.set_index(['AUTO_ID', 'MONTH_TS']).reindex(new_index).reset_index()
    return df

# padded = timestamped_df.groupby('AUTO_ID').apply(pad_missing_months)
# padded.index = range(0, padded.shape[0])
# padded['PADDED'] = padded.RESULT_YEAR.isnull().astype(int)
padded
95/179:
# med_rx = pd.read_csv(cfg.opj(cfg.LONG_IN, 'rx_intervals.csv'))
med_rx = med_rx.groupby(['AUTO_ID', 'DRUG_CLASS']).ACTIVE_DURING.agg('first').unstack().reset_index()
med_rx.columns
95/180:
# med_rx = pd.read_csv(cfg.opj(cfg.LONG_IN, 'rx_intervals.csv'))
med_rx = med_rx.groupby(['AUTO_ID', 'DRUG_CLASS']).ACTIVE_DURING.agg('first').unstack().reset_index()
pd.concat([padded, pd.DataFrame(columns=['5_ASA', 'ANTI_IL12', 'ANTI_INTEGRIN', 'ANTI_TNF','Immunomodulators', 'Psych', 'Systemic_steroids'], index=padded.index)])
95/181:
# med_rx = pd.read_csv(cfg.opj(cfg.LONG_IN, 'rx_intervals.csv'))
# med_rx = med_rx.groupby(['AUTO_ID', 'DRUG_CLASS']).ACTIVE_DURING.agg('first').unstack().reset_index()
pd.concat([padded, pd.DataFrame(columns=['5_ASA', 'ANTI_IL12', 'ANTI_INTEGRIN', 'ANTI_TNF','Immunomodulators', 'Psych', 'Systemic_steroids'], index=padded.index)])
95/182:
# med_rx = pd.read_csv(cfg.opj(cfg.LONG_IN, 'rx_intervals.csv'))
# med_rx = med_rx.groupby(['AUTO_ID', 'DRUG_CLASS']).ACTIVE_DURING.agg('first').unstack().reset_index()
pd.concat([padded, pd.DataFrame(columns=['5_ASA', 'ANTI_IL12', 'ANTI_INTEGRIN', 'ANTI_TNF','Immunomodulators', 'Psych', 'Systemic_steroids'], index=padded.index)], axis=1)
95/183: med_rx
94/7:
import pandas as pd
from config import *
import os
94/8:
charges = pd.read_csv(opj(LONG_IN, 'charges.csv'))
encounters = pd.read_csv(opj(LONG_IN,'encounters.csv'))
diag = pd.read_csv(opj(LONG_IN,'gi_diagnostics.csv'))
diag.columns = [f'DIAG_{x}' if x not in IDENTIFIER_COLS else x for x in diag.columns ]
labs = pd.read_csv(opj(LONG_IN, 'labs.csv'))
labs.columns = [f'LAB_{x}' if x not in IDENTIFIER_COLS else x for x in labs.columns]
94/9:
ce = charges.merge(encounters, how='outer', on=IDENTIFIER_COLS)
ced = ce.merge(diag, how='outer', on=IDENTIFIER_COLS)
cedl = ced.merge(labs, how='outer', on=IDENTIFIER_COLS)
94/10: cedl
94/11: cedl.AUTO_ID
94/12: cedl.AUTO_ID.name
94/13:
med_rx = pd.read_csv(cfg.opj(cfg.LONG_IN, 'all_rx.csv'))
med_rx
94/14:
med_rx = pd.read_csv(opj(LONG_IN, 'all_rx.csv'))
med_rx
92/121:
t = pd.to_datetime(['2018-07-05', '2019-03-28'])
pd.tseries.offsets.MonthBegin(1)
92/122:
t = pd.to_datetime(['2018-07-05', '2019-03-28'])
t-pd.tseries.offsets.MonthBegin(1)
92/123:
# t = pd.to_datetime(['2018-07-05', '2019-03-28'])
t = pd.Timestamp('2018-07-05')
t-pd.tseries.offsets.MonthBegin(1)
92/124:
# t = pd.to_datetime(['2018-07-05', '2019-03-28'])
t = pd.Timestamp('2018-07-05')
t+pd.tseries.offsets.MonthEnd(1)
92/125:
def merge_contiguous_events(y):    
    e = y.END_DATE
    o = y.ORDERING_DATE
    drug_col = y.GROUP.values[0]
    
    start_loc = 0
    end_loc = 0
    
    start_dates = [o.iloc[0]]
    end_dates=[]

    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    df_start = min(start_dates)-pd.tseries.offsets.MonthBegin(1)
    df_end = max(end_dates)+pd.tseries.offsets.MonthEnd(1)
    active_during = list(zip(start_dates, end_dates))
    
    out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
    out_df['RESULT_YEAR'] = out_df.ts.dt.year
    out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    for s,e in active_during:
        out_df.loc[out_df.ts.between(s,e), drug_col] = 1
    out_df.loc[[0,-1],drug_col] = 1
    
    return out_df
92/126:
med = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med.head(100).groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
92/127:
med = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med.head(10).groupby(['AUTO_ID', 'GROUP']).apply(print)
92/128:
med = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med.head(10).groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
92/129:
def merge_contiguous_events(y):    
    e = y.END_DATE
    o = y.ORDERING_DATE
    drug_col = 'chk'#y.GROUP.values[0]
    
    start_loc = 0
    end_loc = 0
    
    start_dates = [o.iloc[0]]
    end_dates=[]

    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
    df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)
    active_during = list(map(pd.Timestamp, zip(start_dates, end_dates)))
    
    out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
    out_df['RESULT_YEAR'] = out_df.ts.dt.year
    out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    for s,e in active_during:
        out_df.loc[out_df.ts.between(s,e), drug_col] = 1
    out_df.loc[[0,-1],drug_col] = 1
    
    return out_df()
92/130:
med = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med.head(10).groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
92/131:
def merge_contiguous_events(y):    
    e = y.END_DATE
    o = y.ORDERING_DATE
    drug_col = 'chk'#y.GROUP.values[0]
    print(y.GROUP)
    
    start_loc = 0
    end_loc = 0
    
    start_dates = [o.iloc[0]]
    end_dates=[]

    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    print(start_dates[0])
    df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
    df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)
    active_during = list(map(pd.Timestamp, zip(start_dates, end_dates)))
    
    out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
    out_df['RESULT_YEAR'] = out_df.ts.dt.year
    out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    for s,e in active_during:
        out_df.loc[out_df.ts.between(s,e), drug_col] = 1
    out_df.loc[[0,-1],drug_col] = 1
    
    return out_df()
92/132:
med = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med.head(10).groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
92/133:
def merge_contiguous_events(y):    
    e = y.END_DATE
    o = y.ORDERING_DATE
    drug_col = 'chk'#y.GROUP.values[0]
#     print(y.GROUP)
    
    start_loc = 0
    end_loc = 0
    
    start_dates = [o.iloc[0]]
    end_dates=[]

    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    print(type(start_dates[0]))
    df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
    df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)
    active_during = list(map(pd.Timestamp, zip(start_dates, end_dates)))
    
    out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
    out_df['RESULT_YEAR'] = out_df.ts.dt.year
    out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    for s,e in active_during:
        out_df.loc[out_df.ts.between(s,e), drug_col] = 1
    out_df.loc[[0,-1],drug_col] = 1
    
    return out_df()
92/134:
med = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med.head(10).groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
92/135:
def merge_contiguous_events(y):    
    e = y.END_DATE
    o = y.ORDERING_DATE
    drug_col = 'chk'#y.GROUP.values[0]
#     print(y.GROUP)
    
    start_loc = 0
    end_loc = 0
    
    start_dates = [o.iloc[0]]
    end_dates=[]

    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    print(type(start_dates[0]))
    df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
    df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)
    active_during = list(map(pd.Timestamp, zip(start_dates, end_dates)))
    
    out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
    out_df['RESULT_YEAR'] = out_df.ts.dt.year
    out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    for s,e in active_during:
        out_df.loc[out_df.ts.between(s,e), drug_col] = 1
    out_df.loc[[0,-1],drug_col] = 1
    
    return out_df()
92/136:
def merge_contiguous_events(y):    
    e = y.END_DATE
    o = y.ORDERING_DATE
    drug_col = y.GROUP.values[0]
#     print(y.GROUP)
    
    start_loc = 0
    end_loc = 0
    
    start_dates = [o.iloc[0]]
    end_dates=[]

    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(pd.Timestamp(end_tm1))
            start_loc=end_loc+1
            start_dates.append(pd.Timestamp(start_t0))
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    print(type(start_dates[0]))
    df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
    df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)
    active_during = list(map(pd.Timestamp, zip(start_dates, end_dates)))
    
    out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
    out_df['RESULT_YEAR'] = out_df.ts.dt.year
    out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    for s,e in active_during:
        out_df.loc[out_df.ts.between(s,e), drug_col] = 1
    out_df.loc[[0,-1],drug_col] = 1
    
    return out_df()
92/137:
med = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med.head(10).groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
92/138:
def merge_contiguous_events(y):    
    e = y.END_DATE
    o = y.ORDERING_DATE
    drug_col = y.GROUP.values[0]
#     print(y.GROUP)
    
    start_loc = 0
    end_loc = 0
    
    start_dates = [pd.Timestamp(o.iloc[0])]
    end_dates=[]

    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(pd.Timestamp(end_tm1))
            start_loc=end_loc+1
            start_dates.append(pd.Timestamp(start_t0))
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    print(type(start_dates[0]))
    df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
    df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)
    active_during = list(map(pd.Timestamp, zip(start_dates, end_dates)))
    
    out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
    out_df['RESULT_YEAR'] = out_df.ts.dt.year
    out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    for s,e in active_during:
        out_df.loc[out_df.ts.between(s,e), drug_col] = 1
    out_df.loc[[0,-1],drug_col] = 1
    
    return out_df()
92/139:
med = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med.head(10).groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
92/140:
def merge_contiguous_events(y):    
    e = y.END_DATE
    o = y.ORDERING_DATE
    drug_col = y.GROUP.values[0]
#     print(y.GROUP)
    
    start_loc = 0
    end_loc = 0
    
    start_dates = [pd.Timestamp(o.iloc[0])]
    end_dates=[]

    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(pd.Timestamp(e.iloc[-1]))
    
    df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
    df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)
    active_during = list(map(pd.Timestamp, zip(start_dates, end_dates)))
    
    out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
    out_df['RESULT_YEAR'] = out_df.ts.dt.year
    out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    for s,e in active_during:
        out_df.loc[out_df.ts.between(s,e), drug_col] = 1
    out_df.loc[[0,-1],drug_col] = 1
    
    return out_df()
92/141:
med = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med.head(10).groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
92/142:
def merge_contiguous_events(y):    
    e = y.END_DATE
    o = y.ORDERING_DATE
    drug_col = y.GROUP.values[0]
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = map(pd.Timestamp, start_dates)
    end_dates = map(pd.Timestamp, end_dates)
        
    out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
    df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
    df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
    out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
    out_df['RESULT_YEAR'] = out_df.ts.dt.year
    out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
    df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
    for s,e in active_during:
        out_df.loc[out_df.ts.between(s,e), drug_col] = 1
    out_df.loc[[0,-1],drug_col] = 1
    
    return out_df()
92/143:
med = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med.head(10).groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
92/144:
def merge_contiguous_events(y):    
    e = y.END_DATE
    o = y.ORDERING_DATE
    drug_col = y.GROUP.values[0]
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))
        
    out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
    df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
    df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
    out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
    out_df['RESULT_YEAR'] = out_df.ts.dt.year
    out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
    df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
    for s,e in active_during:
        out_df.loc[out_df.ts.between(s,e), drug_col] = 1
    out_df.loc[[0,-1],drug_col] = 1
    
    return out_df()
92/145:
med = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med.head(10).groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
92/146:
def merge_contiguous_events(y):    
    e = y.END_DATE
    o = y.ORDERING_DATE
    drug_col = y.GROUP.values[0]
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))
        
    out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
    df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
    df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
    out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
    out_df['RESULT_YEAR'] = out_df.ts.dt.year
    out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    
    active_during = list(zip(start_dates, end_dates))
    for s,e in active_during:
        out_df.loc[out_df.ts.between(s,e), drug_col] = 1
    out_df.loc[[0,-1],drug_col] = 1
    
    return out_df()
92/147:
med = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med.head(10).groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
92/148:
def merge_contiguous_events(y):    
    e = y.END_DATE
    o = y.ORDERING_DATE
    drug_col = y.GROUP.values[0]
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))
        
    out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
    df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
    df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
    out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
    out_df['RESULT_YEAR'] = out_df.ts.dt.year
    out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    
    active_during = list(zip(start_dates, end_dates))
    for s,e in active_during:
        out_df.loc[out_df.ts.between(s,e), drug_col] = 1
    out_df.iloc[[0,-1],drug_col] = 1
    
    return out_df()
92/149:
med = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med.head(10).groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
92/150:
def merge_contiguous_events(y):    
    e = y.END_DATE
    o = y.ORDERING_DATE
    drug_col = y.GROUP.values[0]
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))
        
    out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
    df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
    df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
    out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
    out_df['RESULT_YEAR'] = out_df.ts.dt.year
    out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    
    active_during = list(zip(start_dates, end_dates))
    for s,e in active_during:
        out_df.loc[out_df.ts.between(s,e), drug_col] = 1
    out_df.iloc[[0,-1]][drug_col] = 1
    
    return out_df()
92/151:
def merge_contiguous_events(y):    
    e = y.END_DATE
    o = y.ORDERING_DATE
    drug_col = y.GROUP.values[0]
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))
        
    out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
    df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
    df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
    out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
    out_df['RESULT_YEAR'] = out_df.ts.dt.year
    out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    
    active_during = list(zip(start_dates, end_dates))
    for s,e in active_during:
        out_df.loc[out_df.ts.between(s,e), drug_col] = 1
    out_df.iloc[[0,-1]][drug_col] = 1
    
    return out_df()
92/152:
med = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med.head(10).groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
92/153:
def merge_contiguous_events(y):    
    e = y.END_DATE
    o = y.ORDERING_DATE
    drug_col = y.GROUP.values[0]
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))
        
    out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
    df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
    df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
    out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
    out_df['RESULT_YEAR'] = out_df.ts.dt.year
    out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    
    active_during = list(zip(start_dates, end_dates))
    for s,e in active_during:
        out_df.loc[out_df.ts.between(s,e), drug_col] = 1
    out_df.iloc[[0,-1]][drug_col] = 1
    
    return out_df
92/154:
med = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med.head(10).groupby(['AUTO_ID', 'GROUP']).apply(merge_contiguous_events)
92/155: merge_contiguous_events(med[med.AUTO_ID==0][med.GROUP=='5_ASA'])
92/156: merge_contiguous_events(med[med.AUTO_ID==0][med.GROUP=='5_ASA']).5_ASA.isnull()
92/157: merge_contiguous_events(med[med.AUTO_ID==0][med.GROUP=='5_ASA'])['5_ASA'].isnull()
92/158: merge_contiguous_events(med[med.AUTO_ID==0][med.GROUP=='5_ASA'])['5_ASA'].isnull().sum()
92/159: merge_contiguous_events(med[med.AUTO_ID==0][med.GROUP=='5_ASA'])
92/160:
from functools import reduce

def func1(x):
    l = []
    for grp in x.GROUP.unique():
        l.append(merge_contiguous_events(x[x.GROUP==grp], grp))
        
        
        
    
    
    
def merge_contiguous_events(y, drug_col):    
    e = y.END_DATE
    o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))
        
    out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
    df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
    df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
    out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
    out_df['RESULT_YEAR'] = out_df.ts.dt.year
    out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    
    active_during = list(zip(start_dates, end_dates))
    for s,e in active_during:
        out_df.loc[out_df.ts.between(s,e), drug_col] = 1
    out_df.iloc[[0,-1]][drug_col] = 1
    
    return out_df.drop('ts',axis=1)
92/161: merge_contiguous_events(med[med.AUTO_ID==0][med.GROUP=='5_ASA'])
92/162: merge_contiguous_events(med[med.AUTO_ID==0][med.GROUP=='5_ASA'], '5_ASA')
92/163:
from functools import reduce

def func1(x):
    l = []
    for grp in x.GROUP.unique():
        l.append(merge_contiguous_events(x[x.GROUP==grp], grp))
    
    return reduce(lambda left, right: left.merge(right, how='outer', on=['AUTO_ID', 'RESULT_YEAR','RESULT_MONTH']), l)
    
    
def merge_contiguous_events(y, drug_col):    
    e = y.END_DATE
    o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))
        
    out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
    df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
    df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
    out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
    out_df['RESULT_YEAR'] = out_df.ts.dt.year
    out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    
    active_during = list(zip(start_dates, end_dates))
    for s,e in active_during:
        out_df.loc[out_df.ts.between(s,e), drug_col] = 1
    out_df.iloc[[0,-1]][drug_col] = 1
    
    return out_df.drop('ts',axis=1)
92/164:
# merge_contiguous_events(med[med.AUTO_ID==0][med.GROUP=='5_ASA'], '5_ASA')
func1(med[med.AUTO_ID==0])
92/165:
from functools import reduce

def active_prescriptions(x):
    l = []
    for grp in x.GROUP.unique():
        l.append(active_drug(x[x.GROUP==grp], grp))
    return reduce(lambda left, right: left.merge(right, how='outer', on=['AUTO_ID', 'RESULT_YEAR','RESULT_MONTH']), l)
    

def active_drug(y, drug_col):    
    e = y.END_DATE
    o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))
        
    out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
    df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
    df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
    out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
    out_df['RESULT_YEAR'] = out_df.ts.dt.year
    out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    
    active_during = list(zip(start_dates, end_dates))
    for s,e in active_during:
        out_df.loc[out_df.ts.between(s,e), drug_col] = 1
    out_df.iloc[[0,-1]][drug_col] = 1
    
    return out_df.drop('ts',axis=1)
92/166: med_df = med.copy()
92/167: drug_activity = med_df.groupby(['AUTO_ID']).apply(active_prescriptions)
92/168: drug_activity = med_df.groupby(['AUTO_ID']).apply(active_prescriptions)
92/169: drug_activity = med_df.groupby(['AUTO_ID']).apply(active_prescriptions)
92/170: med_df
92/171:
med_df.drop('Unnamed: 0', axis=1, inplace=True)
med_df
92/172: med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
92/173: med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack(-1).reset_index()
92/174: med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack()#.reset_index()
92/175: med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
92/176: med_df#.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
92/177: med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
92/178:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df
92/179:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.GROUP.unique()
92/180: s
92/181: pd.Series(index=pd.date_range(pd.to_datetime('2019-05-05'),pd.to_datetime('2020-05-05', freq='M'))
92/182: pd.Series(index=pd.date_range(pd.to_datetime('2019-05-05'),pd.to_datetime('2020-05-05', freq='M')))
92/183: pd.Series(index=pd.date_range(pd.to_datetime('2019-05-05'),pd.to_datetime('2020-05-05'), freq='M'))
92/184: pd.Series(index=pd.date_range(pd.to_datetime('2019-05-01'),pd.to_datetime('2020-05-05'), freq='M'))
92/185: pd.Series(index=pd.date_range(pd.to_datetime('2019-05-01'),pd.to_datetime('2020-05-05'), freq='M'), name='drugz')
92/186: type(pd.Series(index=pd.date_range(pd.to_datetime('2019-05-01'),pd.to_datetime('2020-05-05'), freq='M'), name='drugz'))
92/187:
s1=type(pd.Series(index=pd.date_range(pd.to_datetime('2019-05-01'),pd.to_datetime('2020-05-05'), freq='M'), name='drugz'))
d1 = pd.concat([s1,s1], axis=1)
d1
92/188:
s1=pd.Series(index=pd.date_range(pd.to_datetime('2019-05-01'),pd.to_datetime('2020-05-05'), freq='M'), name='drugz')
d1 = pd.concat([s1,s1], axis=1)
d1
92/189:
s1=pd.Series(index=pd.date_range(pd.to_datetime('2019-05-01'),pd.to_datetime('2020-05-05'), freq='M'), name='drugz')
s2=pd.Series(index=pd.date_range(pd.to_datetime('2019-05-01'),pd.to_datetime('2019-10-05'), freq='M'), name='drugz')
d1 = pd.concat([s1,s2], axis=1)
d1
92/190:
s1=pd.Series(index=pd.date_range(pd.to_datetime('2019-05-01'),pd.to_datetime('2020-05-05'), freq='M'), name='drugz')
s1=1
s2=pd.Series(index=pd.date_range(pd.to_datetime('2019-05-01'),pd.to_datetime('2019-10-05'), freq='M'), name='drugz')
s2=1
d1 = pd.concat([s1,s2], axis=1)
d1
92/191:
s1=pd.Series(index=pd.date_range(pd.to_datetime('2019-05-01'),pd.to_datetime('2020-05-05'), freq='M'), name='drugz')
# s1=1
s2=pd.Series(index=pd.date_range(pd.to_datetime('2019-05-01'),pd.to_datetime('2019-10-05'), freq='M'), name='drugz')
# s2=1
d1 = pd.concat([s1,s2], axis=1)
d1
92/192: s1.fillna(1)
92/193:
s1=pd.Series(index=pd.date_range(pd.to_datetime('2019-05-01'),pd.to_datetime('2020-05-05'), freq='M'), name='drugz')
s1.fillna(1)
s2=pd.Series(index=pd.date_range(pd.to_datetime('2019-05-01'),pd.to_datetime('2019-10-05'), freq='M'), name='drugz')
s2.fillna(2)
d1 = pd.concat([s1,s2], axis=1)
d1
92/194:
s1=pd.Series(index=pd.date_range(pd.to_datetime('2019-05-01'),pd.to_datetime('2020-05-05'), freq='M'), name='drugz')
s1=s1.fillna(1)
s2=pd.Series(index=pd.date_range(pd.to_datetime('2019-05-01'),pd.to_datetime('2019-10-05'), freq='M'), name='drugz')
s2=s2.fillna(2)
d1 = pd.concat([s1,s2], axis=1)
d1
92/195:
s1=pd.Series(index=pd.date_range(pd.to_datetime('2019-05-01'),pd.to_datetime('2020-05-05'), freq='M'), name='drugz')
s1=s1.fillna(1)
s2=pd.Series(index=pd.date_range(pd.to_datetime('2019-05-01'),pd.to_datetime('2019-10-05'), freq='M'), name='drugz')
s2=s2.fillna(2)
d1 = pd.concat([s1,s2], axis=1)
d1.index
92/196:
s1=pd.Series(index=pd.date_range(pd.to_datetime('2019-05-01'),pd.to_datetime('2020-05-05'), freq='M'), name='drugz')
s1=s1.fillna(1)
s2=pd.Series(index=pd.date_range(pd.to_datetime('2019-05-01'),pd.to_datetime('2019-10-05'), freq='M'), name='drugz')
s2=s2.fillna(2)
d1 = pd.concat([s1,s2], axis=1)
d1.index.dt.year
92/197:
s1=pd.Series(index=pd.date_range(pd.to_datetime('2019-05-01'),pd.to_datetime('2020-05-05'), freq='M'), name='drugz')
s1=s1.fillna(1)
s2=pd.Series(index=pd.date_range(pd.to_datetime('2019-05-01'),pd.to_datetime('2019-10-05'), freq='M'), name='drugz')
s2=s2.fillna(2)
d1 = pd.concat([s1,s2], axis=1)
d1.index.year
92/198:
s1=pd.Series(index=pd.date_range(pd.to_datetime('2019-05-01'),pd.to_datetime('2020-05-05'), freq='M'), name='drugz')
s1=s1.fillna(1)
s2=pd.Series(index=pd.date_range(pd.to_datetime('2019-05-01'),pd.to_datetime('2019-10-05'), freq='M'), name='drugz')
s2=s2.fillna(2)
d1 = pd.concat([s1,s2], axis=1)
d1.index.month
92/199:
from functools import reduce

def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    
    srs=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        for s,e in intvl:
            s = pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1)
            srs.append()
    df = pd.concat(srs, axis=1)
    df['AUTO_ID'] = med_a.AUTO_ID.values[0]
    df['RESULT_YEAR'] = df.index.year
    df['RESULT_MONTH'] = df.index.month
    return df
                       
    

def active_intervals(o,e):    
#     e = y.END_DATE
#     o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
#     start_dates[0] -= pd.tseries.offsets.MonthBegin(1)
#     end_dates[-1] += pd.tseries.offsets.MonthEnd(1)
    active_during = list(zip(start_dates, end_dates))
    return active_during
        
        
#     out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
#     df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
#     df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
#     out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
#     out_df['RESULT_YEAR'] = out_df.ts.dt.year
#     out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    
    
#     for s,e in active_during:
#         out_df.loc[out_df.ts.between(s,e), drug_col] = 1
#     out_df.iloc[[0,-1]][drug_col] = 1
    
#     return out_df.drop('ts',axis=1)
92/200:
from functools import reduce

def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    
    srs=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        for s,e in intvl:
            s = pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1)
            srs.append()
    df = pd.concat(srs, axis=1)
    df['AUTO_ID'] = med_a.AUTO_ID.values[0]
    df['RESULT_YEAR'] = df.index.year
    df['RESULT_MONTH'] = df.index.month
    return df
                       
    

def active_intervals(o,e):    
#     e = y.END_DATE
#     o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
#     start_dates[0] -= pd.tseries.offsets.MonthBegin(1)
#     end_dates[-1] += pd.tseries.offsets.MonthEnd(1)
    active_during = list(zip(start_dates, end_dates))
    return active_during
        
        
#     out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
#     df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
#     df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
#     out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
#     out_df['RESULT_YEAR'] = out_df.ts.dt.year
#     out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    
    
#     for s,e in active_during:
#         out_df.loc[out_df.ts.between(s,e), drug_col] = 1
#     out_df.iloc[[0,-1]][drug_col] = 1
    
#     return out_df.drop('ts',axis=1)
92/201:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions)
92/202: med_df.empty()
92/203: med_df.empty
92/204:
from functools import reduce

def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        if df.empty():
            continue
        intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    
    srs=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        for s,e in intvl:
            s = pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1)
            srs.append()
    df = pd.concat(srs, axis=1)
    df['AUTO_ID'] = med_a.AUTO_ID.values[0]
    df['RESULT_YEAR'] = df.index.year
    df['RESULT_MONTH'] = df.index.month
    return df
                       
    

def active_intervals(o,e):    
#     e = y.END_DATE
#     o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
#     start_dates[0] -= pd.tseries.offsets.MonthBegin(1)
#     end_dates[-1] += pd.tseries.offsets.MonthEnd(1)
    active_during = list(zip(start_dates, end_dates))
    return active_during
        
        
#     out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
#     df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
#     df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
#     out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
#     out_df['RESULT_YEAR'] = out_df.ts.dt.year
#     out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    
    
#     for s,e in active_during:
#         out_df.loc[out_df.ts.between(s,e), drug_col] = 1
#     out_df.iloc[[0,-1]][drug_col] = 1
    
#     return out_df.drop('ts',axis=1)
92/205:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions)
92/206:
from functools import reduce

def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        if df.empty:
            continue
        intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    
    srs=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        for s,e in intvl:
            s = pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1)
            srs.append()
    df = pd.concat(srs, axis=1)
    df['AUTO_ID'] = med_a.AUTO_ID.values[0]
    df['RESULT_YEAR'] = df.index.year
    df['RESULT_MONTH'] = df.index.month
    return df
                       
    

def active_intervals(o,e):    
#     e = y.END_DATE
#     o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
#     start_dates[0] -= pd.tseries.offsets.MonthBegin(1)
#     end_dates[-1] += pd.tseries.offsets.MonthEnd(1)
    active_during = list(zip(start_dates, end_dates))
    return active_during
        
        
#     out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
#     df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
#     df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
#     out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
#     out_df['RESULT_YEAR'] = out_df.ts.dt.year
#     out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    
    
#     for s,e in active_during:
#         out_df.loc[out_df.ts.between(s,e), drug_col] = 1
#     out_df.iloc[[0,-1]][drug_col] = 1
    
#     return out_df.drop('ts',axis=1)
92/207:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions)
92/208:
from functools import reduce

def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        if df.empty:
            continue
        intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    
    srs=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        for s,e in intvl:
            s = pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1)
            srs.append(s)
    df = pd.concat(srs, axis=1)
    df['AUTO_ID'] = med_a.AUTO_ID.values[0]
    df['RESULT_YEAR'] = df.index.year
    df['RESULT_MONTH'] = df.index.month
    return df
                       
    

def active_intervals(o,e):    
#     e = y.END_DATE
#     o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
#     start_dates[0] -= pd.tseries.offsets.MonthBegin(1)
#     end_dates[-1] += pd.tseries.offsets.MonthEnd(1)
    active_during = list(zip(start_dates, end_dates))
    return active_during
        
        
#     out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
#     df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
#     df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
#     out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
#     out_df['RESULT_YEAR'] = out_df.ts.dt.year
#     out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    
    
#     for s,e in active_during:
#         out_df.loc[out_df.ts.between(s,e), drug_col] = 1
#     out_df.iloc[[0,-1]][drug_col] = 1
    
#     return out_df.drop('ts',axis=1)
92/209:
from functools import reduce

def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        if df.empty:
            continue
        intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    
    srs=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        for s,e in intvl:
            sr = pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1)
            srs.append(sr)
    df = pd.concat(srs, axis=1)
    df['AUTO_ID'] = med_a.AUTO_ID.values[0]
    df['RESULT_YEAR'] = df.index.year
    df['RESULT_MONTH'] = df.index.month
    return df
                       
    

def active_intervals(o,e):    
#     e = y.END_DATE
#     o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
#     start_dates[0] -= pd.tseries.offsets.MonthBegin(1)
#     end_dates[-1] += pd.tseries.offsets.MonthEnd(1)
    active_during = list(zip(start_dates, end_dates))
    return active_during
        
        
#     out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
#     df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
#     df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
#     out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
#     out_df['RESULT_YEAR'] = out_df.ts.dt.year
#     out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    
    
#     for s,e in active_during:
#         out_df.loc[out_df.ts.between(s,e), drug_col] = 1
#     out_df.iloc[[0,-1]][drug_col] = 1
    
#     return out_df.drop('ts',axis=1)
92/210:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions)
92/211: pd.Series(name='dr')
92/212: pd.concat([pd.Series(name='dr'), pd.Series(range(x), name='asd')], axis=1)
92/213: pd.concat([pd.Series(name='dr'), pd.Series(range(5), name='asd')], axis=1)
92/214:
from functools import reduce

def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        if df.empty:
            intervals.append(None)
        else:
            intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    
    drug_columns=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        drug_col = []
        if not intvl:            
            drug_column = pd.Series(name=dr)
        else:
            for s,e in intvl:
                drug_col.append(pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1))
            drug_col = pd.concat(drug_col)
        drug_columns.append(drug_col)
        
    df = pd.concat(srs, axis=1)
    df['AUTO_ID'] = med_a.AUTO_ID.values[0]
    df['RESULT_YEAR'] = df.index.year
    df['RESULT_MONTH'] = df.index.month
    return df
                       
    

def active_intervals(o,e):    
#     e = y.END_DATE
#     o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
#     start_dates[0] -= pd.tseries.offsets.MonthBegin(1)
#     end_dates[-1] += pd.tseries.offsets.MonthEnd(1)
    active_during = list(zip(start_dates, end_dates))
    return active_during
        
        
#     out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
#     df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
#     df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
#     out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
#     out_df['RESULT_YEAR'] = out_df.ts.dt.year
#     out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    
    
#     for s,e in active_during:
#         out_df.loc[out_df.ts.between(s,e), drug_col] = 1
#     out_df.iloc[[0,-1]][drug_col] = 1
    
#     return out_df.drop('ts',axis=1)
92/215:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions)
92/216:
from functools import reduce

def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        if df.empty:
            intervals.append(None)
        else:
            intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    
    drug_columns=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        drug_col = []
        if not intvl:            
            drug_column = pd.Series(name=dr)
        else:
            for s,e in intvl:
                drug_col.append(pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1))
            drug_col = pd.concat(drug_col)
        drug_columns.append(drug_col)
        
    df = pd.concat(drug_columns, axis=1)
    df['AUTO_ID'] = med_a.AUTO_ID.values[0]
    df['RESULT_YEAR'] = df.index.year
    df['RESULT_MONTH'] = df.index.month
    return df
                       
    

def active_intervals(o,e):    
#     e = y.END_DATE
#     o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
#     start_dates[0] -= pd.tseries.offsets.MonthBegin(1)
#     end_dates[-1] += pd.tseries.offsets.MonthEnd(1)
    active_during = list(zip(start_dates, end_dates))
    return active_during
        
        
#     out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
#     df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
#     df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
#     out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
#     out_df['RESULT_YEAR'] = out_df.ts.dt.year
#     out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    
    
#     for s,e in active_during:
#         out_df.loc[out_df.ts.between(s,e), drug_col] = 1
#     out_df.iloc[[0,-1]][drug_col] = 1
    
#     return out_df.drop('ts',axis=1)
92/217:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions)
92/218:
from functools import reduce

def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        if df.empty:
            intervals.append(None)
        else:
            intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    
    drug_columns=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        drug_col = []
        
        if not intvl:            
            drug_col = pd.Series(name=dr)
        else:
            for s,e in intvl:
                drug_col.append(pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1))
            drug_col = pd.concat(drug_col)
        drug_columns.append(drug_col)
        
    df = pd.concat(drug_columns, axis=1)
    df['AUTO_ID'] = med_a.AUTO_ID.values[0]
    df['RESULT_YEAR'] = df.index.year
    df['RESULT_MONTH'] = df.index.month
    return df
                       
    

def active_intervals(o,e):    
#     e = y.END_DATE
#     o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
#     start_dates[0] -= pd.tseries.offsets.MonthBegin(1)
#     end_dates[-1] += pd.tseries.offsets.MonthEnd(1)
    active_during = list(zip(start_dates, end_dates))
    return active_during
        
        
#     out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
#     df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
#     df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
#     out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
#     out_df['RESULT_YEAR'] = out_df.ts.dt.year
#     out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    
    
#     for s,e in active_during:
#         out_df.loc[out_df.ts.between(s,e), drug_col] = 1
#     out_df.iloc[[0,-1]][drug_col] = 1
    
#     return out_df.drop('ts',axis=1)
92/219:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions)
92/220:
from functools import reduce

def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        if df.empty:
            intervals.append(None)
        else:
            intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    
    drug_columns=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        drug_col = []
        
        if not intvl:            
            drug_col = pd.Series(name=dr)
        else:
            for s,e in intvl:
                drug_col.append(pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1))
            drug_col = pd.concat(drug_col)
        drug_columns.append(drug_col)
        
    df = pd.concat(drug_columns, axis=1)
    print(df.index)
    df['AUTO_ID'] = med_a.AUTO_ID.values[0]
    df['RESULT_YEAR'] = df.index.year
    df['RESULT_MONTH'] = df.index.month
    return df
                       
    

def active_intervals(o,e):    
#     e = y.END_DATE
#     o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
#     start_dates[0] -= pd.tseries.offsets.MonthBegin(1)
#     end_dates[-1] += pd.tseries.offsets.MonthEnd(1)
    active_during = list(zip(start_dates, end_dates))
    return active_during
        
        
#     out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
#     df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
#     df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
#     out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
#     out_df['RESULT_YEAR'] = out_df.ts.dt.year
#     out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    
    
#     for s,e in active_during:
#         out_df.loc[out_df.ts.between(s,e), drug_col] = 1
#     out_df.iloc[[0,-1]][drug_col] = 1
    
#     return out_df.drop('ts',axis=1)
92/221:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions)
92/222:
from functools import reduce

def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        if df.empty:
            intervals.append(None)
        else:
            intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    
    drug_columns=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        drug_col = []
        
        if not intvl:            
            drug_col = pd.Series(name=dr)
        else:
            for s,e in intvl:
                drug_col.append(pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1))
            drug_col = pd.concat(drug_col)
        drug_columns.append(drug_col)
        
    df = pd.concat(drug_columns, axis=1)
    
    df['AUTO_ID'] = med_a.AUTO_ID.values[0]
    try:
        df['RESULT_YEAR'] = df.index.year
    except:
        print(df)
    df['RESULT_MONTH'] = df.index.month
    return df
                       
    

def active_intervals(o,e):    
#     e = y.END_DATE
#     o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
#     start_dates[0] -= pd.tseries.offsets.MonthBegin(1)
#     end_dates[-1] += pd.tseries.offsets.MonthEnd(1)
    active_during = list(zip(start_dates, end_dates))
    return active_during
        
        
#     out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
#     df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
#     df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
#     out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
#     out_df['RESULT_YEAR'] = out_df.ts.dt.year
#     out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    
    
#     for s,e in active_during:
#         out_df.loc[out_df.ts.between(s,e), drug_col] = 1
#     out_df.iloc[[0,-1]][drug_col] = 1
    
#     return out_df.drop('ts',axis=1)
92/223:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions)
92/224:
from functools import reduce

def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        if df.empty:
            intervals.append(None)
        else:
            intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    
    drug_columns=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        drug_col = []
        
        if not intvl:            
            drug_col = pd.Series(name=dr)
        else:
            for s,e in intvl:
                drug_col.append(pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1))
            drug_col = pd.concat(drug_col)
        drug_columns.append(drug_col)
        
    df = pd.concat(drug_columns, axis=1)
    
    df['AUTO_ID'] = med_a.AUTO_ID.values[0]
    df['RESULT_YEAR'] = pd.Series(df.index).dt.year
    df['RESULT_MONTH'] = pd.Series(df.index).dt.month
    return df
                       
    

def active_intervals(o,e):    
#     e = y.END_DATE
#     o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
#     start_dates[0] -= pd.tseries.offsets.MonthBegin(1)
#     end_dates[-1] += pd.tseries.offsets.MonthEnd(1)
    active_during = list(zip(start_dates, end_dates))
    return active_during
        
        
#     out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
#     df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
#     df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
#     out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
#     out_df['RESULT_YEAR'] = out_df.ts.dt.year
#     out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    
    
#     for s,e in active_during:
#         out_df.loc[out_df.ts.between(s,e), drug_col] = 1
#     out_df.iloc[[0,-1]][drug_col] = 1
    
#     return out_df.drop('ts',axis=1)
92/225:
from functools import reduce

def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        if df.empty:
            intervals.append(None)
        else:
            intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    
    drug_columns=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        drug_col = []
        
        if not intvl:            
            drug_col = pd.Series(name=dr)
        else:
            for s,e in intvl:
                drug_col.append(pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1))
            drug_col = pd.concat(drug_col)
        drug_columns.append(drug_col)
        
    df = pd.concat(drug_columns, axis=1)
    
    df['AUTO_ID'] = med_a.AUTO_ID.values[0]
    df['RESULT_YEAR'] = pd.Series(df.index).dt.year
    df['RESULT_MONTH'] = pd.Series(df.index).dt.month
    return df
                       
    

def active_intervals(o,e):    
#     e = y.END_DATE
#     o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
#     start_dates[0] -= pd.tseries.offsets.MonthBegin(1)
#     end_dates[-1] += pd.tseries.offsets.MonthEnd(1)
    active_during = list(zip(start_dates, end_dates))
    return active_during
        
        
#     out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
#     df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
#     df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
#     out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
#     out_df['RESULT_YEAR'] = out_df.ts.dt.year
#     out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    
    
#     for s,e in active_during:
#         out_df.loc[out_df.ts.between(s,e), drug_col] = 1
#     out_df.iloc[[0,-1]][drug_col] = 1
    
#     return out_df.drop('ts',axis=1)
92/226:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions)
92/227:
from functools import reduce

def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        if df.empty:
            intervals.append(None)
        else:
            intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    
    drug_columns=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        drug_col = []
        
        if not intvl:            
            drug_col = pd.Series(name=dr)
        else:
            for s,e in intvl:
                drug_col.append(pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1))
            drug_col = pd.concat(drug_col)
        drug_columns.append(drug_col)
        
    df = pd.concat(drug_columns, axis=1)
    
    df['AUTO_ID'] = med_a.AUTO_ID.values[0]
    df['ts'] = df.index
    return df
                       
    

def active_intervals(o,e):    
#     e = y.END_DATE
#     o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
#     start_dates[0] -= pd.tseries.offsets.MonthBegin(1)
#     end_dates[-1] += pd.tseries.offsets.MonthEnd(1)
    active_during = list(zip(start_dates, end_dates))
    return active_during
        
        
#     out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
#     df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
#     df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
#     out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
#     out_df['RESULT_YEAR'] = out_df.ts.dt.year
#     out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    
    
#     for s,e in active_during:
#         out_df.loc[out_df.ts.between(s,e), drug_col] = 1
#     out_df.iloc[[0,-1]][drug_col] = 1
    
#     return out_df.drop('ts',axis=1)
92/228:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions)
92/229:
from functools import reduce

def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        if df.empty:
            intervals.append(None)
        else:
            intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    
    drug_columns=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        drug_col = []
        
        if not intvl:            
            drug_col = pd.Series(name=dr)
        else:
            for s,e in intvl:
                drug_col.append(pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1))
            drug_col = pd.concat(drug_col)
        drug_columns.append(drug_col)
        
    df = pd.concat(drug_columns, axis=1)
    
    df['AUTO_ID'] = med_a.AUTO_ID.values[0]
    df['RESULT_YEAR'] = df.index.dt.year
    df['RESULT_MONTH'] = df.index.dt.month
    return df
                       
    

def active_intervals(o,e):    
#     e = y.END_DATE
#     o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
#     start_dates[0] -= pd.tseries.offsets.MonthBegin(1)
#     end_dates[-1] += pd.tseries.offsets.MonthEnd(1)
    active_during = list(zip(start_dates, end_dates))
    return active_during
        
        
#     out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
#     df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
#     df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
#     out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
#     out_df['RESULT_YEAR'] = out_df.ts.dt.year
#     out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    
    
#     for s,e in active_during:
#         out_df.loc[out_df.ts.between(s,e), drug_col] = 1
#     out_df.iloc[[0,-1]][drug_col] = 1
    
#     return out_df.drop('ts',axis=1)
92/230:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions)
92/231:
from functools import reduce

def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        if df.empty:
            intervals.append(None)
        else:
            intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    
    drug_columns=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        drug_col = []
        
        if not intvl:            
            drug_col = pd.Series(name=dr)
        else:
            for s,e in intvl:
                drug_col.append(pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1))
            drug_col = pd.concat(drug_col)
        drug_columns.append(drug_col)
        
    df = pd.concat(drug_columns, axis=1)
    
    df['AUTO_ID'] = med_a.AUTO_ID.values[0]
    yymm = pd.to_datetime(df.index)
    df['RESULT_YEAR'] = yymm.dt.year
    df['RESULT_MONTH'] = yymm.dt.month
    return df
                       
    

def active_intervals(o,e):    
#     e = y.END_DATE
#     o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
#     start_dates[0] -= pd.tseries.offsets.MonthBegin(1)
#     end_dates[-1] += pd.tseries.offsets.MonthEnd(1)
    active_during = list(zip(start_dates, end_dates))
    return active_during
        
        
#     out_df = pd.DataFrame(columns=['ts','AUTO_ID', 'RESULT_YEAR','RESULT_MONTH', drug_col])
    
#     df_start = start_dates[0]-pd.tseries.offsets.MonthBegin(1)
#     df_end = end_dates[-1]+pd.tseries.offsets.MonthEnd(1)   
#     out_df['ts'] = pd.date_range(df_start, df_end, freq='M')
#     out_df['RESULT_YEAR'] = out_df.ts.dt.year
#     out_df['RESULT_MONTH'] = out_df.ts.dt.month
    
    
    
#     for s,e in active_during:
#         out_df.loc[out_df.ts.between(s,e), drug_col] = 1
#     out_df.iloc[[0,-1]][drug_col] = 1
    
#     return out_df.drop('ts',axis=1)
92/232:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions)
92/233:
from functools import reduce

def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        if df.empty:
            intervals.append(None)
        else:
            intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    
    drug_columns=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        drug_col = []
        
        if not intvl:            
            drug_col = pd.Series(name=dr)
        else:
            for s,e in intvl:
                drug_col.append(pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1))
            drug_col = pd.concat(drug_col)
        drug_columns.append(drug_col)
        
    df = pd.concat(drug_columns, axis=1)
    
    df['AUTO_ID'] = med_a.AUTO_ID.values[0]
    df['DT'] df.index
#     yymm = pd.to_datetime(df.index)
#     df['RESULT_YEAR'] = yymm.dt.year
#     df['RESULT_MONTH'] = yymm.dt.month
    return df
                       
    

def active_intervals(o,e):    
#     e = y.END_DATE
#     o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
#     start_dates[0] -= pd.tseries.offsets.MonthBegin(1)
#     end_dates[-1] += pd.tseries.offsets.MonthEnd(1)
    active_during = list(zip(start_dates, end_dates))
    return active_during
92/234:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions)
92/235:
from functools import reduce

def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        if df.empty:
            intervals.append(None)
        else:
            intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    
    drug_columns=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        drug_col = []
        
        if not intvl:            
            drug_col = pd.Series(name=dr)
        else:
            for s,e in intvl:
                drug_col.append(pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1))
            drug_col = pd.concat(drug_col)
        drug_columns.append(drug_col)
        
    df = pd.concat(drug_columns, axis=1)
    
    df['AUTO_ID'] = med_a.AUTO_ID.values[0]
    df['DT'] df.index
#     yymm = pd.to_datetime(df.index)
#     df['RESULT_YEAR'] = yymm.dt.year
#     df['RESULT_MONTH'] = yymm.dt.month
    return df
                       
    

def active_intervals(o,e):    
#     e = y.END_DATE
#     o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
#     start_dates[0] -= pd.tseries.offsets.MonthBegin(1)
#     end_dates[-1] += pd.tseries.offsets.MonthEnd(1)
    active_during = list(zip(start_dates, end_dates))
    return active_during
92/236:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions)
92/237:
from functools import reduce

def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        if df.empty:
            intervals.append(None)
        else:
            intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    
    drug_columns=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        drug_col = []
        
        if not intvl:            
            drug_col = pd.Series(name=dr)
        else:
            for s,e in intvl:
                drug_col.append(pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1))
            drug_col = pd.concat(drug_col)
        drug_columns.append(drug_col)
        
    df = pd.concat(drug_columns, axis=1)
    
    df['AUTO_ID'] = med_a.AUTO_ID.values[0]
    df['DT'] = df.index
#     yymm = pd.to_datetime(df.index)
#     df['RESULT_YEAR'] = yymm.dt.year
#     df['RESULT_MONTH'] = yymm.dt.month
    return df
                       
    

def active_intervals(o,e):    
#     e = y.END_DATE
#     o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
#     start_dates[0] -= pd.tseries.offsets.MonthBegin(1)
#     end_dates[-1] += pd.tseries.offsets.MonthEnd(1)
    active_during = list(zip(start_dates, end_dates))
    return active_during
92/238:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions)
92/239:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions).reset_index()
92/240:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions).index
92/241:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions).unstack()
92/242:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions).reset_index()
92/243:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions).AUTO_ID
92/244:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions).columns
92/245:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions).reset_index()
92/246:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions).reset_index(0)
92/247:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions).reset_index(-1)
92/248:
from functools import reduce

def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        if df.empty:
            intervals.append(None)
        else:
            intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    
    drug_columns=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        drug_col = []
        
        if not intvl:            
            drug_col = pd.Series(name=dr)
        else:
            for s,e in intvl:
                drug_col.append(pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1))
            drug_col = pd.concat(drug_col)
        drug_columns.append(drug_col)
        
    df = pd.concat(drug_columns, axis=1)
    
    df['AUTO_ID'] = med_a.AUTO_ID.values[0]
    return df
                       
    

def active_intervals(o,e):    
#     e = y.END_DATE
#     o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
    active_during = list(zip(start_dates, end_dates))
    return active_during
92/249:
from functools import reduce

def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        if df.empty:
            intervals.append(None)
        else:
            intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    
    drug_columns=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        drug_col = []
        
        if not intvl:            
            drug_col = pd.Series(name=dr)
        else:
            for s,e in intvl:
                drug_col.append(pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1))
            drug_col = pd.concat(drug_col)
        drug_columns.append(drug_col)
        
    df = pd.concat(drug_columns, axis=1)
    
    df['AUTO_ID'] = med_a.AUTO_ID.values[0]
    return df
                       
    

def active_intervals(o,e):    
#     e = y.END_DATE
#     o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
    active_during = list(zip(start_dates, end_dates))
    return active_during
92/250:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions).reset_index(-1)
92/251:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions).reset_index()
92/252:
from functools import reduce

def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        if df.empty:
            intervals.append(None)
        else:
            intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    
    drug_columns=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        drug_col = []
        
        if not intvl:            
            drug_col = pd.Series(name=dr)
        else:
            for s,e in intvl:
                drug_col.append(pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1))
            drug_col = pd.concat(drug_col)
        drug_columns.append(drug_col)
        
    df = pd.concat(drug_columns, axis=1)
    
#     df['AUTO_ID'] = med_a.AUTO_ID.values[0]
    return df
                       
    

def active_intervals(o,e):    
#     e = y.END_DATE
#     o = y.ORDERING_DATE
    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
    active_during = list(zip(start_dates, end_dates))
    return active_during
92/253:
# med_df.groupby(['AUTO_ID','GROUP']).aggregate('first').unstack().reset_index()
med_df = pd.read_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))
med_df.head(100).groupby('AUTO_ID').apply(active_prescriptions).reset_index()
92/254:
# psych_med_prescrip = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Medications_2018-07-05-09-47-15.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
# psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.THERAPEUTIC_CLASS=='PSYCHOTHERAPEUTIC DRUGS']
# psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.END_DATE.notnull() & psych_med_prescrip.ORDERING_DATE.notnull()] # filter no end_date rows
# psych_med_prescrip['GROUP'] = 'Psych'
# psych_med_prescrip = psych_med_prescrip[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
# psych_med_prescrip.to_csv(os.path.join(cfg.OUT_DATA_PATH,'psych_rx.csv'))

# med_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,'filtered_meds2018.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
# med_df = med_df[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
# med_df = med_df[med_df.END_DATE.notnull() & med_df.ORDERING_DATE.notnull()] # filter no end_date rows
# med_df = med_df[med_df.GROUP!='Vitamin_D']
# med_df = pd.concat([med_df, psych_med_prescrip])


# # drop redundant rows like (Apr-Dec, Apr-Nov) and (Jan-Sept, Mar-Sept) ==> second row is redundant in both tuples 
# med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])
# med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'END_DATE', 'GROUP'])
# med_df.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))

med_df2 = med_df.groupby('AUTO_ID').apply(active_prescriptions).reset_index()
med_Df2.to_csv(cfg.opj(cfg.LONG_IN, 'rx_long.csv'), index=False)
92/255:
# psych_med_prescrip = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Medications_2018-07-05-09-47-15.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
# psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.THERAPEUTIC_CLASS=='PSYCHOTHERAPEUTIC DRUGS']
# psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.END_DATE.notnull() & psych_med_prescrip.ORDERING_DATE.notnull()] # filter no end_date rows
# psych_med_prescrip['GROUP'] = 'Psych'
# psych_med_prescrip = psych_med_prescrip[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
# psych_med_prescrip.to_csv(os.path.join(cfg.OUT_DATA_PATH,'psych_rx.csv'))

# med_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,'filtered_meds2018.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
# med_df = med_df[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
# med_df = med_df[med_df.END_DATE.notnull() & med_df.ORDERING_DATE.notnull()] # filter no end_date rows
# med_df = med_df[med_df.GROUP!='Vitamin_D']
# med_df = pd.concat([med_df, psych_med_prescrip])


# # drop redundant rows like (Apr-Dec, Apr-Nov) and (Jan-Sept, Mar-Sept) ==> second row is redundant in both tuples 
# med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])
# med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'END_DATE', 'GROUP'])
# med_df.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))

# med_df2 = med_df.groupby('AUTO_ID').apply(active_prescriptions).reset_index()
med_df2.to_csv(cfg.opj(cfg.LONG_IN, 'rx_long.csv'), index=False)
92/256:
# psych_med_prescrip = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Medications_2018-07-05-09-47-15.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
# psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.THERAPEUTIC_CLASS=='PSYCHOTHERAPEUTIC DRUGS']
# psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.END_DATE.notnull() & psych_med_prescrip.ORDERING_DATE.notnull()] # filter no end_date rows
# psych_med_prescrip['GROUP'] = 'Psych'
# psych_med_prescrip = psych_med_prescrip[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
# psych_med_prescrip.to_csv(os.path.join(cfg.OUT_DATA_PATH,'psych_rx.csv'))

# med_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,'filtered_meds2018.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
# med_df = med_df[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
# med_df = med_df[med_df.END_DATE.notnull() & med_df.ORDERING_DATE.notnull()] # filter no end_date rows
# med_df = med_df[med_df.GROUP!='Vitamin_D']
# med_df = pd.concat([med_df, psych_med_prescrip])


# # drop redundant rows like (Apr-Dec, Apr-Nov) and (Jan-Sept, Mar-Sept) ==> second row is redundant in both tuples 
# med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])
# med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'END_DATE', 'GROUP'])
# med_df.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))

# med_df2 = med_df.groupby('AUTO_ID').apply(active_prescriptions).reset_index()
med_df2.to_csv(os.path.join(cfg.LONG_IN, 'rx_long.csv'), index=False)
92/257: med_df2
92/258:
# psych_med_prescrip = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Medications_2018-07-05-09-47-15.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
# psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.THERAPEUTIC_CLASS=='PSYCHOTHERAPEUTIC DRUGS']
# psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.END_DATE.notnull() & psych_med_prescrip.ORDERING_DATE.notnull()] # filter no end_date rows
# psych_med_prescrip['GROUP'] = 'Psych'
# psych_med_prescrip = psych_med_prescrip[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
# psych_med_prescrip.to_csv(os.path.join(cfg.OUT_DATA_PATH,'psych_rx.csv'))

# med_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,'filtered_meds2018.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
# med_df = med_df[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
# med_df = med_df[med_df.END_DATE.notnull() & med_df.ORDERING_DATE.notnull()] # filter no end_date rows
# med_df = med_df[med_df.GROUP!='Vitamin_D']
# med_df = pd.concat([med_df, psych_med_prescrip])


# # drop redundant rows like (Apr-Dec, Apr-Nov) and (Jan-Sept, Mar-Sept) ==> second row is redundant in both tuples 
# med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])
# med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'END_DATE', 'GROUP'])
# med_df.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))

# med_df2 = med_df.groupby('AUTO_ID').apply(active_prescriptions).reset_index()
med_df2['RESULT_YEAR'] = med_df2.level_1.dt.year
med_df2['RESULT_MONTH'] = med_df2.level_1.dt.month
med_df2 = med_df2[identifier_cols+med_df.GROUP.unique()]
med_df2.to_csv(os.path.join(cfg.LONG_IN, 'rx_long.csv'), index=False)
92/259:
# psych_med_prescrip = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Medications_2018-07-05-09-47-15.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
# psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.THERAPEUTIC_CLASS=='PSYCHOTHERAPEUTIC DRUGS']
# psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.END_DATE.notnull() & psych_med_prescrip.ORDERING_DATE.notnull()] # filter no end_date rows
# psych_med_prescrip['GROUP'] = 'Psych'
# psych_med_prescrip = psych_med_prescrip[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
# psych_med_prescrip.to_csv(os.path.join(cfg.OUT_DATA_PATH,'psych_rx.csv'))

# med_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,'filtered_meds2018.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
# med_df = med_df[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
# med_df = med_df[med_df.END_DATE.notnull() & med_df.ORDERING_DATE.notnull()] # filter no end_date rows
# med_df = med_df[med_df.GROUP!='Vitamin_D']
# med_df = pd.concat([med_df, psych_med_prescrip])


# # drop redundant rows like (Apr-Dec, Apr-Nov) and (Jan-Sept, Mar-Sept) ==> second row is redundant in both tuples 
# med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])
# med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'END_DATE', 'GROUP'])
# med_df.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))

# med_df2 = med_df.groupby('AUTO_ID').apply(active_prescriptions).reset_index()
med_df2['RESULT_YEAR'] = med_df2.level_1.dt.year
med_df2['RESULT_MONTH'] = med_df2.level_1.dt.month
med_df2 = med_df2[identifier_cols+list(med_df.GROUP.unique())]
med_df2.to_csv(os.path.join(cfg.LONG_IN, 'rx_long.csv'), index=False)
94/15:
import pandas as pd
from config import *
import os
from functools import reduce
94/16:
charges = pd.read_csv(opj(LONG_IN, 'charges.csv'))
encounters = pd.read_csv(opj(LONG_IN,'encounters.csv'))
diag = pd.read_csv(opj(LONG_IN,'gi_diagnostics.csv'))
diag.columns = [f'DIAG_{x}' if x not in IDENTIFIER_COLS else x for x in diag.columns ]
labs = pd.read_csv(opj(LONG_IN, 'labs.csv'))
labs.columns = [f'LAB_{x}' if x not in IDENTIFIER_COLS else x for x in labs.columns]
meds = pd.read_csv(opj((LONG_IN, 'rx_long.csv')))
meds.columns = [f'MED_{x}' if x not in IDENTIFIER_COLS else x for x in meds.columns]
94/17:
charges = pd.read_csv(opj(LONG_IN, 'charges.csv'))
encounters = pd.read_csv(opj(LONG_IN,'encounters.csv'))
diag = pd.read_csv(opj(LONG_IN,'gi_diagnostics.csv'))
diag.columns = [f'DIAG_{x}' if x not in IDENTIFIER_COLS else x for x in diag.columns ]
labs = pd.read_csv(opj(LONG_IN, 'labs.csv'))
labs.columns = [f'LAB_{x}' if x not in IDENTIFIER_COLS else x for x in labs.columns]
meds = pd.read_csv(opj(LONG_IN, 'rx_long.csv'))
meds.columns = [f'MED_{x}' if x not in IDENTIFIER_COLS else x for x in meds.columns]
94/18:
final = reduce(lambda l,r: l.merge(r, how='outer', on=IDENTIFIER_COLS), [charges, encounters, diag, labs, meds])
final
92/260: med_Df2
92/261: med_df2
94/19:
exclude_patients = pd.read_csv(os.path.join(OUT_DATA_PATH, 'exclude_patients.csv'))
ct_aid = exclude_patients.AUTO_ID
CT_filtered = final[~final.AUTO_ID.isin(ct_aid)]
94/20:
cedl.to_csv(opj(LONG_OUT, 'all.csv'), index=False)
CT_filtered.to_csv(opj(LONG_OUT, 'noCT.csv'), index=False)
96/1:
%matplotlib inline
import pandas as pd
import config as cfg
import matplotlib.pyplot as plt
import seaborn as sns
96/2:
def load_irregular_sequence(exclude_ct=True):
    if exclude_ct:
        return pd.read_csv(cfg.opj(cfg.LONG_OUT, 'noCT.csv'))
    return pd.read_csv(cfg.opj(cfg.LONG_OUT, 'all.csv'))
97/1:
%matplotlib inline
import pandas as pd
import config as cfg
import matplotlib.pyplot as plt
import seaborn as sns
97/2:
def load_irregular_sequence(exclude_ct=True):
    if exclude_ct:
        return pd.read_csv(cfg.opj(cfg.LONG_OUT, 'noCT.csv'))
    return pd.read_csv(cfg.opj(cfg.LONG_OUT, 'all.csv'))
97/3: df = load_irregular_sequence()
98/1:
%matplotlib inline
import pandas as pd
import config as cfg
import matplotlib.pyplot as plt
import seaborn as sns
98/2:
%matplotlib inline
import pandas as pd
import config as cfg
import matplotlib.pyplot as plt
import seaborn as sns
98/3:
def load_irregular_sequence(exclude_ct=True):
    if exclude_ct:
        return pd.read_csv(cfg.opj(cfg.LONG_OUT, 'noCT.csv'))
    return pd.read_csv(cfg.opj(cfg.LONG_OUT, 'all.csv'))
98/4: df = load_irregular_sequence()
98/5: df
98/6:
def load_irregular_sequence(exclude_ct=True):
    if exclude_ct:
        return pd.read_csv(cfg.opj(cfg.LONG_OUT, 'noCT.csv'))
    return pd.read_csv(cfg.opj(cfg.LONG_OUT, 'all.csv'))

def apply_monthly_timestamp(df): 
    df['tmp_ts'] = (df.RESULT_YEAR.astype(str)+df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int)
    t0_indices = df.groupby('AUTO_ID').tmp_ts.idxmin()     # find earliest row
    t0_df = df.loc[t0_indices][['AUTO_ID','RESULT_YEAR','RESULT_MONTH']]
    t0_df.columns = ['AUTO_ID', 'tmp_minY', 'tmp_minMo']
    # add tmp columns to df for vectorized calculation of delta in months
    timestamped_df = df.merge(t0_df, how='left', on='AUTO_ID')
    timestamped_df['MONTH_TS'] = (timestamped_df['RESULT_YEAR']-timestamped_df['tmp_minY'])*12 + timestamped_df['RESULT_MONTH']-timestamped_df['tmp_minMo']
    timestamped_df = timestamped_df[[x for x in timestamped_df.columns if x[:3]!='tmp']]
    return timestamped_df
98/7: timestamped_df = apply_monthly_timestamp(df)
98/8:
def load_irregular_sequence(exclude_ct=True):
    if exclude_ct:
        return pd.read_csv(cfg.opj(cfg.LONG_OUT, 'noCT.csv'))
    return pd.read_csv(cfg.opj(cfg.LONG_OUT, 'all.csv'))

def apply_monthly_timestamp(df): 
    df['tmp_ts'] = (df.RESULT_YEAR.astype(str)+df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int)
    t0_indices = df.groupby('AUTO_ID').tmp_ts.idxmin()     # find earliest row
    t0_df = df.loc[t0_indices][['AUTO_ID','RESULT_YEAR','RESULT_MONTH']]
    t0_df.columns = ['AUTO_ID', 'tmp_minY', 'tmp_minMo']
    # add tmp columns to df for vectorized calculation of delta in months
    timestamped_df = df.merge(t0_df, how='left', on='AUTO_ID')
    timestamped_df['MONTH_TS'] = (timestamped_df['RESULT_YEAR']-timestamped_df['tmp_minY'])*12 + timestamped_df['RESULT_MONTH']-timestamped_df['tmp_minMo']
    timestamped_df = timestamped_df[[x for x in timestamped_df.columns if x[:3]!='tmp']]
    return timestamped_df

def pad_missing_months(df):
    df = df.reset_index()
    new_index = pd.MultiIndex.from_product([[df.AUTO_ID.values[0]], list(range(0, df.MONTH_TS.max()+1))], names=['AUTO_ID', 'MONTH_TS'])
    df = df.set_index(['AUTO_ID', 'MONTH_TS']).reindex(new_index).reset_index()
    return df
98/9:
padded = timestamped_df.groupby('AUTO_ID').apply(pad_missing_months)
padded.index = range(0, padded.shape[0])
padded['PADDED'] = padded.RESULT_YEAR.isnull().astype(int)
98/10: padded.shape
98/11: padded.PADDED.sum()
98/12:
padded.drop(['index', 'RESULT_YEAR', 'RESULT_MONTH'], axis=1, inplace=True)
padded.to_pickle(cfg.opj(cfg.LONG_OUT, 'padded'))
98/13: padded.head()
98/14: [x for x in padded.columns if x[:3]=='MED']
98/15:
cols_to_impute = ['ENC_OFF_Related', 'ENC_OFF_Unrelated',\
                'ENC_PROC_Related', 'ENC_PROC_Unrelated', 'ENC_TEL_Related',\
                'ENC_TEL_Unrelated', 'DIAG_COLONOSCOPY', 'DIAG_ENDOSCOPY',\
                'DIAG_SIGMOIDOSCOPY', 'DIAG_ILEOSCOPY', 'DIAG_ERCP', 'DIAG_EGD',\
                'DIAG_UPPER GI ENDO', 'DIAG_UPPER EUS', 'DIAG_GI_PROCEDURE',\
                'LAB_albumin_High', 'LAB_albumin_Low', 'LAB_albumin_Normal',\
                'LAB_crp_High', 'LAB_crp_Low', 'LAB_crp_Normal', 'LAB_eos_High',\
                'LAB_eos_Low', 'LAB_eos_Normal', 'LAB_esr_High', 'LAB_esr_Normal',\
                'LAB_hemoglobin_High', 'LAB_hemoglobin_Low', 'LAB_hemoglobin_Normal',\
                'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_monocytes_Normal',\
                'LAB_vitamin_d_High', 'LAB_vitamin_d_Low', 'LAB_vitamin_d_Normal', \
                'MED_5_ASA', 'MED_Systemic_steroids', 'MED_Immunomodulators', 'MED_Psych',\
                'MED_ANTI_TNF', 'MED_ANTI_IL12', 'MED_ANTI_INTEGRIN']

def commonsense_imputer(df):
    """
    - 0 for encounters (if it wasn't recorded, it probably didn't happen)
    - 0 for diagnoses (-- " --)
    - 1 for LAB_*_Normal (if it wasn't prescribed, it's probably normal)
    """
    enc_diag_cols = [x for x in cols_to_impute if x[:3]=='ENC' or x[:4]=='DIAG']
    df.loc[:, enc_diag_cols] = df.loc[:, enc_diag_cols].fillna(0)
    
    # One-hot encoding of normal labs
    labgrp = ['albumin', 'eos', 'hemoglobin', 'monocytes', 'vitamin_d']
    lablvl = ['High', 'Low', 'Normal']
    lab_fillna = {f'LAB_{g}_{l}':(1 if l=='Normal' else 0) for g in labgrp for l in lablvl}
    df = df.fillna(lab_fillna)
    
    return df
        

def mean_imputer(df):
    obs_mean = df.loc[df.PADDED==0, cols_to_impute].mean()
    return df.fillna(obs)
   
    
def fwd_imputer(df):
    # fillna of 0th row with commonsense values    
    df.loc[df.MONTH_TS==0, cols_to_impute] = commonsense_imputer(df.loc[df.MONTH_TS==0, cols_to_impute])
    # fwd-fill 
    def fwd(x):
        x.loc[:,cols_to_impute] = x.loc[:,cols_to_impute].fillna(None, 'ffill')
    return df.groupby('AUTO_ID').apply(fwd)
98/16:
cols_to_impute = ['ENC_OFF_Related', 'ENC_OFF_Unrelated',\
                'ENC_PROC_Related', 'ENC_PROC_Unrelated', 'ENC_TEL_Related',\
                'ENC_TEL_Unrelated', 'DIAG_COLONOSCOPY', 'DIAG_ENDOSCOPY',\
                'DIAG_SIGMOIDOSCOPY', 'DIAG_ILEOSCOPY', 'DIAG_ERCP', 'DIAG_EGD',\
                'DIAG_UPPER GI ENDO', 'DIAG_UPPER EUS', 'DIAG_GI_PROCEDURE',\
                'LAB_albumin_High', 'LAB_albumin_Low', 'LAB_albumin_Normal',\
                'LAB_crp_High', 'LAB_crp_Low', 'LAB_crp_Normal', 'LAB_eos_High',\
                'LAB_eos_Low', 'LAB_eos_Normal', 'LAB_esr_High', 'LAB_esr_Normal',\
                'LAB_hemoglobin_High', 'LAB_hemoglobin_Low', 'LAB_hemoglobin_Normal',\
                'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_monocytes_Normal',\
                'LAB_vitamin_d_High', 'LAB_vitamin_d_Low', 'LAB_vitamin_d_Normal', \
                'MED_5_ASA', 'MED_Systemic_steroids', 'MED_Immunomodulators', 'MED_Psych',\
                'MED_ANTI_TNF', 'MED_ANTI_IL12', 'MED_ANTI_INTEGRIN']

def commonsense_imputer(df):
    """
    - 0 for encounters (if it wasn't recorded, it probably didn't happen)
    - 0 for diagnoses (-- " --)
    - 0 for RX (-- " --)
    - 1 for LAB_*_Normal (if it wasn't prescribed, it's probably normal)
    """
    enc_diag_rx_cols = [x for x in cols_to_impute if x[:3]=='ENC' or x[:4]=='DIAG' or x[:3]=='MED']
    df.loc[:, enc_diag_rx_cols] = df.loc[:, enc_diag_rx_cols].fillna(0)
    
    # One-hot encoding of normal labs
    labgrp = ['albumin', 'eos', 'hemoglobin', 'monocytes', 'vitamin_d']
    lablvl = ['High', 'Low', 'Normal']
    lab_fillna = {f'LAB_{g}_{l}':(1 if l=='Normal' else 0) for g in labgrp for l in lablvl}
    df = df.fillna(lab_fillna)
    
    return df
        

def mean_imputer(df):
    obs_mean = df.loc[df.PADDED==0, cols_to_impute].mean()
    return df.fillna(obs)
   
    
def fwd_imputer(df):
    # fillna of 0th row with commonsense values    
    df.loc[df.MONTH_TS==0, cols_to_impute] = commonsense_imputer(df.loc[df.MONTH_TS==0, cols_to_impute])
    # fwd-fill 
    def fwd(x):
        x.loc[:,cols_to_impute] = x.loc[:,cols_to_impute].fillna(None, 'ffill')
    return df.groupby('AUTO_ID').apply(fwd)
98/17:
cols_to_impute = ['ENC_OFF_Related', 'ENC_OFF_Unrelated',\
                'ENC_PROC_Related', 'ENC_PROC_Unrelated', 'ENC_TEL_Related',\
                'ENC_TEL_Unrelated', 'DIAG_COLONOSCOPY', 'DIAG_ENDOSCOPY',\
                'DIAG_SIGMOIDOSCOPY', 'DIAG_ILEOSCOPY', 'DIAG_ERCP', 'DIAG_EGD',\
                'DIAG_UPPER GI ENDO', 'DIAG_UPPER EUS', 'DIAG_GI_PROCEDURE',\
                'LAB_albumin_High', 'LAB_albumin_Low', 'LAB_albumin_Normal',\
                'LAB_crp_High', 'LAB_crp_Low', 'LAB_crp_Normal', 'LAB_eos_High',\
                'LAB_eos_Low', 'LAB_eos_Normal', 'LAB_esr_High', 'LAB_esr_Normal',\
                'LAB_hemoglobin_High', 'LAB_hemoglobin_Low', 'LAB_hemoglobin_Normal',\
                'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_monocytes_Normal',\
                'LAB_vitamin_d_High', 'LAB_vitamin_d_Low', 'LAB_vitamin_d_Normal', \
                'MED_5_ASA', 'MED_Systemic_steroids', 'MED_Immunomodulators', 'MED_Psych',\
                'MED_ANTI_TNF', 'MED_ANTI_IL12', 'MED_ANTI_INTEGRIN']

def commonsense_imputer(df):
    """
    - 0 for encounters (if it wasn't recorded, it probably didn't happen)
    - 0 for diagnoses (-- " --)
    - 0 for RX (-- " --)
    - 1 for LAB_*_Normal (if it wasn't prescribed, it's probably normal)
    """
    enc_diag_rx_cols = [x for x in cols_to_impute if x[:3]=='ENC' or x[:4]=='DIAG' or x[:3]=='MED']
    df.loc[:, enc_diag_rx_cols] = df.loc[:, enc_diag_rx_cols].fillna(0)
    
    # One-hot encoding of normal labs
    labgrp = ['albumin', 'eos', 'hemoglobin', 'monocytes', 'vitamin_d']
    lablvl = ['High', 'Low', 'Normal']
    lab_fillna = {f'LAB_{g}_{l}':(1 if l=='Normal' else 0) for g in labgrp for l in lablvl}
    df = df.fillna(lab_fillna)
    
    return df
        

def mean_imputer(df):
    obs_mean = df.loc[df.PADDED==0, cols_to_impute].mean()
    return df.fillna(obs)
   
    
def fwd_imputer(df):
    # fillna of 0th row with commonsense values    
    df.loc[df.MONTH_TS==0, cols_to_impute] = commonsense_imputer(df.loc[df.MONTH_TS==0, cols_to_impute])
    # fwd-fill 
    def fwd(x):
        x.loc[:,cols_to_impute] = x.loc[:,cols_to_impute].fillna(None, 'ffill')
    return df.groupby('AUTO_ID').apply(fwd)
98/18: [x for x in padded.columns if x not in cols_to_impute]
98/19:
missing_mask = padded[['AUTO_ID', 'MONTH_TS']+cols_to_impute]
missing_mask.loc[:, cols_to_impute]/=missing_mask.loc[:, cols_to_impute]
missing_mask
98/20: padded
98/21:
missing_mask = padded[['AUTO_ID', 'MONTH_TS']+cols_to_impute]
missing_mask.loc[:, cols_to_impute].notnull()
98/22:
missing_mask = padded[['AUTO_ID', 'MONTH_TS']+cols_to_impute]
missing_mask.loc[:, cols_to_impute].notnull().astype(int)
98/23: 243563*42
98/24:
missing_mask = padded[['AUTO_ID', 'MONTH_TS']+cols_to_impute]
missing_mask.loc[:, cols_to_impute].notnull().astype(int).sum()
98/25:
missing_mask = padded[['AUTO_ID', 'MONTH_TS']+cols_to_impute]
missing_mask.loc[:, cols_to_impute].notnull().astype(int).sum().sum()
98/26: 243563*42 - 1562926
98/27:
missing_mask = padded[['AUTO_ID', 'MONTH_TS']+cols_to_impute]
missing_mask.loc[:, cols_to_impute] = missing_mask.loc[:, cols_to_impute].notnull().astype(int)
98/28: missing_mask
98/29: missing_mask.shift(1)
98/30: missing_mask#.shift(1)
98/31: missing_mask.shift(1)
98/32: missing_mask#.shift(1)
98/33: missing_mask.groupby('AUTO_ID')['ENC_OFF_Related'].shift(1)
98/34:
test = missing_mask[missing_mask.AUTO_ID==0][['AUTO_ID', 'MONTH_TS','ENC_OFF_Related']]
test
98/35:
test = missing_mask[missing_mask.AUTO_ID==0][['AUTO_ID', 'MONTH_TS','ENC_OFF_Related']]
test.ENC_OFF_Related = test.ENC_OFF_Related.shift(1)
test
98/36:
test = missing_mask[missing_mask.AUTO_ID==0][['AUTO_ID', 'MONTH_TS','ENC_OFF_Related']]
test.ENC_OFF_Related = test.ENC_OFF_Related.shift(1)

test['delta'] = test.ENC_OFF_Related*1
test.delta
98/37:
test = missing_mask[missing_mask.AUTO_ID==0][['AUTO_ID', 'MONTH_TS','ENC_OFF_Related']]
test.ENC_OFF_Related = test.ENC_OFF_Related.shift(1)

test['delta'] = test.ENC_OFF_Related*1
test
98/38:
test = missing_mask[missing_mask.AUTO_ID==0][['AUTO_ID', 'MONTH_TS','ENC_OFF_Related']]
test.ENC_OFF_Related = test.ENC_OFF_Related.shift(1)

test['delta_s'] = (test.ENC_OFF_Related*1).shift(1)
test['delta'] = test.ENC_OFF_Related*1 + (1-test.ENC_OFF_Related)*(1+test.delta_s)
98/39:
test = missing_mask[missing_mask.AUTO_ID==0][['AUTO_ID', 'MONTH_TS','ENC_OFF_Related']]
test.ENC_OFF_Related = test.ENC_OFF_Related.shift(1)

test['delta_s'] = (test.ENC_OFF_Related*1).shift(1)
test['delta'] = test.ENC_OFF_Related*1 + (1-test.ENC_OFF_Related)*(1+test.delta_s)
test
98/40:
test = missing_mask[missing_mask.AUTO_ID==0][['AUTO_ID', 'MONTH_TS','ENC_OFF_Related']]
test.columns = ['aid', 'ts', 'obs']
98/41:
test = missing_mask[missing_mask.AUTO_ID==0][['AUTO_ID', 'MONTH_TS','ENC_OFF_Related']]
test.columns = ['aid', 'ts', 'obs']
test
98/42:
test = missing_mask[missing_mask.AUTO_ID==0][['AUTO_ID', 'MONTH_TS','ENC_OFF_Related']]
test.columns = ['aid', 'ts', 'obs']
test['obs_c'] = test.obs.cumsum()
test
98/43:
test = missing_mask[missing_mask.AUTO_ID==0][['AUTO_ID', 'MONTH_TS','ENC_OFF_Related']]
test.columns = ['aid', 'ts', 'obs']
test['obs_c'] = test.obs.cumsum()
test['delta'] = 1
test = test.groupby(['obs_c']).delta.cumsum()
test
98/44:
test = missing_mask[missing_mask.AUTO_ID==0][['AUTO_ID', 'MONTH_TS','ENC_OFF_Related']]
test.columns = ['aid', 'ts', 'obs']
test['obs_c'] = test.obs.cumsum()
test['delta'] = 1
test.delta = test.groupby(['obs_c']).delta.cumsum()
test
98/45:
tmp_delta = missing_mask.copy()
col = cols_to_impute[0]
tmp_delta.groupby('AUTO_ID').col.cumsum()
98/46:
tmp_delta = missing_mask.copy()
col = cols_to_impute[0]
tmp_delta.groupby('AUTO_ID')[col].cumsum()
98/47: missing_mas[:30]
98/48: missing_mask[:30]
98/49:
tmp_delta = missing_mask[17:25].copy()
col = cols_to_impute[0]
tmp_delta.groupby('AUTO_ID')[col].cumsum()
98/50:
tmp_delta = missing_mask.copy()
for col in cols_to_impute:
    tmptmp_delta.groupby('AUTO_ID')[col].cumsum()
98/51:
tmp_delta = missing_mask[10:30].copy()
for observed in cols_to_impute:
    tmp_delta['tmp_delta'] = 1
    tmp_delta['obs_delays'] = tmp_delta.groupby('AUTO_ID')[observed].cumsum()
    tmp_delta[observed] = tmp_delta.groupby('obs_delays')['tmp_delta'].cumsum()

tmp_delta
98/52: missing_mask[10:30]
98/53:
missing_delta = missing_mask.copy()
for observed in cols_to_impute:
    missing_delta['tmp_delta'] = 1
    missing_delta['obs_delays'] = missing_delta.groupby('AUTO_ID')[observed].cumsum()
    missing_delta[observed] = missing_delta.groupby('obs_delays')['tmp_delta'].cumsum()

missing_delta = missing_delta.drop(['tmp_delta', 'obs_delays'], axis=1)
missing_delta
98/54: missing_delta[102:120]
98/55: missing_mask[102:120]
98/56: missing_mask
98/57: missing_mask.head()
98/58: missing_delta.head()
98/59:
missing_delta = missing_mask.copy()
for observed in cols_to_impute:
    missing_delta['tmp_delta'] = 1
    missing_delta['obs_delays'] = missing_delta.groupby('AUTO_ID')[observed].cumsum()
    missing_delta[observed] = missing_delta.groupby(['AUTO_ID', 'obs_delays'])['tmp_delta'].cumsum()

# missing_delta = missing_delta.drop(['tmp_delta', 'obs_delays'], axis=1)
missing_delta
98/60:
missing_delta = missing_mask.copy()
for observed in cols_to_impute:
    missing_delta['tmp_delta'] = 1
    missing_delta['obs_delays'] = missing_delta.groupby('AUTO_ID')[observed].cumsum()
    missing_delta[observed] = missing_delta.groupby(['AUTO_ID', 'obs_delays'])['tmp_delta'].cumsum()

# missing_delta = missing_delta.drop(['tmp_delta', 'obs_delays'], axis=1)
# missing_delta
98/61: missing_delta.head()
98/62:
def missingness_indicators(padded, timedelta=True):
    missing_mask = padded[['AUTO_ID', 'MONTH_TS']+cols_to_impute]
    missing_mask.loc[:, cols_to_impute] = missing_mask.loc[:, cols_to_impute].notnull().astype(int)
    
    if timedelta:
        missing_delta = missing_mask.copy()
        time_interval = 1 #1 month with padding. For without padding, maintain PADDED col, then filter on that.
        for observed in cols_to_impute:
            missing_delta['tmp_delta'] = time_interval
            missing_delta['obs_delays'] = missing_delta.groupby('AUTO_ID')[observed].cumsum()
            missing_delta[observed] = missing_delta.groupby(['AUTO_ID', 'obs_delays'])['tmp_delta'].cumsum()
        missing_delta = missing_delta.drop(['tmp_delta', 'obs_delays'], axis=1)
        return missing_mask, missing_delta
    
    else:
        return missing_mask, None
98/63:
import math, random
# W = random.random
# decay = lambda s: math.exp(-max(0, ))
random.random(1,2)
98/64:
import math, random
# W = random.random
# decay = lambda s: math.exp(-max(0, ))
random.random()
98/65:
import math, random
# W = random.random
# decay = lambda s: math.exp(-max(0, ))
random.randrange()
98/66:
import math, random
# W = pd.Series([random.random() for x in range(100)])
# s = pd.Series(list(range(100)))
# decay = lambda s: math.exp(-max(0, W*s))

random.randrange(-5,5)
98/67:
import math, random
# W = pd.Series([random.random() for x in range(100)])
# s = pd.Series(list(range(100)))
# decay = lambda s: math.exp(-max(0, W*s))

random.randrange(-5,5)
98/68:
import math, random
# W = pd.Series([random.random() for x in range(100)])
# s = pd.Series(list(range(100)))
# decay = lambda s: math.exp(-max(0, W*s))

random.randrange(-5,5)
98/69:
import math, random
# W = pd.Series([random.random() for x in range(100)])
# s = pd.Series(list(range(100)))
# decay = lambda s: math.exp(-max(0, W*s))

random.randrange(-5,5)
98/70:
import math, random
# W = pd.Series([random.random() for x in range(100)])
# s = pd.Series(list(range(100)))
# decay = lambda s: math.exp(-max(0, W*s))

random.randrange(-5,5)
98/71:
import math, random
# W = pd.Series([random.random() for x in range(100)])
# s = pd.Series(list(range(100)))
# decay = lambda s: math.exp(-max(0, W*s))

random.randrange(-5,5)
98/72:
W = pd.Series([random.randrange(-30,100)/100 for x in range(100)])
s = pd.Series(list(range(100)))
y = W*s
x = max(0, W*s)
print(y, x)
98/73:
W = pd.Series([random.randrange(-30,100)/100 for x in range(100)])
s = pd.Series(list(range(100)))
y = W*s
x = y.apply(lambda x: max(0, x))
print(y, x)
98/74:
W = pd.Series([random.randrange(-30,100)/100 for x in range(100)])
s = pd.Series(list(range(100)))
y = W*s
x = y.apply(lambda x: math.exp(-max(0, x)))
print(y, x)
98/75:
W = pd.Series([random.randrange(-30,100)/100 for x in range(100)])
s = pd.Series(list(range(100)))
y = W*s
x = y.apply(lambda x: math.exp(-max(0, x)))
x.describe()
98/76:
W = pd.Series([random.randrange(-30,100)/100 for x in range(100)])
s = pd.Series(list(range(100)))
y = W*s
x = y.apply(lambda x: math.exp(-max(0, x)))
sns.distplot(x)
98/77: import torch
98/78:
class Imputer:
    def __init__(self, cols_to_impute=None):
        if cols_to_impute:
            self.cols_to_impute = cols_to_impute
        else:
            self.cols_to_impute = ['ENC_OFF_Related', 'ENC_OFF_Unrelated',\
                'ENC_PROC_Related', 'ENC_PROC_Unrelated', 'ENC_TEL_Related',\
                'ENC_TEL_Unrelated', 'DIAG_COLONOSCOPY', 'DIAG_ENDOSCOPY',\
                'DIAG_SIGMOIDOSCOPY', 'DIAG_ILEOSCOPY', 'DIAG_ERCP', 'DIAG_EGD',\
                'DIAG_UPPER GI ENDO', 'DIAG_UPPER EUS', 'DIAG_GI_PROCEDURE',\
                'LAB_albumin_High', 'LAB_albumin_Low', 'LAB_albumin_Normal',\
                'LAB_crp_High', 'LAB_crp_Low', 'LAB_crp_Normal', 'LAB_eos_High',\
                'LAB_eos_Low', 'LAB_eos_Normal', 'LAB_esr_High', 'LAB_esr_Normal',\
                'LAB_hemoglobin_High', 'LAB_hemoglobin_Low', 'LAB_hemoglobin_Normal',\
                'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_monocytes_Normal',\
                'LAB_vitamin_d_High', 'LAB_vitamin_d_Low', 'LAB_vitamin_d_Normal', \
                'MED_5_ASA', 'MED_Systemic_steroids', 'MED_Immunomodulators', 'MED_Psych',\
                'MED_ANTI_TNF', 'MED_ANTI_IL12', 'MED_ANTI_INTEGRIN']
        
        self.df=None
            
    def fit(df):
        self.df = df
        
    def commonsense(df=None):
        if df:
            return _commonsense_imputer(df)
        else:
            return _commonsense_imputer(self.df)
    
    def mean(df=None):
        if df:
            return _mean_imputer(df)
        else:
            return _mean_imputer(self.df)
            
    def forward(df=None):
        if df:
            return _fwd_imputer(df)
        else:
            return _fwd_imputer(self.df)

    def _commonsense_imputer(df):
        """
        - 0 for encounters (if it wasn't recorded, it probably didn't happen)
        - 0 for diagnoses (-- " --)
        - 0 for RX (-- " --)
        - 1 for LAB_*_Normal (if it wasn't prescribed, it's probably normal)
        """
        enc_diag_rx_cols = [x for x in self.cols_to_impute if x[:3]=='ENC' or x[:4]=='DIAG' or x[:3]=='MED']
        df.loc[:, enc_diag_rx_cols] = df.loc[:, enc_diag_rx_cols].fillna(0)

        # One-hot encoding of normal labs
        labgrp = ['albumin', 'eos', 'hemoglobin', 'monocytes', 'vitamin_d']
        lablvl = ['High', 'Low', 'Normal']
        lab_fillna = {f'LAB_{g}_{l}':(1 if l=='Normal' else 0) for g in labgrp for l in lablvl}
        df = df.fillna(lab_fillna)

        return df


    def _mean_imputer(df):
        obs_mean = df.loc[df.PADDED==0, self.cols_to_impute].mean()
        return df.fillna(obs)


    def _fwd_imputer(df):
        # fillna of 0th row with commonsense values    
        df.loc[df.MONTH_TS==0, self.cols_to_impute] = commonsense_imputer(df.loc[df.MONTH_TS==0, self.cols_to_impute])
        # fwd-fill 
        def fwd(x):
            x.loc[:,self.cols_to_impute] = x.loc[:,self.cols_to_impute].fillna(None, 'ffill')
        return df.groupby('AUTO_ID').apply(fwd)
98/79:
imp = Imputer().fit(padded)
padded.commonsense().head()
98/80:
class Imputer:
    def __init__(self, cols_to_impute=None):
        if cols_to_impute:
            self.cols_to_impute = cols_to_impute
        else:
            self.cols_to_impute = ['ENC_OFF_Related', 'ENC_OFF_Unrelated',\
                'ENC_PROC_Related', 'ENC_PROC_Unrelated', 'ENC_TEL_Related',\
                'ENC_TEL_Unrelated', 'DIAG_COLONOSCOPY', 'DIAG_ENDOSCOPY',\
                'DIAG_SIGMOIDOSCOPY', 'DIAG_ILEOSCOPY', 'DIAG_ERCP', 'DIAG_EGD',\
                'DIAG_UPPER GI ENDO', 'DIAG_UPPER EUS', 'DIAG_GI_PROCEDURE',\
                'LAB_albumin_High', 'LAB_albumin_Low', 'LAB_albumin_Normal',\
                'LAB_crp_High', 'LAB_crp_Low', 'LAB_crp_Normal', 'LAB_eos_High',\
                'LAB_eos_Low', 'LAB_eos_Normal', 'LAB_esr_High', 'LAB_esr_Normal',\
                'LAB_hemoglobin_High', 'LAB_hemoglobin_Low', 'LAB_hemoglobin_Normal',\
                'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_monocytes_Normal',\
                'LAB_vitamin_d_High', 'LAB_vitamin_d_Low', 'LAB_vitamin_d_Normal', \
                'MED_5_ASA', 'MED_Systemic_steroids', 'MED_Immunomodulators', 'MED_Psych',\
                'MED_ANTI_TNF', 'MED_ANTI_IL12', 'MED_ANTI_INTEGRIN']
        
        self.df=None
            
    def fit(self, df):
        self.df = df
        
    def commonsense(self, df=None):
        if df:
            return _commonsense_imputer(df)
        else:
            return _commonsense_imputer(self.df)
    
    def mean(self, df=None):
        if df:
            return _mean_imputer(df)
        else:
            return _mean_imputer(self.df)
            
    def forward(self, df=None):
        if df:
            return _fwd_imputer(df)
        else:
            return _fwd_imputer(self.df)

    def _commonsense_imputer(self, df):
        """
        - 0 for encounters (if it wasn't recorded, it probably didn't happen)
        - 0 for diagnoses (-- " --)
        - 0 for RX (-- " --)
        - 1 for LAB_*_Normal (if it wasn't prescribed, it's probably normal)
        """
        enc_diag_rx_cols = [x for x in self.cols_to_impute if x[:3]=='ENC' or x[:4]=='DIAG' or x[:3]=='MED']
        df.loc[:, enc_diag_rx_cols] = df.loc[:, enc_diag_rx_cols].fillna(0)

        # One-hot encoding of normal labs
        labgrp = ['albumin', 'eos', 'hemoglobin', 'monocytes', 'vitamin_d']
        lablvl = ['High', 'Low', 'Normal']
        lab_fillna = {f'LAB_{g}_{l}':(1 if l=='Normal' else 0) for g in labgrp for l in lablvl}
        df = df.fillna(lab_fillna)

        return df


    def _mean_imputer(self, df):
        obs_mean = df.loc[df.PADDED==0, self.cols_to_impute].mean()
        return df.fillna(obs)


    def _fwd_imputer(self, df):
        # fillna of 0th row with commonsense values    
        df.loc[df.MONTH_TS==0, self.cols_to_impute] = commonsense_imputer(df.loc[df.MONTH_TS==0, self.cols_to_impute])
        # fwd-fill 
        def fwd(x):
            x.loc[:,self.cols_to_impute] = x.loc[:,self.cols_to_impute].fillna(None, 'ffill')
        return df.groupby('AUTO_ID').apply(fwd)
98/81:
imp = Imputer().fit(padded)
padded.commonsense().head()
98/82:
imp = Imputer().fit(padded)
imp.commonsense().head()
98/83:
class Imputer:
    def __init__(self, cols_to_impute=None):
        if cols_to_impute:
            self.cols_to_impute = cols_to_impute
        else:
            self.cols_to_impute = ['ENC_OFF_Related', 'ENC_OFF_Unrelated',\
                'ENC_PROC_Related', 'ENC_PROC_Unrelated', 'ENC_TEL_Related',\
                'ENC_TEL_Unrelated', 'DIAG_COLONOSCOPY', 'DIAG_ENDOSCOPY',\
                'DIAG_SIGMOIDOSCOPY', 'DIAG_ILEOSCOPY', 'DIAG_ERCP', 'DIAG_EGD',\
                'DIAG_UPPER GI ENDO', 'DIAG_UPPER EUS', 'DIAG_GI_PROCEDURE',\
                'LAB_albumin_High', 'LAB_albumin_Low', 'LAB_albumin_Normal',\
                'LAB_crp_High', 'LAB_crp_Low', 'LAB_crp_Normal', 'LAB_eos_High',\
                'LAB_eos_Low', 'LAB_eos_Normal', 'LAB_esr_High', 'LAB_esr_Normal',\
                'LAB_hemoglobin_High', 'LAB_hemoglobin_Low', 'LAB_hemoglobin_Normal',\
                'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_monocytes_Normal',\
                'LAB_vitamin_d_High', 'LAB_vitamin_d_Low', 'LAB_vitamin_d_Normal', \
                'MED_5_ASA', 'MED_Systemic_steroids', 'MED_Immunomodulators', 'MED_Psych',\
                'MED_ANTI_TNF', 'MED_ANTI_IL12', 'MED_ANTI_INTEGRIN']
        
        self.df=None
            
    def fit(self, df):
        self.df = df
        return self
        
    def commonsense(self, df=None):
        if df:
            return _commonsense_imputer(df)
        else:
            return _commonsense_imputer(self.df)
    
    def mean(self, df=None):
        if df:
            return _mean_imputer(df)
        else:
            return _mean_imputer(self.df)
            
    def forward(self, df=None):
        if df:
            return _fwd_imputer(df)
        else:
            return _fwd_imputer(self.df)

    def _commonsense_imputer(self, df):
        """
        - 0 for encounters (if it wasn't recorded, it probably didn't happen)
        - 0 for diagnoses (-- " --)
        - 0 for RX (-- " --)
        - 1 for LAB_*_Normal (if it wasn't prescribed, it's probably normal)
        """
        enc_diag_rx_cols = [x for x in self.cols_to_impute if x[:3]=='ENC' or x[:4]=='DIAG' or x[:3]=='MED']
        df.loc[:, enc_diag_rx_cols] = df.loc[:, enc_diag_rx_cols].fillna(0)

        # One-hot encoding of normal labs
        labgrp = ['albumin', 'eos', 'hemoglobin', 'monocytes', 'vitamin_d']
        lablvl = ['High', 'Low', 'Normal']
        lab_fillna = {f'LAB_{g}_{l}':(1 if l=='Normal' else 0) for g in labgrp for l in lablvl}
        df = df.fillna(lab_fillna)

        return df


    def _mean_imputer(self, df):
        obs_mean = df.loc[df.PADDED==0, self.cols_to_impute].mean()
        return df.fillna(obs)


    def _fwd_imputer(self, df):
        # fillna of 0th row with commonsense values    
        df.loc[df.MONTH_TS==0, self.cols_to_impute] = commonsense_imputer(df.loc[df.MONTH_TS==0, self.cols_to_impute])
        # fwd-fill 
        def fwd(x):
            x.loc[:,self.cols_to_impute] = x.loc[:,self.cols_to_impute].fillna(None, 'ffill')
        return df.groupby('AUTO_ID').apply(fwd)
98/84:
imp = Imputer().fit(padded)
imp.commonsense().head()
98/85:
class Imputer:
    def __init__(self, cols_to_impute=None):
        if cols_to_impute:
            self.cols_to_impute = cols_to_impute
        else:
            self.cols_to_impute = ['ENC_OFF_Related', 'ENC_OFF_Unrelated',\
                'ENC_PROC_Related', 'ENC_PROC_Unrelated', 'ENC_TEL_Related',\
                'ENC_TEL_Unrelated', 'DIAG_COLONOSCOPY', 'DIAG_ENDOSCOPY',\
                'DIAG_SIGMOIDOSCOPY', 'DIAG_ILEOSCOPY', 'DIAG_ERCP', 'DIAG_EGD',\
                'DIAG_UPPER GI ENDO', 'DIAG_UPPER EUS', 'DIAG_GI_PROCEDURE',\
                'LAB_albumin_High', 'LAB_albumin_Low', 'LAB_albumin_Normal',\
                'LAB_crp_High', 'LAB_crp_Low', 'LAB_crp_Normal', 'LAB_eos_High',\
                'LAB_eos_Low', 'LAB_eos_Normal', 'LAB_esr_High', 'LAB_esr_Normal',\
                'LAB_hemoglobin_High', 'LAB_hemoglobin_Low', 'LAB_hemoglobin_Normal',\
                'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_monocytes_Normal',\
                'LAB_vitamin_d_High', 'LAB_vitamin_d_Low', 'LAB_vitamin_d_Normal', \
                'MED_5_ASA', 'MED_Systemic_steroids', 'MED_Immunomodulators', 'MED_Psych',\
                'MED_ANTI_TNF', 'MED_ANTI_IL12', 'MED_ANTI_INTEGRIN']
        
        self.df=None
            
    def fit(self, df):
        self.df = df
        return self
        
    def commonsense(self, df=None):
        if df:
            return self._commonsense_imputer(df)
        else:
            return self._commonsense_imputer(self.df)
    
    def mean(self, df=None):
        if df:
            return self._mean_imputer(df)
        else:
            return self._mean_imputer(self.df)
            
    def forward(self, df=None):
        if df:
            return self._fwd_imputer(df)
        else:
            return self._fwd_imputer(self.df)

    def _commonsense_imputer(self, df):
        """
        - 0 for encounters (if it wasn't recorded, it probably didn't happen)
        - 0 for diagnoses (-- " --)
        - 0 for RX (-- " --)
        - 1 for LAB_*_Normal (if it wasn't prescribed, it's probably normal)
        """
        enc_diag_rx_cols = [x for x in self.cols_to_impute if x[:3]=='ENC' or x[:4]=='DIAG' or x[:3]=='MED']
        df.loc[:, enc_diag_rx_cols] = df.loc[:, enc_diag_rx_cols].fillna(0)

        # One-hot encoding of normal labs
        labgrp = ['albumin', 'eos', 'hemoglobin', 'monocytes', 'vitamin_d']
        lablvl = ['High', 'Low', 'Normal']
        lab_fillna = {f'LAB_{g}_{l}':(1 if l=='Normal' else 0) for g in labgrp for l in lablvl}
        df = df.fillna(lab_fillna)

        return df


    def _mean_imputer(self, df):
        obs_mean = df.loc[df.PADDED==0, self.cols_to_impute].mean()
        return df.fillna(obs)


    def _fwd_imputer(self, df):
        # fillna of 0th row with commonsense values    
        df.loc[df.MONTH_TS==0, self.cols_to_impute] = commonsense_imputer(df.loc[df.MONTH_TS==0, self.cols_to_impute])
        # fwd-fill 
        def fwd(x):
            x.loc[:,self.cols_to_impute] = x.loc[:,self.cols_to_impute].fillna(None, 'ffill')
        return df.groupby('AUTO_ID').apply(fwd)
98/86:
imp = Imputer().fit(padded)
imp.commonsense().head()
98/87:
imp = Imputer().fit(padded)
imp.mean().head()
98/88:
class Imputer:
    def __init__(self, cols_to_impute=None):
        if cols_to_impute:
            self.cols_to_impute = cols_to_impute
        else:
            self.cols_to_impute = ['ENC_OFF_Related', 'ENC_OFF_Unrelated',\
                'ENC_PROC_Related', 'ENC_PROC_Unrelated', 'ENC_TEL_Related',\
                'ENC_TEL_Unrelated', 'DIAG_COLONOSCOPY', 'DIAG_ENDOSCOPY',\
                'DIAG_SIGMOIDOSCOPY', 'DIAG_ILEOSCOPY', 'DIAG_ERCP', 'DIAG_EGD',\
                'DIAG_UPPER GI ENDO', 'DIAG_UPPER EUS', 'DIAG_GI_PROCEDURE',\
                'LAB_albumin_High', 'LAB_albumin_Low', 'LAB_albumin_Normal',\
                'LAB_crp_High', 'LAB_crp_Low', 'LAB_crp_Normal', 'LAB_eos_High',\
                'LAB_eos_Low', 'LAB_eos_Normal', 'LAB_esr_High', 'LAB_esr_Normal',\
                'LAB_hemoglobin_High', 'LAB_hemoglobin_Low', 'LAB_hemoglobin_Normal',\
                'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_monocytes_Normal',\
                'LAB_vitamin_d_High', 'LAB_vitamin_d_Low', 'LAB_vitamin_d_Normal', \
                'MED_5_ASA', 'MED_Systemic_steroids', 'MED_Immunomodulators', 'MED_Psych',\
                'MED_ANTI_TNF', 'MED_ANTI_IL12', 'MED_ANTI_INTEGRIN']
        
        self.df=None
            
    def fit(self, df):
        self.df = df
        return self
        
    def commonsense(self, df=None):
        if df:
            return self._commonsense_imputer(df)
        else:
            return self._commonsense_imputer(self.df)
    
    def mean(self, df=None):
        if df:
            return self._mean_imputer(df)
        else:
            return self._mean_imputer(self.df)
            
    def forward(self, df=None):
        if df:
            return self._fwd_imputer(df)
        else:
            return self._fwd_imputer(self.df)

    def _commonsense_imputer(self, df):
        """
        - 0 for encounters (if it wasn't recorded, it probably didn't happen)
        - 0 for diagnoses (-- " --)
        - 0 for RX (-- " --)
        - 1 for LAB_*_Normal (if it wasn't prescribed, it's probably normal)
        """
        enc_diag_rx_cols = [x for x in self.cols_to_impute if x[:3]=='ENC' or x[:4]=='DIAG' or x[:3]=='MED']
        df.loc[:, enc_diag_rx_cols] = df.loc[:, enc_diag_rx_cols].fillna(0)

        # One-hot encoding of normal labs
        labgrp = ['albumin', 'eos', 'hemoglobin', 'monocytes', 'vitamin_d']
        lablvl = ['High', 'Low', 'Normal']
        lab_fillna = {f'LAB_{g}_{l}':(1 if l=='Normal' else 0) for g in labgrp for l in lablvl}
        df = df.fillna(lab_fillna)

        return df


    def _mean_imputer(self, df):
        obs_mean = df.loc[df.PADDED==0, self.cols_to_impute].mean()
        return df.fillna(obs_mean)


    def _fwd_imputer(self, df):
        # fillna of 0th row with commonsense values    
        df.loc[df.MONTH_TS==0, self.cols_to_impute] = commonsense_imputer(df.loc[df.MONTH_TS==0, self.cols_to_impute])
        # fwd-fill 
        def fwd(x):
            x.loc[:,self.cols_to_impute] = x.loc[:,self.cols_to_impute].fillna(None, 'ffill')
        return df.groupby('AUTO_ID').apply(fwd)
98/89:
imp = Imputer().fit(padded)
imp.mean().head()
98/90:
imp = Imputer().fit(padded)
imp.forward().head()
98/91:
imp = Imputer().fit(padded)
imp.forward(padded.head()).head()
98/92:
imp = Imputer().fit(padded)
imp.forward(padded.head())
98/93:
class Imputer:
    def __init__(self, cols_to_impute=None):
        if cols_to_impute:
            self.cols_to_impute = cols_to_impute
        else:
            self.cols_to_impute = ['ENC_OFF_Related', 'ENC_OFF_Unrelated',\
                'ENC_PROC_Related', 'ENC_PROC_Unrelated', 'ENC_TEL_Related',\
                'ENC_TEL_Unrelated', 'DIAG_COLONOSCOPY', 'DIAG_ENDOSCOPY',\
                'DIAG_SIGMOIDOSCOPY', 'DIAG_ILEOSCOPY', 'DIAG_ERCP', 'DIAG_EGD',\
                'DIAG_UPPER GI ENDO', 'DIAG_UPPER EUS', 'DIAG_GI_PROCEDURE',\
                'LAB_albumin_High', 'LAB_albumin_Low', 'LAB_albumin_Normal',\
                'LAB_crp_High', 'LAB_crp_Low', 'LAB_crp_Normal', 'LAB_eos_High',\
                'LAB_eos_Low', 'LAB_eos_Normal', 'LAB_esr_High', 'LAB_esr_Normal',\
                'LAB_hemoglobin_High', 'LAB_hemoglobin_Low', 'LAB_hemoglobin_Normal',\
                'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_monocytes_Normal',\
                'LAB_vitamin_d_High', 'LAB_vitamin_d_Low', 'LAB_vitamin_d_Normal', \
                'MED_5_ASA', 'MED_Systemic_steroids', 'MED_Immunomodulators', 'MED_Psych',\
                'MED_ANTI_TNF', 'MED_ANTI_IL12', 'MED_ANTI_INTEGRIN']
        
        self.df=None
            
    def fit(self, df):
        self.df = df
        return self
        
    def commonsense(self, df=None):
        if df:
            return self._commonsense_imputer(df)
        else:
            return self._commonsense_imputer(self.df)
    
    def mean(self, df=None):
        if df:
            return self._mean_imputer(df)
        else:
            return self._mean_imputer(self.df)
            
    def forward(self, df=None):
        if df not None:
            return self._fwd_imputer(df)
        else:
            return self._fwd_imputer(self.df)

    def _commonsense_imputer(self, df):
        """
        - 0 for encounters (if it wasn't recorded, it probably didn't happen)
        - 0 for diagnoses (-- " --)
        - 0 for RX (-- " --)
        - 1 for LAB_*_Normal (if it wasn't prescribed, it's probably normal)
        """
        enc_diag_rx_cols = [x for x in self.cols_to_impute if x[:3]=='ENC' or x[:4]=='DIAG' or x[:3]=='MED']
        df.loc[:, enc_diag_rx_cols] = df.loc[:, enc_diag_rx_cols].fillna(0)

        # One-hot encoding of normal labs
        labgrp = ['albumin', 'eos', 'hemoglobin', 'monocytes', 'vitamin_d']
        lablvl = ['High', 'Low', 'Normal']
        lab_fillna = {f'LAB_{g}_{l}':(1 if l=='Normal' else 0) for g in labgrp for l in lablvl}
        df = df.fillna(lab_fillna)

        return df


    def _mean_imputer(self, df):
        obs_mean = df.loc[df.PADDED==0, self.cols_to_impute].mean()
        return df.fillna(obs_mean)


    def _fwd_imputer(self, df):
        # fillna of 0th row with commonsense values    
        df.loc[df.MONTH_TS==0, self.cols_to_impute] = commonsense_imputer(df.loc[df.MONTH_TS==0, self.cols_to_impute])
        # fwd-fill 
        def fwd(x):
            x.loc[:,self.cols_to_impute] = x.loc[:,self.cols_to_impute].fillna(None, 'ffill')
        return df.groupby('AUTO_ID').apply(fwd)
98/94:
class Imputer:
    def __init__(self, cols_to_impute=None):
        if cols_to_impute:
            self.cols_to_impute = cols_to_impute
        else:
            self.cols_to_impute = ['ENC_OFF_Related', 'ENC_OFF_Unrelated',\
                'ENC_PROC_Related', 'ENC_PROC_Unrelated', 'ENC_TEL_Related',\
                'ENC_TEL_Unrelated', 'DIAG_COLONOSCOPY', 'DIAG_ENDOSCOPY',\
                'DIAG_SIGMOIDOSCOPY', 'DIAG_ILEOSCOPY', 'DIAG_ERCP', 'DIAG_EGD',\
                'DIAG_UPPER GI ENDO', 'DIAG_UPPER EUS', 'DIAG_GI_PROCEDURE',\
                'LAB_albumin_High', 'LAB_albumin_Low', 'LAB_albumin_Normal',\
                'LAB_crp_High', 'LAB_crp_Low', 'LAB_crp_Normal', 'LAB_eos_High',\
                'LAB_eos_Low', 'LAB_eos_Normal', 'LAB_esr_High', 'LAB_esr_Normal',\
                'LAB_hemoglobin_High', 'LAB_hemoglobin_Low', 'LAB_hemoglobin_Normal',\
                'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_monocytes_Normal',\
                'LAB_vitamin_d_High', 'LAB_vitamin_d_Low', 'LAB_vitamin_d_Normal', \
                'MED_5_ASA', 'MED_Systemic_steroids', 'MED_Immunomodulators', 'MED_Psych',\
                'MED_ANTI_TNF', 'MED_ANTI_IL12', 'MED_ANTI_INTEGRIN']
        
        self.df=None
            
    def fit(self, df):
        self.df = df
        return self
        
    def commonsense(self, df=None):
        if df:
            return self._commonsense_imputer(df)
        else:
            return self._commonsense_imputer(self.df)
    
    def mean(self, df=None):
        if df:
            return self._mean_imputer(df)
        else:
            return self._mean_imputer(self.df)
            
    def forward(self, df=None):
        if df != None:
            return self._fwd_imputer(df)
        else:
            return self._fwd_imputer(self.df)

    def _commonsense_imputer(self, df):
        """
        - 0 for encounters (if it wasn't recorded, it probably didn't happen)
        - 0 for diagnoses (-- " --)
        - 0 for RX (-- " --)
        - 1 for LAB_*_Normal (if it wasn't prescribed, it's probably normal)
        """
        enc_diag_rx_cols = [x for x in self.cols_to_impute if x[:3]=='ENC' or x[:4]=='DIAG' or x[:3]=='MED']
        df.loc[:, enc_diag_rx_cols] = df.loc[:, enc_diag_rx_cols].fillna(0)

        # One-hot encoding of normal labs
        labgrp = ['albumin', 'eos', 'hemoglobin', 'monocytes', 'vitamin_d']
        lablvl = ['High', 'Low', 'Normal']
        lab_fillna = {f'LAB_{g}_{l}':(1 if l=='Normal' else 0) for g in labgrp for l in lablvl}
        df = df.fillna(lab_fillna)

        return df


    def _mean_imputer(self, df):
        obs_mean = df.loc[df.PADDED==0, self.cols_to_impute].mean()
        return df.fillna(obs_mean)


    def _fwd_imputer(self, df):
        # fillna of 0th row with commonsense values    
        df.loc[df.MONTH_TS==0, self.cols_to_impute] = commonsense_imputer(df.loc[df.MONTH_TS==0, self.cols_to_impute])
        # fwd-fill 
        def fwd(x):
            x.loc[:,self.cols_to_impute] = x.loc[:,self.cols_to_impute].fillna(None, 'ffill')
        return df.groupby('AUTO_ID').apply(fwd)
98/95:
imp = Imputer().fit(padded)
imp.forward(padded.head())
98/96:
class Imputer:
    def __init__(self, cols_to_impute=None):
        if cols_to_impute:
            self.cols_to_impute = cols_to_impute
        else:
            self.cols_to_impute = ['ENC_OFF_Related', 'ENC_OFF_Unrelated',\
                'ENC_PROC_Related', 'ENC_PROC_Unrelated', 'ENC_TEL_Related',\
                'ENC_TEL_Unrelated', 'DIAG_COLONOSCOPY', 'DIAG_ENDOSCOPY',\
                'DIAG_SIGMOIDOSCOPY', 'DIAG_ILEOSCOPY', 'DIAG_ERCP', 'DIAG_EGD',\
                'DIAG_UPPER GI ENDO', 'DIAG_UPPER EUS', 'DIAG_GI_PROCEDURE',\
                'LAB_albumin_High', 'LAB_albumin_Low', 'LAB_albumin_Normal',\
                'LAB_crp_High', 'LAB_crp_Low', 'LAB_crp_Normal', 'LAB_eos_High',\
                'LAB_eos_Low', 'LAB_eos_Normal', 'LAB_esr_High', 'LAB_esr_Normal',\
                'LAB_hemoglobin_High', 'LAB_hemoglobin_Low', 'LAB_hemoglobin_Normal',\
                'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_monocytes_Normal',\
                'LAB_vitamin_d_High', 'LAB_vitamin_d_Low', 'LAB_vitamin_d_Normal', \
                'MED_5_ASA', 'MED_Systemic_steroids', 'MED_Immunomodulators', 'MED_Psych',\
                'MED_ANTI_TNF', 'MED_ANTI_IL12', 'MED_ANTI_INTEGRIN']
        
        self.df=None
            
    def fit(self, df):
        self.df = df
        return self
        
    def commonsense(self, df=None):
        if df:
            return self._commonsense_imputer(df)
        else:
            return self._commonsense_imputer(self.df)
    
    def mean(self, df=None):
        if df:
            return self._mean_imputer(df)
        else:
            return self._mean_imputer(self.df)
            
    def forward(self, df=None):
        if df is not None:
            return self._fwd_imputer(df)
        else:
            return self._fwd_imputer(self.df)

    def _commonsense_imputer(self, df):
        """
        - 0 for encounters (if it wasn't recorded, it probably didn't happen)
        - 0 for diagnoses (-- " --)
        - 0 for RX (-- " --)
        - 1 for LAB_*_Normal (if it wasn't prescribed, it's probably normal)
        """
        enc_diag_rx_cols = [x for x in self.cols_to_impute if x[:3]=='ENC' or x[:4]=='DIAG' or x[:3]=='MED']
        df.loc[:, enc_diag_rx_cols] = df.loc[:, enc_diag_rx_cols].fillna(0)

        # One-hot encoding of normal labs
        labgrp = ['albumin', 'eos', 'hemoglobin', 'monocytes', 'vitamin_d']
        lablvl = ['High', 'Low', 'Normal']
        lab_fillna = {f'LAB_{g}_{l}':(1 if l=='Normal' else 0) for g in labgrp for l in lablvl}
        df = df.fillna(lab_fillna)

        return df


    def _mean_imputer(self, df):
        obs_mean = df.loc[df.PADDED==0, self.cols_to_impute].mean()
        return df.fillna(obs_mean)


    def _fwd_imputer(self, df):
        # fillna of 0th row with commonsense values    
        df.loc[df.MONTH_TS==0, self.cols_to_impute] = commonsense_imputer(df.loc[df.MONTH_TS==0, self.cols_to_impute])
        # fwd-fill 
        def fwd(x):
            x.loc[:,self.cols_to_impute] = x.loc[:,self.cols_to_impute].fillna(None, 'ffill')
        return df.groupby('AUTO_ID').apply(fwd)
98/97:
imp = Imputer().fit(padded)
imp.forward(padded.head())
98/98:
class Imputer:
    def __init__(self, cols_to_impute=None):
        if cols_to_impute:
            self.cols_to_impute = cols_to_impute
        else:
            self.cols_to_impute = ['ENC_OFF_Related', 'ENC_OFF_Unrelated',\
                'ENC_PROC_Related', 'ENC_PROC_Unrelated', 'ENC_TEL_Related',\
                'ENC_TEL_Unrelated', 'DIAG_COLONOSCOPY', 'DIAG_ENDOSCOPY',\
                'DIAG_SIGMOIDOSCOPY', 'DIAG_ILEOSCOPY', 'DIAG_ERCP', 'DIAG_EGD',\
                'DIAG_UPPER GI ENDO', 'DIAG_UPPER EUS', 'DIAG_GI_PROCEDURE',\
                'LAB_albumin_High', 'LAB_albumin_Low', 'LAB_albumin_Normal',\
                'LAB_crp_High', 'LAB_crp_Low', 'LAB_crp_Normal', 'LAB_eos_High',\
                'LAB_eos_Low', 'LAB_eos_Normal', 'LAB_esr_High', 'LAB_esr_Normal',\
                'LAB_hemoglobin_High', 'LAB_hemoglobin_Low', 'LAB_hemoglobin_Normal',\
                'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_monocytes_Normal',\
                'LAB_vitamin_d_High', 'LAB_vitamin_d_Low', 'LAB_vitamin_d_Normal', \
                'MED_5_ASA', 'MED_Systemic_steroids', 'MED_Immunomodulators', 'MED_Psych',\
                'MED_ANTI_TNF', 'MED_ANTI_IL12', 'MED_ANTI_INTEGRIN']
        
        self.df=None
            
    def fit(self, df):
        self.df = df
        return self
        
    def commonsense(self, df=None):
        if df:
            return self._commonsense_imputer(df)
        else:
            return self._commonsense_imputer(self.df)
    
    def mean(self, df=None):
        if df:
            return self._mean_imputer(df)
        else:
            return self._mean_imputer(self.df)
            
    def forward(self, df=None):
        if df is not None:
            return self._fwd_imputer(df)
        else:
            return self._fwd_imputer(self.df)

    def _commonsense_imputer(self, df):
        """
        - 0 for encounters (if it wasn't recorded, it probably didn't happen)
        - 0 for diagnoses (-- " --)
        - 0 for RX (-- " --)
        - 1 for LAB_*_Normal (if it wasn't prescribed, it's probably normal)
        """
        enc_diag_rx_cols = [x for x in self.cols_to_impute if x[:3]=='ENC' or x[:4]=='DIAG' or x[:3]=='MED']
        df.loc[:, enc_diag_rx_cols] = df.loc[:, enc_diag_rx_cols].fillna(0)

        # One-hot encoding of normal labs
        labgrp = ['albumin', 'eos', 'hemoglobin', 'monocytes', 'vitamin_d']
        lablvl = ['High', 'Low', 'Normal']
        lab_fillna = {f'LAB_{g}_{l}':(1 if l=='Normal' else 0) for g in labgrp for l in lablvl}
        df = df.fillna(lab_fillna)

        return df


    def _mean_imputer(self, df):
        obs_mean = df.loc[df.PADDED==0, self.cols_to_impute].mean()
        return df.fillna(obs_mean)


    def _fwd_imputer(self, df):
        # fillna of 0th row with commonsense values    
        df.loc[df.MONTH_TS==0, self.cols_to_impute] = self.commonsense_imputer(df.loc[df.MONTH_TS==0, self.cols_to_impute])
        # fwd-fill 
        def fwd(x):
            x.loc[:,self.cols_to_impute] = x.loc[:,self.cols_to_impute].fillna(None, 'ffill')
        return df.groupby('AUTO_ID').apply(fwd)
98/99:
imp = Imputer().fit(padded)
imp.forward(padded.head())
98/100:
class Imputer:
    def __init__(self, cols_to_impute=None):
        if cols_to_impute:
            self.cols_to_impute = cols_to_impute
        else:
            self.cols_to_impute = ['ENC_OFF_Related', 'ENC_OFF_Unrelated',\
                'ENC_PROC_Related', 'ENC_PROC_Unrelated', 'ENC_TEL_Related',\
                'ENC_TEL_Unrelated', 'DIAG_COLONOSCOPY', 'DIAG_ENDOSCOPY',\
                'DIAG_SIGMOIDOSCOPY', 'DIAG_ILEOSCOPY', 'DIAG_ERCP', 'DIAG_EGD',\
                'DIAG_UPPER GI ENDO', 'DIAG_UPPER EUS', 'DIAG_GI_PROCEDURE',\
                'LAB_albumin_High', 'LAB_albumin_Low', 'LAB_albumin_Normal',\
                'LAB_crp_High', 'LAB_crp_Low', 'LAB_crp_Normal', 'LAB_eos_High',\
                'LAB_eos_Low', 'LAB_eos_Normal', 'LAB_esr_High', 'LAB_esr_Normal',\
                'LAB_hemoglobin_High', 'LAB_hemoglobin_Low', 'LAB_hemoglobin_Normal',\
                'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_monocytes_Normal',\
                'LAB_vitamin_d_High', 'LAB_vitamin_d_Low', 'LAB_vitamin_d_Normal', \
                'MED_5_ASA', 'MED_Systemic_steroids', 'MED_Immunomodulators', 'MED_Psych',\
                'MED_ANTI_TNF', 'MED_ANTI_IL12', 'MED_ANTI_INTEGRIN']
        
        self.df=None
            
    def fit(self, df):
        self.df = df
        return self
        
    def commonsense(self, df=None):
        if df:
            return self._commonsense_imputer(df)
        else:
            return self._commonsense_imputer(self.df)
    
    def mean(self, df=None):
        if df:
            return self._mean_imputer(df)
        else:
            return self._mean_imputer(self.df)
            
    def forward(self, df=None):
        if df is not None:
            return self._fwd_imputer(df)
        else:
            return self._fwd_imputer(self.df)

    def _commonsense_imputer(self, df):
        """
        - 0 for encounters (if it wasn't recorded, it probably didn't happen)
        - 0 for diagnoses (-- " --)
        - 0 for RX (-- " --)
        - 1 for LAB_*_Normal (if it wasn't prescribed, it's probably normal)
        """
        enc_diag_rx_cols = [x for x in self.cols_to_impute if x[:3]=='ENC' or x[:4]=='DIAG' or x[:3]=='MED']
        df.loc[:, enc_diag_rx_cols] = df.loc[:, enc_diag_rx_cols].fillna(0)

        # One-hot encoding of normal labs
        labgrp = ['albumin', 'eos', 'hemoglobin', 'monocytes', 'vitamin_d']
        lablvl = ['High', 'Low', 'Normal']
        lab_fillna = {f'LAB_{g}_{l}':(1 if l=='Normal' else 0) for g in labgrp for l in lablvl}
        df = df.fillna(lab_fillna)

        return df


    def _mean_imputer(self, df):
        obs_mean = df.loc[df.PADDED==0, self.cols_to_impute].mean()
        return df.fillna(obs_mean)


    def _fwd_imputer(self, df):
        # fillna of 0th row with commonsense values    
        df.loc[df.MONTH_TS==0, self.cols_to_impute] = self._commonsense_imputer(df.loc[df.MONTH_TS==0, self.cols_to_impute])
        # fwd-fill 
        def fwd(x):
            x.loc[:,self.cols_to_impute] = x.loc[:,self.cols_to_impute].fillna(None, 'ffill')
        return df.groupby('AUTO_ID').apply(fwd)
98/101:
imp = Imputer().fit(padded)
imp.forward(padded.head())
98/102: padded
98/103:
padded = timestamped_df.groupby('AUTO_ID').apply(pad_missing_months)
padded.index = range(0, padded.shape[0])
padded['PADDED'] = padded.RESULT_YEAR.isnull().astype(int)
98/104: padded
98/105:
class Imputer:
    def __init__(self, cols_to_impute=None):
        if cols_to_impute:
            self.cols_to_impute = cols_to_impute
        else:
            self.cols_to_impute = ['ENC_OFF_Related', 'ENC_OFF_Unrelated',\
                'ENC_PROC_Related', 'ENC_PROC_Unrelated', 'ENC_TEL_Related',\
                'ENC_TEL_Unrelated', 'DIAG_COLONOSCOPY', 'DIAG_ENDOSCOPY',\
                'DIAG_SIGMOIDOSCOPY', 'DIAG_ILEOSCOPY', 'DIAG_ERCP', 'DIAG_EGD',\
                'DIAG_UPPER GI ENDO', 'DIAG_UPPER EUS', 'DIAG_GI_PROCEDURE',\
                'LAB_albumin_High', 'LAB_albumin_Low', 'LAB_albumin_Normal',\
                'LAB_crp_High', 'LAB_crp_Low', 'LAB_crp_Normal', 'LAB_eos_High',\
                'LAB_eos_Low', 'LAB_eos_Normal', 'LAB_esr_High', 'LAB_esr_Normal',\
                'LAB_hemoglobin_High', 'LAB_hemoglobin_Low', 'LAB_hemoglobin_Normal',\
                'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_monocytes_Normal',\
                'LAB_vitamin_d_High', 'LAB_vitamin_d_Low', 'LAB_vitamin_d_Normal', \
                'MED_5_ASA', 'MED_Systemic_steroids', 'MED_Immunomodulators', 'MED_Psych',\
                'MED_ANTI_TNF', 'MED_ANTI_IL12', 'MED_ANTI_INTEGRIN']
        
        self.df=None
            
    def fit(self, df):
        self.df = df
        return self
        
    def commonsense(self, df=None):
        if df is not None:
            _ = self.fit(df)
        return self._commonsense_imputer(self.df)
    
    def mean(self, df=None):
        if df is not None:
            _ = self.fit(df)
        return self._mean_imputer(self.df)
            
    def forward(self, df=None):
        if df is not None:
            _ = self.fit(df)
        return self._fwd_imputer(self.df)

    def _commonsense_imputer(self, df):
        """
        - 0 for encounters (if it wasn't recorded, it probably didn't happen)
        - 0 for diagnoses (-- " --)
        - 0 for RX (-- " --)
        - 1 for LAB_*_Normal (if it wasn't prescribed, it's probably normal)
        """
        enc_diag_rx_cols = [x for x in self.cols_to_impute if x[:3]=='ENC' or x[:4]=='DIAG' or x[:3]=='MED']
        df.loc[:, enc_diag_rx_cols] = df.loc[:, enc_diag_rx_cols].fillna(0)

        # One-hot encoding of normal labs
        labgrp = ['albumin', 'eos', 'hemoglobin', 'monocytes', 'vitamin_d']
        lablvl = ['High', 'Low', 'Normal']
        lab_fillna = {f'LAB_{g}_{l}':(1 if l=='Normal' else 0) for g in labgrp for l in lablvl}
        df = df.fillna(lab_fillna)

        return df


    def _mean_imputer(self, df):
        obs_mean = df.loc[df.PADDED==0, self.cols_to_impute].mean()
        return df.fillna(obs_mean)


    def _fwd_imputer(self, df):
        # fillna of 0th row with commonsense values    
        df.loc[df.MONTH_TS==0, self.cols_to_impute] = self._commonsense_imputer(df.loc[df.MONTH_TS==0, self.cols_to_impute])
        # fwd-fill 
        def fwd(x):
            x.loc[:,self.cols_to_impute] = x.loc[:,self.cols_to_impute].fillna(None, 'ffill')
        return df.groupby('AUTO_ID').apply(fwd)
98/106:
imp = Imputer()
imp.forward(padded.head())
98/107: padded
98/108:
class Imputer:
    def __init__(self, cols_to_impute=None):
        if cols_to_impute:
            self.cols_to_impute = cols_to_impute
        else:
            self.cols_to_impute = ['ENC_OFF_Related', 'ENC_OFF_Unrelated',\
                'ENC_PROC_Related', 'ENC_PROC_Unrelated', 'ENC_TEL_Related',\
                'ENC_TEL_Unrelated', 'DIAG_COLONOSCOPY', 'DIAG_ENDOSCOPY',\
                'DIAG_SIGMOIDOSCOPY', 'DIAG_ILEOSCOPY', 'DIAG_ERCP', 'DIAG_EGD',\
                'DIAG_UPPER GI ENDO', 'DIAG_UPPER EUS', 'DIAG_GI_PROCEDURE',\
                'LAB_albumin_High', 'LAB_albumin_Low', 'LAB_albumin_Normal',\
                'LAB_crp_High', 'LAB_crp_Low', 'LAB_crp_Normal', 'LAB_eos_High',\
                'LAB_eos_Low', 'LAB_eos_Normal', 'LAB_esr_High', 'LAB_esr_Normal',\
                'LAB_hemoglobin_High', 'LAB_hemoglobin_Low', 'LAB_hemoglobin_Normal',\
                'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_monocytes_Normal',\
                'LAB_vitamin_d_High', 'LAB_vitamin_d_Low', 'LAB_vitamin_d_Normal', \
                'MED_5_ASA', 'MED_Systemic_steroids', 'MED_Immunomodulators', 'MED_Psych',\
                'MED_ANTI_TNF', 'MED_ANTI_IL12', 'MED_ANTI_INTEGRIN']
        
        self.df=None
            
    def fit(self, df):
        self.df = df
        return self
        
    def commonsense(self, df=None):
        if df is not None:
            _ = self.fit(df)
        return self._commonsense_imputer(self.df)
    
    def mean(self, df=None):
        if df is not None:
            _ = self.fit(df)
        return self._mean_imputer(self.df)
            
    def forward(self, df=None):
        if df is not None:
            _ = self.fit(df)
        return self._fwd_imputer(self.df)

    def _commonsense_imputer(self, df):
        """
        - 0 for encounters (if it wasn't recorded, it probably didn't happen)
        - 0 for diagnoses (-- " --)
        - 0 for RX (-- " --)
        - 1 for LAB_*_Normal (if it wasn't prescribed, it's probably normal)
        """
        enc_diag_rx_cols = [x for x in self.cols_to_impute if x[:3]=='ENC' or x[:4]=='DIAG' or x[:3]=='MED']
        df.loc[:, enc_diag_rx_cols] = df.loc[:, enc_diag_rx_cols].fillna(0)

        # One-hot encoding of normal labs
        labgrp = ['albumin', 'eos', 'hemoglobin', 'monocytes', 'vitamin_d']
        lablvl = ['High', 'Low', 'Normal']
        lab_fillna = {f'LAB_{g}_{l}':(1 if l=='Normal' else 0) for g in labgrp for l in lablvl}
        df = df.fillna(lab_fillna)

        return df


    def _mean_imputer(self, df):
        obs_mean = df.loc[df.PADDED==0, self.cols_to_impute].mean()
        return df.fillna(obs_mean)


    def _fwd_imputer(self, df):
        # fillna of 0th row with commonsense values    
        df.loc[df.MONTH_TS==0, self.cols_to_impute] = self._commonsense_imputer(df.loc[df.MONTH_TS==0, self.cols_to_impute])
        # fwd-fill 
        def fwd(x):
            return x.loc[:,self.cols_to_impute] = x.loc[:,self.cols_to_impute].fillna(None, 'ffill')
        return df.groupby('AUTO_ID').apply(fwd)
98/109:
class Imputer:
    def __init__(self, cols_to_impute=None):
        if cols_to_impute:
            self.cols_to_impute = cols_to_impute
        else:
            self.cols_to_impute = ['ENC_OFF_Related', 'ENC_OFF_Unrelated',\
                'ENC_PROC_Related', 'ENC_PROC_Unrelated', 'ENC_TEL_Related',\
                'ENC_TEL_Unrelated', 'DIAG_COLONOSCOPY', 'DIAG_ENDOSCOPY',\
                'DIAG_SIGMOIDOSCOPY', 'DIAG_ILEOSCOPY', 'DIAG_ERCP', 'DIAG_EGD',\
                'DIAG_UPPER GI ENDO', 'DIAG_UPPER EUS', 'DIAG_GI_PROCEDURE',\
                'LAB_albumin_High', 'LAB_albumin_Low', 'LAB_albumin_Normal',\
                'LAB_crp_High', 'LAB_crp_Low', 'LAB_crp_Normal', 'LAB_eos_High',\
                'LAB_eos_Low', 'LAB_eos_Normal', 'LAB_esr_High', 'LAB_esr_Normal',\
                'LAB_hemoglobin_High', 'LAB_hemoglobin_Low', 'LAB_hemoglobin_Normal',\
                'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_monocytes_Normal',\
                'LAB_vitamin_d_High', 'LAB_vitamin_d_Low', 'LAB_vitamin_d_Normal', \
                'MED_5_ASA', 'MED_Systemic_steroids', 'MED_Immunomodulators', 'MED_Psych',\
                'MED_ANTI_TNF', 'MED_ANTI_IL12', 'MED_ANTI_INTEGRIN']
        
        self.df=None
            
    def fit(self, df):
        self.df = df
        return self
        
    def commonsense(self, df=None):
        if df is not None:
            _ = self.fit(df)
        return self._commonsense_imputer(self.df)
    
    def mean(self, df=None):
        if df is not None:
            _ = self.fit(df)
        return self._mean_imputer(self.df)
            
    def forward(self, df=None):
        if df is not None:
            _ = self.fit(df)
        return self._fwd_imputer(self.df)

    def _commonsense_imputer(self, df):
        """
        - 0 for encounters (if it wasn't recorded, it probably didn't happen)
        - 0 for diagnoses (-- " --)
        - 0 for RX (-- " --)
        - 1 for LAB_*_Normal (if it wasn't prescribed, it's probably normal)
        """
        enc_diag_rx_cols = [x for x in self.cols_to_impute if x[:3]=='ENC' or x[:4]=='DIAG' or x[:3]=='MED']
        df.loc[:, enc_diag_rx_cols] = df.loc[:, enc_diag_rx_cols].fillna(0)

        # One-hot encoding of normal labs
        labgrp = ['albumin', 'eos', 'hemoglobin', 'monocytes', 'vitamin_d']
        lablvl = ['High', 'Low', 'Normal']
        lab_fillna = {f'LAB_{g}_{l}':(1 if l=='Normal' else 0) for g in labgrp for l in lablvl}
        df = df.fillna(lab_fillna)

        return df


    def _mean_imputer(self, df):
        obs_mean = df.loc[df.PADDED==0, self.cols_to_impute].mean()
        return df.fillna(obs_mean)


    def _fwd_imputer(self, df):
        # fillna of 0th row with commonsense values    
        df.loc[df.MONTH_TS==0, self.cols_to_impute] = self._commonsense_imputer(df.loc[df.MONTH_TS==0, self.cols_to_impute])
        # fwd-fill 
        def fwd(x):
            x.loc[:,self.cols_to_impute] = x.loc[:,self.cols_to_impute].fillna(None, 'ffill')
            return x
        return df.groupby('AUTO_ID').apply(fwd)
98/110: padded = padded2.copy()
98/111: padded2 = padded.copy()
98/112:
imp = Imputer()
imp.forward(padded.head())
98/113: import torch
98/114:
import torch
torch.randn(1,3)
98/115:
import torch
torch.randn(1,1,3)
98/116:
import torch
torch.randn(1,3).view()
98/117:
import torch
torch.randn(1,3).view(1,1,-1)
98/118:
import torch
torch.randn(1,3).view(1,1,1)
98/119:
import torch
torch.randn(1,3).view(1,1,-1)
98/120:
import torch
torch.randn(1,3).view(1,-1,1)
98/121:
import torch
torch.randn(1,3).view(1,-1)
98/122:
import torch
a = torch.randn(1,3)
print(a.view(-1,1), a.view(1,-1))
98/123:
import torch
a = torch.randn(1,3)
print(a.view(1,1), a.view(1,-1))
98/124:
import torch
a = torch.randn(1,3)
print(a.view(1,1,-1), a.view(-1,1,1))
98/125:
import torch
a = torch.randn(1,3)
print(a.view(1,1,-1), a.view(1,-1,1))
98/126:
import torch
a = torch.randn(1,3)
print(a.view(-1,1,1), a.view(1,-1,1))
98/127:
import torch
a = torch.randn(1,3)
print(a.view(1,1,-1))
98/128:
import torch
a = [torch.randn(1,3) for x in range(5)]
print(a.view(5,1,-1))
98/129:
import torch
a = [torch.randn(1,3) for x in range(5)]
print(a.view(5,1,-1))
98/130:
import torch
a = [torch.randn(1,3) for x in range(5)]
print(torch.cat(a).view(5,1,-1))
98/131:
import torch
a = [torch.randn(1,3) for x in range(5)]
print(torch.cat(a).view(1,1,-1))
98/132:
import torch
a = [torch.randn(1,3) for x in range(5)]
print(torch.cat(a).view(5,2,-1))
98/133:
import torch
a = [torch.randn(1,3) for x in range(5)]
print(torch.cat(a).view(5,1,-1))
98/134:
import torch
a = [torch.randn(1,3) for x in range(5)]
print(torch.cat(a).view(3,5,-1))
98/135:
import torch
a = [torch.randn(1,3) for x in range(5)]
print(torch.cat(a).view(3,1,-5))
98/136:
import torch
a = [torch.randn(1,3) for x in range(5)]
print(torch.cat(a).view(3,-1,5))
106/1:
n_rows = 10
seq_len = 4
lookfowd = 2
min = 6
list(range(0,4))
106/2:
n_rows = 10
seq_len = 4
lookfowd = 2
min = 6
list(range(0,5))
106/3:
import torch
import torch.nn as nn
import data_utils as du
106/4: dat = du.X_raw()
109/1:
import torch
import torch.nn as nn
import data_utils as du
109/2:
import torch
import torch.nn as nn
import data_utils as du
109/3: dat = du.X_raw()
109/4:
dat = du.X_raw()
missing_mask, missing_delta = du.missingness_indicators(dataset)

print(dat.shape, missing_delta.shape, missing_mask.shape)
109/5:
# dat = du.X_raw()
missing_mask, missing_delta = du.missingness_indicators(dat)

print(dat.shape, missing_delta.shape, missing_mask.shape)
109/6:
# dat = du.X_raw()
# missing_mask, missing_delta = du.missingness_indicators(dat)

# print(dat.shape, missing_delta.shape, missing_mask.shape)
dat.shape
109/7:
# dat = du.X_raw()

def missingness_indicators(padded, timedelta=True):
    cols_to_impute = ['ENC_OFF_Related', 'ENC_OFF_Unrelated',\
                'ENC_PROC_Related', 'ENC_PROC_Unrelated', 'ENC_TEL_Related',\
                'ENC_TEL_Unrelated', 'DIAG_COLONOSCOPY', 'DIAG_ENDOSCOPY',\
                'DIAG_SIGMOIDOSCOPY', 'DIAG_ILEOSCOPY', 'DIAG_ERCP', 'DIAG_EGD',\
                'DIAG_UPPER GI ENDO', 'DIAG_UPPER EUS', 'DIAG_GI_PROCEDURE',\
                'LAB_albumin_High', 'LAB_albumin_Low', 'LAB_albumin_Normal',\
                'LAB_crp_High', 'LAB_crp_Low', 'LAB_crp_Normal', 'LAB_eos_High',\
                'LAB_eos_Low', 'LAB_eos_Normal', 'LAB_esr_High', 'LAB_esr_Normal',\
                'LAB_hemoglobin_High', 'LAB_hemoglobin_Low', 'LAB_hemoglobin_Normal',\
                'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_monocytes_Normal',\
                'LAB_vitamin_d_High', 'LAB_vitamin_d_Low', 'LAB_vitamin_d_Normal', \
                'MED_5_ASA', 'MED_Systemic_steroids', 'MED_Immunomodulators', 'MED_Psych',\
                'MED_ANTI_TNF', 'MED_ANTI_IL12', 'MED_ANTI_INTEGRIN']
    missing_mask = padded[['AUTO_ID', 'MONTH_TS']+cols_to_impute]
    missing_mask.loc[:, cols_to_impute] = missing_mask.loc[:, cols_to_impute].notnull().astype(int)
    
    if timedelta:
        missing_delta = missing_mask.copy()
        time_interval = 1 #1 month with padding. For without padding, maintain PADDED col, then filter on that.
        for observed in cols_to_impute:
            missing_delta['tmp_delta'] = time_interval
            missing_delta['obs_delays'] = missing_delta.groupby('AUTO_ID')[observed].cumsum()
            missing_delta[observed] = missing_delta.groupby(['AUTO_ID', 'obs_delays'])['tmp_delta'].cumsum()
        missing_delta = missing_delta.drop(['tmp_delta', 'obs_delays'], axis=1)
        return missing_mask, missing_delta
    
    else:
        return missing_mask, None
    
    
missing_mask, missing_delta = missingness_indicators(dat)

print(dat.shape, missing_delta.shape, missing_mask.shape)
# dat.shape
109/8:
l = list(range(10))
np.random.randint(0,10,8)
109/9:
l = list(range(10))
np.random.randint(0,10,8, replace=False)
109/10:
l = list(range(10))
np.random.choice(range(10),8, replace=False)
109/11:
l = list(range(10))
a=np.random.choice(range(10),8, replace=False)
print(a)
109/12:
l = list(range(10))
# a=np.random.choice(range(10),8, replace=False)
print(a)
a[2,3,5]
109/13:
l = list(range(10))
# a=np.random.choice(range(10),8, replace=False)
print(a)
a[[2,3,5]]
109/14: dat.columns
109/15: missing_delta.columns
109/16: dat.columns
109/17: np.setdiff1d(dat.columns, ['AUTO_ID'])
109/18: dat.columns
109/19:
dat.to_pickle('pickles/dataset.pkl')
missing_delta.to_pickle('pickles/delta.pkl')
missing_mask.to_pickle('pickles/mask.pkl')
109/20:
# x = dat[dat.AUTO_ID==0]
# m = missing_mask.
m
109/21:
# x = dat[dat.AUTO_ID==0]
# m = missing_mask.
missing_mask
109/22:
x = dat[dat.AUTO_ID==0]
m = missing_mask[missing_mask.AUTO_ID==0]
t = missing_delta[missing_delta.AUTO_ID==0]
109/23: x
109/24: m
109/25: t
109/26: xo = x.shift(1)
109/27: xo
109/28: xo.fillna(x.iloc[0])
109/29:
# xo.fillna(x.iloc[0])
m
109/30:
# xo.fillna(x.iloc[0])
list(range(0,96,4))
109/31:
# xo.fillna(x.iloc[0])
len(list(range(0,96,4)))
90/32: import os
90/33: os.chdir(..)
90/34: os.chdir('..')
90/35: os.get_cwd()
90/36: os.curdir()
90/37: os.curdir
90/38: os.chdir('../pickles')
90/39: os.curdir
90/40: df = pd.read_pickle('dataset.pkl')
90/41: df.groupby('AUTO_ID').count()
90/42: nb_seq = df.groupby('AUTO_ID').MONTH_TS.count()
90/43: %matplotlib inle
90/44: %matplotlib inline
90/45: nb_seq.value_counts()
90/46: nb_seq.describe()
90/47: nb_mos = nb_seq.copy()
90/48: nb_seq = nb_mos - 47
90/49: nb_seq.describe()
90/50: nb_seq[nb_seq>0].describe()
90/51: nb_seq.sum()
90/52: df
90/53: df.columns
114/1:
import torch
import torch.nn as nn
import data_utils as du
114/2:
import torch
import torch.nn as nn
import data_utils as du
114/3:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.seq=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,self.train_size*seq_shape, replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), self.valid_size*seq_shape)
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_seq_label(self, dataset):
        groups = dataset.groupby('AUTO_ID')
#       (nb_seq x 3 x seq_len x input_size)
        seq = np.array([])
        labels = np.array([])
        min_rows_reqd = self.seq_len+self.lookfwd_months

        for grp in groups:
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: continue
            for i in range(n_rows-min_rows_reqd+1): #CHK
                df = grp.iloc[i:i+self.seq_len]
                x = df[self.col_dict.inputs].values
                m = df[self.col_dict.missing].values
                t = df[self.col_dict.delta].values
                x_obs = x.shift(1).fillna(x.loc[0]).values
#               (3 x seq_len x input_size)
                tmp_seq = np.array([x , m , t, x_obs])
                seq = np.append(seq, tmp_seq)
                np.append(labels, grp.iloc[i+min_rows_reqd-1].values)
        
        print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
        self.seq = seq
        self.labels = labels
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.seq[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit():
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        imputed_dataset = du.Imputer().fit(dataset[col_dict.input]).forward()  
        self.get_seq_labels(imputed_dataset)
        #ttsplit
        self.partition = self.seq_split(self.seq.shape[0])
114/4: dataset, self.col_dict = du.X_raw()
90/54: 'abc_123'.replace('_','')
117/1: import data_utils as du
117/2: df, dic = du.X_raw()
118/1: import data_utils as du
118/2: df, dic = du.X_raw()
118/3: df.columns
118/4: list(df.columns)
118/5: df.head()
119/1:
import torch
import torch.nn as nn
import data_utils as du
119/2:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.seq=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,self.train_size*seq_shape, replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), self.valid_size*seq_shape)
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_seq_label(self, dataset):
        groups = dataset.groupby('AUTO_ID')
#       (nb_seq x 3 x seq_len x input_size)
        seq = np.array([])
        labels = np.array([])
        min_rows_reqd = self.seq_len+self.lookfwd_months

        for grp in groups:
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: continue
            for i in range(n_rows-min_rows_reqd+1): #CHK
                df = grp.iloc[i:i+self.seq_len]
                x = df[self.col_dict.inputs].values
                m = df[self.col_dict.missing].values
                t = df[self.col_dict.delta].values
                x_obs = x.shift(1).fillna(x.loc[0]).values
#               (3 x seq_len x input_size)
                tmp_seq = np.array([x , m , t, x_obs])
                seq = np.append(seq, tmp_seq)
                np.append(labels, grp.iloc[i+min_rows_reqd-1].values)
        
        print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
        self.seq = seq
        self.labels = labels
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.seq[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit():
        dataset2, self.col_dict = du.X_raw() #(243563 rows)
        dataset = dataset2.head(1000).copy()
        
        imputed_dataset = du.Imputer().forward(dataset)  
        self.get_seq_labels(imputed_dataset)
        #ttsplit
        self.partition = self.seq_ttsplit(self.seq.shape[0])
119/3:
dl = IBDDataLoader()
dl.fit()
120/1:
import torch
import torch.nn as nn
import data_utils as du
120/2:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.seq=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,self.train_size*seq_shape, replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), self.valid_size*seq_shape)
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_seq_label(self, dataset):
        groups = dataset.groupby('AUTO_ID')
#       (nb_seq x 3 x seq_len x input_size)
        seq = np.array([])
        labels = np.array([])
        min_rows_reqd = self.seq_len+self.lookfwd_months

        for grp in groups:
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: continue
            for i in range(n_rows-min_rows_reqd+1): #CHK
                df = grp.iloc[i:i+self.seq_len]
                x = df[self.col_dict.inputs].values
                m = df[self.col_dict.missing].values
                t = df[self.col_dict.delta].values
                x_obs = x.shift(1).fillna(x.loc[0]).values
#               (3 x seq_len x input_size)
                tmp_seq = np.array([x , m , t, x_obs])
                seq = np.append(seq, tmp_seq)
                np.append(labels, grp.iloc[i+min_rows_reqd-1].values)
        
        print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
        self.seq = seq
        self.labels = labels
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.seq[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit(self):
        dataset2, self.col_dict = du.X_raw() #(243563 rows)
        dataset = dataset2.head(1000).copy()
        
        imputed_dataset = du.Imputer().forward(dataset)  
        self.get_seq_labels(imputed_dataset)
        #ttsplit
        self.partition = self.seq_ttsplit(self.seq.shape[0])
120/3:
dl = IBDDataLoader()
dl.fit()
118/6:
df.to_pickle('pickles/inputs.pkl')
dic.to_pickle('pickles/col_dict.pkl')
118/7:
import pickle

# df.to_pickle('pickles/inputs.pkl')
with open('pickles/col_dict.pkl', 'wb') as f:
    pickle.dump(dic, f)
121/1:
import torch
import torch.nn as nn
import data_utils as du
121/2:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.seq=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,self.train_size*seq_shape, replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), self.valid_size*seq_shape)
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_seq_label(self, dataset):
        groups = dataset.groupby('AUTO_ID')
#       (nb_seq x 3 x seq_len x input_size)
        seq = np.array([])
        labels = np.array([])
        min_rows_reqd = self.seq_len+self.lookfwd_months

        for grp in groups:
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: continue
            for i in range(n_rows-min_rows_reqd+1): #CHK
                df = grp.iloc[i:i+self.seq_len]
                x = df[self.col_dict.inputs].values
                m = df[self.col_dict.missing].values
                t = df[self.col_dict.delta].values
                x_obs = x.shift(1).fillna(x.loc[0]).values
#               (3 x seq_len x input_size)
                tmp_seq = np.array([x , m , t, x_obs])
                seq = np.append(seq, tmp_seq)
                np.append(labels, grp.iloc[i+min_rows_reqd-1].values)
        
        print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
        self.seq = seq
        self.labels = labels
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.seq[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit(self):
        dataset2, self.col_dict = du.X_raw() #(243563 rows)
        dataset = dataset2.head(1000).copy()
        
        imputed_dataset = du.Imputer().forward(dataset)  
        self.get_seq_labels(imputed_dataset)
        #ttsplit
        self.partition = self.seq_ttsplit(self.seq.shape[0])
121/3:
dl = IBDDataLoader()
dl.fit()
122/1:
import torch
import torch.nn as nn
import data_utils as du
122/2:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.seq=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,self.train_size*seq_shape, replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), self.valid_size*seq_shape)
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_seq_label(self, dataset):
        groups = dataset.groupby('AUTO_ID')
#       (nb_seq x 3 x seq_len x input_size)
        seq = np.array([])
        labels = np.array([])
        min_rows_reqd = self.seq_len+self.lookfwd_months

        for grp in groups:
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: continue
            for i in range(n_rows-min_rows_reqd+1): #CHK
                df = grp.iloc[i:i+self.seq_len]
                x = df[self.col_dict.inputs].values
                m = df[self.col_dict.missing].values
                t = df[self.col_dict.delta].values
                x_obs = x.shift(1).fillna(x.loc[0]).values
#               (3 x seq_len x input_size)
                tmp_seq = np.array([x , m , t, x_obs])
                seq = np.append(seq, tmp_seq)
                np.append(labels, grp.iloc[i+min_rows_reqd-1].values)
        
        print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
        self.seq = seq
        self.labels = labels
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.seq[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit(self):
        dataset2, self.col_dict = du.X_raw() #(243563 rows)
        dataset = dataset2.head(1000).copy()
        
        imputed_dataset = du.Imputer().forward(dataset)  
        self.get_seq_labels(imputed_dataset)
        #ttsplit
        self.partition = self.seq_ttsplit(self.seq.shape[0])
122/3:
dl = IBDDataLoader()
dl.fit()
122/4:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.seq=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,self.train_size*seq_shape, replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), self.valid_size*seq_shape)
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_seq_labels(self, dataset):
        groups = dataset.groupby('AUTO_ID')
#       (nb_seq x 3 x seq_len x input_size)
        seq = np.array([])
        labels = np.array([])
        min_rows_reqd = self.seq_len+self.lookfwd_months

        for grp in groups:
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: continue
            for i in range(n_rows-min_rows_reqd+1): #CHK
                df = grp.iloc[i:i+self.seq_len]
                x = df[self.col_dict.inputs].values
                m = df[self.col_dict.missing].values
                t = df[self.col_dict.delta].values
                x_obs = x.shift(1).fillna(x.loc[0]).values
#               (3 x seq_len x input_size)
                tmp_seq = np.array([x , m , t, x_obs])
                seq = np.append(seq, tmp_seq)
                np.append(labels, grp.iloc[i+min_rows_reqd-1].values)
        
        print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
        self.seq = seq
        self.labels = labels
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.seq[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit(self):
        dataset2, self.col_dict = du.X_raw() #(243563 rows)
        dataset = dataset2.head(1000).copy()
        
        imputed_dataset = du.Imputer().forward(dataset)  
        self.get_seq_labels(imputed_dataset)
        #ttsplit
        self.partition = self.seq_ttsplit(self.seq.shape[0])
122/5:
dl = IBDDataLoader()
dl.fit()
122/6:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.seq=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,self.train_size*seq_shape, replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), self.valid_size*seq_shape)
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_seq_labels(self, dataset):
        groups = dataset.groupby('AUTO_ID')
#       (nb_seq x 3 x seq_len x input_size)
        seq = np.array([])
        labels = np.array([])
        min_rows_reqd = self.seq_len+self.lookfwd_months

        for _,grp in groups:
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: continue
            for i in range(n_rows-min_rows_reqd+1): #CHK
                df = grp.iloc[i:i+self.seq_len]
                x = df[self.col_dict.inputs].values
                m = df[self.col_dict.missing].values
                t = df[self.col_dict.delta].values
                x_obs = x.shift(1).fillna(x.loc[0]).values
#               (3 x seq_len x input_size)
                tmp_seq = np.array([x , m , t, x_obs])
                seq = np.append(seq, tmp_seq)
                np.append(labels, grp.iloc[i+min_rows_reqd-1].values)
        
        print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
        self.seq = seq
        self.labels = labels
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.seq[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit(self):
        dataset2, self.col_dict = du.X_raw() #(243563 rows)
        dataset = dataset2.head(1000).copy()
        
        imputed_dataset = du.Imputer().forward(dataset)  
        self.get_seq_labels(imputed_dataset)
        #ttsplit
        self.partition = self.seq_ttsplit(self.seq.shape[0])
122/7:
dl = IBDDataLoader()
dl.fit()
118/8: dic.input
118/9: dic
118/10: dic['input']
118/11: list(dic['input'])
122/8:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.seq=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,self.train_size*seq_shape, replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), self.valid_size*seq_shape)
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_seq_labels(self, dataset):
        groups = dataset.groupby('AUTO_ID')
#       (nb_seq x 3 x seq_len x input_size)
        seq = np.array([])
        labels = np.array([])
        min_rows_reqd = self.seq_len+self.lookfwd_months

        for _,grp in groups:
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: continue
            for i in range(n_rows-min_rows_reqd+1): #CHK
                df = grp.iloc[i:i+self.seq_len]
                x = df[self.col_dict['inputs']].values
                m = df[self.col_dict.['missing']].values
                t = df[self.col_dict.['delta']].values
                x_obs = x.shift(1).fillna(x.loc[0]).values
#               (3 x seq_len x input_size)
                tmp_seq = np.array([x , m , t, x_obs])
                seq = np.append(seq, tmp_seq)
                np.append(labels, grp.iloc[i+min_rows_reqd-1].values)
        
        print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
        self.seq = seq
        self.labels = labels
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.seq[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit(self):
        dataset2, self.col_dict = du.X_raw() #(243563 rows)
        dataset = dataset2.head(1000).copy()
        
        imputed_dataset = du.Imputer().forward(dataset)  
        self.get_seq_labels(imputed_dataset)
        #ttsplit
        self.partition = self.seq_ttsplit(self.seq.shape[0])
122/9:
dl = IBDDataLoader()
dl.fit()
122/10:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.seq=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,self.train_size*seq_shape, replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), self.valid_size*seq_shape)
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_seq_labels(self, dataset):
        groups = dataset.groupby('AUTO_ID')
#       (nb_seq x 3 x seq_len x input_size)
        seq = np.array([])
        labels = np.array([])
        min_rows_reqd = self.seq_len+self.lookfwd_months

        for _,grp in groups:
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: continue
            for i in range(n_rows-min_rows_reqd+1): #CHK
                df = grp.iloc[i:i+self.seq_len]
                x = df[self.col_dict['inputs']].values
                m = df[self.col_dict['missing']].values
                t = df[self.col_dict['delta']].values
                x_obs = x.shift(1).fillna(x.loc[0]).values
#               (3 x seq_len x input_size)
                tmp_seq = np.array([x , m , t, x_obs])
                seq = np.append(seq, tmp_seq)
                np.append(labels, grp.iloc[i+min_rows_reqd-1].values)
        
        print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
        self.seq = seq
        self.labels = labels
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.seq[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit(self):
        dataset2, self.col_dict = du.X_raw() #(243563 rows)
        dataset = dataset2.head(1000).copy()
        
        imputed_dataset = du.Imputer().forward(dataset)  
        self.get_seq_labels(imputed_dataset)
        #ttsplit
        self.partition = self.seq_ttsplit(self.seq.shape[0])
122/11:
dl = IBDDataLoader()
dl.fit()
122/12: dl.col_dict
122/13:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.seq=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,self.train_size*seq_shape, replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), self.valid_size*seq_shape)
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_seq_labels(self, dataset):
        groups = dataset.groupby('AUTO_ID')
#       (nb_seq x 3 x seq_len x input_size)
        seq = np.array([])
        labels = np.array([])
        min_rows_reqd = self.seq_len+self.lookfwd_months

        for _,grp in groups:
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: continue
            for i in range(n_rows-min_rows_reqd+1): #CHK
                df = grp.iloc[i:i+self.seq_len]
                x = df[self.col_dict['input']].values
                m = df[self.col_dict['missing']].values
                t = df[self.col_dict['delta']].values
                x_obs = x.shift(1).fillna(x.loc[0]).values
#               (3 x seq_len x input_size)
                tmp_seq = np.array([x , m , t, x_obs])
                seq = np.append(seq, tmp_seq)
                np.append(labels, grp.iloc[i+min_rows_reqd-1].values)
        
        print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
        self.seq = seq
        self.labels = labels
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.seq[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit(self):
        dataset2, self.col_dict = du.X_raw() #(243563 rows)
        dataset = dataset2.head(1000).copy()
        
        imputed_dataset = du.Imputer().forward(dataset)  
        self.get_seq_labels(imputed_dataset)
        #ttsplit
        self.partition = self.seq_ttsplit(self.seq.shape[0])
122/14:
dl = IBDDataLoader()
dl.fit()
122/15:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.seq=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,self.train_size*seq_shape, replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), self.valid_size*seq_shape)
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_seq_labels(self, dataset):
        groups = dataset.groupby('AUTO_ID')
#       (nb_seq x 3 x seq_len x input_size)
        seq = np.array([])
        labels = np.array([])
        min_rows_reqd = self.seq_len+self.lookfwd_months

        for _,grp in groups:
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: continue
            for i in range(n_rows-min_rows_reqd+1): #CHK
                df = grp.iloc[i:i+self.seq_len]
                x = df[self.col_dict['input']].values
                m = df[self.col_dict['missing']].values
                t = df[self.col_dict['delta']].values
                x_obs = df[self.col_dict['input']].shift(1).fillna(x.loc[0]).values
#               (3 x seq_len x input_size)
                tmp_seq = np.array([x , m , t, x_obs])
                seq = np.append(seq, tmp_seq)
                np.append(labels, grp.iloc[i+min_rows_reqd-1].values)
        
        print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
        self.seq = seq
        self.labels = labels
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.seq[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit(self):
        dataset2, self.col_dict = du.X_raw() #(243563 rows)
        dataset = dataset2.head(1000).copy()
        
        imputed_dataset = du.Imputer().forward(dataset)  
        self.get_seq_labels(imputed_dataset)
        #ttsplit
        self.partition = self.seq_ttsplit(self.seq.shape[0])
122/16:
dl = IBDDataLoader()
dl.fit()
122/17:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.seq=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,self.train_size*seq_shape, replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), self.valid_size*seq_shape)
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_seq_labels(self, dataset):
        groups = dataset.groupby('AUTO_ID')
#       (nb_seq x 3 x seq_len x input_size)
        seq = np.array([])
        labels = np.array([])
        min_rows_reqd = self.seq_len+self.lookfwd_months

        for _,grp in groups:
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: continue
            for i in range(n_rows-min_rows_reqd+1): #CHK
                df = grp.iloc[i:i+self.seq_len]
                x = df[self.col_dict['input']]
                x_obs = x.shift(1).fillna(x.loc[0]).values
                m = df[self.col_dict['missing']].values
                t = df[self.col_dict['delta']].values
#               (3 x seq_len x input_size)
                tmp_seq = np.array([x.values , m , t, x_obs])
                seq = np.append(seq, tmp_seq)
                np.append(labels, grp.iloc[i+min_rows_reqd-1].values)
        
        print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
        self.seq = seq
        self.labels = labels
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.seq[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit(self):
        dataset2, self.col_dict = du.X_raw() #(243563 rows)
        dataset = dataset2.head(1000).copy()
        
        imputed_dataset = du.Imputer().forward(dataset)  
        self.get_seq_labels(imputed_dataset)
        #ttsplit
        self.partition = self.seq_ttsplit(self.seq.shape[0])
122/18:
dl = IBDDataLoader()
dl.fit()
122/19:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.seq=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,self.train_size*seq_shape, replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), self.valid_size*seq_shape)
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_seq_labels(self, dataset):
        groups = dataset.groupby('AUTO_ID')
#       (nb_seq x 3 x seq_len x input_size)
        seq = np.array([])
        labels = np.array([])
        min_rows_reqd = self.seq_len+self.lookfwd_months

        for _,grp in groups:
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: continue
            for i in range(n_rows-min_rows_reqd+1): #CHK
                df = grp.iloc[i:i+self.seq_len]
                x = df[self.col_dict['input']]
                x_obs = x.shift(1).fillna(x.loc[0]).values
                m = df[self.col_dict['missing']].values
                t = df[self.col_dict['delta']].values
                x=x.values
#               (3 x seq_len x input_size)
                tmp_seq = np.array([x, m , t, x_obs])
                seq = np.append(seq, tmp_seq)
                np.append(labels, grp.iloc[i+min_rows_reqd-1].values)
        
        print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
        self.seq = seq
        self.labels = labels
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.seq[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit(self):
        dataset2, self.col_dict = du.X_raw() #(243563 rows)
        dataset = dataset2.head(1000).copy()
        
        imputed_dataset = du.Imputer().forward(dataset)  
        self.get_seq_labels(imputed_dataset)
        #ttsplit
        self.partition = self.seq_ttsplit(self.seq.shape[0])
122/20: len(dl.col_dict['input'])
122/21: len(dl.col_dict['missing'])
118/12: list(dic['missing'])
122/22: len(dl.col_dict['delta'])
122/23:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.seq=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,self.train_size*seq_shape, replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), self.valid_size*seq_shape)
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_seq_labels(self, dataset):
        groups = dataset.groupby('AUTO_ID')
#       (nb_seq x 3 x seq_len x input_size)
        seq = np.array([])
        labels = np.array([])
        min_rows_reqd = self.seq_len+self.lookfwd_months

        for _,grp in groups:
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: continue
            for i in range(n_rows-min_rows_reqd+1): #CHK
                df = grp.iloc[i:i+self.seq_len]
                x = df[self.col_dict['input']]
                x_obs = x.shift(1).fillna(x.loc[0]).values
                print(x_obs.shape)
                m = df[self.col_dict['missing']].values
                t = df[self.col_dict['delta']].values
                x=x.values
#               (3 x seq_len x input_size)
                tmp_seq = np.array([x, m, t, x_obs])
                seq = np.append(seq, tmp_seq)
                np.append(labels, grp.iloc[i+min_rows_reqd-1].values)
        
        print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
        self.seq = seq
        self.labels = labels
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.seq[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit(self):
        dataset2, self.col_dict = du.X_raw() #(243563 rows)
        dataset = dataset2.head(1000).copy()
        
        imputed_dataset = du.Imputer().forward(dataset)  
        self.get_seq_labels(imputed_dataset)
        #ttsplit
        self.partition = self.seq_ttsplit(self.seq.shape[0])
122/24:
dl = IBDDataLoader()
dl.fit()
122/25:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.seq=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,self.train_size*seq_shape, replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), self.valid_size*seq_shape)
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_seq_labels(self, dataset):
        groups = dataset.groupby('AUTO_ID')
#       (nb_seq x 3 x seq_len x input_size)
        seq = np.array([])
        labels = np.array([])
        min_rows_reqd = self.seq_len+self.lookfwd_months

        for _,grp in groups:
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: continue
            for i in range(n_rows-min_rows_reqd+1): #CHK
                df = grp.iloc[i:i+self.seq_len]
                x = df[self.col_dict['input']]
                x_obs = x.shift(1).fillna(x.loc[0]).values
                print(x_obs.shape)
                m = df[self.col_dict['missing']].values
                t = df[self.col_dict['delta']].values
                x=x.values
#               (3 x seq_len x input_size)
                tmp_seq = np.concatenate((x, m, t, x_obs),axis=1)
                seq = np.append(seq, tmp_seq)
                np.append(labels, grp.iloc[i+min_rows_reqd-1].values)
        
        print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
        self.seq = seq
        self.labels = labels
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.seq[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit(self):
        dataset2, self.col_dict = du.X_raw() #(243563 rows)
        dataset = dataset2.head(1000).copy()
        
        imputed_dataset = du.Imputer().forward(dataset)  
        self.get_seq_labels(imputed_dataset)
        #ttsplit
        self.partition = self.seq_ttsplit(self.seq.shape[0])
122/26:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.seq=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,self.train_size*seq_shape, replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), self.valid_size*seq_shape)
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_seq_labels(self, dataset):
        groups = dataset.groupby('AUTO_ID')
#       (nb_seq x 3 x seq_len x input_size)
        seq = np.array([])
        labels = np.array([])
        min_rows_reqd = self.seq_len+self.lookfwd_months

        for _,grp in groups:
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: continue
            for i in range(n_rows-min_rows_reqd+1): #CHK
                df = grp.iloc[i:i+self.seq_len]
                x = df[self.col_dict['input']]
                x_obs = x.shift(1).fillna(x.loc[0]).values
                m = df[self.col_dict['missing']].values
                t = df[self.col_dict['delta']].values
                x=x.values
#               (3 x seq_len x input_size)
                tmp_seq = np.concatenate((x, m, t, x_obs),axis=1)
                seq = np.append(seq, tmp_seq)
                np.append(labels, grp.iloc[i+min_rows_reqd-1].values)
        
        print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
        self.seq = seq
        self.labels = labels
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.seq[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit(self):
        dataset2, self.col_dict = du.X_raw() #(243563 rows)
        dataset = dataset2.head(1000).copy()
        
        imputed_dataset = du.Imputer().forward(dataset)  
        self.get_seq_labels(imputed_dataset)
        #ttsplit
        self.partition = self.seq_ttsplit(self.seq.shape[0])
122/27:
dl = IBDDataLoader()
dl.fit()
122/28:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.seq=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,self.train_size*seq_shape, replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), self.valid_size*seq_shape)
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_seq_labels(self, dataset):
        groups = dataset.groupby('AUTO_ID')
#       (nb_seq x 3 x seq_len x input_size)
        seq = np.array([])
        labels = np.array([])
        min_rows_reqd = self.seq_len+self.lookfwd_months

        for _,grp in groups:
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: continue
            for i in range(n_rows-min_rows_reqd+1): #CHK
                df = grp.iloc[i:i+self.seq_len]
                x = df[self.col_dict['input']]
                x_obs = x.shift(1).fillna(x.iloc[0]).values
                m = df[self.col_dict['missing']].values
                t = df[self.col_dict['delta']].values
                x=x.values
#               (3 x seq_len x input_size)
                tmp_seq = np.concatenate((x, m, t, x_obs),axis=1)
                seq = np.append(seq, tmp_seq)
                np.append(labels, grp.iloc[i+min_rows_reqd-1].values)
        
        print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
        self.seq = seq
        self.labels = labels
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.seq[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit(self):
        dataset2, self.col_dict = du.X_raw() #(243563 rows)
        dataset = dataset2.head(1000).copy()
        
        imputed_dataset = du.Imputer().forward(dataset)  
        self.get_seq_labels(imputed_dataset)
        #ttsplit
        self.partition = self.seq_ttsplit(self.seq.shape[0])
122/29:
dl = IBDDataLoader()
dl.fit()
122/30:
import torch
import torch.nn as nn
import data_utils as du
import math
122/31:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.seq=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_seq_labels(self, dataset):
        groups = dataset.groupby('AUTO_ID')
#       (nb_seq x 3 x seq_len x input_size)
        seq = np.array([])
        labels = np.array([])
        min_rows_reqd = self.seq_len+self.lookfwd_months

        for _,grp in groups:
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: continue
            for i in range(n_rows-min_rows_reqd+1): #CHK
                df = grp.iloc[i:i+self.seq_len]
                x = df[self.col_dict['input']]
                x_obs = x.shift(1).fillna(x.iloc[0]).values
                m = df[self.col_dict['missing']].values
                t = df[self.col_dict['delta']].values
                x=x.values
#               (3 x seq_len x input_size)
                tmp_seq = np.concatenate((x, m, t, x_obs),axis=1)
                seq = np.append(seq, tmp_seq)
                np.append(labels, grp.iloc[i+min_rows_reqd-1].values)
        
        print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
        self.seq = seq
        self.labels = labels
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.seq[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit(self):
        dataset2, self.col_dict = du.X_raw() #(243563 rows)
        dataset = dataset2.head(1000).copy()
        
        imputed_dataset = du.Imputer().forward(dataset)  
        self.get_seq_labels(imputed_dataset)
        #ttsplit
        self.partition = self.seq_ttsplit(self.seq.shape[0])
122/32:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.seq=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_seq_labels(self, dataset):
        groups = dataset.groupby('AUTO_ID')
#       (nb_seq x 3 x seq_len x input_size)
        seq = np.array([])
        labels = np.array([])
        min_rows_reqd = self.seq_len+self.lookfwd_months

        for _,grp in groups:
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: continue
            for i in range(n_rows-min_rows_reqd+1): #CHK
                df = grp.iloc[i:i+self.seq_len]
                x = df[self.col_dict['input']]
                x_obs = x.shift(1).fillna(x.iloc[0]).values
                m = df[self.col_dict['missing']].values
                t = df[self.col_dict['delta']].values
                x=x.values
#               (3 x seq_len x input_size)
                tmp_seq = np.concatenate((x, m, t, x_obs),axis=1)
                seq = np.append(seq, tmp_seq)
                labels = np.append(labels, grp.iloc[i+min_rows_reqd-1].values)
        
        print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
        self.seq = seq
        self.labels = labels
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.seq[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit(self):
        dataset2, self.col_dict = du.X_raw() #(243563 rows)
        dataset = dataset2.head(1000).copy()
        
        imputed_dataset = du.Imputer().forward(dataset)  
        self.get_seq_labels(imputed_dataset)
        #ttsplit
        self.partition = self.seq_ttsplit(self.seq.shape[0])
122/33:
dl = IBDDataLoader()
dl.fit()
122/34: dl.partition
122/35: dl.seq[0[]]
122/36: dl.seq[0]
122/37: dl.seq[1]
122/38: dl.seq[2]
122/39:
dataset2, self.col_dict = du.X_raw() #(243563 rows)
dataset = dataset2.head(1000).copy()
imputed_dataset = du.Imputer().forward(dataset)
122/40:
dataset2, col_dict = du.X_raw() #(243563 rows)
dataset = dataset2.head(1000).copy()
imputed_dataset = du.Imputer().forward(dataset)
122/41: imputed_dataset
122/42:
groups = imputed_dataset.groupby('AUTO_ID')
#(nb_seq x 3 x seq_len x input_size)
seq = np.array([])
labels = np.array([])
min_rows_reqd = self.seq_len+self.lookfwd_months

# for _,grp in groups:
#     n_rows = grp.shape[0]
#     if n_rows<min_rows_reqd: continue
#     for i in range(n_rows-min_rows_reqd+1): #CHK
#         df = grp.iloc[i:i+self.seq_len]
#         x = df[self.col_dict['input']]
#         x_obs = x.shift(1).fillna(x.iloc[0]).values
#         m = df[self.col_dict['missing']].values
#         t = df[self.col_dict['delta']].values
#         x=x.values
# #               (3 x seq_len x input_size)
#         tmp_seq = np.concatenate((x, m, t, x_obs),axis=1)
#         seq = np.append(seq, tmp_seq)
#         labels = np.append(labels, grp.iloc[i+min_rows_reqd-1].values)

# print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
# self.seq = seq
# self.labels = labels
122/43:
groups = imputed_dataset.groupby('AUTO_ID')
#(nb_seq x 3 x seq_len x input_size)
seq = np.array([])
labels = np.array([])
min_rows_reqd = 48

# for _,grp in groups:
#     n_rows = grp.shape[0]
#     if n_rows<min_rows_reqd: continue
#     for i in range(n_rows-min_rows_reqd+1): #CHK
#         df = grp.iloc[i:i+self.seq_len]
#         x = df[self.col_dict['input']]
#         x_obs = x.shift(1).fillna(x.iloc[0]).values
#         m = df[self.col_dict['missing']].values
#         t = df[self.col_dict['delta']].values
#         x=x.values
# #               (3 x seq_len x input_size)
#         tmp_seq = np.concatenate((x, m, t, x_obs),axis=1)
#         seq = np.append(seq, tmp_seq)
#         labels = np.append(labels, grp.iloc[i+min_rows_reqd-1].values)

# print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
# self.seq = seq
# self.labels = labels
122/44:
grp = imputed_dataset[imputed_dataset.AUTO_ID==0]
for i in range(n_rows-min_rows_reqd+1): #CHK
    df = grp.iloc[i:i+seq_len]
    x = df[col_dict['input']]
    x_obs = x.shift(1).fillna(x.iloc[0]).values
    m = df[col_dict['missing']].values
    t = df[col_dict['delta']].values
    x=x.values
    tmp_seq = np.concatenate((x, m, t, x_obs),axis=1)
    seq = np.append(seq, tmp_seq)
122/45:
grp = imputed_dataset[imputed_dataset.AUTO_ID==0]
n_rows = grp.shape[0]
for i in range(n_rows-min_rows_reqd+1): #CHK
    df = grp.iloc[i:i+seq_len]
    x = df[col_dict['input']]
    x_obs = x.shift(1).fillna(x.iloc[0]).values
    m = df[col_dict['missing']].values
    t = df[col_dict['delta']].values
    x=x.values
    tmp_seq = np.concatenate((x, m, t, x_obs),axis=1)
    seq = np.append(seq, tmp_seq)
122/46:
grp = imputed_dataset[imputed_dataset.AUTO_ID==0]
n_rows = grp.shape[0]
min_rows_reqd=9
for i in range(n_rows-min_rows_reqd+1): #CHK
    df = grp.iloc[i:i+8]
    x = df[col_dict['input']]
    x_obs = x.shift(1).fillna(x.iloc[0]).values
    m = df[col_dict['missing']].values
    t = df[col_dict['delta']].values
    x=x.values
    tmp_seq = np.concatenate((x, m, t, x_obs),axis=1)
    seq = np.append(seq, tmp_seq)
    labels = np.append(labels, grp.iloc[i+min_rows_reqd-1].values)
122/47: seq
122/48: seq.shape
122/49: tmp_seq
122/50: tmp_seq.shape
118/13:
for k in dic.keys():
    print(list(dic[k]), len(list(dic[k])))
122/51:
grp = imputed_dataset[imputed_dataset.AUTO_ID==0]
n_rows = grp.shape[0]
min_rows_reqd=9
for i in range(n_rows-min_rows_reqd+1): #CHK
    df = grp.iloc[i:i+8]
    x = df[col_dict['input']]
    x_obs = x.shift(1).fillna(x.iloc[0]).values
    m = df[col_dict['missing']].values
    t = df[col_dict['delta']].values
    x=x.values
    tmp_seq = np.concatenate((x, m, t, x_obs),axis=1)
    seq = np.append(seq, [tmp_seq])
    labels = np.append(labels, grp.iloc[i+min_rows_reqd-1].values)
122/52: seq.shape
122/53: seq[0]
122/54:
seq = np.array([])
labels = np.array([])
grp = imputed_dataset[imputed_dataset.AUTO_ID==0]
n_rows = grp.shape[0]
min_rows_reqd=9
for i in range(n_rows-min_rows_reqd+1): #CHK
    df = grp.iloc[i:i+8]
    x = df[col_dict['input']]
    x_obs = x.shift(1).fillna(x.iloc[0]).values
    m = df[col_dict['missing']].values
    t = df[col_dict['delta']].values
    x=x.values
    tmp_seq = np.concatenate((x, m, t, x_obs),axis=1)
    seq = np.append(seq, [tmp_seq])
    labels = np.append(labels, grp.iloc[i+min_rows_reqd-1].values)
122/55: seq.shape
122/56: tmp_seq
122/57:
l = np.array([[]])
l.append([1,2])
122/58:
l = np.array([[]])
np.append(l,[1,2])
122/59:
l = np.array([[]])
l = np.append(l,[1,2])
np.append(l,[3,4])
122/60:
l = np.array([[]])
l = np.append(l,[[1,2]])
# np.append(l,[3,4])
122/61:
l = np.array([[]])
l = np.append(l,[[1,2]])
# np.append(l,[3,4])
l
122/62:
l = np.array([[]])
l = np.append(l,[[1,2]])
np.append(l,[3,4])
122/63:
l = np.array([[]])
l = np.append(l,[[1,2]])
np.append(l,[[3,4]])
122/64:
seq = np.array([])
labels = np.array([])
grp = imputed_dataset[imputed_dataset.AUTO_ID==0]
n_rows = grp.shape[0]
min_rows_reqd=9
for i in range(n_rows-min_rows_reqd+1): #CHK
    df = grp.iloc[i:i+8]
    x = df[col_dict['input']]
    x_obs = x.shift(1).fillna(x.iloc[0]).values
    m = df[col_dict['missing']].values
    t = df[col_dict['delta']].values
    x=x.values
    tmp_seq = np.concatenate((x, m, t, x_obs),axis=1)
    seq.append(tmp_seq)
    labels.append(grp.iloc[i+min_rows_reqd-1].values)
122/65:
seq = []
labels = []
grp = imputed_dataset[imputed_dataset.AUTO_ID==0]
n_rows = grp.shape[0]
min_rows_reqd=9
for i in range(n_rows-min_rows_reqd+1): #CHK
    df = grp.iloc[i:i+8]
    x = df[col_dict['input']]
    x_obs = x.shift(1).fillna(x.iloc[0]).values
    m = df[col_dict['missing']].values
    t = df[col_dict['delta']].values
    x=x.values
    tmp_seq = np.concatenate((x, m, t, x_obs),axis=1)
    seq.append(tmp_seq)
    labels.append(grp.iloc[i+min_rows_reqd-1].values)
122/66: seq
122/67: seq.shape
122/68: np.array(seq).shape
122/69:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.seq=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_seq_labels(self, dataset):
        groups = dataset.groupby('AUTO_ID')
#       (nb_seq x 3 x seq_len x input_size)
        seq = []
        labels = []
        min_rows_reqd = self.seq_len+self.lookfwd_months

        for _,grp in groups:
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: continue
            for i in range(n_rows-min_rows_reqd+1): #CHK
                df = grp.iloc[i:i+self.seq_len]
                x = df[self.col_dict['input']]
                x_obs = x.shift(1).fillna(x.iloc[0]).values
                m = df[self.col_dict['missing']].values
                t = df[self.col_dict['delta']].values
                x=x.values
#               (3 x seq_len x input_size)
                tmp_seq = np.concatenate((x, m, t, x_obs),axis=1)
                seq.append(tmp_seq)
                labels.append(grp.iloc[i+min_rows_reqd-1].values)
        
        self.seq = np.array(seq)
        self.labels = np.array(labels)
        print(f"Sequences shape: {self.seq.shape} Labels shape: {self.labels.shape}")
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.seq[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit(self):
        dataset2, self.col_dict = du.X_raw() #(243563 rows)
        dataset = dataset2.head(1000).copy()
        
        imputed_dataset = du.Imputer().forward(dataset)  
        self.get_seq_labels(imputed_dataset)
        #ttsplit
        self.partition = self.seq_ttsplit(self.seq.shape[0])
122/70:
dl = IBDDataLoader()
dl.fit()
122/71: dl.seq[0], dl.labels[0]
90/55: np
90/56: a=np.random.uniform(-np.sqrt(1./180), np.sqrt(1./180), (180, 300))
90/57: a.shape
90/58: a
122/72:
%matplotlib inline
a=np.random.uniform(-np.sqrt(1./180), np.sqrt(1./180), (180, 300))
plt.plot(a)
122/73:
import torch
import torch.nn as nn
import data_utils as du
import math

import matplotlib.pyplot as plt
%matplotlib inline
122/74:

a=np.random.uniform(-np.sqrt(1./180), np.sqrt(1./180), (180, 300))
plt.plot(a)
122/75:
import seaborn as sns
a=np.random.uniform(-np.sqrt(1./180), np.sqrt(1./180), (180, 300))
sns.distplot(np.random.uniform(-np.sqrt(1./180), np.sqrt(1./180), (100,)))
122/76: -np.sqrt(1./180)
122/77: np.array([1,2])*np.array([3,4])
122/78:
a=np.array([[1,2],[3,4]])
a.shape
122/79:
a=np.array([[1,2],[3,4]])
np.array([a,a,a]).shape
122/80:
a=np.array([[1,2],[3,4]])
b = torch.cat((a,a), axis=1)
b.size()
122/81:
a=torch.from_numpy(np.array([[1,2],[3,4]]))
b = torch.cat((a,a), axis=1)
b.size()
122/82:
a=torch.from_numpy(np.array([[1,2],[3,4]]))
b = torch.cat((a,a), axis=1)
c = torch.cat((b,b,b), axis=0)
c.size()
122/83:
a=torch.from_numpy(np.array([[1,2],[3,4]]))
b = torch.cat((a,a), axis=1)
c = torch.cat((b,b,b), axis=0)
d = torch.from_numpy([c,c,c])
d.size()
122/84:
a=torch.from_numpy(np.array([[1,2],[3,4]]))
b = torch.cat((a,a), axis=1)
c = torch.cat((b,b,b), axis=0)
d = torch.from_numpy(np.array([c,c,c]))
d.size()
122/85:
a=torch.from_numpy(np.array([[1,2],[3,4]]))
b = torch.cat((a,a), axis=1)
c = torch.cat((b,b,b), axis=0)
c
122/86:
a=torch.from_numpy(np.array([[1,2],[3,4]]))
b = torch.cat((a,a), axis=1)
c = torch.cat((b,b,b), axis=0)
torch.squeeze(c[:,1])
122/87:
a=torch.from_numpy(np.array([[1,2],[3,4]]))
b = torch.cat((a,a), axis=1)
c = torch.cat((b,b,b), axis=0)
d = torch.tensor([c,c,c])
d.size()
122/88:
a=torch.from_numpy(np.array([[1,2],[3,4]]))
b = torch.cat((a,a), axis=1)
c = torch.cat((b,b,b), axis=0)
d = torch.tensor([c,c,c])
d
122/89:
a=torch.from_numpy(np.array([[1,2],[3,4]]))
b = torch.cat((a,a), axis=1)
c = torch.cat((b,b,b), axis=0)
c
122/90:
a=torch.from_numpy(np.array([[1,2],[3,4]]))
b = torch.cat((a,a), axis=1)
c = torch.cat((b,b,b), axis=0)
torch.squeeze(c[:,1:3])
122/91:
a=torch.from_numpy(np.array([[1,2],[3,4]]))
b = torch.cat((a,a), axis=1)
c = torch.cat((b,b,b), axis=0)
c[:,1:3]
122/92:
    def get_params(self, input_dim, hidden_dim, ctx):
        # Initialize Weight matrices
        def normal(shape):
            return torch.randn(size=shape, device=ctx)

        def three():
            return (normal((input_dim, hidden_dim)),
                    normal((input_dim, hidden_dim)),
                    np.zeros(hidden_dim, device=ctx))
        
        W_xz, W_hz, b_z = three()  # Update gate parameter
        W_xr, W_hr, b_r = three()  # Reset gate parameter
        W_xh, W_hh, b_h = three()  # Candidate hidden state parameter
        
        W_hq = normal((num_hiddens, num_outputs))
        b_q = np.zeros(num_outputs, device=ctx)
        params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]
#         for param in params:
#             param.attach_grad()
        return params

    def reset_parameters(self):
        stdv = 1.0 / math.sqrt(self.hidden_size)
        for weight in self.parameters():
            init.uniform_(weight, -stdv, stdv)
122/93:
a=torch.from_numpy(np.array([[1,2],[3,4]]))
b = torch.cat((a,a), axis=1)
c = torch.cat((b,b,b), axis=0)
c
122/94:
a=torch.from_numpy(np.array([[1,2],[3,4]]))
b = torch.cat((a,a), axis=1)
c = torch.cat((b,b,b), axis=0)
b
122/95:
a=torch.from_numpy(np.array([[1,2],[3,4]]))
b = torch.cat((a.unsqueeze(1),a.unsqueeze(1)), axis=1)
c = torch.cat((b,b,b), axis=0)
b
122/96:
a=torch.from_numpy(np.array([[1,2],[3,4]]))
b = torch.cat((a,a), axis=1)
c = torch.cat((b,b,b), axis=0)
b
122/97:
a=torch.from_numpy(np.array([[1,2],[3,4]]))
b = torch.cat((a,a), axis=1)
c = torch.cat((b,b,b), axis=0)
b.unqueeze(1)
122/98:
a=torch.from_numpy(np.array([[1,2],[3,4]]))
b = torch.cat((a,a), axis=1)
c = torch.cat((b,b,b), axis=0)
b.unsqueeze(1)
122/99:
a=torch.from_numpy(np.array([[1,2],[3,4]]))
b = torch.cat((a,a), axis=1)
c = torch.cat((b,b,b), axis=0)
b, b.unsqueeze(1)
122/100:
a=torch.from_numpy(np.array([[1,2],[3,4]]))
b = torch.cat((a,a), axis=1)
c = torch.cat((b,b,b), axis=0)
print(b, b.unsqueeze(1))
print(b.shape, b.unsqueeze(1).shape)
122/101:
a=torch.from_numpy(np.array([[1,2],[3,4]]))
b = torch.cat((a,a), axis=1)
c = torch.cat((b,b,b), axis=0)
print(b, b.unsqueeze(2))
print(b.shape, b.unsqueeze(2).shape)
122/102: torch.tensor((3,2))
122/103: torch.tensor(3,2)
122/104: torch.tensor(3)
122/105:
a=torch.from_numpy(np.array([[1,2],[3,4]]))
b = torch.cat((a,a), axis=1)
c = torch.cat((b,b,b), axis=0)
c.size()
122/106:
a=torch.from_numpy(np.array([[1,2],[3,4]]))
b = torch.cat((a,a), axis=1)
c = torch.cat((b,b,b), axis=0)
c
122/107:
a=torch.from_numpy(np.array([[1,2],[3,4]]))
b = torch.cat((a,a), axis=1)
c = torch.cat((b,b,b), axis=0)
c*c
122/108:
class GRUD:
    
    def __init__(self, input_dim, hidden_dim, hidden_layers, use_decay=False, mask_dim=0, delta_dim=0):
        # Assign input and hidden dim
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.hidden_layers = hidden_layers
        self.use_decay = use_decay
        
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
            self.step_fn = self.step_decay
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.step_fn = self.step_vanilla
            
            
    def init_hidden(self, batch_size, ctx):
        return torch.zeros(batch_size, self.hidden_dim, device=ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.shape[0]
        step_size = inputs.shape[2]
        hidden = self.init_hidden()
        outputs = []

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:,:]), \
                            torch.squeeze(inputs[:,1,:,:]), \
                            torch.squeeze(inputs[:,2,:,:]). \
                            torch.squeeze(inputs[:,3,:,:])
        if self.use_decay:
            gamma_x = torch.exp(-1*torch.max(torch.zeros(), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-1*torch.max(torch.zeros(), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*x.mean() ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.combined((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.combined((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
122/109:
class GRUD:
    
    def __init__(self, input_dim, hidden_dim, hidden_layers, use_decay=False, mask_dim=0, delta_dim=0):
        # Assign input and hidden dim
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.hidden_layers = hidden_layers
        self.use_decay = use_decay
        
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
            self.step_fn = self.step_decay
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.step_fn = self.step_vanilla
            
            
    def init_hidden(self, batch_size, ctx):
        return torch.zeros(batch_size, self.hidden_dim, device=ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.shape[0]
        step_size = inputs.shape[2]
        hidden = self.init_hidden()
        outputs = []

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:,:]), \
                            torch.squeeze(inputs[:,1,:,:]), \
                            torch.squeeze(inputs[:,2,:,:]). \
                            torch.squeeze(inputs[:,3,:,:])
        if self.use_decay:
            gamma_x = torch.exp(-1*torch.max(torch.zeros(), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-1*torch.max(torch.zeros(), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*x.mean() ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.combined((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.combined((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
122/110:
class GRUD:
    
    def __init__(self, input_dim, hidden_dim, hidden_layers, use_decay=False, mask_dim=0, delta_dim=0):
        # Assign input and hidden dim
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.hidden_layers = hidden_layers
        self.use_decay = use_decay
        
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
            self.step_fn = self.step_decay
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.step_fn = self.step_vanilla
            
            
    def init_hidden(self, batch_size, ctx):
        return torch.zeros(batch_size, self.hidden_dim, device=ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.shape[0]
        step_size = inputs.shape[2]
        hidden = self.init_hidden()
        outputs = []

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:,:]), \
                            torch.squeeze(inputs[:,1,:,:]), \
                            torch.squeeze(inputs[:,2,:,:]). \
                            torch.squeeze(inputs[:,3,:,:])
        if self.use_decay:
            gamma_x = torch.exp(-1*torch.max(torch.zeros(), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-1*torch.max(torch.zeros(), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*x.mean() ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.combined((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.combined((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
122/111: seq
122/112: seq.shape
122/113: len(seq)
122/114: len(labels)
122/115: np.array(labels).shape
122/116: np.array(seq).shape
122/117:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.seq=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_seq_labels(self, dataset):
        """
        [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
        """
        groups = dataset.groupby('AUTO_ID')
        seq = []
        labels = []
        min_rows_reqd = self.seq_len+self.lookfwd_months

        for _,grp in groups:
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: continue
            for i in range(n_rows-min_rows_reqd+1): #CHK
                df = grp.iloc[i:i+self.seq_len]
                df.drop('AUTO_ID', axis=1, inplace=True)
                x = df[self.col_dict['input']]
                x_obs = x.shift(1).fillna(x.iloc[0]).values
                m = df[self.col_dict['missing']].values
                t = df[self.col_dict['delta']].values
                x=x.values
#               (seq_len x input_size)
                tmp_seq = [x, m, t, x_obs]
                seq.append(tmp_seq)
                labels.append(grp.iloc[i+min_rows_reqd-1].values)
        
        self.seq = np.array(seq)
        self.labels = np.array(labels)
        print(f"Sequences shape: {self.seq.shape} Labels shape: {self.labels.shape}")
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.seq[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def get_sequences(self):
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        imputed_dataset = du.Imputer().forward(dataset)  
        self.get_seq_labels(imputed_dataset)
    
    def generator():
        # WIP
        pass
122/118:
dl = IBDDataLoader()
train, valid, test = dl.get_loaders()
122/119:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    

    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def get_loaders(self):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months

            for _,grp in groups:
                n_rows = grp.shape[0]
                if n_rows<min_rows_reqd: continue
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    df = grp.iloc[i:i+self.seq_len]
                    df.drop(['AUTO_ID','PADDED'], axis=1, inplace=True)
                    x = df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = df[self.col_dict['missing']].values
                    t = df[self.col_dict['delta']].values
                    x=x.values
    #               (seq_len x input_size)
                    tmp_seq = [x, m, t, x_obs]
                    seq.append(tmp_seq)
                    labels.append(grp.iloc[i+min_rows_reqd-1].values)    
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        imputed_dataset = du.Imputer().forward(dataset)  
        self.sequences, self.labels = load(imputed_dataset)
        tvt_partition = seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
122/120:
dl = IBDDataLoader()
train, valid, test = dl.get_loaders()
122/121:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    

    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def get_loaders(self):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months

            for _,grp in groups:
                n_rows = grp.shape[0]
                if n_rows<min_rows_reqd: continue
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    df = grp.iloc[i:i+self.seq_len]
                    df.drop(['AUTO_ID'], axis=1, inplace=True)
                    x = df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = df[self.col_dict['missing']].values
                    t = df[self.col_dict['delta']].values
                    x=x.values
    #               (seq_len x input_size)
                    tmp_seq = [x, m, t, x_obs]
                    seq.append(tmp_seq)
                    labels.append(grp.iloc[i+min_rows_reqd-1].values)    
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        imputed_dataset = du.Imputer().forward(dataset)  
        self.sequences, self.labels = load(imputed_dataset)
        tvt_partition = seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
122/122:
dl = IBDDataLoader()
train, valid, test = dl.get_loaders()
122/123: seq[0]
122/124: seq[0].shape
122/125: seq[0][:,0]
122/126: seq[1][:,0]
122/127: seq[100][:,0]
122/128: seq[2][:,0]
122/129: len(seq)
122/130: seq[95][:,0]
122/131:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    

    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def get_loaders(self):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months

            for _,grp in groups:
                n_rows = grp.shape[0]
                if n_rows<min_rows_reqd: continue
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    df = grp.iloc[i:i+self.seq_len]
                    df.drop(['AUTO_ID'], axis=1, inplace=True)
                    x = df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = df[self.col_dict['missing']].values
                    t = df[self.col_dict['delta']].values
                    x=x.values
    #               (seq_len x input_size)
                    tmp_seq = [x[:,1:], m, t, x_obs]
                    seq.append(tmp_seq)
                    labels.append(grp.iloc[i+min_rows_reqd-1].values)    
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        imputed_dataset = du.Imputer().forward(dataset.head(1000))  
        self.sequences, self.labels = load(imputed_dataset)
        tvt_partition = seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
122/132:
dl = IBDDataLoader()
train, valid, test = dl.get_loaders()
122/133:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    

    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def get_loaders(self):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months

            for _,grp in groups:
                n_rows = grp.shape[0]
                if n_rows<min_rows_reqd: continue
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    df = grp.iloc[i:i+self.seq_len]
                    x = df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = df[self.col_dict['missing']].values
                    t = df[self.col_dict['delta']].values
                    x=x.values
    #               (seq_len x input_size)
                    tmp_seq = [x[:,1:], m, t, x_obs]
                    seq.append(tmp_seq)
                    labels.append(grp.iloc[i+min_rows_reqd-1].values)    
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        imputed_dataset = du.Imputer().forward(dataset.head(1000))  
        self.sequences, self.labels = load(imputed_dataset)
        tvt_partition = seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
122/134:
dl = IBDDataLoader()
train, valid, test = dl.get_loaders()
122/135:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    

    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def get_loaders(self):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months

            for _,grp in groups:
                n_rows = grp.shape[0]
                if n_rows<min_rows_reqd: continue
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    df = grp.iloc[i:i+self.seq_len]
                    x = df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = df[self.col_dict['missing']].values
                    t = df[self.col_dict['delta']].values
                    x=x.values
    #               (seq_len x input_size)
                    tmp_seq = [x[:,1:], m, t, x_obs]
                    seq.append(tmp_seq)
                    labels.append(grp.iloc[i+min_rows_reqd-1].values)    
            return seq, labels
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        imputed_dataset = du.Imputer().forward(dataset.head(1000))  
        self.sequences, self.labels = load(imputed_dataset)
        tvt_partition = seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
122/136:
dl = IBDDataLoader()
train, valid, test = dl.get_loaders()
122/137:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    

    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def get_loaders(self):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months

            for _,grp in groups:
                n_rows = grp.shape[0]
                if n_rows<min_rows_reqd: continue
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    df = grp.iloc[i:i+self.seq_len]
                    x = df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = df[self.col_dict['missing']].values
                    t = df[self.col_dict['delta']].values
                    x=x.values
    #               (seq_len x input_size)
                    tmp_seq = [x[:,1:], m, t, x_obs]
                    seq.append(tmp_seq)
                    labels.append(grp.iloc[i+min_rows_reqd-1].values)    
            return seq, labels
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        imputed_dataset = du.Imputer().forward(dataset.head(1000))  
        self.sequences, self.labels = load(imputed_dataset)
        return self.sequences, self.labels
        tvt_partition = seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
122/138:
dl = IBDDataLoader()
# train, valid, test = dl.get_loaders()
s,l=dl.get_loaders()
122/139: s
122/140: s[0]
122/141: s[0][0]
122/142: s[0][0].shape
122/143: s[0][1].shape
122/144: s[0][2].shape
122/145: s[0][3].shape
122/146:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    

    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def get_loaders(self):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months

            for _,grp in groups:
                n_rows = grp.shape[0]
                if n_rows<min_rows_reqd: continue
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    df = grp.iloc[i:i+self.seq_len]
                    x = df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = df[self.col_dict['missing']].values
                    t = df[self.col_dict['delta']].values
                    x=x.values
                    tmp_seq = [x[:,1:], m, t, x_obs[:,1:]] #strip AUTO_ID
                    seq.append(tmp_seq)
                    labels.append(grp.iloc[i+min_rows_reqd-1].values)    
            return seq, labels
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        imputed_dataset = du.Imputer().forward(dataset.head(1000))  
        self.sequences, self.labels = load(imputed_dataset)
        return self.sequences, self.labels
        tvt_partition = seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
122/147: len(s)
122/148: np.ndarray(s).shape
122/149: np.array(s)
122/150: s[0,2]
122/151: np.array([s[0],s[2]]).shape
122/152:
# np.array([s[0],s[2]]).shape
s[0]
122/153:
# np.array([s[0],s[2]]).shape
len(s[0])
122/154:
# np.array([s[0],s[2]]).shape
len(s[1])
122/155:
# np.array([s[0],s[2]]).shape
len(s)
122/156:
# np.array([s[0],s[2]]).shape
len(s[0])
122/157:
# np.array([s[0],s[2]]).shape
len(s[3])
122/158:
# np.array([s[0],s[2]]).shape
len(s[0][0])
122/159:
# np.array([s[0],s[2]]).shape
len(s[0][3])
122/160:
# np.array([s[0],s[2]]).shape
len(s[0][0][0])
122/161:
# np.array([s[0],s[2]]).shape
len(s[0][0][1])
122/162:
# np.array([s[0],s[2]]).shape
len(s[0][0][2])
122/163:
# np.array([s[0],s[2]]).shape
len(s[0][0][3])
122/164:
# np.array([s[0],s[2]]).shape
s[0][0][3]
122/165:
# np.array([s[0],s[2]]).shape
s[0][0][0]
122/166:
# np.array([s[0],s[2]]).shape
s[0][0][1]
122/167:
# np.array([s[0],s[2]]).shape
s[1][0][1]
122/168:
# np.array([s[0],s[2]]).shape
s[2][0][1]
122/169:
# np.array([s[0],s[2]]).shape
# [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
len(s[0])
122/170:
# np.array([s[0],s[2]]).shape
# [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
len(s)
122/171:
# np.array([s[0],s[2]]).shape
# [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
len(s[0])
122/172:
# np.array([s[0],s[2]]).shape
# [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
len(s[0][0])
122/173:
# np.array([s[0],s[2]]).shape
# [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
len(s[0][0][0])
122/174:
# np.array([s[0],s[2]]).shape
# [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
len(s[1][0][0])
122/175:
# np.array([s[0],s[2]]).shape
# [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
len(s[5][0][0])
122/176:
# np.array([s[0],s[2]]).shape
# [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
len(s)
122/177:
# np.array([s[0],s[2]]).shape
# [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
s[0]
122/178:
# np.array([s[0],s[2]]).shape
# [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
len(s[0])
122/179:
# np.array([s[0],s[2]]).shape
# [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
s[0][1][0]
122/180:
# np.array([s[0],s[2]]).shape
# [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
len(s[0][1][0])
122/181:
# np.array([s[0],s[2]]).shape
# [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
len(s[0][1][1])
122/182:
# np.array([s[0],s[2]]).shape
# [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
len(s[0][1][3])
122/183:
# np.array([s[0],s[2]]).shape
# [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
len(s[0][3][0])
122/184:
# np.array([s[0],s[2]]).shape
# [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
len(s[0][1][0])
122/185:
grp = imputed_dataset[imputed_dataset.AUTO_ID==0]
n_rows = grp.shape[0]
if n_rows<min_rows_reqd: continue
i=0
df = grp.iloc[i:i+self.seq_len]
x = df[self.col_dict['input']]
x_obs = x.shift(1).fillna(x.iloc[0])
m = df[self.col_dict['missing']]
t = df[self.col_dict['delta']]
# x=x.values
# tmp_seq = [x[:,1:], m, t, x_obs[:,1:]] #strip AUTO_ID
# seq.append(tmp_seq)
# labels.append(grp.iloc[i+min_rows_reqd-1].values)
122/186:
grp = imputed_dataset[imputed_dataset.AUTO_ID==0]
n_rows = grp.shape[0]

i=0
df = grp.iloc[i:i+self.seq_len]
x = df[self.col_dict['input']]
x_obs = x.shift(1).fillna(x.iloc[0])
m = df[self.col_dict['missing']]
t = df[self.col_dict['delta']]
# x=x.values
# tmp_seq = [x[:,1:], m, t, x_obs[:,1:]] #strip AUTO_ID
# seq.append(tmp_seq)
# labels.append(grp.iloc[i+min_rows_reqd-1].values)
122/187:
grp = imputed_dataset[imputed_dataset.AUTO_ID==0]
n_rows = grp.shape[0]

i=0
df = grp.iloc[i:i+36]
x = df[col_dict['input']]
x_obs = x.shift(1).fillna(x.iloc[0])
m = df[col_dict['missing']]
t = df[col_dict['delta']]
# x=x.values
# tmp_seq = [x[:,1:], m, t, x_obs[:,1:]] #strip AUTO_ID
# seq.append(tmp_seq)
# labels.append(grp.iloc[i+min_rows_reqd-1].values)
122/188: x.columns
122/189: m.columns
122/190: x.columns
122/191: m.columns
122/192: t.columns
122/193: x_obs.columns
122/194:
def load_irregular_sequence_(exclude_ct=True):
    if exclude_ct:
        return pd.read_csv(cfg.opj(cfg.LONG_OUT, 'noCT.csv'))
    return pd.read_csv(cfg.opj(cfg.LONG_OUT, 'all.csv'))

def apply_monthly_timestamp_(df): 
    df['tmp_ts'] = (df.RESULT_YEAR.astype(str)+df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int)
    t0_indices = df.groupby('AUTO_ID').tmp_ts.idxmin()     # find earliest row
    t0_df = df.loc[t0_indices][['AUTO_ID','RESULT_YEAR','RESULT_MONTH']]
    t0_df.columns = ['AUTO_ID', 'tmp_minY', 'tmp_minMo']
    # add tmp columns to df for vectorized calculation of delta in months
    timestamped_df = df.merge(t0_df, how='left', on='AUTO_ID')
    timestamped_df['MONTH_TS'] = (timestamped_df['RESULT_YEAR']-timestamped_df['tmp_minY'])*12 + timestamped_df['RESULT_MONTH']-timestamped_df['tmp_minMo']
    timestamped_df = timestamped_df[[x for x in timestamped_df.columns if x[:3]!='tmp']]
    return timestamped_df

def pad_missing_months_(timestamped_df):
    def pad(df):
        df = df.reset_index()
        new_index = pd.MultiIndex.from_product([[df.AUTO_ID.values[0]], list(range(0, df.MONTH_TS.max()+1))], names=['AUTO_ID', 'MONTH_TS'])
        df = df.set_index(['AUTO_ID', 'MONTH_TS']).reindex(new_index).reset_index()
        return df
    padded = timestamped_df.groupby('AUTO_ID').apply(pad)
    padded.index = range(0, padded.shape[0])
#         padded['PADDED'] = padded.RESULT_YEAR.isnull().astype(int)
    padded.drop(['index', 'RESULT_YEAR', 'RESULT_MONTH'], axis=1, inplace=True)
    return padded

def missingness_indicators(padded):
    missing_mask = padded.copy()
    missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
    missing_delta = missing_mask.copy()

    time_interval = 1 #1 month with padding. For without padding, maintain PADDED col, then filter on that.
    missing_delta['tmp_delta'] = time_interval
    for observed in cols_to_impute:
        missing_delta['obs_delays'] = missing_delta.groupby('AUTO_ID')[observed].cumsum()
        missing_delta[observed] = missing_delta.groupby(['AUTO_ID', 'obs_delays'])['tmp_delta'].cumsum()
    missing_delta = missing_delta.drop(['tmp_delta', 'obs_delays'], axis=1)

    missing_mask.columns = ['AUTO_ID', 'MONTH_TS']+[f'MISSING_{x}' for x in cols_to_impute]
    missing_delta.columns = ['AUTO_ID', 'MONTH_TS']+[f'DELTA_{x}' for x in cols_to_impute]
    return missing_mask.drop(['AUTO_ID'],axis=1), missing_delta.drop(['AUTO_ID'],axis=1)
122/195:
df = load_irregular_sequence_(exclude_ct)
df = apply_monthly_timestamp_(df.head(2000)))
df = pad_missing_months_(df)
m, t = missingness_indicators(df)
122/196:
df = load_irregular_sequence_(exclude_ct)
df = apply_monthly_timestamp_(df.head(2000))
df = pad_missing_months_(df)
m, t = missingness_indicators(df)
122/197:
df = load_irregular_sequence_()
df = apply_monthly_timestamp_(df.head(2000))
df = pad_missing_months_(df)
m, t = missingness_indicators(df)
122/198:
import pandas as pd
import config as cfg
import matplotlib.pyplot as plt
import pickle

def load_irregular_sequence_(exclude_ct=True):
    if exclude_ct:
        return pd.read_csv(cfg.opj(cfg.LONG_OUT, 'noCT.csv'))
    return pd.read_csv(cfg.opj(cfg.LONG_OUT, 'all.csv'))

def apply_monthly_timestamp_(df): 
    df['tmp_ts'] = (df.RESULT_YEAR.astype(str)+df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int)
    t0_indices = df.groupby('AUTO_ID').tmp_ts.idxmin()     # find earliest row
    t0_df = df.loc[t0_indices][['AUTO_ID','RESULT_YEAR','RESULT_MONTH']]
    t0_df.columns = ['AUTO_ID', 'tmp_minY', 'tmp_minMo']
    # add tmp columns to df for vectorized calculation of delta in months
    timestamped_df = df.merge(t0_df, how='left', on='AUTO_ID')
    timestamped_df['MONTH_TS'] = (timestamped_df['RESULT_YEAR']-timestamped_df['tmp_minY'])*12 + timestamped_df['RESULT_MONTH']-timestamped_df['tmp_minMo']
    timestamped_df = timestamped_df[[x for x in timestamped_df.columns if x[:3]!='tmp']]
    return timestamped_df

def pad_missing_months_(timestamped_df):
    def pad(df):
        df = df.reset_index()
        new_index = pd.MultiIndex.from_product([[df.AUTO_ID.values[0]], list(range(0, df.MONTH_TS.max()+1))], names=['AUTO_ID', 'MONTH_TS'])
        df = df.set_index(['AUTO_ID', 'MONTH_TS']).reindex(new_index).reset_index()
        return df
    padded = timestamped_df.groupby('AUTO_ID').apply(pad)
    padded.index = range(0, padded.shape[0])
#         padded['PADDED'] = padded.RESULT_YEAR.isnull().astype(int)
    padded.drop(['index', 'RESULT_YEAR', 'RESULT_MONTH'], axis=1, inplace=True)
    return padded

def missingness_indicators(padded):
    missing_mask = padded.copy()
    missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
    missing_delta = missing_mask.copy()

    time_interval = 1 #1 month with padding. For without padding, maintain PADDED col, then filter on that.
    missing_delta['tmp_delta'] = time_interval
    for observed in cols_to_impute:
        missing_delta['obs_delays'] = missing_delta.groupby('AUTO_ID')[observed].cumsum()
        missing_delta[observed] = missing_delta.groupby(['AUTO_ID', 'obs_delays'])['tmp_delta'].cumsum()
    missing_delta = missing_delta.drop(['tmp_delta', 'obs_delays'], axis=1)

    missing_mask.columns = ['AUTO_ID', 'MONTH_TS']+[f'MISSING_{x}' for x in cols_to_impute]
    missing_delta.columns = ['AUTO_ID', 'MONTH_TS']+[f'DELTA_{x}' for x in cols_to_impute]
    return missing_mask.drop(['AUTO_ID'],axis=1), missing_delta.drop(['AUTO_ID'],axis=1)
122/199:
df = load_irregular_sequence_()
df = apply_monthly_timestamp_(df.head(2000))
df = pad_missing_months_(df)
m, t = missingness_indicators(df)
122/200:
import pandas as pd
import config as cfg
import matplotlib.pyplot as plt
import pickle

def load_irregular_sequence_(exclude_ct=True):
    if exclude_ct:
        return pd.read_csv(cfg.opj(cfg.LONG_OUT, 'noCT.csv'))
    return pd.read_csv(cfg.opj(cfg.LONG_OUT, 'all.csv'))

def apply_monthly_timestamp_(df): 
    df['tmp_ts'] = (df.RESULT_YEAR.astype(str)+df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int)
    t0_indices = df.groupby('AUTO_ID').tmp_ts.idxmin()     # find earliest row
    t0_df = df.loc[t0_indices][['AUTO_ID','RESULT_YEAR','RESULT_MONTH']]
    t0_df.columns = ['AUTO_ID', 'tmp_minY', 'tmp_minMo']
    # add tmp columns to df for vectorized calculation of delta in months
    timestamped_df = df.merge(t0_df, how='left', on='AUTO_ID')
    timestamped_df['MONTH_TS'] = (timestamped_df['RESULT_YEAR']-timestamped_df['tmp_minY'])*12 + timestamped_df['RESULT_MONTH']-timestamped_df['tmp_minMo']
    timestamped_df = timestamped_df[[x for x in timestamped_df.columns if x[:3]!='tmp']]
    return timestamped_df

def pad_missing_months_(timestamped_df):
    def pad(df):
        df = df.reset_index()
        new_index = pd.MultiIndex.from_product([[df.AUTO_ID.values[0]], list(range(0, df.MONTH_TS.max()+1))], names=['AUTO_ID', 'MONTH_TS'])
        df = df.set_index(['AUTO_ID', 'MONTH_TS']).reindex(new_index).reset_index()
        return df
    padded = timestamped_df.groupby('AUTO_ID').apply(pad)
    padded.index = range(0, padded.shape[0])
#         padded['PADDED'] = padded.RESULT_YEAR.isnull().astype(int)
    padded.drop(['index', 'RESULT_YEAR', 'RESULT_MONTH'], axis=1, inplace=True)
    return padded

def missingness_indicators(padded):
    missing_mask = padded.copy()
    missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
    missing_delta = missing_mask.copy()

    time_interval = 1 #1 month with padding. For without padding, maintain PADDED col, then filter on that.
    missing_delta['tmp_delta'] = time_interval
    for observed in missing_delta.columns:
        missing_delta['obs_delays'] = missing_delta.groupby('AUTO_ID')[observed].cumsum()
        missing_delta[observed] = missing_delta.groupby(['AUTO_ID', 'obs_delays'])['tmp_delta'].cumsum()
    missing_delta = missing_delta.drop(['tmp_delta', 'obs_delays'], axis=1)

    missing_mask.columns = ['AUTO_ID', 'MONTH_TS']+[f'MISSING_{x}' for x in cols_to_impute]
    missing_delta.columns = ['AUTO_ID', 'MONTH_TS']+[f'DELTA_{x}' for x in cols_to_impute]
    return missing_mask.drop(['AUTO_ID'],axis=1), missing_delta.drop(['AUTO_ID'],axis=1)
122/201:
# df = load_irregular_sequence_()
# df = apply_monthly_timestamp_(df.head(2000))
# df = pad_missing_months_(df)
m, t = missingness_indicators(df)
122/202:
import pandas as pd
import config as cfg
import matplotlib.pyplot as plt
import pickle

def load_irregular_sequence_(exclude_ct=True):
    if exclude_ct:
        return pd.read_csv(cfg.opj(cfg.LONG_OUT, 'noCT.csv'))
    return pd.read_csv(cfg.opj(cfg.LONG_OUT, 'all.csv'))

def apply_monthly_timestamp_(df): 
    df['tmp_ts'] = (df.RESULT_YEAR.astype(str)+df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int)
    t0_indices = df.groupby('AUTO_ID').tmp_ts.idxmin()     # find earliest row
    t0_df = df.loc[t0_indices][['AUTO_ID','RESULT_YEAR','RESULT_MONTH']]
    t0_df.columns = ['AUTO_ID', 'tmp_minY', 'tmp_minMo']
    # add tmp columns to df for vectorized calculation of delta in months
    timestamped_df = df.merge(t0_df, how='left', on='AUTO_ID')
    timestamped_df['MONTH_TS'] = (timestamped_df['RESULT_YEAR']-timestamped_df['tmp_minY'])*12 + timestamped_df['RESULT_MONTH']-timestamped_df['tmp_minMo']
    timestamped_df = timestamped_df[[x for x in timestamped_df.columns if x[:3]!='tmp']]
    return timestamped_df

def pad_missing_months_(timestamped_df):
    def pad(df):
        df = df.reset_index()
        new_index = pd.MultiIndex.from_product([[df.AUTO_ID.values[0]], list(range(0, df.MONTH_TS.max()+1))], names=['AUTO_ID', 'MONTH_TS'])
        df = df.set_index(['AUTO_ID', 'MONTH_TS']).reindex(new_index).reset_index()
        return df
    padded = timestamped_df.groupby('AUTO_ID').apply(pad)
    padded.index = range(0, padded.shape[0])
#         padded['PADDED'] = padded.RESULT_YEAR.isnull().astype(int)
    padded.drop(['index', 'RESULT_YEAR', 'RESULT_MONTH'], axis=1, inplace=True)
    return padded

def missingness_indicators(padded):
    missing_mask = padded.copy()
    missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
    missing_delta = missing_mask.copy()

    time_interval = 1 #1 month with padding. For without padding, maintain PADDED col, then filter on that.
    missing_delta['tmp_delta'] = time_interval
    for observed in missing_delta.columns:
        missing_delta['obs_delays'] = missing_delta.groupby('AUTO_ID')[observed].cumsum()
        missing_delta[observed] = missing_delta.groupby(['AUTO_ID', 'obs_delays'])['tmp_delta'].cumsum()
    missing_delta = missing_delta.drop(['tmp_delta', 'obs_delays'], axis=1)

    missing_mask.columns = ['AUTO_ID', 'MONTH_TS']+[f'MISSING_{x}' for x in missing_mask.columns[2:]]
    missing_delta.columns = ['AUTO_ID', 'MONTH_TS']+[f'DELTA_{x}' for x in  missing_mask.columns[2:]]
    return missing_mask.drop(['AUTO_ID'],axis=1), missing_delta.drop(['AUTO_ID'],axis=1)
122/203:
# df = load_irregular_sequence_()
# df = apply_monthly_timestamp_(df.head(2000))
# df = pad_missing_months_(df)
m, t = missingness_indicators(df)
122/204: m.columns
122/205:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    

    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def get_loaders(self):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months

            for _,grp in groups:
                n_rows = grp.shape[0]
                if n_rows<min_rows_reqd: continue
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    df = grp.iloc[i:i+self.seq_len]
                    x = df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = df[self.col_dict['missing']].values
                    t = df[self.col_dict['delta']].values
                    x=x.values
                    tmp_seq = [x[:,1:], m, t, x_obs[:,1:]] #strip AUTO_ID
                    seq.append(tmp_seq)
                    labels.append(grp.iloc[i+min_rows_reqd-1].values)    
            return seq, labels
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        imputed_dataset = du.Imputer().forward(dataset.head(1000))  
        self.sequences, self.labels = load(imputed_dataset)
        return self.sequences, self.labels
        tvt_partition = seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
115/1:
import data_utils as du
du.X_raw()
115/2:
import data_utils as du
du.X_raw(False)
115/3:
import data_utils as du
df, cd = du.X_raw(False)
115/4:
# import data_utils as du
# df, cd = du.X_raw(False)
df
122/206:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    

    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def get_loaders(self):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months

            for _,grp in groups:
                n_rows = grp.shape[0]
                if n_rows<min_rows_reqd: continue
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    df = grp.iloc[i:i+self.seq_len]
                    x = df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = df[self.col_dict['missing']].values
                    t = df[self.col_dict['delta']].values
                    x=x.values
                    tmp_seq = [x[:,1:], m, t, x_obs[:,1:]] #strip AUTO_ID
                    seq.append(tmp_seq)
                    labels.append(grp.iloc[i+min_rows_reqd-1].values)    
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        imputed_dataset = du.Imputer().forward(dataset.head(1000))  
        self.sequences, self.labels = load(imputed_dataset)
        return self.sequences, self.labels
        tvt_partition = seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
122/207:
dl = IBDDataLoader()
# train, valid, test = dl.get_loaders()
# X, y =dl.get_loaders()
print(X.shape)
122/208:
dl = IBDDataLoader()
train, valid, test = dl.get_loaders()
115/5:
# import data_utils as du
# df, cd = du.X_raw(False)
df.shape
115/6:
# import data_utils as du
# df, cd = du.X_raw(False)
df.shape
142-(47*2)
123/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
import data_utils as du
import math

import matplotlib.pyplot as plt
%matplotlib inline
123/2:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    

    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def get_loaders(self):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months

            for _,grp in groups:
                n_rows = grp.shape[0]
                if n_rows<min_rows_reqd: continue
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    df = grp.iloc[i:i+self.seq_len]
                    x = df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = df[self.col_dict['missing']].values
                    t = df[self.col_dict['delta']].values
                    x=x.values
                    tmp_seq = [x[:,1:], m, t, x_obs[:,1:]] #strip AUTO_ID
                    seq.append(tmp_seq)
                    labels.append(grp.iloc[i+min_rows_reqd-1].values)    
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        imputed_dataset = du.Imputer().forward(dataset.head(1000))  
        self.sequences, self.labels = load(imputed_dataset)
        return self.sequences, self.labels
        tvt_partition = seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
123/3:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def get_loaders(self):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months

            for _,grp in groups:
                n_rows = grp.shape[0]
                if n_rows<min_rows_reqd: continue
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    df = grp.iloc[i:i+self.seq_len]
                    x = df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = df[self.col_dict['missing']].values
                    t = df[self.col_dict['delta']].values
                    x=x.values
                    tmp_seq = [x[:,1:], m, t, x_obs[:,1:]] #strip AUTO_ID
                    seq.append(tmp_seq)
                    labels.append(grp.iloc[i+min_rows_reqd-1].values)    
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        imputed_dataset = du.Imputer().forward(dataset.head(1000))  
        self.sequences, self.labels = load(imputed_dataset)
        return self.sequences, self.labels
        tvt_partition = seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
123/4:
dl = IBDDataLoader()
train, valid, test = dl.get_loaders()
123/5:
df, _ = du.X_raw()
df.columns
123/6:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def get_loaders(self):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months

            for _,grp in groups:
                n_rows = grp.shape[0]
                if n_rows<min_rows_reqd: continue
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    df = grp.iloc[i:i+self.seq_len]
                    x = df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = df[self.col_dict['missing']].values
                    t = df[self.col_dict['delta']].values
                    x=x.values
                    tmp_seq = [x[:,1:], m, t, x_obs[:,1:]] #strip AUTO_ID
                    seq.append(tmp_seq)
                    labels.append(grp.iloc[i+min_rows_reqd-1].values)    
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        imputed_dataset = du.Imputer().forward(dataset.head(1000))  
        self.sequences, self.labels = load(imputed_dataset)
        return self.sequences, self.labels
        tvt_partition = seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
123/7:
dl = IBDDataLoader()
train, valid, test = dl.get_loaders()
124/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
import data_utils as du
import math

import matplotlib.pyplot as plt
%matplotlib inline
124/2:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def get_loaders(self):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months

            for _,grp in groups:
                n_rows = grp.shape[0]
                if n_rows<min_rows_reqd: continue
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    df = grp.iloc[i:i+self.seq_len]
                    x = df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = df[self.col_dict['missing']].values
                    t = df[self.col_dict['delta']].values
                    x=x.values
                    tmp_seq = [x[:,1:], m, t, x_obs[:,1:]] #strip AUTO_ID
                    seq.append(tmp_seq)
                    labels.append(grp.iloc[i+min_rows_reqd-1].values)    
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        imputed_dataset = du.Imputer().forward(dataset.head(1000))  
        self.sequences, self.labels = load(imputed_dataset)
        return self.sequences, self.labels
        tvt_partition = seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
124/3:
dl = IBDDataLoader()
train, valid, test = dl.get_loaders()
124/4:
df, _ = du.X_raw()
df.loc[df.]
124/5:
df, _ = du.X_raw()
df.loc[df.MONTH_TS==0,:]
124/6:
df, _ = du.X_raw()
df.loc[df.MONTH_TS==0]
124/7:
df, _ = du.X_raw()
df.MONTH_TS==0
125/1:
import data_utils as du
du.X_raw(False)
126/1:
df, _ = du.X_raw()
df.MONTH_TS==0
126/2:
import torch
import torch.nn as nn
import torch.nn.functional as F
import data_utils as du
import math

import matplotlib.pyplot as plt
%matplotlib inline
126/3:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def get_loaders(self):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months

            for _,grp in groups:
                n_rows = grp.shape[0]
                if n_rows<min_rows_reqd: continue
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    df = grp.iloc[i:i+self.seq_len]
                    x = df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = df[self.col_dict['missing']].values
                    t = df[self.col_dict['delta']].values
                    x=x.values
                    tmp_seq = [x[:,1:], m, t, x_obs[:,1:]] #strip AUTO_ID
                    seq.append(tmp_seq)
                    labels.append(grp.iloc[i+min_rows_reqd-1].values)    
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        imputed_dataset = du.Imputer().forward(dataset.head(1000))  
        self.sequences, self.labels = load(imputed_dataset)
        return self.sequences, self.labels
        tvt_partition = seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
126/4:
df, _ = du.X_raw()
df.MONTH_TS==0
126/5:
df, _ = du.X_raw()
df.loc[df.MONTH_TS==0, :]
126/6:
dl = IBDDataLoader()
train, valid, test = dl.get_loaders()
126/7:
dl = IBDDataLoader()
train, valid, test = dl.get_loaders()
126/8:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def get_loaders(self):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months

            for _,grp in groups:
                n_rows = grp.shape[0]
                if n_rows<min_rows_reqd: continue
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    df = grp.iloc[i:i+self.seq_len]
                    x = df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = df[self.col_dict['missing']].values
                    t = df[self.col_dict['delta']].values
                    x=x.values
                    tmp_seq = [x[:,1:], m, t, x_obs[:,1:]] #strip AUTO_ID
                    seq.append(tmp_seq)
                    labels.append(grp.iloc[i+min_rows_reqd-1].values)    
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        imputed_dataset = du.Imputer().forward(dataset)  
        self.sequences, self.labels = load(imputed_dataset)
        tvt_partition = seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
126/9:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def get_loaders(self):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months

            for _,grp in groups:
                n_rows = grp.shape[0]
                if n_rows<min_rows_reqd: continue
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    df = grp.iloc[i:i+self.seq_len]
                    x = df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = df[self.col_dict['missing']].values
                    t = df[self.col_dict['delta']].values
                    x=x.values
                    tmp_seq = [x[:,1:], m, t, x_obs[:,1:]] #strip AUTO_ID
                    seq.append(tmp_seq)
                    labels.append(grp.iloc[i+min_rows_reqd-1].values)    
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        imputed_dataset = du.Imputer().forward(dataset)  
        self.sequences, self.labels = load(imputed_dataset)
        tvt_partition = seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
126/10:
dl = IBDDataLoader()
train, valid, test = dl.get_loaders()
126/11:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months

            for _,grp in groups:
                n_rows = grp.shape[0]
                if n_rows<min_rows_reqd: continue
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    df = grp.iloc[i:i+self.seq_len]
                    x = df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = df[self.col_dict['missing']].values
                    t = df[self.col_dict['delta']].values
                    x=x.values
                    tmp_seq = [x[:,1:], m, t, x_obs[:,1:]] #strip AUTO_ID
                    seq.append(tmp_seq)
                    labels.append(grp.iloc[i+min_rows_reqd-1].values)    
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        imputed_dataset = du.Imputer().forward(dataset)  
        self.sequences, self.labels = load(imputed_dataset)
        
    def TVT_loaders(self):
        if not self.sequences:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
126/12:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months

            for _,grp in groups:
                n_rows = grp.shape[0]
                if n_rows<min_rows_reqd: continue
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    df = grp.iloc[i:i+self.seq_len]
                    x = df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = df[self.col_dict['missing']].values
                    t = df[self.col_dict['delta']].values
                    x=x.values
                    tmp_seq = [x[:,1:], m, t, x_obs[:,1:]] #strip AUTO_ID
                    seq.append(tmp_seq)
                    labels.append(grp.iloc[i+min_rows_reqd-1].values)    
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        imputed_dataset = du.Imputer().forward(dataset)  
        self.sequences, self.labels = load(imputed_dataset)
        
    def TVT_loaders(self):
        if not self.sequences:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
126/13:
dl = IBDDataLoader()
train, valid, test = dl.get_loaders()
126/14:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months

            for _,grp in groups:
                n_rows = grp.shape[0]
                if n_rows<min_rows_reqd: continue
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    df = grp.iloc[i:i+self.seq_len]
                    x = df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = df[self.col_dict['missing']].values
                    t = df[self.col_dict['delta']].values
                    x=x.values
                    tmp_seq = [x[:,1:], m, t, x_obs[:,1:]] #strip AUTO_ID
                    seq.append(tmp_seq)
                    labels.append(grp.iloc[i+min_rows_reqd-1].values)    
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        imputed_dataset = du.Imputer().forward(dataset)  
        self.sequences, self.labels = load(imputed_dataset)
        
    def TVT_loaders(self):
        if not self.sequences:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
126/15:
dl = IBDDataLoader()
dl.fit_data()
train, valid, test = dl.TVT_loaders()
126/16:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months
            
            nb_tot_grp = len(groups)
            processed=0
            skipped = 0
            cumseq=0
            
            for _,grp in groups:
                n_rows = grp.shape[0]
                if n_rows<min_rows_reqd: 
                    skipped+=1
                    continue
                processed+=1
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    df = grp.iloc[i:i+self.seq_len]
                    x = df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = df[self.col_dict['missing']].values
                    t = df[self.col_dict['delta']].values
                    x=x.values
                    tmp_seq = [x[:,1:], m, t, x_obs[:,1:]] #strip AUTO_ID
                    seq.append(tmp_seq)
                    labels.append(grp.iloc[i+min_rows_reqd-1].values)    
                    cumseq+=1
                if processed%10==0:
                    print(f"Patients processed: {processed}/{nb_tot_grp} | Skipped:{skipped} | Average seq per patient: {cumseq/10}")
                    cumseq=0
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        imputed_dataset = du.Imputer().forward(dataset)  
        self.sequences, self.labels = load(imputed_dataset)
        
    def TVT_loaders(self):
        if not self.sequences:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
126/17:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months
            
            nb_tot_grp = len(groups)
            processed=0
            skipped = 0
            cumseq=0
            
            for _,grp in groups:
                n_rows = grp.shape[0]
                if n_rows<min_rows_reqd: 
                    skipped+=1
                    continue
                processed+=1
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    df = grp.iloc[i:i+self.seq_len]
                    x = df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = df[self.col_dict['missing']].values
                    t = df[self.col_dict['delta']].values
                    x=x.values
                    tmp_seq = [x[:,1:], m, t, x_obs[:,1:]] #strip AUTO_ID
                    seq.append(tmp_seq)
                    labels.append(grp.iloc[i+min_rows_reqd-1].values)    
                    cumseq+=1
                if processed%10==0:
                    print(f"Patients processed: {processed}/{nb_tot_grp} | Skipped:{skipped} | Average seq per patient: {cumseq/10}")
                    cumseq=0
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample:
            dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().forward(dataset)  
        self.sequences, self.labels = load(imputed_dataset)
        
    def TVT_loaders(self):
        if not self.sequences:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
126/18:
dl = IBDDataLoader()
dl.fit_data(10000)
train, valid, test = dl.TVT_loaders()
126/19:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months
            
            nb_tot_grp = len(groups)
            processed=0
            skipped = 0
            cumseq=0
            
            for _,grp in groups:
                n_rows = grp.shape[0]
                if n_rows<min_rows_reqd: 
                    skipped+=1
                    continue
                processed+=1
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    df = grp.iloc[i:i+self.seq_len]
                    x = df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = df[self.col_dict['missing']].values
                    t = df[self.col_dict['delta']].values
                    x=x.values
                    tmp_seq = [x[:,1:], m, t, x_obs[:,1:]] #strip AUTO_ID
                    seq.append(tmp_seq)
                    labels.append(grp.iloc[i+min_rows_reqd-1].values)    
                    cumseq+=1
                if processed%10==0:
                    print(f"Patients processed: {processed}/{nb_tot_grp} | Skipped:{skipped} | Average seq per patient: {cumseq/10}")
                    cumseq=0
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample:
            dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().forward(dataset)  
        self.sequences, self.labels = load(imputed_dataset)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
126/20: dl.labels[0]
126/21: dl.seq[0]
126/22: dl.sequences[0]
126/23: dl.sequences[0][0].sha[e]
126/24: dl.sequences[0][0].shape
126/25: dl.sequences[0][0]
126/26: dl.labels
126/27: dl.labels.shape
126/28: dl.labels[0]
126/29: dl.labels[0].shape
126/30: dl.labels[0]
126/31: df.columns
126/32: df.iloc[3,'AUTO_ID']
126/33: df.iloc[3]['AUTO_ID']
126/34:
import torch
import torch.nn as nn
import torch.nn.functional as F
import data_utils as du
import math

import matplotlib.pyplot as plt
%matplotlib inline
127/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
import data_utils as du
import math

import matplotlib.pyplot as plt
%matplotlib inline
127/2:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None, target_col='TGT_gt100k'):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months
            
            nb_tot_grp = len(groups)
            processed=0
            skipped = 0
            cumseq=0
            
            for _,grp in groups:
                n_rows = grp.shape[0]
                
                # Skip patients who have lesser number of months
                if n_rows<min_rows_reqd: 
                    skipped+=1
                    continue
                
                processed+=1
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    input_df = grp.iloc[i:i+self.seq_len]
                    target_df = grp.iloc[i+min_rows_reqd-1][target_col]
                    
                    x = input_df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = input_df[self.col_dict['missing']].values
                    t = input_df[self.col_dict['delta']].values
                    x=x.values
                    seq.append( [x[:,1:], m, t, x_obs[:,1:]] ) #strip AUTO_ID
                    labels.append(target_df.values)    
                    cumseq+=1
                if processed%10==0:
                    print(f"Patients processed: {processed}/{nb_tot_grp} | Skipped:{skipped} | Average seq per patient: {cumseq/10}")
                    cumseq=0
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample:
            dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().forward(dataset)  
        self.sequences, self.labels = load(imputed_dataset)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
127/3:
dl = IBDDataLoader()
dl.fit_data(10000)
train, valid, test = dl.TVT_loaders()
127/4:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None, target_col='TGT_CUMCHARGES'):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months
            
            nb_tot_grp = len(groups)
            processed=0
            skipped = 0
            cumseq=0
            
            for _,grp in groups:
                n_rows = grp.shape[0]
                
                # Skip patients who have lesser number of months
                if n_rows<min_rows_reqd: 
                    skipped+=1
                    continue
                
                processed+=1
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    input_df = grp.iloc[i:i+self.seq_len]
                    target_df = grp.iloc[i+min_rows_reqd-1][target_col]
                    
                    x = input_df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = input_df[self.col_dict['missing']].values
                    t = input_df[self.col_dict['delta']].values
                    x=x.values
                    seq.append( [x[:,1:], m, t, x_obs[:,1:]] ) #strip AUTO_ID
                    labels.append(target_df.values)    
                    cumseq+=1
                if processed%10==0:
                    print(f"Patients processed: {processed}/{nb_tot_grp} | Skipped:{skipped} | Average seq per patient: {cumseq/10}")
                    cumseq=0
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample:
            dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().forward(dataset)  
        self.sequences, self.labels = load(imputed_dataset)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
127/5:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None, target_col='TGT_CUMCHARGES'):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months
            
            nb_tot_grp = len(groups)
            processed=0
            skipped = 0
            cumseq=0
            
            for _,grp in groups:
                n_rows = grp.shape[0]
                
                # Skip patients who have lesser number of months
                if n_rows<min_rows_reqd: 
                    skipped+=1
                    continue
                
                processed+=1
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    input_df = grp.iloc[i:i+self.seq_len]
                    target_df = grp.iloc[i+min_rows_reqd-1][target_col]
                    
                    x = input_df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = input_df[self.col_dict['missing']].values
                    t = input_df[self.col_dict['delta']].values
                    x=x.values
                    seq.append( [x[:,1:], m, t, x_obs[:,1:]] ) #strip AUTO_ID
                    labels.append(target_df.values)    
                    cumseq+=1
                if processed%10==0:
                    print(f"Patients processed: {processed}/{nb_tot_grp} | Skipped:{skipped} | Average seq per patient: {cumseq/10}")
                    cumseq=0
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw(False) #(243563 rows)
        if sample:
            dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().forward(dataset)  
        self.sequences, self.labels = load(imputed_dataset)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
127/6:
dl = IBDDataLoader()
dl.fit_data(10000)
train, valid, test = dl.TVT_loaders()
129/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
import data_utils as du
import math

import matplotlib.pyplot as plt
%matplotlib inline
129/2:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None, target_col='TGT_CUMCHARGES'):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months
            
            nb_tot_grp = len(groups)
            processed=0
            skipped = 0
            cumseq=0
            
            for _,grp in groups:
                n_rows = grp.shape[0]
                
                # Skip patients who have lesser number of months
                if n_rows<min_rows_reqd: 
                    skipped+=1
                    continue
                
                processed+=1
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    input_df = grp.iloc[i:i+self.seq_len]
                    target_df = grp.iloc[i+min_rows_reqd-1][target_col]
                    
                    x = input_df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = input_df[self.col_dict['missing']].values
                    t = input_df[self.col_dict['delta']].values
                    x=x.values
                    seq.append( [x[:,1:], m, t, x_obs[:,1:]] ) #strip AUTO_ID
                    labels.append(target_df.values)    
                    cumseq+=1
                if processed%10==0:
                    print(f"Patients processed: {processed}/{nb_tot_grp} | Skipped:{skipped} | Average seq per patient: {cumseq/10}")
                    cumseq=0
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw(False) #(243563 rows)
        if sample:
            dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().forward(dataset)  
        self.sequences, self.labels = load(imputed_dataset)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
129/3:
dl = IBDDataLoader()
dl.fit_data(10000)
train, valid, test = dl.TVT_loaders()
129/4:
import torch
import torch.nn as nn
import torch.nn.functional as F
import data_utils as du
import math

import matplotlib.pyplot as plt
%matplotlib inline
129/5:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None, target_col='TGT_CUMCHARGES'):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months
            
            nb_tot_grp = len(groups)
            processed=0
            skipped = 0
            cumseq=0
            
            for _,grp in groups:
                n_rows = grp.shape[0]
                
                # Skip patients who have lesser number of months
                if n_rows<min_rows_reqd: 
                    skipped+=1
                    continue
                
                processed+=1
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    input_df = grp.iloc[i:i+self.seq_len]
                    target_df = grp.iloc[i+min_rows_reqd-1][target_col]
                    
                    x = input_df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = input_df[self.col_dict['missing']].values
                    t = input_df[self.col_dict['delta']].values
                    x=x.values
                    seq.append( [x[:,1:], m, t, x_obs[:,1:]] ) #strip AUTO_ID
                    labels.append(target_df.values)    
                    cumseq+=1
                if processed%10==0:
                    print(f"Patients processed: {processed}/{nb_tot_grp} | Skipped:{skipped} | Average seq per patient: {cumseq/10}")
                    cumseq=0
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw(False) #(243563 rows)
        if sample:
            dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().forward(dataset)  
        self.sequences, self.labels = load(imputed_dataset)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
129/6:
import importlib as imp
imp.reload(du)
129/7:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None, target_col='TGT_CUMCHARGES'):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months
            
            nb_tot_grp = len(groups)
            processed=0
            skipped = 0
            cumseq=0
            
            for _,grp in groups:
                n_rows = grp.shape[0]
                
                # Skip patients who have lesser number of months
                if n_rows<min_rows_reqd: 
                    skipped+=1
                    continue
                
                processed+=1
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    input_df = grp.iloc[i:i+self.seq_len]
                    target_df = grp.iloc[i+min_rows_reqd-1][target_col]
                    
                    x = input_df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = input_df[self.col_dict['missing']].values
                    t = input_df[self.col_dict['delta']].values
                    x=x.values
                    seq.append( [x[:,1:], m, t, x_obs[:,1:]] ) #strip AUTO_ID
                    labels.append(target_df.values)    
                    cumseq+=1
                if processed%10==0:
                    print(f"Patients processed: {processed}/{nb_tot_grp} | Skipped:{skipped} | Average seq per patient: {cumseq/10}")
                    cumseq=0
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw(False) #(243563 rows)
        if sample:
            dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().forward(dataset)  
        self.sequences, self.labels = load(imputed_dataset)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
129/8:
dl = IBDDataLoader()
dl.fit_data(10000)
train, valid, test = dl.TVT_loaders()
129/9: 90000//50000
129/10: round(90000//50000)
129/11: np.round(90000//50000)
129/12: np.rint(90000//50000)
129/13: int(90000//50000 + 0.5)
129/14: 90000//50000 + 0.5
129/15: int(90000//50000 + 0.5)
129/16: round(90000//50000 + 0.5)
129/17: round(90000/50000 + 0.5)
129/18: int(90000/50000 + 0.5)
129/19:
import importlib as imp
imp.reload(du)
129/20:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None, target_col='TGT_CUMCHARGES'):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months
            
            nb_tot_grp = len(groups)
            processed=0
            skipped = 0
            cumseq=0
            
            for _,grp in groups:
                n_rows = grp.shape[0]
                
                # Skip patients who have lesser number of months
                if n_rows<min_rows_reqd: 
                    skipped+=1
                    continue
                
                processed+=1
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    input_df = grp.iloc[i:i+self.seq_len]
                    target_df = grp.iloc[i+min_rows_reqd-1][target_col]
                    
                    x = input_df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = input_df[self.col_dict['missing']].values
                    t = input_df[self.col_dict['delta']].values
                    x=x.values
                    seq.append( [x[:,1:], m, t, x_obs[:,1:]] ) #strip AUTO_ID
                    labels.append(target_df.values)    
                    cumseq+=1
                if processed%10==0:
                    print(f"Patients processed: {processed}/{nb_tot_grp} | Skipped:{skipped} | Average seq per patient: {cumseq/10}")
                    cumseq=0
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw(False) #(243563 rows)
        if sample:
            dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().forward(dataset)  
        self.sequences, self.labels = load(imputed_dataset)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
129/21:
dl = IBDDataLoader()
dl.fit_data(10000)
train, valid, test = dl.TVT_loaders()
129/22:
import pandas as pd
df=pd.DataFrame([[1,2],[3,4]])
df
129/23:
import pandas as pd
df=pd.DataFrame([[1,2],[3,4]])
pd.get_dummies(df[1])
129/24:
import pandas as pd
df=pd.DataFrame([[1,2],[3,4]])
pd.get_dummies(df[1], prefix='TGT_')
129/25:
import pandas as pd
df=pd.DataFrame([[1,2],[3,4]])
pd.get_dummies(df[1], prefix='TGT')
129/26:
import pandas as pd
df=pd.DataFrame([[1,2],[3,4]])
df
# pd.get_dummies(df[1], prefix='TGT')
129/27:
import torch
import torch.nn as nn
import torch.nn.functional as F
import data_utils as du
import math

import matplotlib.pyplot as plt
%matplotlib inline
129/28:
import importlib as imp
imp.reload(du)
129/29:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None, target_col='TGT_CUMCHARGES'):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months
            
            nb_tot_grp = len(groups)
            processed=0
            skipped = 0
            cumseq=0
            
            for _,grp in groups:
                n_rows = grp.shape[0]
                
                # Skip patients who have lesser number of months
                if n_rows<min_rows_reqd: 
                    skipped+=1
                    continue
                
                processed+=1
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    input_df = grp.iloc[i:i+self.seq_len]
                    target_df = grp.iloc[i+min_rows_reqd-1][target_col]
                    
                    x = input_df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = input_df[self.col_dict['missing']].values
                    t = input_df[self.col_dict['delta']].values
                    x=x.values
                    seq.append( [x[:,1:], m, t, x_obs[:,1:]] ) #strip AUTO_ID
                    labels.append(target_df.values)    
                    cumseq+=1
                if processed%10==0:
                    print(f"Patients processed: {processed}/{nb_tot_grp} | Skipped:{skipped} | Average seq per patient: {cumseq/10}")
                    cumseq=0
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw(False) #(243563 rows)
        if sample:
            dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().forward(dataset)  
        self.sequences, self.labels = load(imputed_dataset)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
129/30:
dl = IBDDataLoader()
dl.fit_data(10000)
train, valid, test = dl.TVT_loaders()
129/31:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months
            
            nb_tot_grp = len(groups)
            processed=0
            skipped = 0
            cumseq=0
            
            for _,grp in groups:
                n_rows = grp.shape[0]
                
                # Skip patients who have lesser number of months
                if n_rows<min_rows_reqd: 
                    skipped+=1
                    continue
                
                processed+=1
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    input_df = grp.iloc[i:i+self.seq_len]
                    target_df = grp.iloc[i+min_rows_reqd-1][self.col_dict['tgt']]
                    
                    x = input_df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = input_df[self.col_dict['missing']].values
                    t = input_df[self.col_dict['delta']].values
                    x=x.values
                    seq.append( [x[:,1:], m, t, x_obs[:,1:]] ) #strip AUTO_ID
                    labels.append(target_df.values)    
                    cumseq+=1
                if processed%10==0:
                    print(f"Patients processed: {processed}/{nb_tot_grp} | Skipped:{skipped} | Average seq per patient: {cumseq/10}")
                    cumseq=0
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw(False) #(243563 rows)
        if sample:
            dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().forward(dataset)  
        self.sequences, self.labels = load(imputed_dataset)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
129/32:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months
            
            nb_tot_grp = len(groups)
            processed=0
            skipped = 0
            cumseq=0
            
            for _,grp in groups:
                n_rows = grp.shape[0]
                
                # Skip patients who have lesser number of months
                if n_rows<min_rows_reqd: 
                    skipped+=1
                    continue
                
                processed+=1
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    input_df = grp.iloc[i:i+self.seq_len]
                    target_df = grp.iloc[i+min_rows_reqd-1][self.col_dict['tgt']]
                    
                    x = input_df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = input_df[self.col_dict['missing']].values
                    t = input_df[self.col_dict['delta']].values
                    x=x.values
                    seq.append( [x[:,1:], m, t, x_obs[:,1:]] ) #strip AUTO_ID
                    labels.append(target_df.values)    
                    cumseq+=1
                if processed%10==0:
                    print(f"Patients processed: {processed}/{nb_tot_grp} | Skipped:{skipped} | Average seq per patient: {cumseq/10}")
                    cumseq=0
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample:
            dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().forward(dataset)  
        self.sequences, self.labels = load(imputed_dataset)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
129/33:
dl = IBDDataLoader()
dl.fit_data(10000)
train, valid, test = dl.TVT_loaders()
129/34:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
import data_utils as du
import math


import matplotlib.pyplot as plt
%matplotlib inline
129/35:
# dl = IBDDataLoader()
# dl.fit_data(10000)
train, valid, test = dl.TVT_loaders()
129/36:
X, y = next(valid)

# model = GRU(X.squeeze(0).size(0), 300, 0, use_decay=True, mask_dim=0, delta_dim=0)
129/37:
X, y = next(iter(valid))

# model = GRU(X.squeeze(0).size(0), 300, 0, use_decay=True, mask_dim=0, delta_dim=0)
129/38: X.size()
129/39:
X.size()
X.squeeze(0).size()
129/40:
X.size()
X.squeeze(1).size()
129/41:

X.squeeze(1).size()
129/42: X.squeeze(1).size(0)
129/43: X.squeeze(1).size()
129/44: X.squeeze(2).size()
129/45: X.squeeze()
129/46: X.squeeze().size()
129/47: X.squeeze()
129/48: torch.squeeze(X[:,0,:,:])
129/49: torch.squeeze(X[:,0,:,:]).size()
129/50: torch.squeeze(X[0,:,:,:]).size()
129/51:
X, y = next(iter(valid))
model = GRU(X.size(3), 300, 0, use_decay=True, mask_dim=0, delta_dim=0)
129/52:
class GRUD(nn.module):
    
    def __init__(self, input_dim, hidden_dim, hidden_layers, use_decay=True):
        super(GRUD, self).__init__()
        
        # Assign input and hidden dim
        input_dim = mask_dim = delta_dim = input_dim
        self.use_decay = use_decay
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size, ctx):
        return torch.zeros(batch_size, self.hidden_dim, device=ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden()
        outputs = []

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:,:]), \
                            torch.squeeze(inputs[:,1,:,:]), \
                            torch.squeeze(inputs[:,2,:,:]). \
                            torch.squeeze(inputs[:,3,:,:])
        if self.use_decay:
            gamma_x = torch.exp(-1*torch.max(torch.zeros(), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-1*torch.max(torch.zeros(), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*x.mean() ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.combined((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.combined((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
129/53:
X, y = next(iter(valid))
model = GRUD(X.size(3), 300, 0, use_decay=True, mask_dim=0, delta_dim=0)
129/54:
class GRUD(nn.module):
    
    def __init__(self, input_dim, hidden_dim, hidden_layers, use_decay=True):
        super(GRUD, self).__init__()
        
        # Assign input and hidden dim
        input_dim = mask_dim = delta_dim = input_dim
        self.use_decay = use_decay
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size, ctx):
        return torch.zeros(batch_size, self.hidden_dim, device=ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden()
        outputs = []

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:,:]), \
                            torch.squeeze(inputs[:,1,:,:]), \
                            torch.squeeze(inputs[:,2,:,:]). \
                            torch.squeeze(inputs[:,3,:,:])
        if self.use_decay:
            gamma_x = torch.exp(-1*torch.max(torch.zeros(), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-1*torch.max(torch.zeros(), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*x.mean() ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.combined((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.combined((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
129/55:
class GRUD(nn.Module):
    
    def __init__(self, input_dim, hidden_dim, hidden_layers, use_decay=True):
        super(GRUD, self).__init__()
        
        # Assign input and hidden dim
        input_dim = mask_dim = delta_dim = input_dim
        self.use_decay = use_decay
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size, ctx):
        return torch.zeros(batch_size, self.hidden_dim, device=ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden()
        outputs = []

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:,:]), \
                            torch.squeeze(inputs[:,1,:,:]), \
                            torch.squeeze(inputs[:,2,:,:]). \
                            torch.squeeze(inputs[:,3,:,:])
        if self.use_decay:
            gamma_x = torch.exp(-1*torch.max(torch.zeros(), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-1*torch.max(torch.zeros(), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*x.mean() ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.combined((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.combined((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
129/56:
X, y = next(iter(valid))
model = GRUD(X.size(3), 300, 0, use_decay=True, mask_dim=0, delta_dim=0)
129/57:
X, y = next(iter(valid))
model = GRUD(X.size(3), 300, 0, use_decay=True)
129/58:
class GRUD(nn.Module):
    
    def __init__(self, input_dim, hidden_dim, hidden_layers, use_decay=True):
        super(GRUD, self).__init__()
        
        # Assign input and hidden dim
        input_dim = mask_dim = delta_dim = input_dim
        self.use_decay = use_decay
        self.hidden = None
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size, ctx):
        self.hidden = torch.zeros(batch_size, self.hidden_dim, device=ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        self.init_hidden()
        outputs = []

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:,:]), \
                            torch.squeeze(inputs[:,1,:,:]), \
                            torch.squeeze(inputs[:,2,:,:]). \
                            torch.squeeze(inputs[:,3,:,:])
        if self.use_decay:
            gamma_x = torch.exp(-1*torch.max(torch.zeros(), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-1*torch.max(torch.zeros(), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*x.mean() ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.combined((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.combined((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
129/59:
X, y = next(iter(valid))
model = GRUD(X.size(3), 300, 0, use_decay=True)
129/60:
class GRUD(nn.Module):
    
    def __init__(self, input_dim, hidden_dim, hidden_layers, use_decay=True):
        super(GRUD, self).__init__()
        
        # Assign input and hidden dim
        input_dim = mask_dim = delta_dim = input_dim
        self.use_decay = use_decay
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size, ctx):
        return torch.zeros(batch_size, self.hidden_dim, device=ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden()
        outputs = []

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:,:]), \
                            torch.squeeze(inputs[:,1,:,:]), \
                            torch.squeeze(inputs[:,2,:,:]). \
                            torch.squeeze(inputs[:,3,:,:])
        if self.use_decay:
            gamma_x = torch.exp(-1*torch.max(torch.zeros(), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-1*torch.max(torch.zeros(), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*x.mean() ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.combined((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.combined((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
129/61: len(valid)
129/62: valid[0[]]
129/63: valid[0]
129/64:
state = model.init_hidden()
out = model(X, state)
y-out
129/65:
def try_gpu():
    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
    is_cuda = torch.cuda.is_available()
    print(f"CUDA available?: {is_cuda}")
    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    return device
129/66:
state = model.init_hidden(X.size(0), try_gpu())
out = model(X, state)
y-out
129/67:
class GRUD(nn.Module):
    
    def __init__(self, input_dim, hidden_dim, hidden_layers, use_decay=True):
        super(GRUD, self).__init__()
        
        # Assign input and hidden dim
        input_dim = mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size, ctx):
        return torch.zeros(batch_size, self.hidden_dim, device=ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden()
        outputs = []

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:,:]), \
                            torch.squeeze(inputs[:,1,:,:]), \
                            torch.squeeze(inputs[:,2,:,:]). \
                            torch.squeeze(inputs[:,3,:,:])
        if self.use_decay:
            gamma_x = torch.exp(-1*torch.max(torch.zeros(), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-1*torch.max(torch.zeros(), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*x.mean() ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.combined((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.combined((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
129/68:
X, y = next(iter(valid))
model = GRUD(X.size(3), 300, 0, use_decay=True)
129/69:
state = model.init_hidden(X.size(0), try_gpu())
out = model(X, state)
y-out
129/70:
class GRUD(nn.Module):
    
    def __init__(self, input_dim, hidden_dim, hidden_layers, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        input_dim = mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size, ctx=self.ctx):
        return torch.zeros(batch_size, self.hidden_dim, device=ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:,:]), \
                            torch.squeeze(inputs[:,1,:,:]), \
                            torch.squeeze(inputs[:,2,:,:]). \
                            torch.squeeze(inputs[:,3,:,:])
        if self.use_decay:
            gamma_x = torch.exp(-1*torch.max(torch.zeros(), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-1*torch.max(torch.zeros(), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*x.mean() ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.combined((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.combined((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
129/71:
class GRUD(nn.Module):
    
    def __init__(self, input_dim, hidden_dim, hidden_layers, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        input_dim = mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:,:]), \
                            torch.squeeze(inputs[:,1,:,:]), \
                            torch.squeeze(inputs[:,2,:,:]). \
                            torch.squeeze(inputs[:,3,:,:])
        if self.use_decay:
            gamma_x = torch.exp(-1*torch.max(torch.zeros(), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-1*torch.max(torch.zeros(), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*x.mean() ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.combined((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.combined((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
129/72:
X, y = next(iter(valid))
model = GRUD(X.size(3), 300, 0, use_decay=True)
129/73:
state = model.init_hidden(X.size(0), try_gpu())
out = model(X, state)
y-out
129/74:
state = model.init_hidden(X.size(0))
out = model(X, state)
y-out
129/75:
class GRUD(nn.Module):
    
    def __init__(self, input_dim, hidden_dim, hidden_layers, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        input_dim = mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:,:]), \
                            torch.squeeze(inputs[:,1,:,:]), \
                            torch.squeeze(inputs[:,2,:,:]), \
                            torch.squeeze(inputs[:,3,:,:])
        if self.use_decay:
            gamma_x = torch.exp(-1*torch.max(torch.zeros(), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-1*torch.max(torch.zeros(), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*x.mean() ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.combined((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.combined((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
129/76:
X, y = next(iter(valid))
model = GRUD(X.size(3), 300, 0, use_decay=True)
129/77:
state = model.init_hidden(X.size(0))
out = model(X, state)
y-out
129/78:
class GRUD(nn.Module):
    
    def __init__(self, input_dim, hidden_dim, hidden_layers, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        input_dim = mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h):
        print(inputs.size())
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:,:]), \
                            torch.squeeze(inputs[:,1,:,:]), \
                            torch.squeeze(inputs[:,2,:,:]), \
                            torch.squeeze(inputs[:,3,:,:])
        if self.use_decay:
            gamma_x = torch.exp(-1*torch.max(torch.zeros(), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-1*torch.max(torch.zeros(), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*x.mean() ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.combined((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.combined((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
129/79:
X, y = next(iter(valid))
model = GRUD(X.size(3), 300, 0, use_decay=True)
129/80:
X, y = next(iter(valid))
model = GRUD(X.size(3), 300, 0, use_decay=True)
state = model.init_hidden(X.size(0))
out = model(X, state)
y - out
129/81:
class GRUD(nn.Module):
    
    def __init__(self, input_dim, hidden_dim, hidden_layers, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        input_dim = mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:]), \
                            torch.squeeze(inputs[:,1,:]), \
                            torch.squeeze(inputs[:,2,:]), \
                            torch.squeeze(inputs[:,3,:])
        if self.use_decay:
            gamma_x = torch.exp(-1*torch.max(torch.zeros(), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-1*torch.max(torch.zeros(), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*x.mean() ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.combined((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.combined((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
129/82:
X, y = next(iter(valid))
model = GRUD(X.size(3), 300, 0, use_decay=True)
state = model.init_hidden(X.size(0))
out = model(X, state)
y - out
129/83:
def try_gpu():
    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
    is_cuda = torch.cuda.is_available()
    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    return device
129/84:
class GRUD(nn.Module):
    
    def __init__(self, input_dim, hidden_dim, hidden_layers, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = Variable(torch.zeros(delta_dim))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.Tensor(torch.squeeze(np.mean(inputs[:,0,:,:]))))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        print(xmean.size())
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:]), \
                            torch.squeeze(inputs[:,1,:]), \
                            torch.squeeze(inputs[:,2,:]), \
                            torch.squeeze(inputs[:,3,:])
        if self.use_decay:
            gamma_x = torch.exp(-1*torch.max(self.zeros, self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-1*torch.max(self.zeros, self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.combined((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.combined((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
129/85:
X, y = next(iter(valid))
model = GRUD(X.size(3), 300, 0, use_decay=True)
state = model.init_hidden(X.size(0))
out = model(X, state)
y - out
129/86:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.autograd import Variable
import data_utils as du
import math


import matplotlib.pyplot as plt
%matplotlib inline
129/87:
class GRUD(nn.Module):
    
    def __init__(self, input_dim, hidden_dim, hidden_layers, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = Variable(torch.zeros(delta_dim))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.Tensor(torch.squeeze(np.mean(inputs[:,0,:,:]))))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        print(xmean.size())
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:]), \
                            torch.squeeze(inputs[:,1,:]), \
                            torch.squeeze(inputs[:,2,:]), \
                            torch.squeeze(inputs[:,3,:])
        if self.use_decay:
            gamma_x = torch.exp(-1*torch.max(self.zeros, self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-1*torch.max(self.zeros, self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.combined((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.combined((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
129/88:
X, y = next(iter(valid))
model = GRUD(X.size(3), 300, 0, use_decay=True)
state = model.init_hidden(X.size(0))
out = model(X, state)
y - out
129/89: X.size()
129/90:
xx = torch.squeeze(X[:,0,:,:])
xx.size()
129/91:
xx = torch.squeeze(X[:,0,:,:])
xx.size()
torch.mean(xx, 2).size()
129/92:
xx = torch.squeeze(X[:,0,:,:])
xx.size()
torch.mean(xx, 1).size()
129/93:
xx = torch.squeeze(X[:,0,:,:])
xx.size()
torch.mean(xx, [0,1]).size()
129/94:
xx = torch.squeeze(X[:,0,:,:])
xx.size()
torch.mean(xx, [0,1])
129/95:
xx = torch.squeeze(X[:,0,:,:])
xx
129/96: dl.seq[0]
129/97: dl.sequences[0]
129/98: dl.sequences[3]
129/99: dl.sequences[3][0]
129/100: dl.sequences[3][0].shape
129/101: dl.sequences[3][0]
129/102:
dataset, self.col_dict = du.X_raw() #(243563 rows)
dataset = dataset.head(1000)
imputed_dataset = du.Imputer().forward(dataset)
129/103:
dataset,_ = du.X_raw() #(243563 rows)
dataset = dataset.head(1000)
imputed_dataset = du.Imputer().forward(dataset)
129/104: imputed_dataset
129/105:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months
            
            nb_tot_grp = len(groups)
            processed=0
            skipped = 0
            cumseq=0
            
            for _,grp in groups:
                n_rows = grp.shape[0]
                
                # Skip patients who have lesser number of months
                if n_rows<min_rows_reqd: 
                    skipped+=1
                    continue
                
                processed+=1
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    input_df = grp.iloc[i:i+self.seq_len]
                    target_df = grp.iloc[i+min_rows_reqd-1][self.col_dict['tgt']]
                    
                    x = input_df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = input_df[self.col_dict['missing']].values
                    t = input_df[self.col_dict['delta']].values
                    x=x.values
                    seq.append( [x[:,1:], m, t, x_obs[:,1:]] ) #strip AUTO_ID
                    labels.append(target_df.values)    
                    cumseq+=1
                if processed%10==0:
                    print(f"Patients processed: {processed}/{nb_tot_grp} | Skipped:{skipped} | Average seq per patient: {cumseq/10}")
                    cumseq=0
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample:
            dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().forward(dataset).fillna(0) ########################!!!!!!!!!!!############### 
        self.sequences, self.labels = load(imputed_dataset)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
129/106:
xx = torch.squeeze(X[:,0,:,:])
xx.size()
torch.mean(xx, [0,1]).size()
129/107:
xx = torch.squeeze(X[:,0,:,:])
xx.size()
torch.mean(np.nan_to_num(xx,0), [0,1]).size()
129/108:
xx = torch.squeeze(X[:,0,:,:])
xx.size()
torch.mean(torch.from_numpy(np.nan_to_num(xx,0)), [0,1]).size()
129/109:
xx = torch.squeeze(X[:,0,:,:])
xx.size()
torch.mean(torch.from_numpy(np.nan_to_num(xx,0)), [0,1])
129/110:
class GRUD(nn.Module):
    
    def __init__(self, input_dim, hidden_dim, hidden_layers, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = Variable(torch.zeros(delta_dim))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        print(xmean.size())
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:]), \
                            torch.squeeze(inputs[:,1,:]), \
                            torch.squeeze(inputs[:,2,:]), \
                            torch.squeeze(inputs[:,3,:])
        if self.use_decay:
            gamma_x = torch.exp(-1*torch.max(self.zeros, self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-1*torch.max(self.zeros, self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.combined((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.combined((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
129/111:
X, y = next(iter(valid))
model = GRUD(X.size(3), 300, 0, use_decay=True)
state = model.init_hidden(X.size(0))
out = model(X, state)
y - out
129/112:
class GRUD(nn.Module):
    
    def __init__(self, input_dim, hidden_dim, hidden_layers, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = Variable(torch.zeros(delta_dim))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:]), \
                            torch.squeeze(inputs[:,1,:]), \
                            torch.squeeze(inputs[:,2,:]), \
                            torch.squeeze(inputs[:,3,:])
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros, self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros, self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.combined((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.combined((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
129/113:
X, y = next(iter(valid))
model = GRUD(X.size(3), 300, 0, use_decay=True)
state = model.init_hidden(X.size(0))
out = model(X, state)
y - out
129/114:
class GRUD(nn.Module):
    
    def __init__(self, input_dim, hidden_dim, hidden_layers, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = Variable(torch.zeros(delta_dim))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:]), \
                            torch.squeeze(inputs[:,1,:]), \
                            torch.squeeze(inputs[:,2,:]), \
                            torch.squeeze(inputs[:,3,:])
        
        print(delta.size())
        print(self.gamma_x_lin(delta).size())
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros, self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros, self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.combined((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.combined((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
129/115:
X, y = next(iter(valid))
model = GRUD(X.size(3), 300, 0, use_decay=True)
state = model.init_hidden(X.size(0))
out = model(X, state)
y - out
129/116:
X, y = next(iter(valid))
model = GRUD(X.size(3), 300, 0, use_decay=False)
state = model.init_hidden(X.size(0))
out = model(X, state)
y - out
129/117:
class GRUD(nn.Module):
    
    def __init__(self, input_dim, hidden_dim, hidden_layers, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = Variable(torch.zeros(delta_dim))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:]), \
                            torch.squeeze(inputs[:,1,:]), \
                            torch.squeeze(inputs[:,2,:]), \
                            torch.squeeze(inputs[:,3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros, self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros, self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.combined((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.combined((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
129/118:
X, y = next(iter(valid))
model = GRUD(X.size(3), 300, 0, use_decay=False)
state = model.init_hidden(X.size(0))
out = model(X, state)
y - out
129/119:
class GRUD(nn.Module):
    
    def __init__(self, input_dim, hidden_dim, hidden_layers, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = Variable(torch.zeros(delta_dim))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:]), \
                            torch.squeeze(inputs[:,1,:]), \
                            torch.squeeze(inputs[:,2,:]), \
                            torch.squeeze(inputs[:,3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros, self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros, self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.cat((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
129/120:
X, y = next(iter(valid))
model = GRUD(X.size(3), 300, 0, use_decay=False)
state = model.init_hidden(X.size(0))
out = model(X, state)
y - out
129/121:
class GRUD(nn.Module):
    
    def __init__(self, input_dim, hidden_dim, hidden_layers, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = Variable(torch.zeros(delta_dim))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:]), \
                            torch.squeeze(inputs[:,1,:]), \
                            torch.squeeze(inputs[:,2,:]), \
                            torch.squeeze(inputs[:,3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros, self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros, self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.cat((x,h.double(),obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
129/122:
X, y = next(iter(valid))
model = GRUD(X.size(3), 300, 0, use_decay=False)
state = model.init_hidden(X.size(0))
out = model(X, state)
y - out
131/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.autograd import Variable
import data_utils as du
import math


import matplotlib.pyplot as plt
%matplotlib inline
131/2:
import importlib as imp
imp.reload(du)
131/3:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        def load(dataset):
            """
            [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim], label WIP !!!
            """
            groups = dataset.groupby('AUTO_ID')
            seq = []
            labels = []
            min_rows_reqd = self.seq_len+self.lookfwd_months
            
            nb_tot_grp = len(groups)
            processed=0
            skipped = 0
            cumseq=0
            
            for _,grp in groups:
                n_rows = grp.shape[0]
                
                # Skip patients who have lesser number of months
                if n_rows<min_rows_reqd: 
                    skipped+=1
                    continue
                
                processed+=1
                for i in range(n_rows-min_rows_reqd+1): #CHK
                    input_df = grp.iloc[i:i+self.seq_len]
                    target_df = grp.iloc[i+min_rows_reqd-1][self.col_dict['tgt']]
                    
                    x = input_df[self.col_dict['input']]
                    x_obs = x.shift(1).fillna(x.iloc[0]).values
                    m = input_df[self.col_dict['missing']].values
                    t = input_df[self.col_dict['delta']].values
                    x=x.values
                    seq.append( [x[:,1:], m, t, x_obs[:,1:]] ) #strip AUTO_ID
                    labels.append(target_df.values)    
                    cumseq+=1
                if processed%10==0:
                    print(f"Patients processed: {processed}/{nb_tot_grp} | Skipped:{skipped} | Average seq per patient: {cumseq/10}")
                    cumseq=0
            seq = np.array(seq)
            labels = np.array(labels)
            print(f"Sequences shape: {seq.shape} Labels shape: {labels.shape}")
            return seq, labels
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample:
            dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().forward(dataset).fillna(0) ########################!!!!!!!!!!!############### 
        self.sequences, self.labels = load(imputed_dataset)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
131/4:
def try_gpu():
    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
    is_cuda = torch.cuda.is_available()
    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    return device
131/5:
    def get_params(self, input_dim, hidden_dim, ctx):
        # Initialize Weight matrices
        def normal(shape):
            return torch.randn(size=shape, device=ctx)

        def three():
            return (normal((input_dim, hidden_dim)),
                    normal((input_dim, hidden_dim)),
                    np.zeros(hidden_dim, device=ctx))
        
        W_xz, W_hz, b_z = three()  # Update gate parameter
        W_xr, W_hr, b_r = three()  # Reset gate parameter
        W_xh, W_hh, b_h = three()  # Candidate hidden state parameter
        
        W_hq = normal((num_hiddens, num_outputs))
        b_q = np.zeros(num_outputs, device=ctx)
        params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]
#         for param in params:
#             param.attach_grad()
        return params

    def reset_parameters(self):
        stdv = 1.0 / math.sqrt(self.hidden_size)
        for weight in self.parameters():
            init.uniform_(weight, -stdv, stdv)
131/6:
class GRUD(nn.Module):
    
    def __init__(self, input_dim, hidden_dim, hidden_layers, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = Variable(torch.zeros(delta_dim))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:]), \
                            torch.squeeze(inputs[:,1,:]), \
                            torch.squeeze(inputs[:,2,:]), \
                            torch.squeeze(inputs[:,3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros, self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros, self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.cat((x,h.double(),obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
131/7:
dl = IBDDataLoader()
dl.fit_data(10000)
train, valid, test = dl.TVT_loaders()
131/8:
df = pd.DataFrame([list(range(10)), list(range(100,120))])
df
131/9:
import pandas as pddf = pd.DataFrame([list(range(10)), list(range(100,120))])
df
131/10:
import pandas as pd
df = pd.DataFrame([list(range(10)), list(range(100,120))])
df
131/11:
import pandas as pd
df = pd.DataFrame([list(range(10)), list(range(100,120))]).T
df
131/12:
import pandas as pd
df = pd.DataFrame([list(range(10)), list(range(100,120))]).T
df_subset = lambda df,(s,e): df.iloc[s:e]
x = lambda df:df[0].values, df[1].values
df.apply(x)
131/13:
import pandas as pd
df = pd.DataFrame([list(range(10)), list(range(100,120))]).T
x = lambda df:df[0].values, df[1].values
df.apply(x)
131/14:
import pandas as pd
df = pd.DataFrame([list(range(10)), list(range(100,120))]).T
x = lambda df:[df[0], df[1]]
df.apply(x)
131/15:
import pandas as pd
df = pd.DataFrame([list(range(10)), list(range(100,120))]).T
x = lambda df:tuple((df[0], df[1]))
df.apply(x)
131/16:
import pandas as pd
df = pd.DataFrame([list(range(10)), list(range(100,120))]).T
x = lambda df:df[0]
df.apply(x)
131/17:
import pandas as pd
df = pd.DataFrame([list(range(10)), list(range(100,120))]).T
x = lambda df:df[0]
type(df.apply(x))
131/18:
import pandas as pd
df = pd.DataFrame([list(range(10)), list(range(100,120))]).T
x = lambda df:df[0],df[1]
type(df.apply(x))
131/19:
import pandas as pd
df = pd.DataFrame([list(range(10)), list(range(100,120))]).T
x = lambda df:(df[0],df[1])
type(df.apply(x))
131/20:
import pandas as pd
df = pd.DataFrame([list(range(10)), list(range(100,120))]).T
x = lambda df:(df[0],df[1])
df.apply(x))
131/21:
import pandas as pd
df = pd.DataFrame([list(range(10)), list(range(100,120))]).T
x = lambda df:(df[0],df[1])
df.apply(x)
131/22: list(zip([1,2,3],[5,6,7]))
131/23: zip(*zip([1,2,3],[5,6,7]))
131/24:
x1,y1=zip(*zip([1,2,3],[5,6,7]))
x1, y1
131/25:
df = pd.read_pickle('pickles/inputs.pkl')
df.columns
131/26:
df = pd.read_pickle('pickles/inputs.pkl')
df.head
131/27:
df = pd.read_pickle('pickles/inputs.pkl')
df.columns
131/28:
df = pd.read_pickle('pickles/inputs.pkl')
df.columns
x=df[[['AUTO_ID', 'MONTH_TS', 'RELATED_OP', 'UNRELATED_OP', 'RELATED_IP',
       \'UNRELATED_IP','TGT_1', 'TGT_2', 'TGT_3']]
x
131/29:
df = pd.read_pickle('pickles/inputs.pkl')
df.columns
x=df[['AUTO_ID', 'MONTH_TS', 'RELATED_OP', 'UNRELATED_OP', 'RELATED_IP',
       \'UNRELATED_IP','TGT_1', 'TGT_2', 'TGT_3']]
x
131/30:
df = pd.read_pickle('pickles/inputs.pkl')
df.columns
x=df[['AUTO_ID', 'MONTH_TS', 'RELATED_OP', 'UNRELATED_OP', 'RELATED_IP','UNRELATED_IP','TGT_1', 'TGT_2', 'TGT_3']]
x
131/31:
# df = pd.read_pickle('pickles/inputs.pkl')
def get_target(df):
    y = df.copy()
    y['tmpsum'] = df.RELATED_OP.fillna(0) + df.RELATED_IP.fillna(0)
    y['tmpsum'] = y.groupby('AUTO_ID').tmpsum.cumsum()
    y['tmpsum'] = np.where(y['tmpsum'].between(0,30000),1,np.where(y['tmpsum'].between(30000,60000), 2, 3))
    y = pd.get_dummies(y.tmpsum, prefix='TGT')
    return y

xdf = get_target(df)
xdf
131/32:
# df = pd.read_pickle('pickles/inputs.pkl')
def get_target(df):
    y = df.copy()
    y['tmpsum'] = df.RELATED_OP.fillna(0) + df.RELATED_IP.fillna(0)
    y['tmpsum'] = y.groupby('AUTO_ID').tmpsum.cumsum()
    y['tmpsum'] = np.where(y['tmpsum'].between(0,30000),1,np.where(y['tmpsum'].between(30000,60000), 2, 3))
    y = pd.get_dummies(y.tmpsum, prefix='TGT')
    return y

# xdf = get_target(df)
xdf.TGT_3.value_counts()
131/33:
# df = pd.read_pickle('pickles/inputs.pkl')
def get_target(df):
    y = df.copy()
    y['tmpsum'] = df.RELATED_OP.fillna(0) + df.RELATED_IP.fillna(0)
    y['tmpsum'] = y.groupby('AUTO_ID').tmpsum.cumsum()
    y['tmpsum'] = np.where(y['tmpsum'].between(0,30000),1,np.where(y['tmpsum'].between(30000,60000), 2, 3))
    y = pd.get_dummies(y.tmpsum, prefix='TGT')
    return y

# xdf = get_target(df)
xdf.TGT_2.value_counts()
131/34:
# df = pd.read_pickle('pickles/inputs.pkl')
def get_target(df):
    y = df.copy()
    y['tmpsum'] = df.RELATED_OP.fillna(0) + df.RELATED_IP.fillna(0)
    y['tmpsum'] = y.groupby('AUTO_ID').tmpsum.cumsum()
    y['tmpsum'] = np.where(y['tmpsum'].between(0,30000),1,np.where(y['tmpsum'].between(30000,60000), 2, 3))
    y = pd.get_dummies(y.tmpsum, prefix='TGT')
    return y

# xdf = get_target(df)
xdf.TGT_1.value_counts()
131/35:
# df = pd.read_pickle('pickles/inputs.pkl')
def get_target(df):
    y = df.copy()
    y['tmpsum'] = df.RELATED_OP.fillna(0) + df.RELATED_IP.fillna(0)
    y['tmpsum'] = y.groupby('AUTO_ID').tmpsum.cumsum()
    y['tmpsum'] = np.where(y['tmpsum'].between(0,30000),1,np.where(y['tmpsum'].between(30000,60000), 2, 3))
    y = pd.get_dummies(y.tmpsum, prefix='TGT')
    return y

# xdf = get_target(df)
xdf.TGT_2.value_counts()
131/36:
# df = pd.read_pickle('pickles/inputs.pkl')
def get_target(df):
    y = df.copy()
    y['tmpsum'] = df.RELATED_OP.fillna(0) + df.RELATED_IP.fillna(0)
    y['tmpsum'] = y.groupby('AUTO_ID').tmpsum.cumsum()
    y['tmpsum'] = np.where(y['tmpsum'].between(0,30000),1,np.where(y['tmpsum'].between(30000,60000), 2, 3))
    y = pd.get_dummies(y.tmpsum, prefix='TGT')
    return y

# xdf = get_target(df)
xdf.TGT_3.value_counts()
131/37:
# df = pd.read_pickle('pickles/inputs.pkl')
def get_target(df):
    y = df.copy()
    y['tmpsum'] = df.RELATED_OP.fillna(0) + df.RELATED_IP.fillna(0)
    y['tmpsum'] = y.groupby('AUTO_ID').tmpsum.cumsum()
    y['tmpsum'] = np.where(y['tmpsum'].between(0,30000),1,np.where(y['tmpsum'].between(30000,60000), 2, 3))
    y = pd.get_dummies(y.tmpsum, prefix='TGT')
    return y

# xdf = get_target(df)
xdf
131/38:
# df = pd.read_pickle('pickles/inputs.pkl')
def get_target(df):
    y = df.copy()
    y['tmpsum'] = df.RELATED_OP.fillna(0) + df.RELATED_IP.fillna(0)
    y['tmpsum'] = y.groupby('AUTO_ID').tmpsum.cumsum()
    y['tmpsum'] = np.where(y['tmpsum'].between(0,30000),1,np.where(y['tmpsum'].between(30000,60000), 2, 3))
    y = pd.get_dummies(y.tmpsum, prefix='TGT')
    return y

# xdf = get_target(df)
df[['TGT_1','TGT_2','TGT_3']] = xdf
df
131/39:
# df = pd.read_pickle('pickles/inputs.pkl')
def get_target(df):
    y = df.copy()
    y['tmpsum'] = df.RELATED_OP.fillna(0) + df.RELATED_IP.fillna(0)
    y['tmpsum'] = y.groupby('AUTO_ID').tmpsum.cumsum()
    y['tmpsum'] = np.where(y['tmpsum'].between(0,30000),1,np.where(y['tmpsum'].between(30000,60000), 2, 3))
    y = pd.get_dummies(y.tmpsum, prefix='TGT')
    return y

# xdf = get_target(df)
# df[['TGT_1','TGT_2','TGT_3']] = xdf
df.to_pickle('pickles/inputs.pkl')
131/40:
%timeit
df = pd.read_pickle('pickles/inputs.pkl')
131/41: %timeit "df = pd.read_pickle('pickles/inputs.pkl')""
131/42: %timeit "df = pd.read_pickle('pickles/inputs.pkl')"
131/43:
%timeit "df = pd.read_pickle('pickles/inputs.pkl')"

df
131/44:
dl = IBDDataLoader()
%timeit dl.fit_data(500)
# train, valid, test = dl.TVT_loaders()
131/45:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
         """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        def subset_df(df, start_end_ilocs):
            return [df.iloc[s:e] for s,e in start_end_ilocs]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, input_ilocs):
            labels = []
            for i in range(n_rows-min_rows_reqd+1):
                target_df = grp.iloc[i+min_rows_reqd-1][self.col_dict['tgt']]
                labels.append(target_df.values)   
            return np.array(labels)
        
        def process_patients(grp, min_rows_reqd):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            print(f"{grp.head(0).AUTO_ID} ")
            input_ilocs = [(i, i+self.seq_len) for i in range(n_rows-min_rows_reqd+1)]
            input_seq = subset_df(grp, input_ilocs)
            sequences = np.array(map(decompose_inputs, input_seq))
            labels = get_targets(grp, n_rows, min_rows_reqd)
            return zip(sequences, labels)
           
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().impute(dataset, 'forward')
        groups = dataset.groupby('AUTO_ID')

        S_L_zip = groups.apply(process_patients, args=(self.seq_len+self.lookfwd_months, ))
        
        self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
131/47:
dl2 = IBDDataLoader()
%timeit dl2.fit_data(500)
# train, valid, test = dl.TVT_loaders()
131/48:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
         """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        def subset_df(df, start_end_ilocs):
            return [df.iloc[s:e] for s,e in start_end_ilocs]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, input_ilocs):
            labels = []
            for i in range(n_rows-min_rows_reqd+1):
                target_df = grp.iloc[i+min_rows_reqd-1][self.col_dict['tgt']]
                labels.append(target_df.values)   
            return np.array(labels)
        
        def process_patients(grp, min_rows_reqd):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            print(f"{grp.head(0).AUTO_ID} ")
            input_ilocs = [(i, i+self.seq_len) for i in range(n_rows-min_rows_reqd+1)]
            input_seq = subset_df(grp, input_ilocs)
            sequences = np.array(map(decompose_inputs, input_seq))
            labels = get_targets(grp, n_rows, min_rows_reqd)
            return zip(sequences, labels)
           
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().impute(dataset, 'forward')
        groups = dataset.groupby('AUTO_ID')

        S_L_zip = groups.apply(process_patients, args=(self.seq_len+self.lookfwd_months, ))
        
        self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
131/50:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
         """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        def subset_df(df, start_end_ilocs):
            return [df.iloc[s:e] for s,e in start_end_ilocs]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, input_ilocs):
            labels = []
            for i in range(n_rows-min_rows_reqd+1):
                target_df = grp.iloc[i+min_rows_reqd-1][self.col_dict['tgt']]
                labels.append(target_df.values)   
            return np.array(labels)
        
        def process_patients(grp, min_rows_reqd):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            print(f"{grp.head(0).AUTO_ID} ")
            input_ilocs = [(i, i+self.seq_len) for i in range(n_rows-min_rows_reqd+1)]
            input_seq = subset_df(grp, input_ilocs)
            sequences = np.array(map(decompose_inputs, input_seq))
            labels = get_targets(grp, n_rows, min_rows_reqd)
            return zip(sequences, labels)
           
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().impute(dataset, 'forward')
        groups = dataset.groupby('AUTO_ID')

        S_L_zip = groups.apply(process_patients, args=(self.seq_len+self.lookfwd_months, ))
        
        self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
131/52:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        def subset_df(df, start_end_ilocs):
            return [df.iloc[s:e] for s,e in start_end_ilocs]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, input_ilocs):
            labels = []
            for i in range(n_rows-min_rows_reqd+1):
                target_df = grp.iloc[i+min_rows_reqd-1][self.col_dict['tgt']]
                labels.append(target_df.values)   
            return np.array(labels)
        
        def process_patients(grp, min_rows_reqd):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            print(f"{grp.head(0).AUTO_ID} ")
            input_ilocs = [(i, i+self.seq_len) for i in range(n_rows-min_rows_reqd+1)]
            input_seq = subset_df(grp, input_ilocs)
            sequences = np.array(map(decompose_inputs, input_seq))
            labels = get_targets(grp, n_rows, min_rows_reqd)
            return zip(sequences, labels)
           
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().impute(dataset, 'forward')
        groups = dataset.groupby('AUTO_ID')

        S_L_zip = groups.apply(process_patients, args=(self.seq_len+self.lookfwd_months, ))
        
        self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
131/53:
dl2 = IBDDataLoader()
%timeit dl2.fit_data(500)
# train, valid, test = dl.TVT_loaders()
131/54:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        def subset_df(df, start_end_ilocs):
            return [df.iloc[s:e] for s,e in start_end_ilocs]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, input_ilocs):
            labels = []
            for i in range(n_rows-min_rows_reqd+1):
                target_df = grp.iloc[i+min_rows_reqd-1][self.col_dict['tgt']]
                labels.append(target_df.values)   
            return np.array(labels)
        
        def process_patients(grp, min_rows_reqd):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            print(f"{grp.head(0).AUTO_ID} ")
            input_ilocs = [(i, i+self.seq_len) for i in range(n_rows-min_rows_reqd+1)]
            input_seq = subset_df(grp, input_ilocs)
            sequences = np.array(map(decompose_inputs, input_seq))
            labels = get_targets(grp, n_rows, min_rows_reqd)
            return zip(sequences, labels)
           
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().impute(dataset, 'forward')
        groups = dataset.groupby('AUTO_ID')

        S_L_zip = groups.apply(process_patients, args=(self.seq_len+self.lookfwd_months, ))
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
131/55:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        def subset_df(df, start_end_ilocs):
            return [df.iloc[s:e] for s,e in start_end_ilocs]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, input_ilocs):
            labels = []
            for i in range(n_rows-min_rows_reqd+1):
                target_df = grp.iloc[i+min_rows_reqd-1][self.col_dict['tgt']]
                labels.append(target_df.values)   
            return np.array(labels)
        
        def process_patients(grp, min_rows_reqd):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            print(f"{grp.head(0).AUTO_ID} ")
            input_ilocs = [(i, i+self.seq_len) for i in range(n_rows-min_rows_reqd+1)]
            input_seq = subset_df(grp, input_ilocs)
            sequences = np.array(map(decompose_inputs, input_seq))
            labels = get_targets(grp, n_rows, min_rows_reqd)
            return zip(sequences, labels)
           
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().impute(dataset, 'forward')
        groups = dataset.groupby('AUTO_ID')

        S_L_zip = groups.apply(process_patients, min_rows_reqd=self.seq_len+self.lookfwd_months)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
131/56:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        def subset_df(df, start_end_ilocs):
            return [df.iloc[s:e] for s,e in start_end_ilocs]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, input_ilocs):
            labels = []
            for i in range(n_rows-min_rows_reqd+1):
                target_df = grp.iloc[i+min_rows_reqd-1][self.col_dict['tgt']]
                labels.append(target_df.values)   
            return np.array(labels)
        
        def process_patients(grp, min_rows_reqd):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            print(f"{grp.head(0).AUTO_ID} ")
            input_ilocs = [(i, i+self.seq_len) for i in range(n_rows-min_rows_reqd+1)]
            input_seq = subset_df(grp, input_ilocs)
            sequences = np.array(map(decompose_inputs, input_seq))
            labels = get_targets(grp, n_rows, min_rows_reqd)
            return zip(sequences, labels)
           
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().impute(dataset, 'forward')
        groups = dataset.groupby('AUTO_ID')

        S_L_zip = groups.apply(process_patients, min_rows_reqd=self.seq_len+self.lookfwd_months)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
131/57:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        def subset_df(df, start_end_ilocs):
            return [df.iloc[s:e] for s,e in start_end_ilocs]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, input_ilocs):
            labels = []
            for i in range(n_rows-min_rows_reqd+1):
                target_df = grp.iloc[i+min_rows_reqd-1][self.col_dict['tgt']]
                labels.append(target_df.values)   
            return np.array(labels)
        
        def process_patients(grp, min_rows_reqd):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            print(f"{grp.head(0).AUTO_ID} ")
            input_ilocs = [(i, i+self.seq_len) for i in range(n_rows-min_rows_reqd+1)]
            input_seq = subset_df(grp, input_ilocs)
            sequences = np.array(map(decompose_inputs, input_seq))
            labels = get_targets(grp, n_rows, min_rows_reqd)
            return zip(sequences, labels)
           
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().impute(dataset, 'forward')
        groups = dataset.groupby('AUTO_ID')

        S_L_zip = groups.apply(process_patients, min_rows_reqd=self.seq_len+self.lookfwd_months)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
131/58:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        def subset_df(df, start_end_ilocs):
            return [df.iloc[s:e] for s,e in start_end_ilocs]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, input_ilocs):
            labels = []
            for i in range(n_rows-min_rows_reqd+1):
                target_df = grp.iloc[i+min_rows_reqd-1][self.col_dict['tgt']]
                labels.append(target_df.values)   
            return np.array(labels)
        
        def process_patients(grp, min_rows_reqd):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            print(f"{grp.head(0).AUTO_ID} ")
            input_ilocs = [(i, i+self.seq_len) for i in range(n_rows-min_rows_reqd+1)]
            input_seq = subset_df(grp, input_ilocs)
            sequences = np.array(map(decompose_inputs, input_seq))
            labels = get_targets(grp, n_rows, min_rows_reqd)
            return zip(sequences, labels)
           
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().impute(dataset, 'forward')
        groups = dataset.groupby('AUTO_ID')

        S_L_zip = groups.apply(process_patients, min_rows_reqd=self.seq_len+self.lookfwd_months)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
131/59:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        def subset_df(df, start_end_ilocs):
            return [df.iloc[s:e] for s,e in start_end_ilocs]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, input_ilocs):
            labels = []
            for i in range(n_rows-min_rows_reqd+1):
                target_df = grp.iloc[i+min_rows_reqd-1][self.col_dict['tgt']]
                labels.append(target_df.values)   
            return np.array(labels)
        
        def process_patients(grp, min_rows_reqd):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            print(f"{grp.head(0).AUTO_ID} ")
            input_ilocs = [(i, i+self.seq_len) for i in range(n_rows-min_rows_reqd+1)]
            input_seq = subset_df(grp, input_ilocs)
            sequences = np.array(map(decompose_inputs, input_seq))
            labels = get_targets(grp, n_rows, min_rows_reqd)
            return zip(sequences, labels)
           
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().impute(dataset, 'forward')
        groups = dataset.groupby('AUTO_ID')

        S_L_zip = groups.apply(process_patients, min_rows_reqd=self.seq_len+self.lookfwd_months)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
131/60:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        def subset_df(df, start_end_ilocs):
            return [df.iloc[s:e] for s,e in start_end_ilocs]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, input_ilocs):
            labels = []
            for i in range(n_rows-min_rows_reqd+1):
                target_df = grp.iloc[i+min_rows_reqd-1][self.col_dict['tgt']]
                labels.append(target_df.values)   
            return np.array(labels)
        
        def process_patients(grp, min_rows_reqd):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            print(f"{grp.head(0).AUTO_ID} ")
            input_ilocs = [(i, i+self.seq_len) for i in range(n_rows-min_rows_reqd+1)]
            input_seq = subset_df(grp, input_ilocs)
            sequences = np.array(map(decompose_inputs, input_seq))
            labels = get_targets(grp, n_rows, min_rows_reqd)
            return zip(sequences, labels)
           
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().impute(dataset, 'forward')
        groups = dataset.groupby('AUTO_ID')

        S_L_zip = groups.apply(process_patients, min_rows_reqd=self.seq_len+self.lookfwd_months)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
131/61:
dl2 = IBDDataLoader()
%timeit zipped = dl2.fit_data(500)
# train, valid, test = dl.TVT_loaders()
131/62:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        def subset_df(df, start_end_ilocs):
            return [df.iloc[s:e] for s,e in start_end_ilocs]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, input_ilocs):
            labels = []
            for i in range(n_rows-min_rows_reqd+1):
                target_df = grp.iloc[i+min_rows_reqd-1][self.col_dict['tgt']]
                labels.append(target_df.values)   
            return np.array(labels)
        
        def process_patients(grp, min_rows_reqd):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            input_ilocs = [(i, i+self.seq_len) for i in range(n_rows-min_rows_reqd+1)]
            input_seq = subset_df(grp, input_ilocs)
            sequences = np.array(map(decompose_inputs, input_seq))
            labels = get_targets(grp, n_rows, min_rows_reqd)
            return zip(sequences, labels)
           
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().impute(dataset, 'forward')
        groups = dataset.groupby('AUTO_ID')

        S_L_zip = groups.apply(process_patients, min_rows_reqd=self.seq_len+self.lookfwd_months)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
131/63:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        def subset_df(df, start_end_ilocs):
            return [df.iloc[s:e] for s,e in start_end_ilocs]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, input_ilocs):
            labels = []
            for i in range(n_rows-min_rows_reqd+1):
                target_df = grp.iloc[i+min_rows_reqd-1][self.col_dict['tgt']]
                labels.append(target_df.values)   
            return np.array(labels)
        
        def process_patients(grp, min_rows_reqd):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            input_ilocs = [(i, i+self.seq_len) for i in range(n_rows-min_rows_reqd+1)]
            input_seq = subset_df(grp, input_ilocs)
            sequences = np.array(map(decompose_inputs, input_seq))
            labels = get_targets(grp, n_rows, min_rows_reqd)
            return zip(sequences, labels)
           
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().impute(dataset, 'forward')
        groups = dataset.groupby('AUTO_ID')

        S_L_zip = groups.apply(process_patients, min_rows_reqd=self.seq_len+self.lookfwd_months)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
131/64:
dl2 = IBDDataLoader()
%timeit zipped = dl2.fit_data(500)
# train, valid, test = dl.TVT_loaders()
131/65:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        def subset_df(df, start_end_ilocs):
            return [df.iloc[s:e] for s,e in start_end_ilocs]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, n_rows, min_rows_reqd):
            labels = []
            tgt_cols = self.col_dict['tgt']
            for i in range(n_rows-min_rows_reqd+1):
                target_df = grp.iloc[i+min_rows_reqd-1][tgt_cols]
                labels.append(target_df.values)   
            return np.array(labels)
        
        def process_patients(grp, min_rows_reqd):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            input_ilocs = [(i, i+self.seq_len) for i in range(n_rows-min_rows_reqd+1)]
            input_seq = subset_df(grp, input_ilocs)
            sequences = np.array(map(decompose_inputs, input_seq))
            labels = get_targets(grp, n_rows, min_rows_reqd)
            return zip(sequences, labels)
           
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().impute(dataset, 'forward')
        groups = dataset.groupby('AUTO_ID')

        S_L_zip = groups.apply(process_patients, min_rows_reqd=self.seq_len+self.lookfwd_months)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
131/66:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        def subset_df(df, start_end_ilocs):
            return [df.iloc[s:e] for s,e in start_end_ilocs]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, n_rows, min_rows_reqd):
            labels = []
            tgt_cols = self.col_dict['tgt']
            for i in range(n_rows-min_rows_reqd+1):
                target_df = grp.iloc[i+min_rows_reqd-1][tgt_cols]
                labels.append(target_df.values)   
            return np.array(labels)
        
        def process_patients(grp, min_rows_reqd):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            input_ilocs = [(i, i+self.seq_len) for i in range(n_rows-min_rows_reqd+1)]
            input_seq = subset_df(grp, input_ilocs)
            sequences = np.array(map(decompose_inputs, input_seq))
            labels = get_targets(grp, n_rows, min_rows_reqd)
            return zip(sequences, labels)
           
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().impute(dataset, 'forward')
        groups = dataset.groupby('AUTO_ID')

        S_L_zip = groups.apply(process_patients, min_rows_reqd=self.seq_len+self.lookfwd_months)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
131/67:
dl2 = IBDDataLoader()
%timeit zipped = dl2.fit_data(500)
# train, valid, test = dl.TVT_loaders()
131/68:
class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        def subset_df(df, start_end_ilocs):
            return [df.iloc[s:e] for s,e in start_end_ilocs]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, n_rows, min_rows_reqd):
            labels = []
            tgt_cols = self.col_dict['tgt']
            for i in range(n_rows-min_rows_reqd+1):
                target_df = grp.iloc[i+min_rows_reqd-1][tgt_cols]
                labels.append(target_df.values)   
            return np.array(labels)
        
        def process_patients(grp, min_rows_reqd):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            input_ilocs = [(i, i+self.seq_len) for i in range(n_rows-min_rows_reqd+1)]
            input_seq = subset_df(grp, input_ilocs)
            sequences = np.array(list(map(decompose_inputs, input_seq)))
            labels = get_targets(grp, n_rows, min_rows_reqd)
            return zip(sequences, labels)
           
        
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().impute(dataset, 'forward')
        groups = dataset.groupby('AUTO_ID')

        S_L_zip = groups.apply(process_patients, min_rows_reqd=self.seq_len+self.lookfwd_months)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
131/69:
dl2 = IBDDataLoader()
%timeit zipped = dl2.fit_data(500)
# train, valid, test = dl.TVT_loaders()
131/70:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        def subset_df(df, start_end_ilocs):
            return [df.iloc[s:e] for s,e in start_end_ilocs]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, n_rows, min_rows_reqd):
            labels = []
            tgt_cols = self.col_dict['tgt']
            for i in range(n_rows-min_rows_reqd+1):
                target_df = grp.iloc[i+min_rows_reqd-1][tgt_cols]
                labels.append(target_df.values)   
            return np.array(labels)
        
        def process_patients(grp, min_rows_reqd):
            start = time.time()
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            input_ilocs = [(i, i+self.seq_len) for i in range(n_rows-min_rows_reqd+1)]
            
            t1 = time.time()-start
            
            input_seq = subset_df(grp, input_ilocs)
            
            t2 = time.time()-t1
            
            sequences = np.array(list(map(decompose_inputs, input_seq)))
            
            t3 = time.time()-t2
            
            labels = get_targets(grp, n_rows, min_rows_reqd)
            
            t4 = time.time()-t3
            return zip(sequences, labels)
           
        t0 = time.time()
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().impute(dataset, 'forward')
        print(f'Time taken for imputation: {time.time()-t0}')
        groups = dataset.groupby('AUTO_ID')

        S_L_zip = groups.apply(process_patients, min_rows_reqd=self.seq_len+self.lookfwd_months)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
131/71:
dl2 = IBDDataLoader()
%timeit zipped = dl2.fit_data(300)
# train, valid, test = dl.TVT_loaders()
131/72:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        def subset_df(df, start_end_ilocs):
            return [df.iloc[s:e] for s,e in start_end_ilocs]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, n_rows, min_rows_reqd):
            labels = []
            tgt_cols = self.col_dict['tgt']
            for i in range(n_rows-min_rows_reqd+1):
                target_df = grp.iloc[i+min_rows_reqd-1][tgt_cols]
                labels.append(target_df.values)   
            return np.array(labels)
        
        def process_patients(grp, min_rows_reqd):
            start = time.time()
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            input_ilocs = [(i, i+self.seq_len) for i in range(n_rows-min_rows_reqd+1)]
            
            t1 = time.time()-start
            
            input_seq = subset_df(grp, input_ilocs)
            
            t2 = time.time()-t1
            
            sequences = np.array(list(map(decompose_inputs, input_seq)))
            
            t3 = time.time()-t2
            
            labels = get_targets(grp, n_rows, min_rows_reqd)
            
            t4 = time.time()-t3
            t = [t1,t2,t3,t4]
            print([round(x,2) for x in t])
            return zip(sequences, labels)
           
        t0 = time.time()
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().impute(dataset, 'forward')
        print(f'Time taken for imputation: {time.time()-t0}')
        groups = dataset.groupby('AUTO_ID')

        S_L_zip = groups.apply(process_patients, min_rows_reqd=self.seq_len+self.lookfwd_months)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
131/73:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        def subset_df(df, start_end_ilocs):
            return [df.iloc[s:e] for s,e in start_end_ilocs]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, n_rows, min_rows_reqd):
            labels = []
            tgt_cols = self.col_dict['tgt']
            for i in range(n_rows-min_rows_reqd+1):
                target_df = grp.iloc[i+min_rows_reqd-1][tgt_cols]
                labels.append(target_df.values)   
            return np.array(labels)
        
        def process_patients(grp, min_rows_reqd):
            start = time.time()
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            input_ilocs = [(i, i+self.seq_len) for i in range(n_rows-min_rows_reqd+1)]
            
            t1 = time.time()-start
            
            input_seq = subset_df(grp, input_ilocs)
            
            t2 = time.time()-t1
            
            sequences = np.array(list(map(decompose_inputs, input_seq)))
            
            t3 = time.time()-t2
            
            labels = get_targets(grp, n_rows, min_rows_reqd)
            
            t4 = time.time()-t3
            t = [t1,t2,t3,t4]
            print([round(x,2) for x in t])
            
            return zip(sequences, labels)
           
        t0 = time.time()
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().impute(dataset, 'forward')
        print(f'Time taken for imputation: {time.time()-t0}')
        groups = dataset.groupby('AUTO_ID')

        S_L_zip = groups.apply(process_patients, min_rows_reqd=self.seq_len+self.lookfwd_months)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
131/74:
dl2 = IBDDataLoader()
%timeit zipped = dl2.fit_data(300)
# train, valid, test = dl.TVT_loaders()
131/75:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        get = lambda 
        def subset_df(df, start_end_ilocs):
            return [df.iloc[s:e] for s,e in start_end_ilocs]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, n_rows, min_rows_reqd):
            labels = []
            tgt_cols = self.col_dict['tgt']
            for i in range(n_rows-min_rows_reqd+1):
                t0 = time.time()
                target_df = grp.iloc[i+min_rows_reqd-1][tgt_cols]
                t1=round(time.time()-t0,2)
                labels.append(target_df.values)   
                t2=round(time.time()-t1,2)
                print(t1,t2)
            return np.array(labels)
        
        def process_patients(grp, min_rows_reqd):
            start = time.time()
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            input_ilocs = [(i, i+self.seq_len) for i in range(n_rows-min_rows_reqd+1)]
            
            t1 = time.time()-start
            
            input_seq = subset_df(grp, input_ilocs)
            
            t2 = time.time()-t1
            
            sequences = np.array(list(map(decompose_inputs, input_seq)))
            
            t3 = time.time()-t2
            
            labels = get_targets(grp, n_rows, min_rows_reqd)
            
            t4 = time.time()-t3
            t = [t1,t2,t3,t4]
#             print([round(x,2) for x in t])
            
            return zip(sequences, labels)
           
        t0 = time.time()
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().impute(dataset, 'forward')
        print(f'Time taken for imputation: {time.time()-t0}')
        groups = dataset.groupby('AUTO_ID')

        S_L_zip = groups.apply(process_patients, min_rows_reqd=self.seq_len+self.lookfwd_months)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
131/76:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        
        def subset_df(df, start_end_ilocs):
            return [df.iloc[s:e] for s,e in start_end_ilocs]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, n_rows, min_rows_reqd):
            labels = []
            tgt_cols = self.col_dict['tgt']
            for i in range(n_rows-min_rows_reqd+1):
                t0 = time.time()
                target_df = grp.iloc[i+min_rows_reqd-1][tgt_cols]
                t1=round(time.time()-t0,2)
                labels.append(target_df.values)   
                t2=round(time.time()-t1,2)
                print(t1,t2)
            return np.array(labels)
        
        def process_patients(grp, min_rows_reqd):
            start = time.time()
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            input_ilocs = [(i, i+self.seq_len) for i in range(n_rows-min_rows_reqd+1)]
            
            t1 = time.time()-start
            
            input_seq = subset_df(grp, input_ilocs)
            
            t2 = time.time()-t1
            
            sequences = np.array(list(map(decompose_inputs, input_seq)))
            
            t3 = time.time()-t2
            
            labels = get_targets(grp, n_rows, min_rows_reqd)
            
            t4 = time.time()-t3
            t = [t1,t2,t3,t4]
#             print([round(x,2) for x in t])
            
            return zip(sequences, labels)
           
        t0 = time.time()
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().impute(dataset, 'forward')
        print(f'Time taken for imputation: {time.time()-t0}')
        groups = dataset.groupby('AUTO_ID')

        S_L_zip = groups.apply(process_patients, min_rows_reqd=self.seq_len+self.lookfwd_months)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
131/77:
dl2 = IBDDataLoader()
zipped = dl2.fit_data(300)
# train, valid, test = dl.TVT_loaders()
131/78: x,y = zip(*zipped)
131/79: type(zipped)
131/80: zipped
131/81:
l = zipped.values
l
131/82:
l = zipped.values
x,y=zipped(*l[0])
131/83:
l = zipped.values
x,y=zip(*l[0])
131/84:
l = zipped.values
l[0]
131/85:
l = zipped.values
zip(*l[0])
131/86: z = zip([1,2,3],[4,5,6])
131/87:
z = zip([1,2,3],[4,5,6])
x,y = zip(*z)
131/88:
z = zip([1,2,3],[4,5,6])
zip(*z)
131/89:
z = zip([1,2,3],[4,5,6])
x,y=zip(*z)
131/90:
z = zip([1,2,3],[4,5,6])
x,y=zip(*z)
x,y
131/91:
l = zipped.values
x,y=zip(*l[0])
x,y
131/92:
l = zipped.values
list(l[0])
x,y
131/93:
l = zipped.values
list(l[0])
131/94:
l = zipped.values
list(l[1])
131/95:
l = zipped.values
# list(l[1])
zip(*l[1])
131/96:
l = zipped.values
# list(l[1])
x,y=zip(*l[1])
x,y
131/97:
l = zipped.values
list(l[1])
# x,y=zip(*l[1])
# x,y
131/98:
l = zipped.values
# list(l[1])
x,y=zip(*l[2])
x,y
131/99:
l = zipped.values
list(l[2])
# x,y=zip(*l[2])
# x,y
131/100:
z = zip([1,2,3],[4,5,6])
s = pd.Series([z,z,z])
f = lambda z:tuple(zip(*z))
x=s.apply(f)
print(x)
131/101:
z = zip([1,2,3],[4,5,6])
s = pd.Series([z,z,z])
print(s)
# f = lambda z:tuple(zip(*z))
# x=s.apply(f)
# print(x)
131/102:
z = zip([1,2,3],[4,5,6])
s = pd.Series([z,z,z])
print(s)
s.iloc[:3]
131/103:
z = zip([1,2,3],[4,5,6])
s = pd.Series([z,z,z])
print(s)
s.iloc[:4]
131/104: df
131/105:
grp = df.head(30)
grp.iloc[9:30]
131/106:
# grp = df.head(30)
input_ilocs = [(i, i+8) for i in range(30-10+1)]
input_ilocs
131/107:
# grp = df.head(30)
input_ilocs = [(i, i+8) for i in range(30-10+1)]
len(input_ilocs)
131/108:
# grp = df.head(30)
# input_ilocs = [(i, i+8) for i in range(30-10+1)]
# len(input_ilocs)
pd.np.r_[10:12, 25:28]
131/109:
# grp = df.head(30)
# input_ilocs = [(i, i+8) for i in range(30-10+1)]
# len(input_ilocs)
df[['TGT_1', 'TGT_2']].values
131/110:
# grp = df.head(30)
# input_ilocs = [(i, i+8) for i in range(30-10+1)]
[grp.iloc[s:e] for s,e in start_end_ilocs]
131/111:
# grp = df.head(30)
# input_ilocs = [(i, i+8) for i in range(30-10+1)]
[grp.iloc[s:e] for s,e in input_ilocs]
131/112:
# grp = df.head(30)
# input_ilocs = [(i, i+8) for i in range(30-10+1)]
%timeit [grp.iloc[i:i+8] for i in range(30-10+1)]
131/113:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        
        def subset_df(df, n_rows, min_rows_reqd):
#             return [df.iloc[s:e] for s,e in start_end_ilocs]
            return [grp.iloc[i:i+self.seq_len] for i in range(n_rows-min_rows_reqd+1)]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, n_rows, min_rows_reqd):
            tgt_cols = self.col_dict['tgt']
            target_df = grp.iloc[min_rows_reqd-1:n_rows-1][tgt_cols].values               
            return target_df
        
        def process_patients(grp, min_rows_reqd):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
#             input_ilocs = [(i, i+self.seq_len) for i in range(n_rows-min_rows_reqd+1)] 
            t = time.time()
            input_seq = subset_df(grp, n_rows, min_rows_reqd)
            print(time.time()-t)
            sequences = np.array(list(map(decompose_inputs, input_seq)))
            labels = get_targets(grp, n_rows, min_rows_reqd)
            return zip(sequences, labels)
           
        t0 = time.time()
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        imputed_dataset = du.Imputer().impute(dataset, 'forward')
        print(f'Time taken for imputation: {time.time()-t0}')
        groups = dataset.groupby('AUTO_ID')
        min_rows_reqd=self.seq_len+self.lookfwd_months
        S_L_zip = groups.apply(process_patients, min_rows_reqd=min_rows_reqd)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
131/114:
dl2 = IBDDataLoader()
zipped = dl2.fit_data(300)
# train, valid, test = dl.TVT_loaders()
131/115:
input_start = 0
input_end = 21 #n_rows-min_rows_reqd+1
input_step = 8 #self.sql_len

target_start = min_rows_reqd-1
target_end = n_rows-1
target_step = 1

list(range(input_start, input_end, input_step))
131/116:
input_start = 0
input_end = 21 #n_rows-min_rows_reqd+1
input_step = 8 #self.sql_len

# target_start = min_rows_reqd-1
# target_end = n_rows-1
# target_step = 1

list(range(input_start, input_end, input_step))
131/117:
input_start = 0
input_end = 21 #n_rows-min_rows_reqd+1
input_step = 8 #self.sql_len

# target_start = min_rows_reqd-1
# target_end = n_rows-1
# target_step = 1

list(range(0,21))
132/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.autograd import Variable
import data_utils as du
import math


import matplotlib.pyplot as plt
%matplotlib inline
132/2:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        def subset_df(df, n_rows, min_rows_reqd):
            return [grp.iloc[i:i+self.seq_len] for i in range(n_rows-min_rows_reqd+1)]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, n_rows, min_rows_reqd):
            tgt_cols = self.col_dict['tgt']
            target_df = grp.iloc[min_rows_reqd-1:n_rows-1][tgt_cols].values               
            return target_df
        
        def process_patients(grp, min_rows_reqd, imputer=None):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            if imputer: grp = imputer(grp)
                
            def data(pos): # This returns a sequence of the length num_steps starting from pos
                if grp.shape[0] < pos+self.seq_len: return
                return grp.iloc[pos: pos + self.seq_len]
            
            t0 = time.time()
            input_start_ix = 0
            input_final_ix = n_rows-min_rows_reqd+1
            input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
            t1 = time.time()-t0
            sequences = np.array(list(map(decompose_inputs, input_seq)))
            t2 = time.time()-t1
            target_start_ix = min_rows_reqd-1
            target_final_ix = n_rows-1
            labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values
            t3 = time.time()-t2
#             input_seq = subset_df(grp, n_rows, min_rows_reqd)
#             labels = get_targets(grp, n_rows, min_rows_reqd)
            print([t1,t2,t3])
            return zip(sequences, labels)
           

        dataset, self.col_dict = du.X_raw() #(243563 rows)

        if sample: dataset = dataset.head(sample)
#         imputed_dataset = du.Imputer().impute(dataset, 'forward')

        groups = dataset.groupby('AUTO_ID')
        min_rows_reqd=self.seq_len+self.lookfwd_months
        
        imputer = du.Imputer().get_imputer('forward-mini')
        S_L_zip = groups.apply(process_patients, min_rows_reqd=min_rows_reqd, imputer=)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
132/3:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.autograd import Variable
import data_utils as du
import math


import matplotlib.pyplot as plt
%matplotlib inline
132/4:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        def subset_df(df, n_rows, min_rows_reqd):
            return [grp.iloc[i:i+self.seq_len] for i in range(n_rows-min_rows_reqd+1)]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, n_rows, min_rows_reqd):
            tgt_cols = self.col_dict['tgt']
            target_df = grp.iloc[min_rows_reqd-1:n_rows-1][tgt_cols].values               
            return target_df
        
        def process_patients(grp, min_rows_reqd, imputer=None):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            if imputer: grp = imputer(grp)
                
            def data(pos): # This returns a sequence of the length num_steps starting from pos
                if grp.shape[0] < pos+self.seq_len: return
                return grp.iloc[pos: pos + self.seq_len]
            
            t0 = time.time()
            input_start_ix = 0
            input_final_ix = n_rows-min_rows_reqd+1
            input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
            t1 = time.time()-t0
            sequences = np.array(list(map(decompose_inputs, input_seq)))
            t2 = time.time()-t1
            target_start_ix = min_rows_reqd-1
            target_final_ix = n_rows-1
            labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values
            t3 = time.time()-t2
#             input_seq = subset_df(grp, n_rows, min_rows_reqd)
#             labels = get_targets(grp, n_rows, min_rows_reqd)
            print([t1,t2,t3])
            return zip(sequences, labels)
           

        dataset, self.col_dict = du.X_raw() #(243563 rows)

        if sample: dataset = dataset.head(sample)
#         imputed_dataset = du.Imputer().impute(dataset, 'forward')

        groups = dataset.groupby('AUTO_ID')
        min_rows_reqd=self.seq_len+self.lookfwd_months
        
        imputer = du.Imputer().get_imputer('forward-mini')
        S_L_zip = groups.apply(process_patients, min_rows_reqd=min_rows_reqd, imputer=)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
132/5:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        def subset_df(df, n_rows, min_rows_reqd):
            return [grp.iloc[i:i+self.seq_len] for i in range(n_rows-min_rows_reqd+1)]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, n_rows, min_rows_reqd):
            tgt_cols = self.col_dict['tgt']
            target_df = grp.iloc[min_rows_reqd-1:n_rows-1][tgt_cols].values               
            return target_df
        
        def process_patients(grp, min_rows_reqd, imputer=None):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            if imputer: grp = imputer(grp)
                
            def data(pos): # This returns a sequence of the length num_steps starting from pos
                if grp.shape[0] < pos+self.seq_len: return
                return grp.iloc[pos: pos + self.seq_len]
            
            t0 = time.time()
            input_start_ix = 0
            input_final_ix = n_rows-min_rows_reqd+1
            input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
            t1 = time.time()-t0
            sequences = np.array(list(map(decompose_inputs, input_seq)))
            t2 = time.time()-t1
            target_start_ix = min_rows_reqd-1
            target_final_ix = n_rows-1
            labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values
            t3 = time.time()-t2
#             input_seq = subset_df(grp, n_rows, min_rows_reqd)
#             labels = get_targets(grp, n_rows, min_rows_reqd)
            print([t1,t2,t3])
            return zip(sequences, labels)
           

        dataset, self.col_dict = du.X_raw() #(243563 rows)

        if sample: dataset = dataset.head(sample)
#         imputed_dataset = du.Imputer().impute(dataset, 'forward')

        groups = dataset.groupby('AUTO_ID')
        min_rows_reqd=self.seq_len+self.lookfwd_months
        
        imputer = du.Imputer().get_imputer('forward-mini')
        S_L_zip = groups.apply(process_patients, min_rows_reqd=min_rows_reqd, imputer=imputer)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
132/6:
dl2 = IBDDataLoader()
zipped = dl2.fit_data(300)
# train, valid, test = dl.TVT_loaders()
133/1:
import data_utils as du
import pandas as pd
imputer = du.Imputer().get_imputer('forward-mini')
df = pd.read_pickle('pickles/inputs.pkl')
df0 = df[df.AUTO_ID==0]
133/2: imputer(df0)
133/3: df0
133/4: df0.loc[0,:]
133/5: df0.loc[df0.MONTH_TS==0,:]
134/1:
import data_utils as du
import pandas as pd
imputer = du.Imputer().get_imputer('forward-mini')
df = pd.read_pickle('pickles/inputs.pkl')
df0 = df[df.AUTO_ID==0]
134/2: imputer(df0)
135/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.autograd import Variable
import data_utils as du
import math


import matplotlib.pyplot as plt
%matplotlib inline
135/2:
import importlib as imp
imp.reload(du)
135/3:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        def subset_df(df, n_rows, min_rows_reqd):
            return [grp.iloc[i:i+self.seq_len] for i in range(n_rows-min_rows_reqd+1)]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, n_rows, min_rows_reqd):
            tgt_cols = self.col_dict['tgt']
            target_df = grp.iloc[min_rows_reqd-1:n_rows-1][tgt_cols].values               
            return target_df
        
        def process_patients(grp, min_rows_reqd, imputer=None):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            if imputer: grp = imputer(grp)
                
            def data(pos): # This returns a sequence of the length num_steps starting from pos
                if grp.shape[0] < pos+self.seq_len: return
                return grp.iloc[pos: pos + self.seq_len]
            
            t0 = time.time()
            input_start_ix = 0
            input_final_ix = n_rows-min_rows_reqd+1
            input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
            t1 = time.time()-t0
            sequences = np.array(list(map(decompose_inputs, input_seq)))
            t2 = time.time()-t1
            target_start_ix = min_rows_reqd-1
            target_final_ix = n_rows-1
            labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values
            t3 = time.time()-t2
#             input_seq = subset_df(grp, n_rows, min_rows_reqd)
#             labels = get_targets(grp, n_rows, min_rows_reqd)
            print([t1,t2,t3])
            return zip(sequences, labels)
           

        dataset, self.col_dict = du.X_raw() #(243563 rows)

        if sample: dataset = dataset.head(sample)
#         imputed_dataset = du.Imputer().impute(dataset, 'forward')

        groups = dataset.groupby('AUTO_ID')
        min_rows_reqd=self.seq_len+self.lookfwd_months
        
        imputer = du.Imputer().get_imputer('forward-mini')
        S_L_zip = groups.apply(process_patients, min_rows_reqd=min_rows_reqd, imputer=imputer)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
135/4:
def try_gpu():
    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
    is_cuda = torch.cuda.is_available()
    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    return device
135/5:
    def get_params(self, input_dim, hidden_dim, ctx):
        # Initialize Weight matrices
        def normal(shape):
            return torch.randn(size=shape, device=ctx)

        def three():
            return (normal((input_dim, hidden_dim)),
                    normal((input_dim, hidden_dim)),
                    np.zeros(hidden_dim, device=ctx))
        
        W_xz, W_hz, b_z = three()  # Update gate parameter
        W_xr, W_hr, b_r = three()  # Reset gate parameter
        W_xh, W_hh, b_h = three()  # Candidate hidden state parameter
        
        W_hq = normal((num_hiddens, num_outputs))
        b_q = np.zeros(num_outputs, device=ctx)
        params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]
#         for param in params:
#             param.attach_grad()
        return params

    def reset_parameters(self):
        stdv = 1.0 / math.sqrt(self.hidden_size)
        for weight in self.parameters():
            init.uniform_(weight, -stdv, stdv)
135/6:
class GRUD(nn.Module):
    
    def __init__(self, input_dim, hidden_dim, hidden_layers, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = Variable(torch.zeros(delta_dim))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:]), \
                            torch.squeeze(inputs[:,1,:]), \
                            torch.squeeze(inputs[:,2,:]), \
                            torch.squeeze(inputs[:,3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros, self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros, self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.cat((x,h.double(),obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
135/7:
dl2 = IBDDataLoader()
zipped = dl2.fit_data(300)
# train, valid, test = dl.TVT_loaders()
135/8:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.autograd import Variable
import numpy as np
import data_utils as du
import math
135/9:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
        def subset_df(df, n_rows, min_rows_reqd):
            return [grp.iloc[i:i+self.seq_len] for i in range(n_rows-min_rows_reqd+1)]
            
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, n_rows, min_rows_reqd):
            tgt_cols = self.col_dict['tgt']
            target_df = grp.iloc[min_rows_reqd-1:n_rows-1][tgt_cols].values               
            return target_df
        
        def process_patients(grp, min_rows_reqd, imputer=None):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            if imputer: grp = imputer(grp)
                
            def data(pos): # This returns a sequence of the length num_steps starting from pos
                if grp.shape[0] < pos+self.seq_len: return
                return grp.iloc[pos: pos + self.seq_len]
            
            t0 = time.time()
            input_start_ix = 0
            input_final_ix = n_rows-min_rows_reqd+1
            input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
            t1 = time.time()-t0
            sequences = np.array(list(map(decompose_inputs, input_seq)))
            t2 = time.time()-t1
            target_start_ix = min_rows_reqd-1
            target_final_ix = n_rows-1
            labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values
            t3 = time.time()-t2
#             input_seq = subset_df(grp, n_rows, min_rows_reqd)
#             labels = get_targets(grp, n_rows, min_rows_reqd)
            print([t1,t2,t3])
            return zip(sequences, labels)
           

        dataset, self.col_dict = du.X_raw() #(243563 rows)

        if sample: dataset = dataset.head(sample)
#         imputed_dataset = du.Imputer().impute(dataset, 'forward')

        groups = dataset.groupby('AUTO_ID')
        min_rows_reqd=self.seq_len+self.lookfwd_months
        
        imputer = du.Imputer().get_imputer('forward-mini')
        S_L_zip = groups.apply(process_patients, min_rows_reqd=min_rows_reqd, imputer=imputer)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
135/10:
dl2 = IBDDataLoader()
zipped = dl2.fit_data(300)
# train, valid, test = dl.TVT_loaders()
136/1:
import time

t0 = time.time()

import data_utils as du
import pandas as pd
imputer = du.Imputer().get_imputer('forward-mini')
df = pd.read_pickle('pickles/inputs.pkl')

print(time.time()-t0)
135/11:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
           
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, n_rows, min_rows_reqd):
            tgt_cols = self.col_dict['tgt']
            target_df = grp.iloc[min_rows_reqd-1:n_rows-1][tgt_cols].values               
            return target_df
        
        def process_patients(grp, min_rows_reqd, imputer=None):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            if imputer: grp = imputer(grp)
                
            def data(pos): # This returns a sequence of the length num_steps starting from pos
                if grp.shape[0] < pos+self.seq_len: return
                return grp.iloc[pos: pos + self.seq_len]
            
            input_start_ix = 0
            input_final_ix = n_rows-min_rows_reqd+1
            input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
            t0 = time.time()
            sequences = np.array(list(map(decompose_inputs, input_seq)))
            t2 = time.time()-t0
            target_start_ix = min_rows_reqd-1
            target_final_ix = n_rows-1
            labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values
#             input_seq = subset_df(grp, n_rows, min_rows_reqd)
#             labels = get_targets(grp, n_rows, min_rows_reqd)
            print(t2)
            return zip(sequences, labels)
           

        dataset, self.col_dict = du.X_raw() #(243563 rows)

        if sample: dataset = dataset.head(sample)
#         imputed_dataset = du.Imputer().impute(dataset, 'forward')

        groups = dataset.groupby('AUTO_ID')
        min_rows_reqd=self.seq_len+self.lookfwd_months
        
        imputer = du.Imputer().get_imputer('forward-mini')
        S_L_zip = groups.apply(process_patients, min_rows_reqd=min_rows_reqd, imputer=imputer)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
135/12:
dl2 = IBDDataLoader()
zipped = dl2.fit_data(300)
# train, valid, test = dl.TVT_loaders()
135/13: zipped
135/14:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
           
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, n_rows, min_rows_reqd):
            tgt_cols = self.col_dict['tgt']
            target_df = grp.iloc[min_rows_reqd-1:n_rows-1][tgt_cols].values               
            return target_df
        
        def process_patients(grp, min_rows_reqd, imputer=None):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            if imputer: grp = imputer(grp)
                
            def data(pos): # This returns a sequence of the length num_steps starting from pos
                if grp.shape[0] < pos+self.seq_len: return
                return grp.iloc[pos: pos + self.seq_len]
            
            input_start_ix = 0
            input_final_ix = n_rows-min_rows_reqd+1
            input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]

            sequences = np.array(list(map(decompose_inputs, input_seq)))
            target_start_ix = min_rows_reqd-1
            target_final_ix = n_rows-1
            labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values
#             input_seq = subset_df(grp, n_rows, min_rows_reqd)
#             labels = get_targets(grp, n_rows, min_rows_reqd)
            print(t2)
            print(len(sequences), len(labels))
            return zip(sequences, labels)
           

        dataset, self.col_dict = du.X_raw() #(243563 rows)

        if sample: dataset = dataset.head(sample)
#         imputed_dataset = du.Imputer().impute(dataset, 'forward')

        groups = dataset.groupby('AUTO_ID')
        min_rows_reqd=self.seq_len+self.lookfwd_months
        
        imputer = du.Imputer().get_imputer('forward-mini')
        S_L_zip = groups.apply(process_patients, min_rows_reqd=min_rows_reqd, imputer=imputer)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
135/15:
dl2 = IBDDataLoader()
zipped = dl2.fit_data(300)
# train, valid, test = dl.TVT_loaders()
135/16:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        
           
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 
        
        def get_targets(grp, n_rows, min_rows_reqd):
            tgt_cols = self.col_dict['tgt']
            target_df = grp.iloc[min_rows_reqd-1:n_rows-1][tgt_cols].values               
            return target_df
        
        def process_patients(grp, min_rows_reqd, imputer=None):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            if imputer: grp = imputer(grp)
                
            def data(pos): # This returns a sequence of the length num_steps starting from pos
                if grp.shape[0] < pos+self.seq_len: return
                return grp.iloc[pos: pos + self.seq_len]
            
            input_start_ix = 0
            input_final_ix = n_rows-min_rows_reqd+1
            input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]

            sequences = np.array(list(map(decompose_inputs, input_seq)))
            target_start_ix = min_rows_reqd-1
            target_final_ix = n_rows-1
            labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values
#             input_seq = subset_df(grp, n_rows, min_rows_reqd)
#             labels = get_targets(grp, n_rows, min_rows_reqd)
            print(len(sequences), len(labels))
            return zip(sequences, labels)
           

        dataset, self.col_dict = du.X_raw() #(243563 rows)

        if sample: dataset = dataset.head(sample)
#         imputed_dataset = du.Imputer().impute(dataset, 'forward')

        groups = dataset.groupby('AUTO_ID')
        min_rows_reqd=self.seq_len+self.lookfwd_months
        
        imputer = du.Imputer().get_imputer('forward-mini')
        S_L_zip = groups.apply(process_patients, min_rows_reqd=min_rows_reqd, imputer=imputer)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
135/17:
dl2 = IBDDataLoader()
zipped = dl2.fit_data(300)
# train, valid, test = dl.TVT_loaders()
135/18: zipped
135/19:
a = zipped.loc[0]
seq, labels = zip(*a)
135/20: seq
135/21: len(seq)
135/22: len(labels)
135/23: zipped
135/24:
a = zipped.loc[1]
seq, labels = zip(*a)
135/25: len(seq)
135/26: len(seq)
135/27: len(seq)
135/28: len(labels)
135/29:
a = zipped.loc[0]
seq, labels = zip(*a)
135/30:
a = zipped.loc[2]
seq, labels = zip(*a)
135/31: len(labels)
135/32: labels
135/33: seq
135/34: seq[0]
135/35: len(seq[0])
135/36: len(seq[0][0])
135/37: len(seq[0][0][0])
135/38: seq[0][0][0]
135/39: seq[0][0][1]
135/40:
dl2 = IBDDataLoader()
zipped = dl2.fit_data(300)
# train, valid, test = dl.TVT_loaders()
135/41: zipped
135/42: zipped.values
136/2: df.head(300).AUTO_ID.unique()
136/3:
x=df.head(300)
x[x.AUTO_ID==4].shape
135/43:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        def process_patients(grp, min_rows_reqd, imputer=None):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            if imputer: grp = imputer(grp)
                
            def data(pos): # This returns a sequence of the length num_steps starting from pos
                if grp.shape[0] < pos+self.seq_len: return
                return grp.iloc[pos: pos + self.seq_len]
            def decompose_inputs(input_df):
                x = input_df[self.col_dict['input']]
                x_obs = x.shift(1).fillna(x.iloc[0]).values
                m = input_df[self.col_dict['missing']].values
                t = input_df[self.col_dict['delta']].values
                x=x.values
                return [x[:,1:], m, t, x_obs[:,1:]] 
            
            input_start_ix = 0
            input_final_ix = n_rows-min_rows_reqd+1
            
            input_seq=[]
            for j in range(input_start_ix, input_final_ix):
                d = data(j)
                if d: input_seq.append(d)

            sequences = np.array(list(map(decompose_inputs, input_seq)))
            target_start_ix = min_rows_reqd-1
            target_final_ix = n_rows-1
            labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values
#             input_seq = subset_df(grp, n_rows, min_rows_reqd)
#             labels = get_targets(grp, n_rows, min_rows_reqd)
            print(len(sequences), len(labels))
            return zip(sequences, labels)
           

        dataset, self.col_dict = du.X_raw() #(243563 rows)

        if sample: dataset = dataset.head(sample)
#         imputed_dataset = du.Imputer().impute(dataset, 'forward')

        groups = dataset.groupby('AUTO_ID')
        min_rows_reqd=self.seq_len+self.lookfwd_months
        
        imputer = du.Imputer().get_imputer('forward-mini')
        S_L_zip = groups.apply(process_patients, min_rows_reqd=min_rows_reqd, imputer=imputer)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
135/44:
dl2 = IBDDataLoader()
zipped = dl2.fit_data(300)
# train, valid, test = dl.TVT_loaders()
135/45:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.sequences=None
        self.labels=None
        self.partition=None
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        def process_patients(grp, min_rows_reqd, imputer=None):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            if imputer: grp = imputer(grp)
                
            def data(pos): # This returns a sequence of the length num_steps starting from pos
                if grp.shape[0] < pos+self.seq_len: return
                return grp.iloc[pos: pos + self.seq_len]
            def decompose_inputs(input_df):
                x = input_df[self.col_dict['input']]
                x_obs = x.shift(1).fillna(x.iloc[0]).values
                m = input_df[self.col_dict['missing']].values
                t = input_df[self.col_dict['delta']].values
                x=x.values
                return [x[:,1:], m, t, x_obs[:,1:]] 
            
            input_start_ix = 0
            input_final_ix = n_rows-min_rows_reqd+1
            
            input_seq=[]
            for j in range(input_start_ix, input_final_ix):
                d = data(j)
                if d is not None: input_seq.append(d)

            sequences = np.array(list(map(decompose_inputs, input_seq)))
            target_start_ix = min_rows_reqd-1
            target_final_ix = n_rows-1
            labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values
#             input_seq = subset_df(grp, n_rows, min_rows_reqd)
#             labels = get_targets(grp, n_rows, min_rows_reqd)
            print(len(sequences), len(labels))
            return zip(sequences, labels)
           

        dataset, self.col_dict = du.X_raw() #(243563 rows)

        if sample: dataset = dataset.head(sample)
#         imputed_dataset = du.Imputer().impute(dataset, 'forward')

        groups = dataset.groupby('AUTO_ID')
        min_rows_reqd=self.seq_len+self.lookfwd_months
        
        imputer = du.Imputer().get_imputer('forward-mini')
        S_L_zip = groups.apply(process_patients, min_rows_reqd=min_rows_reqd, imputer=imputer)
        return S_L_zip
#         self.sequences, self.labels = zip(*S_L_zip)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
135/46:
dl2 = IBDDataLoader()
zipped = dl2.fit_data(300)
# train, valid, test = dl.TVT_loaders()
135/47: zipped.values
136/4:
x=df.head(300)
x[x.AUTO_ID==0].shape
136/5:
def data(j):
    return x[j:j+8]

input_start_ix = 0
input_final_ix = x.shape[0]-10+1
input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
137/1:
import time



import data_utils as du
import pandas as pd
df = pd.read_pickle('pickles/inputs.pkl')
137/2:
x=df.head(300)
x = x[x.AUTO_ID==0]
137/3:
def data(j):
    return x[j:j+8]

input_start_ix = 0
input_final_ix = x.shape[0]-10+1
input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
137/4: len(input_seq)
137/5: input_seq[-1]
137/6: x
137/7:
seq_len = 15
lf = 4
def data(j):
    return x[j:j+seq_len]

input_start_ix = 0
input_final_ix = x.shape[0]-seq_len-lf+1
input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
137/8: input_seq[-1]
137/9: input_seq[0]
137/10:
target_start_ix = seq_len+lf-1
x.iloc[target_start_ix]
# target_final_ix = x.shape[0]-1
# labels = grp.iloc[target_start_ix:target_final_ix]
137/11:
target_start_ix = seq_len+lf-1
x.iloc[-1]
# target_final_ix = x.shape[0]-1
# labels = grp.iloc[target_start_ix:target_final_ix]
137/12: input_seq[-1]
135/48:
dl2 = IBDDataLoader()
zipped = dl2.fit_data(300)
# train, valid, test = dl.TVT_loaders()
135/49:
unzip = lambda z:tuple(zip(*z))
a = list(map(unzip, zipped[:1]))
135/50: a
135/51: len(a)
135/52: a
135/53: a[0]
135/54: len(a[0])
135/55:
dl2 = IBDDataLoader()
zipped = dl2.fit_data(500)
# train, valid, test = dl.TVT_loaders()
135/56: zipped.values
135/57: len(a[0][0])
135/58:
unzip = lambda z:tuple(zip(*z))
a = list(map(unzip, zipped[:1]))
135/59: len(a)
135/60: len(a[0])
135/61:
b=a[0]
len(b[0])
135/62:
b=a[0] #l=2
c=b[0] #57
len(c)
135/63:
b=a[0] #l=2
c=b[0] #57
len(c[0])
135/64:
b=a[0] #l=2
c=b[0] #57
c[0]
135/65: aa = np.array(a)
135/66:
# aa = np.array(a)
aa.shape
135/67:
b=a[0] #l=2
c=b[0] #57
d=c[0] #4
len(d[0])
135/68:
b=a[0] #l=2
c=b[0] #57 number of sequences for that patient
d=c[0] #4 input types
e=d[0] #36 seq_len
len(e[0])
135/69:
b=a[1] #l=2
# c=b[0] #57 number of sequences for that patient
# d=c[0] #4 input types
# e=d[0] #36 seq_len
# f=e[0] #47 input_dim
135/70: len(a)
135/71:
dl2 = IBDDataLoader()
zipped = dl2.fit_data(500)
# train, valid, test = dl.TVT_loaders()
135/72: zipped.values
135/73:
unzip = lambda z:tuple(zip(*z))
a = map(unzip, zipped)
135/74:
aa = np.array(a)
aa.shape
135/75: a
135/76:
aa = np.array(list(a))
aa.shape
135/77:
s = aa[:,0]
l = aa[:,1]
135/78: l
135/79: len(l)
135/80: len(l[0])
135/81: l.shape
135/82: l[0][0].shape
135/83: len(s)
135/84: len(s[0])
135/85: len(s[0][0])
135/86: len(l)
135/87:
flat_l = l.flatten()
flat_l.shape
135/88:
flat_l = l.flatten()
flat_l
135/89: l
135/90:
dl2 = IBDDataLoader()
zipped = dl2.fit_data(500)
unzip = lambda z:list(zip(*z))
a = map(unzip, zipped)
# train, valid, test = dl.TVT_loaders()
135/91:
aa = np.array(list(a))
aa.shape # (patients x 2)
135/92:
s = aa[:,0]
l = aa[:,1]
135/93:
s = aa[:,0]
l = aa[:,1]
135/94: l
135/95: l.shape
135/96:
s = aa[:,0]
l = aa[:,1]
135/97: l
135/98: l.flatten
135/99: l.flatten()
135/100: len(a)
135/101: len(aa)
135/102:
x = aa[0]
len(x)
135/103:
dl2 = IBDDataLoader()
%timeit zipped = dl2.fit_data(500)
# unzip = lambda z:list(zip(*z))
# a = map(unzip, zipped)
# train, valid, test = dl.TVT_loaders()
135/104:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.partition=None
        
        self.ehr_sequences = None # [patient_id ; seq_id ; inputs (type_id ; step(t) ; features)]
        self.ehr_labels = None # [patient_id ; seq_id ; targets (1*3) ]
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        def process_patients(grp, min_rows_reqd, imputer=None):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            if imputer: grp = imputer(grp)
                
            def data(pos): # This returns a sequence of the length num_steps starting from pos
                if grp.shape[0] < pos+self.seq_len: return
                return grp.iloc[pos: pos + self.seq_len]
            def decompose_inputs(input_df):
                x = input_df[self.col_dict['input']]
                x_obs = x.shift(1).fillna(x.iloc[0]).values
                m = input_df[self.col_dict['missing']].values
                t = input_df[self.col_dict['delta']].values
                x=x.values
                return [x[:,1:], m, t, x_obs[:,1:]] 
            
            input_start_ix = 0
            input_final_ix = n_rows-min_rows_reqd+1
            input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
            sequences = np.array(list(map(decompose_inputs, input_seq)))
            
            target_start_ix = min_rows_reqd-1
            target_final_ix = n_rows-1
            labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values            
            print(len(sequences), len(labels))
            return zip(sequences, labels)
           

        dataset, self.col_dict = du.X_raw() #(243563 rows)

        if sample: dataset = dataset.head(sample)
#         imputed_dataset = du.Imputer().impute(dataset, 'forward')
        min_rows_reqd=self.seq_len+self.lookfwd_months
        groups = dataset.groupby('AUTO_ID')
        
        imputer = du.Imputer().get_imputer('forward-mini')
#         S_L_zip = groups.apply(process_patients, min_rows_reqd=min_rows_reqd, imputer=imputer)
        
        S_L_zip=[]
        for g in groups:
            S_L_zip.append(process_patients(g, min_rows_reqd, imputer))
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
135/105:
dl2 = IBDDataLoader()
%timeit zipped = dl2.fit_data(500)
# unzip = lambda z:list(zip(*z))
# a = map(unzip, zipped)
# train, valid, test = dl.TVT_loaders()
135/106:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.partition=None
        
        self.ehr_sequences = None # [patient_id ; seq_id ; inputs (type_id ; step(t) ; features)]
        self.ehr_labels = None # [patient_id ; seq_id ; targets (1*3) ]
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    def fit_data(self, sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        def process_patients(grp, min_rows_reqd, imputer=None):
            n_rows = grp.shape[0]
            if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
            if imputer: grp = imputer(grp)
                
            def data(pos): # This returns a sequence of the length num_steps starting from pos
                if grp.shape[0] < pos+self.seq_len: return
                return grp.iloc[pos: pos + self.seq_len]
            def decompose_inputs(input_df):
                x = input_df[self.col_dict['input']]
                x_obs = x.shift(1).fillna(x.iloc[0]).values
                m = input_df[self.col_dict['missing']].values
                t = input_df[self.col_dict['delta']].values
                x=x.values
                return [x[:,1:], m, t, x_obs[:,1:]] 
            
            input_start_ix = 0
            input_final_ix = n_rows-min_rows_reqd+1
            input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
            sequences = np.array(list(map(decompose_inputs, input_seq)))
            
            target_start_ix = min_rows_reqd-1
            target_final_ix = n_rows-1
            labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values            
            print(len(sequences), len(labels))
            return zip(sequences, labels)
           

        dataset, self.col_dict = du.X_raw() #(243563 rows)

        if sample: dataset = dataset.head(sample)
#         imputed_dataset = du.Imputer().impute(dataset, 'forward')
        min_rows_reqd=self.seq_len+self.lookfwd_months
        groups = dataset.groupby('AUTO_ID')
        
        imputer = du.Imputer().get_imputer('forward-mini')
#         S_L_zip = groups.apply(process_patients, min_rows_reqd=min_rows_reqd, imputer=imputer)
        
        S_L_zip=[]
        for _,g in groups:
            S_L_zip.append(process_patients(g, min_rows_reqd, imputer))
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
135/107:
dl2 = IBDDataLoader()
%timeit zipped = dl2.fit_data(500)
# unzip = lambda z:list(zip(*z))
# a = map(unzip, zipped)
# train, valid, test = dl.TVT_loaders()
135/108:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.partition=None
        
        self.ehr_sequences = None # [patient_id ; seq_id ; inputs (type_id ; step(t) ; features)]
        self.ehr_labels = None # [patient_id ; seq_id ; targets (1*3) ]
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    
    def process_patients_(grp, min_rows_reqd, imputer=None):
        n_rows = grp.shape[0]
        if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
        if imputer: grp = imputer(grp)

        def data(pos): # This returns a sequence of the length num_steps starting from pos
            if grp.shape[0] < pos+self.seq_len: return
            return grp.iloc[pos: pos + self.seq_len]
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 

        input_start_ix = 0
        input_final_ix = n_rows-min_rows_reqd+1
        input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
        sequences = np.array(list(map(decompose_inputs, input_seq)))

        target_start_ix = min_rows_reqd-1
        target_final_ix = n_rows-1
        labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values            

        return sequences, labels
    
    
    def write_to_disk(self, fout='./pt_data/', sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        def pkl_dump(obj, f):
            with open(f, 'wb') as fo:
                pickle.dump(onj, fo)
                
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        min_rows_reqd=self.seq_len+self.lookfwd_months
        groups = dataset.groupby('AUTO_ID')
        imputer = du.Imputer().get_imputer('forward-mini')
        
        for autoid,g in groups:
            s, l = process_patients_(g, min_rows_reqd, imputer)
            s_out = cfg.opj(fout, 'inputs', f'{auto_id}.npy')
            l_out = cfg.opj(fout, 'labels', f'{auto_id}.npy')
            pkl_dump(s, s_out)
            pkl_dump(l, l_out)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
138/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.autograd import Variable
import numpy as np
import data_utils as du
import math
import config as cfg
138/2:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.partition=None
        
        self.ehr_sequences = None # [patient_id ; seq_id ; inputs (type_id ; step(t) ; features)]
        self.ehr_labels = None # [patient_id ; seq_id ; targets (1*3) ]
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    
    def process_patients_(grp, min_rows_reqd, imputer=None):
        n_rows = grp.shape[0]
        if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
        if imputer: grp = imputer(grp)

        def data(pos): # This returns a sequence of the length num_steps starting from pos
            if grp.shape[0] < pos+self.seq_len: return
            return grp.iloc[pos: pos + self.seq_len]
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 

        input_start_ix = 0
        input_final_ix = n_rows-min_rows_reqd+1
        input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
        sequences = np.array(list(map(decompose_inputs, input_seq)))

        target_start_ix = min_rows_reqd-1
        target_final_ix = n_rows-1
        labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values            

        return sequences, labels
    
    
    def write_to_disk(self, fout='./pt_data/', sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        def pkl_dump(obj, f):
            with open(f, 'wb') as fo:
                pickle.dump(onj, fo)
                
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        min_rows_reqd=self.seq_len+self.lookfwd_months
        groups = dataset.groupby('AUTO_ID')
        imputer = du.Imputer().get_imputer('forward-mini')
        
        for autoid,g in groups:
            s, l = process_patients_(g, min_rows_reqd, imputer)
            s_out = cfg.opj(fout, 'inputs', f'{auto_id}.npy')
            l_out = cfg.opj(fout, 'labels', f'{auto_id}.npy')
            pkl_dump(s, s_out)
            pkl_dump(l, l_out)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
138/3:
dl2 = IBDDataLoader()
t0 = time.time()
dl2.write_to_disk(500)
print(time.time()-t0)
# unzip = lambda z:list(zip(*z))
# a = map(unzip, zipped)
# train, valid, test = dl.TVT_loaders()
138/4:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.partition=None
        
        self.ehr_sequences = None # [patient_id ; seq_id ; inputs (type_id ; step(t) ; features)]
        self.ehr_labels = None # [patient_id ; seq_id ; targets (1*3) ]
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    
    def process_patients_(self, grp, min_rows_reqd, imputer=None):
        n_rows = grp.shape[0]
        if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
        if imputer: grp = imputer(grp)

        def data(pos): # This returns a sequence of the length num_steps starting from pos
            if grp.shape[0] < pos+self.seq_len: return
            return grp.iloc[pos: pos + self.seq_len]
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 

        input_start_ix = 0
        input_final_ix = n_rows-min_rows_reqd+1
        input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
        sequences = np.array(list(map(decompose_inputs, input_seq)))

        target_start_ix = min_rows_reqd-1
        target_final_ix = n_rows-1
        labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values            

        return sequences, labels
    
    
    def write_to_disk(self, fout='./pt_data/', sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        def pkl_dump(obj, f):
            with open(f, 'wb') as fo:
                pickle.dump(onj, fo)
                
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        min_rows_reqd=self.seq_len+self.lookfwd_months
        groups = dataset.groupby('AUTO_ID')
        imputer = du.Imputer().get_imputer('forward-mini')
        
        for autoid,g in groups:
            s, l = self.process_patients_(g, min_rows_reqd, imputer)
            s_out = cfg.opj(fout, 'inputs', f'{auto_id}.npy')
            l_out = cfg.opj(fout, 'labels', f'{auto_id}.npy')
            pkl_dump(s, s_out)
            pkl_dump(l, l_out)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
138/5:
dl2 = IBDDataLoader()
t0 = time.time()
dl2.write_to_disk(500)
print(time.time()-t0)
# unzip = lambda z:list(zip(*z))
# a = map(unzip, zipped)
# train, valid, test = dl.TVT_loaders()
138/6:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.partition=None
        
        self.ehr_sequences = None # [patient_id ; seq_id ; inputs (type_id ; step(t) ; features)]
        self.ehr_labels = None # [patient_id ; seq_id ; targets (1*3) ]
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    
    def process_patients_(self, grp, min_rows_reqd, imputer=None):
        n_rows = grp.shape[0]
        if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
        if imputer: grp = imputer(grp)

        def data(pos): # This returns a sequence of the length num_steps starting from pos
            if grp.shape[0] < pos+self.seq_len: return
            return grp.iloc[pos: pos + self.seq_len]
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 

        input_start_ix = 0
        input_final_ix = n_rows-min_rows_reqd+1
        input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
        sequences = np.array(list(map(decompose_inputs, input_seq)))

        target_start_ix = min_rows_reqd-1
        target_final_ix = n_rows-1
        labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values            

        return sequences, labels
    
    
    def write_to_disk(self, fout='./pt_data/', sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        def pkl_dump(obj, f):
            with open(f, 'wb') as fo:
                pickle.dump(onj, fo)
                
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        min_rows_reqd=self.seq_len+self.lookfwd_months
        groups = dataset.groupby('AUTO_ID')
        imputer = du.Imputer().get_imputer('forward-mini')
        
        for auto_id,g in groups:
            s, l = self.process_patients_(g, min_rows_reqd, imputer)
            s_out = cfg.opj(fout, 'inputs', f'{auto_id}.npy')
            l_out = cfg.opj(fout, 'labels', f'{auto_id}.npy')
            pkl_dump(s, s_out)
            pkl_dump(l, l_out)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
138/7:
dl2 = IBDDataLoader()
t0 = time.time()
dl2.write_to_disk(500)
print(time.time()-t0)
# unzip = lambda z:list(zip(*z))
# a = map(unzip, zipped)
# train, valid, test = dl.TVT_loaders()
138/8:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.partition=None
        
        self.ehr_sequences = None # [patient_id ; seq_id ; inputs (type_id ; step(t) ; features)]
        self.ehr_labels = None # [patient_id ; seq_id ; targets (1*3) ]
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    
    def process_patients_(self, grp, min_rows_reqd, imputer=None):
        n_rows = grp.shape[0]
        if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
        if imputer: grp = imputer(grp)

        def data(pos): # This returns a sequence of the length num_steps starting from pos
            if grp.shape[0] < pos+self.seq_len: return
            return grp.iloc[pos: pos + self.seq_len]
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 

        input_start_ix = 0
        input_final_ix = n_rows-min_rows_reqd+1
        input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
        sequences = np.array(list(map(decompose_inputs, input_seq)))

        target_start_ix = min_rows_reqd-1
        target_final_ix = n_rows-1
        labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values            

        return sequences, labels
    
    
    def write_to_disk(self, fout='./pt_data/', sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        def pkl_dump(obj, f):
            with open(f, 'wb') as fo:
                pickle.dump(onj, fo)
                
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        min_rows_reqd=self.seq_len+self.lookfwd_months
        groups = dataset.groupby('AUTO_ID')
        imputer = du.Imputer().get_imputer('forward-mini')
        
        for auto_id,g in groups:
            s, l = self.process_patients_(g, min_rows_reqd, imputer)
            filename = f"{auto_id}.npy"
            s_out = cfg.opj(fout, 'inputs', filename)
            l_out = cfg.opj(fout, 'labels', filename)
            pkl_dump(s, s_out)
            pkl_dump(l, l_out)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
138/9:
dl2 = IBDDataLoader()
t0 = time.time()
dl2.write_to_disk(500)
print(time.time()-t0)
# unzip = lambda z:list(zip(*z))
# a = map(unzip, zipped)
# train, valid, test = dl.TVT_loaders()
138/10:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.partition=None
        
        self.ehr_sequences = None # [patient_id ; seq_id ; inputs (type_id ; step(t) ; features)]
        self.ehr_labels = None # [patient_id ; seq_id ; targets (1*3) ]
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    
    def process_patients_(self, grp, min_rows_reqd, imputer=None):
        n_rows = grp.shape[0]
        if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
        if imputer: grp = imputer(grp)

        def data(pos): # This returns a sequence of the length num_steps starting from pos
            if grp.shape[0] < pos+self.seq_len: return
            return grp.iloc[pos: pos + self.seq_len]
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 

        input_start_ix = 0
        input_final_ix = n_rows-min_rows_reqd+1
        input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
        sequences = np.array(list(map(decompose_inputs, input_seq)))

        target_start_ix = min_rows_reqd-1
        target_final_ix = n_rows-1
        labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values            

        return sequences, labels
    
    
    def write_to_disk(self,sample=None,  fout='./pt_data/'):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        def pkl_dump(obj, f):
            with open(f, 'wb') as fo:
                pickle.dump(onj, fo)
                
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        min_rows_reqd=self.seq_len+self.lookfwd_months
        groups = dataset.groupby('AUTO_ID')
        imputer = du.Imputer().get_imputer('forward-mini')
        
        for auto_id,g in groups:
            s, l = self.process_patients_(g, min_rows_reqd, imputer)
            filename = f"{auto_id}.npy"
            s_out = cfg.opj(fout, 'inputs', filename)
            l_out = cfg.opj(fout, 'labels', filename)
            pkl_dump(s, s_out)
            pkl_dump(l, l_out)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
138/11:
dl2 = IBDDataLoader()
t0 = time.time()
dl2.write_to_disk(500)
print(time.time()-t0)
138/12:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.autograd import Variable
import numpy as np
import data_utils as du
import math
import config as cfg
import pickle
138/13:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.partition=None
        
        self.ehr_sequences = None # [patient_id ; seq_id ; inputs (type_id ; step(t) ; features)]
        self.ehr_labels = None # [patient_id ; seq_id ; targets (1*3) ]
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    
    def process_patients_(self, grp, min_rows_reqd, imputer=None):
        n_rows = grp.shape[0]
        if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
        if imputer: grp = imputer(grp)

        def data(pos): # This returns a sequence of the length num_steps starting from pos
            if grp.shape[0] < pos+self.seq_len: return
            return grp.iloc[pos: pos + self.seq_len]
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 

        input_start_ix = 0
        input_final_ix = n_rows-min_rows_reqd+1
        input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
        sequences = np.array(list(map(decompose_inputs, input_seq)))

        target_start_ix = min_rows_reqd-1
        target_final_ix = n_rows-1
        labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values            

        return sequences, labels
    
    
    def write_to_disk(self,sample=None,  fout='./pt_data/'):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        def pkl_dump(obj, f):
            with open(f, 'wb') as fo:
                pickle.dump(onj, fo)
                
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        min_rows_reqd=self.seq_len+self.lookfwd_months
        groups = dataset.groupby('AUTO_ID')
        imputer = du.Imputer().get_imputer('forward-mini')
        
        for auto_id,g in groups:
            s, l = self.process_patients_(g, min_rows_reqd, imputer)
            filename = f"{auto_id}.npy"
            s_out = cfg.opj(fout, 'inputs', filename)
            l_out = cfg.opj(fout, 'labels', filename)
            pkl_dump(s, s_out)
            pkl_dump(l, l_out)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
138/14:
dl2 = IBDDataLoader()
t0 = time.time()
dl2.write_to_disk(500)
print(time.time()-t0)
138/15:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   train_size = 0.8, \
                   valid_size = 0.1, \
                   mask_ones_proportion = 0.8):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.train_size = train_size
        self.valid_size = valid_size
        
        self.col_dict=None
        self.partition=None
        
        self.ehr_sequences = None # [patient_id ; seq_id ; inputs (type_id ; step(t) ; features)]
        self.ehr_labels = None # [patient_id ; seq_id ; targets (1*3) ]
    
    
    #Split train-v-test -- ARCHIVED FOR NOW
    def aid_ttsplit(self, dataset):
        partition={}
        print(f"Splitting on {train_size}, {valid_size}, {1-train_size-valid_size}")
        unique_patients = dataset.AUTO_ID.unique() 
        partition['train'] = np.random.choice(unique_patients, math.floor(self.train_size*len(unique_patients)))
        partition['valid'] = np.random.choice(np.setdiff1d(unique_patients, partition['train']), math.floor(self.valid_size*len(unique_patients)))
        partition['test'] = np.setdiff1d(unique_patients, np.append(partition['train'], partition['valid']))    
        return partition
        
    
    def seq_ttsplit(self, seq_shape):
        all_ix = range(seq_shape)
        partition = {}
        partition['train_ix'] = np.random.choice(all_ix,math.floor(self.train_size*seq_shape), replace=False)
        partition['valid_ix'] = np.random.choice(np.setdiff1d(all_ix, partition['train_ix']), math.floor(self.valid_size*seq_shape))
        partition['test_ix'] = np.setdiff1d(all_ix, np.append(partition['train_ix'], partition['valid_ix']))
        return partition
    
    
    
    def get_dataloader(self, partition, type='train'):
        x = self.sequences[partition[f'{type}_ix']]
        y = self.labels[partition[f'{type}_ix']]
        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))
        loader = DataLoader(data, shuffle=True, batch_size=self.BATCH_SIZE) 
        return loader
    
    
    
    def process_patients_(self, grp, min_rows_reqd, imputer=None):
        n_rows = grp.shape[0]
        if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
        if imputer: grp = imputer(grp)

        def data(pos): # This returns a sequence of the length num_steps starting from pos
            if grp.shape[0] < pos+self.seq_len: return
            return grp.iloc[pos: pos + self.seq_len]
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 

        input_start_ix = 0
        input_final_ix = n_rows-min_rows_reqd+1
        input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
        sequences = np.array(list(map(decompose_inputs, input_seq)))

        target_start_ix = min_rows_reqd-1
        target_final_ix = n_rows-1
        labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values            

        return sequences, labels
    
    
    def write_to_disk(self,sample=None,  fout='./pt_data/'):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        def pkl_dump(obj, f):
            with open(f, 'wb') as fo:
                pickle.dump(obj, fo)
                
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        min_rows_reqd=self.seq_len+self.lookfwd_months
        groups = dataset.groupby('AUTO_ID')
        imputer = du.Imputer().get_imputer('forward-mini')
        
        for auto_id,g in groups:
            s, l = self.process_patients_(g, min_rows_reqd, imputer)
            filename = f"{auto_id}.npy"
            s_out = cfg.opj(fout, 'inputs', filename)
            l_out = cfg.opj(fout, 'labels', filename)
            pkl_dump(s, s_out)
            pkl_dump(l, l_out)
        
    def TVT_loaders(self):
        if self.sequences is None:
            print("Fit a dataset first!")
        tvt_partition = self.seq_ttsplit(len(self.sequences))
        return (self.get_dataloader(tvt_partition), self.get_dataloader(tvt_partition, 'valid'), self.get_dataloader(tvt_partition, 'test'))
138/16:
dl2 = IBDDataLoader()
t0 = time.time()
dl2.write_to_disk(500)
print(time.time()-t0)
138/17:
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   fout='./pt_data/'):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.fout = fout
        
        self.col_dict=None

    
    def process_patients_(self, grp, min_rows_reqd, imputer=None):
        n_rows = grp.shape[0]
        if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
        if imputer: grp = imputer(grp)

        def data(pos): # This returns a sequence of the length num_steps starting from pos
            if grp.shape[0] < pos+self.seq_len: return
            return grp.iloc[pos: pos + self.seq_len]
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 

        input_start_ix = 0
        input_final_ix = n_rows-min_rows_reqd+1
        input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
        sequences = np.array(list(map(decompose_inputs, input_seq)))

        target_start_ix = min_rows_reqd-1
        target_final_ix = n_rows-1
        labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values            

        return sequences, labels
    
    
    def write_to_disk(self,sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        def pkl_dump(obj, f):
            with open(f, 'wb') as fo:
                pickle.dump(obj, fo)
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        min_rows_reqd=self.seq_len+self.lookfwd_months
        groups = dataset.groupby('AUTO_ID')
        imputer = du.Imputer().get_imputer('forward-mini')
        
        for auto_id,g in groups:
            s, l = self.process_patients_(g, min_rows_reqd, imputer)
            filename = f"{auto_id}.npy"
            s_out = cfg.opj(self.fout, 'inputs', filename)
            l_out = cfg.opj(self.fout, 'labels', filename)
            pkl_dump(s, s_out)
            pkl_dump(l, l_out)
139/1:
import numpy as np
import data_utils as du
import math
import config as cfg
import pickle
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   fout='./pt_data/'):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.fout = fout
        
        self.col_dict=None

    
    def process_patients_(self, grp, min_rows_reqd, imputer=None):
        n_rows = grp.shape[0]
        if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
        if imputer: grp = imputer(grp)

        def data(pos): # This returns a sequence of the length num_steps starting from pos
            if grp.shape[0] < pos+self.seq_len: return
            return grp.iloc[pos: pos + self.seq_len]
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 

        input_start_ix = 0
        input_final_ix = n_rows-min_rows_reqd+1
        input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
        sequences = np.array(list(map(decompose_inputs, input_seq)))

        target_start_ix = min_rows_reqd-1
        target_final_ix = n_rows-1
        labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values            

        return sequences, labels
    
    
    def write_to_disk(self,sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        def pkl_dump(obj, f):
            with open(f, 'wb') as fo:
                pickle.dump(obj, fo)
                
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        min_rows_reqd=self.seq_len+self.lookfwd_months
        groups = dataset.groupby('AUTO_ID')
        imputer = du.Imputer().get_imputer('forward-mini')
        
        for auto_id,g in groups:
            s, l = self.process_patients_(g, min_rows_reqd, imputer)
            filename = f"{auto_id}.npy"
            s_out = cfg.opj(self.fout, 'inputs', filename)
            l_out = cfg.opj(self.fout, 'labels', filename)
            pkl_dump(s, s_out)
            pkl_dump(l, l_out)
139/2:
import numpy as np
import data_utils as du
import math
import config as cfg
import pickle
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   fout='./pt_data/'):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.fout = fout
        
        self.col_dict=None

    
    def process_patients_(self, grp, min_rows_reqd, imputer=None):
        n_rows = grp.shape[0]
        if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
        if imputer: grp = imputer(grp)

        def data(pos): # This returns a sequence of the length num_steps starting from pos
            if grp.shape[0] < pos+self.seq_len: return
            return grp.iloc[pos: pos + self.seq_len]
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 

        input_start_ix = 0
        input_final_ix = n_rows-min_rows_reqd+1
        input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
        sequences = np.array(list(map(decompose_inputs, input_seq)))

        target_start_ix = min_rows_reqd-1
        target_final_ix = n_rows-1
        labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values            

        return sequences, labels
    
    
    def write_to_disk(self,sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        def pkl_dump(obj, f):
            with open(f, 'wb') as fo:
                pickle.dump(obj, fo)
                
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        min_rows_reqd=self.seq_len+self.lookfwd_months
        groups = dataset.groupby('AUTO_ID')
        imputer = du.Imputer().get_imputer('forward-mini')
        
        for auto_id,g in groups:
            s, l = self.process_patients_(g, min_rows_reqd, imputer)
            filename = f"{auto_id}.npy"
            s_out = cfg.opj(self.fout, 'inputs', filename)
            l_out = cfg.opj(self.fout, 'labels', filename)
            pkl_dump(s, s_out)
            pkl_dump(l, l_out)
139/3:
dl2 = IBDDataLoader()
t0 = time.time()
dl2.write_to_disk()
print(time.time()-t0)
142/1:
import numpy as np
import data_utils as du
import math
import config as cfg
import pickle
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   fout='./pt_data/'):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.fout = fout
        
        self.col_dict=None

    
    def process_patients_(self, grp, min_rows_reqd, imputer=None):
        n_rows = grp.shape[0]
        if n_rows<min_rows_reqd: return # Skip patients who have lesser number of months
        if imputer: grp = imputer(grp)

        def data(pos): # This returns a sequence of the length num_steps starting from pos
            if grp.shape[0] < pos+self.seq_len: return
            return grp.iloc[pos: pos + self.seq_len]
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 

        input_start_ix = 0
        input_final_ix = n_rows-min_rows_reqd+1
        input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
        sequences = np.array(list(map(decompose_inputs, input_seq)))

        target_start_ix = min_rows_reqd-1
        target_final_ix = n_rows-1
        labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values            

        return sequences, labels
    
    
    def write_to_disk(self,sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        def pkl_dump(obj, f):
            with open(f, 'wb') as fo:
                pickle.dump(obj, fo)
                
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        min_rows_reqd=self.seq_len+self.lookfwd_months
        groups = dataset.groupby('AUTO_ID')
        imputer = du.Imputer().get_imputer('forward-mini')
        
        total_nb = len(groups)
        for c, (auto_id,g) in enumerate(groups):
            if c%100==0:
                print(f"{c}/{total_nb} patients done.")
            s, l = self.process_patients_(g, min_rows_reqd, imputer)
            filename = f"{auto_id}.npy"
            s_out = cfg.opj(self.fout, 'inputs', filename)
            l_out = cfg.opj(self.fout, 'labels', filename)
            pkl_dump(s, s_out)
            pkl_dump(l, l_out)
142/2:
dl2 = IBDDataLoader()
t0 = time.time()
dl2.write_to_disk()
print(time.time()-t0)
142/3:
import numpy as np
import data_utils as du
import math
import config as cfg
import pickle
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   fout='./pt_data/'):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.fout = fout
        
        self.col_dict=None

    
    def process_patients_(self, grp, min_rows_reqd, imputer=None):
        if imputer: grp = imputer(grp)

        def data(pos): # This returns a sequence of the length num_steps starting from pos
            if grp.shape[0] < pos+self.seq_len: return
            return grp.iloc[pos: pos + self.seq_len]
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 

        input_start_ix = 0
        input_final_ix = n_rows-min_rows_reqd+1
        input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
        sequences = np.array(list(map(decompose_inputs, input_seq)))

        target_start_ix = min_rows_reqd-1
        target_final_ix = n_rows-1
        labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values            

        return sequences, labels
    
    
    def write_to_disk(self,sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        def pkl_dump(obj, f):
            with open(f, 'wb') as fo:
                pickle.dump(obj, fo)
                
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        if sample: dataset = dataset.head(sample)
        min_rows_reqd=self.seq_len+self.lookfwd_months
        groups = dataset.groupby('AUTO_ID')
        imputer = du.Imputer().get_imputer('forward-mini')
        
        total_nb = len(groups)
        for c, (auto_id,g) in enumerate(groups):
            if c%100==0: print(f"{c}/{total_nb} patients done.")
            
            # Skip patients who have lesser number of months 
            if g.shape[0]<min_rows_reqd: continue 
            
            s, l = self.process_patients_(g, min_rows_reqd, imputer)
            filename = f"{auto_id}.npy"
            s_out = cfg.opj(self.fout, 'inputs', filename)
            l_out = cfg.opj(self.fout, 'labels', filename)
            pkl_dump(s, s_out)
            pkl_dump(l, l_out)
142/4:
dl2 = IBDDataLoader()
t0 = time.time()
dl2.write_to_disk()
print(time.time()-t0)
142/5:
import numpy as np
import data_utils as du
import math
import config as cfg
import pickle
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   fout='./pt_data/'):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.fout = fout
        
        self.min_rows_reqd=self.seq_len+self.lookfwd_months
        self.col_dict=None

    
    def process_patients_(self, grp, imputer=None):
        if imputer: grp = imputer(grp)
        n_rows = grp.shape[0]
        
        def data(pos): # This returns a sequence of the length num_steps starting from pos
            return grp.iloc[pos: pos + self.seq_len]
        
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 

        input_start_ix = 0
        input_final_ix = n_rows-self.min_rows_reqd+1
        input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
        sequences = np.array(list(map(decompose_inputs, input_seq)))

        target_start_ix = self.min_rows_reqd-1
        target_final_ix = n_rows-1
        labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values            

        return sequences, labels
    
    
    def write_to_disk(self,sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        def pkl_dump(obj, f):
            with open(f, 'wb') as fo:
                pickle.dump(obj, fo)
                
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        
        groups = dataset.groupby('AUTO_ID')
        imputer = du.Imputer().get_imputer('forward-mini')
        
        total_nb = len(groups)
        for c, (auto_id,g) in enumerate(groups):
            if c%100==0: print(f"{c}/{total_nb} patients done.")
            
            # Skip patients who have lesser number of months 
            if g.shape[0]<self.min_rows_reqd: continue 
            
            s, l = self.process_patients_(g, imputer)
            filename = f"{auto_id}.npy"
            s_out = cfg.opj(self.fout, 'inputs', filename)
            l_out = cfg.opj(self.fout, 'labels', filename)
            pkl_dump(s, s_out)
            pkl_dump(l, l_out)
142/6:
dl2 = IBDDataLoader()
t0 = time.time()
dl2.write_to_disk()
print(time.time()-t0)
142/7:
import pickle
with open('pt_data/inputs/0.npy','rb') as f:
    X = pickle.load(f)
with open('pt_data/labels/0.npy','rb') as f:
    y = pickle.load(f)
142/8:
import pickle
with open('pt_data/inputs/0.npy','rb') as f:
    X = pickle.load(f)
with open('pt_data/labels/0.npy','rb') as f:
    y = pickle.load(f)
142/9: X.shape
142/10: X.shape, y.shape
142/11:
import pickle
with open('pt_data/inputs/41.npy','rb') as f:
    X = pickle.load(f)
with open('pt_data/labels/41.npy','rb') as f:
    y = pickle.load(f)
142/12: X.shape, y.shape
142/13:
import pickle
with open('pt_data/inputs/48.npy','rb') as f:
    X = pickle.load(f)
with open('pt_data/labels/48.npy','rb') as f:
    y = pickle.load(f)
142/14: X.shape, y.shape
142/15:
import data_utils as du
data,_ = du.X_raw()
142/16: data[data.AUTO_ID==48]
142/17: y[-1]
142/18:
import pickle
with open('pt_data/inputs/41.npy','rb') as f:
    X = pickle.load(f)
with open('pt_data/labels/41.npy','rb') as f:
    y = pickle.load(f)
142/19: X.shape, y.shape, y[-1]
142/20: X.shape, y.shape
142/21: data[data.AUTO_ID==41]
142/22:
import numpy as np
import data_utils as du
import math
import config as cfg
import pickle
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   fout='./pt_data/'):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.fout = fout
        
        self.min_rows_reqd=self.seq_len+self.lookfwd_months
        self.col_dict=None

    
    def process_patients_(self, grp, imputer=None, labels_only=False):
        if imputer: grp = imputer(grp)
        n_rows = grp.shape[0]
        
        def data(pos): # This returns a sequence of the length num_steps starting from pos
            return grp.iloc[pos: pos + self.seq_len]
        
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 

#         input_start_ix = 0
#         input_final_ix = n_rows-self.min_rows_reqd+1
#         input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
#         sequences = np.array(list(map(decompose_inputs, input_seq)))

        target_start_ix = self.min_rows_reqd-1
        target_final_ix = n_rows
        labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values            

#         return sequences, labels
        return _, labels
    
    
    def write_to_disk(self,sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        def pkl_dump(obj, f):
            with open(f, 'wb') as fo:
                pickle.dump(obj, fo)
                
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        
        groups = dataset.groupby('AUTO_ID')
        imputer = du.Imputer().get_imputer('forward-mini')
        
        total_nb = len(groups)
        for c, (auto_id,g) in enumerate(groups):
            if c%100==0: print(f"{c}/{total_nb} patients done.")
            
            # Skip patients who have lesser number of months 
            if g.shape[0]<self.min_rows_reqd: continue 
            
            s, l = self.process_patients_(g, imputer)
            filename = f"{auto_id}.npy"
#             s_out = cfg.opj(self.fout, 'inputs', filename)
#             pkl_dump(s, s_out)
            l_out = cfg.opj(self.fout, 'labels', filename)
            pkl_dump(l, l_out)
142/23:
import numpy as np
import data_utils as du
import math
import config as cfg
import pickle
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   fout='./pt_data/'):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.fout = fout
        
        self.min_rows_reqd=self.seq_len+self.lookfwd_months
        self.col_dict=None

    
    def process_patients_(self, grp, imputer=None, labels_only=False):
        if imputer: grp = imputer(grp)
        n_rows = grp.shape[0]
        
        def data(pos): # This returns a sequence of the length num_steps starting from pos
            return grp.iloc[pos: pos + self.seq_len]
        
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 

#         input_start_ix = 0
#         input_final_ix = n_rows-self.min_rows_reqd+1
#         input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
#         sequences = np.array(list(map(decompose_inputs, input_seq)))

        target_start_ix = self.min_rows_reqd-1
        target_final_ix = n_rows
        labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values            

#         return sequences, labels
        return _, labels
    
    
    def write_to_disk(self,sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        
        groups = dataset.groupby('AUTO_ID')
        imputer = du.Imputer().get_imputer('forward-mini')
        
        total_nb = len(groups)
        for c, (auto_id,g) in enumerate(groups):
            if c%100==0: print(f"{c}/{total_nb} patients done.")
            
            # Skip patients who have lesser number of months 
            if g.shape[0]<self.min_rows_reqd: continue 
            
            s, l = self.process_patients_(g, imputer)
            filename = f"{auto_id}.npy"
#             s_out = cfg.opj(self.fout, 'inputs', filename)
#             cfg.pkl_dump(s, s_out)
            l_out = cfg.opj(self.fout, 'labels', filename)
            cfg.pkl_dump(l, l_out)
142/24:
import numpy as np
import data_utils as du
import math
import config as cfg
import pickle
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   fout='./pt_data/'):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.fout = fout
        
        self.min_rows_reqd=self.seq_len+self.lookfwd_months
        self.col_dict=None

    
    def process_patients_(self, grp, imputer=None, labels_only=False):
        if imputer: grp = imputer(grp)
        n_rows = grp.shape[0]
        
        def data(pos): # This returns a sequence of the length num_steps starting from pos
            return grp.iloc[pos: pos + self.seq_len]
        
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 

#         input_start_ix = 0
#         input_final_ix = n_rows-self.min_rows_reqd+1
#         input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
#         sequences = np.array(list(map(decompose_inputs, input_seq)))

        target_start_ix = self.min_rows_reqd-1
        target_final_ix = n_rows
        labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values            

#         return sequences, labels
        return _, labels
    
    
    def write_to_disk(self,sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        
        groups = dataset.groupby('AUTO_ID')
        imputer = du.Imputer().get_imputer('forward-mini')
        
        total_nb = len(groups)
        for c, (auto_id,g) in enumerate(groups):
            if c%100==0: print(f"{c}/{total_nb} patients done.")
            
            # Skip patients who have lesser number of months 
            if g.shape[0]<self.min_rows_reqd: continue 
            
            s, l = self.process_patients_(g, imputer)
            filename = f"{auto_id}.npy"
#             s_out = cfg.opj(self.fout, 'inputs', filename)
#             cfg.pkl_dump(s, s_out)
            l_out = cfg.opj(self.fout, 'labels', filename)
            cfg.pkl_dump(l, l_out)
142/25:
import numpy as np
import data_utils as du
import math
import config as cfg
import pickle
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   fout='./pt_data/'):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.fout = fout
        
        self.min_rows_reqd=self.seq_len+self.lookfwd_months
        self.col_dict=None

    
    def process_patients_(self, grp, imputer=None, labels_only=False):
        if imputer: grp = imputer(grp)
        n_rows = grp.shape[0]
        
        def data(pos): # This returns a sequence of the length num_steps starting from pos
            return grp.iloc[pos: pos + self.seq_len]
        
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 

#         input_start_ix = 0
#         input_final_ix = n_rows-self.min_rows_reqd+1
#         input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
#         sequences = np.array(list(map(decompose_inputs, input_seq)))

        target_start_ix = self.min_rows_reqd-1
        target_final_ix = n_rows
        labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values            

#         return sequences, labels
        return _, labels
    
    
    def write_to_disk(self,sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        
        groups = dataset.groupby('AUTO_ID')
        imputer = du.Imputer().get_imputer('forward-mini')
        
        total_nb = len(groups)
        for c, (auto_id,g) in enumerate(groups):
            if c%100==0: print(f"{c}/{total_nb} patients done.")
            
            # Skip patients who have lesser number of months 
            if g.shape[0]<self.min_rows_reqd: continue 
            
            s, l = self.process_patients_(g, imputer)
            filename = f"{auto_id}.npy"
#             s_out = cfg.opj(self.fout, 'inputs', filename)
#             cfg.pkl_dump(s, s_out)
            l_out = cfg.opj(self.fout, 'labels', filename)
            cfg.pkl_dump(l, l_out)
142/26:
dl2 = IBDDataLoader()
t0 = time.time()
dl2.write_to_disk()
print(time.time()-t0)
145/1:
import numpy as np
import data_utils as du
import math
import config as cfg
import pickle
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   fout='./pt_data/'):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.fout = fout
        
        self.min_rows_reqd=self.seq_len+self.lookfwd_months
        self.col_dict=None

    
    def process_patients_(self, grp, imputer=None, labels_only=False):
        if imputer: grp = imputer(grp)
        n_rows = grp.shape[0]
        
        def data(pos): # This returns a sequence of the length num_steps starting from pos
            return grp.iloc[pos: pos + self.seq_len]
        
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 

#         input_start_ix = 0
#         input_final_ix = n_rows-self.min_rows_reqd+1
#         input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
#         sequences = np.array(list(map(decompose_inputs, input_seq)))

        target_start_ix = self.min_rows_reqd-1
        target_final_ix = n_rows
        labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values            

#         return sequences, labels
        return _, labels
    
    
    def write_to_disk(self,sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        
        groups = dataset.groupby('AUTO_ID')
        imputer = du.Imputer().get_imputer('forward-mini')
        
        total_nb = len(groups)
        for c, (auto_id,g) in enumerate(groups):
            if c%100==0: print(f"{c}/{total_nb} patients done.")
            
            # Skip patients who have lesser number of months 
            if g.shape[0]<self.min_rows_reqd: continue 
            
            s, l = self.process_patients_(g, imputer)
            filename = f"{auto_id}.npy"
#             s_out = cfg.opj(self.fout, 'inputs', filename)
#             cfg.pkl_dump(s, s_out)
            l_out = cfg.opj(self.fout, 'labels', filename)
            cfg.pkl_dump(l, l_out)
145/2:
dl2 = IBDDataLoader()
t0 = time.time()
dl2.write_to_disk()
print(time.time()-t0)
147/1:
import numpy as np
import data_utils as du
import math
import config as cfg
import pickle
import time

class IBDDataLoader:
    
    def __init__(self, BATCH_SIZE = 64, \
                   seq_len = 36, \
                   pred_len = 1, \
                   lookfwd_months = 12,\
                   fout='./pt_data/'):
        self.BATCH_SIZE = BATCH_SIZE
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lookfwd_months = lookfwd_months
        self.fout = fout
        
        self.min_rows_reqd=self.seq_len+self.lookfwd_months
        self.col_dict=None

    
    def process_patients_(self, grp, imputer=None, labels_only=False):
        if imputer: grp = imputer(grp)
        n_rows = grp.shape[0]
        
        def data(pos): # This returns a sequence of the length num_steps starting from pos
            return grp.iloc[pos: pos + self.seq_len]
        
        def decompose_inputs(input_df):
            x = input_df[self.col_dict['input']]
            x_obs = x.shift(1).fillna(x.iloc[0]).values
            m = input_df[self.col_dict['missing']].values
            t = input_df[self.col_dict['delta']].values
            x=x.values
            return [x[:,1:], m, t, x_obs[:,1:]] 

#         input_start_ix = 0
#         input_final_ix = n_rows-self.min_rows_reqd+1
#         input_seq = [data(j) for j in range(input_start_ix, input_final_ix)]
#         sequences = np.array(list(map(decompose_inputs, input_seq)))

        target_start_ix = self.min_rows_reqd-1
        target_final_ix = n_rows
        labels = grp.iloc[target_start_ix:target_final_ix][self.col_dict['tgt']].values            

#         return sequences, labels
        return _, labels
    
    
    def write_to_disk(self,sample=None):
        """
         Returns
           sequence: [batch_size ; type (x/m/t/xtm1) ; seq_len ; input_dim]
           labels: one-hot encoding of cost
        """
        dataset, self.col_dict = du.X_raw() #(243563 rows)
        
        groups = dataset.groupby('AUTO_ID')
        imputer = du.Imputer().get_imputer('forward-mini')
        
        total_nb = len(groups)
        for c, (auto_id,g) in enumerate(groups):
            if c%100==0: print(f"{c}/{total_nb} patients done.")
            
            # Skip patients who have lesser number of months 
            if g.shape[0]<self.min_rows_reqd: continue 
            
            s, l = self.process_patients_(g, imputer)
            filename = f"{auto_id}.npy"
#             s_out = cfg.opj(self.fout, 'inputs', filename)
#             cfg.pkl_dump(s, s_out)
            l_out = cfg.opj(self.fout, 'labels', filename)
            cfg.pkl_dump(l, l_out)
147/2:
dl2 = IBDDataLoader()
t0 = time.time()
dl2.write_to_disk()
print(time.time()-t0)
143/1:
import pickle
with open('pt_data/inputs/41.npy','rb') as f:
    X = pickle.load(f)
with open('pt_data/labels/41.npy','rb') as f:
    y = pickle.load(f)
143/2:
import pickle
with open('pt_data/inputs/41.npy','rb') as f:
    X = pickle.load(f)
with open('pt_data/labels/41.npy','rb') as f:
    y = pickle.load(f)
X.shape, y.shape
143/3:
model = GRUD().double()
state = model.init_hidden()
yhat = model(X)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/4:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = Variable(torch.zeros(delta_dim))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:]), \
                            torch.squeeze(inputs[:,1,:]), \
                            torch.squeeze(inputs[:,2,:]), \
                            torch.squeeze(inputs[:,3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros, self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros, self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.cat((x,h.double(),obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/5:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.autograd import Variable
import numpy as np
import data_utils as du
import math
import config as cfg
import pickle
143/6:
def try_gpu():
    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
    is_cuda = torch.cuda.is_available()
    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    return device
143/7:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = Variable(torch.zeros(delta_dim))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:]), \
                            torch.squeeze(inputs[:,1,:]), \
                            torch.squeeze(inputs[:,2,:]), \
                            torch.squeeze(inputs[:,3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros, self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros, self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.cat((x,h.double(),obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/8:
model = GRUD().double()
state = model.init_hidden()
yhat = model(X)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/9:
model = GRUD().double()
state = model.init_hidden(X.shape[0])
yhat = model(X)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/10:
model = GRUD().double()
state = model.init_hidden(X.shape[0])
yhat = model(X, state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/11:
model = GRUD().double()
state = model.init_hidden(X.shape[0])
print(state)
# yhat = model(X, state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/12:
model = GRUD().double()
state = model.init_hidden(X.shape[0])
print(state.size())
# yhat = model(X, state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/13:
model = GRUD().double()
state = model.init_hidden(X.shape[0])
print(state.size()[0])
# yhat = model(X, state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/14:
model = GRUD().double()
state = model.init_hidden(X.shape[0])
print(state.size(0))
# yhat = model(X, state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/15:
model = GRUD().double()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/16:
import pickle
with open('pt_data/inputs/41.npy','rb') as f:
    X = torch.tensor(pickle.load(f))
with open('pt_data/labels/41.npy','rb') as f:
    y = torch.tensor(pickle.load(f))
X.shape, y.shape
143/17: torch.squeeze(X[:,:,0:1,:])
143/18: torch.squeeze(X[:,:,0:1,:]).size()
143/19:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = Variable(torch.zeros(delta_dim))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, delta_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros, self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros, self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.cat((x,h.double(),obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/20:
model = GRUD().double()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/21:
model = GRUD().double()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/22:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/23:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/24:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/25:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = Variable(torch.zeros(delta_dim))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(hidden_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros, self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros, self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.cat((x,h.double(),obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/26:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/27:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = Variable(torch.zeros(delta_dim))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(hidden_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros, self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros, self.gamma_h_lin(h)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.cat((x,h.double(),obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/28:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/29:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = Variable(torch.zeros(delta_dim))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros, self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros, self.gamma_h_lin(h)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.cat((x,h.double(),obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/30:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/31:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = Variable(torch.zeros(delta_dim))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros, self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros, self.gamma_h_lin(h)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.cat((x,h.double(),obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/32:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/33:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(h)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.cat((x,h.double(),obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/34:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/35:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = gamma_x*h
        
        gate_in = torch.cat((x,h.double(),obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/36:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/37:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = gamma_h*h
        
        gate_in = torch.cat((x,h.double(),obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/38:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/39:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = gamma_h*h
        
        gate_in = torch.cat((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/40:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/41:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = gamma_h*h
        
        print(x.size(), h.size(), obs_mask.size())
        
        gate_in = torch.cat((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/42:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/43:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)
        
        print(f"x:{x.size()}, h:{h.size()}, m:{obs_mask.size()}")
        
        gate_in = torch.cat((x,h,obs_mask), axis=1)
        
        print(f"Input gate:{gate_in.size()}")
        
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/44:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/45:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)
        
        print(f"x:{x.size()}, h:{h.size()}, m:{obs_mask.size()}")
        
        gate_in = torch.cat((x,h,obs_mask), axis=-1)
        
        print(f"Input gate:{gate_in.size()}")
        
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/46:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/47:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = []
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if not outputs:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)
        
        print(f"x:{x.size()}, h:{h.size()}, m:{obs_mask.size()}")
        
        gate_in = torch.cat((x,h,obs_mask))
        
        print(f"Input gate:{gate_in.size()}")
        
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/48:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/49:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if outputs is None:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)
        
        print(f"x:{x.size()}, h:{h.size()}, m:{obs_mask.size()}")
        
        gate_in = torch.cat((x,h,obs_mask))
        
        print(f"Input gate:{gate_in.size()}")
        
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/50:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/51:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if outputs is None:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=1)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/52:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/53:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if outputs is None:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)))      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/54:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/55:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if outputs is None:
                outputs = hidden.unsqueeze(1)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=0)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/56:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if outputs is None:
                outputs = hidden.unsqueeze(1)
            else:
                print(outputs.size())
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=0)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/57:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/58:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if outputs is None:
                outputs = hidden.unsqueeze(0)
            else:
                print(outputs.size())
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=0)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/59:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if outputs is None:
                outputs = hidden.unsqueeze(0)
            else:
#                 print(outputs.size())
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=0)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/60:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/61:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if outputs is None:
                outputs = hidden.unsqueeze(0)
            else:
                print(outputs.size())
                outputs = torch.cat((outputs, hidden.unsqueeze(1)), axis=0)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/62:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/63:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if outputs is None:
                outputs = hidden.unsqueeze(0)
            else:
                print(outputs.size())
                outputs = torch.cat((outputs, hidden.unsqueeze(0)), axis=0)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/64:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/65:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, hidden_layers=0, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
            if outputs is None:
                outputs = hidden.unsqueeze(0)
            else:
                outputs = torch.cat((outputs, hidden.unsqueeze(0)), axis=0)      
        return outputs

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/66:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape}  Yhat: {yhat.shape}  Y: {y.shape}")
143/67:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \n Yhat: {yhat.shape}  Y: {y.shape}")
143/68:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
143/69:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
143/70:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, state):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        hidden = self.init_hidden(inputs.size(0))
        state = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            state = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), state, xmean)
            if outputs is None:
                state = hidden.unsqueeze(0)
            else:
                state = torch.cat((state, hidden.unsqueeze(0)), axis=0) 
        
        outputs = F.log_softmax(self.fc1(state))
        
        return outputs, state

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/71:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
143/72:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, hidden):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
        
        outputs = F.log_softmax(self.fc1(hidden))
        
        return outputs, hidden

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/73:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
143/74:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
143/75:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, hidden):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
        
        outputs = torch.unsqueeze(F.log_softmax(self.fc1(hidden)), 1)
        
        return outputs, hidden

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/76:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
143/77:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, hidden):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
        
        outputs = torch.unsqueeze(F.log_softmax(self.fc1(hidden)), 0)
        
        return outputs, hidden

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/78:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
143/79: yhat
143/80:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, hidden):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
        
        print(self.fc1(hidden))
        outputs = torch.unsqueeze(F.log_softmax(self.fc1(hidden)), 0)
        
        return outputs, hidden

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/81:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
143/82:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, hidden):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
        
        print(self.fc1(hidden))
        outputs = torch.unsqueeze(F.softmax(self.fc1(hidden)), 0)
        
        return outputs, hidden

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/83:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, hidden):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
        
        print(self.fc1(hidden))
        outputs = torch.unsqueeze(F.softmax(self.fc1(hidden)), 0)
        
        return outputs, hidden

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/84:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
143/85:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        self.m = nn.Softmax()
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, hidden):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
        
        print(self.fc1(hidden))
        outputs = F.softmax(self.fc1(hidden))
        
        return outputs, hidden

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/86:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        self.m = nn.Softmax()
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, hidden):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
        
        print(self.fc1(hidden))
        print(self.m(self.fc1(hidden)))
        outputs = F.softmax(self.fc1(hidden))
        
        return outputs, hidden

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/87:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
143/88:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, hidden):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
        
        outputs = torch.unsqueeze(nn.Softmax(self.fc1(hidden)), 0)
        
        return outputs, hidden

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/89:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
143/90:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, hidden):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
        
        outputs = nn.Softmax(self.fc1(hidden))
        
        return torch.unsqueeze(outputs, 0), hidden

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/91:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
143/92:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, hidden):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
        
        outputs = nn.Softmax(self.fc1(hidden))
        print(outputs)
        
        return outputs, hidden

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/93:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, hidden):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
        
        m = nn.LogSoftmax()
        outputs = m(self.fc1(hidden))
        
        return outputs, hidden

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/94:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
143/95: yhat
143/96:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, hidden):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
        
        m = nn.Softmax()
        outputs = torch.unsqueeze(m(self.fc1(hidden)), 0)
        
        return outputs, hidden

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/97:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
143/98: yhat
143/99:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, hidden):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
        
        m = F.softmax()
        outputs = torch.unsqueeze(m(self.fc1(hidden)), 0)
        
        return outputs, hidden

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/100:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
143/101:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, hidden):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
        
        m = F.softmax
        outputs = torch.unsqueeze(m(self.fc1(hidden)), 0)
        
        return outputs, hidden

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/102:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
143/103: yhat
143/104:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True, op_activation=None):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        # Output layer
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        self.op_activation = op_activation if op_activation is not None else F.softmax
        
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, hidden):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
        
        outputs = torch.unsqueeze(self.op_activation(self.fc1(hidden)), 0)
        
        return outputs, hidden

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[0,:]), \
                            torch.squeeze(inputs[1,:]), \
                            torch.squeeze(inputs[2,:]), \
                            torch.squeeze(inputs[3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/105:
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
143/106:
import numpy as np
a = np.array([[0,0,1],[1,0,0],[0,1,0]])
a.argmax()
143/107:
import numpy as np
a = np.array([[0,0,1],[1,0,0],[0,1,0]])
a.argmax(1)
143/108:
import numpy as np
a = np.array([[0,0,1],[1,0,0],[0,1,0]])
a.argmax(1)
143/109:
import numpy as np
a = np.array([[0,0,1],[1,0,0],[0,1,0]])
a.argmax(1)
143/110:
import numpy as np
a = np.array([[0,0,1],[1,0,0],[0,1,0]])
a.argmax(1)
143/111:
import numpy as np
a = np.array([[0,0,1],[1,0,0],[0,1,0]])
a.argmax(1)
143/112:
import numpy as np
a = np.array([[0,0,1],[1,0,0],[0,1,0]])
a.argmax(1)
143/113:
import numpy as np
a = np.array([[0,0,1],[1,0,0],[0,1,0]])
a.argmax(2)
143/114:
import numpy as np
a = np.array([[0,0,1],[1,0,0],[0,1,0]])
a.argmax(0)
143/115:
import numpy as np
a = np.array([[0,0,1],[1,0,0],[0,1,0]])
a.argmax(1)
143/116:
step = 1
seq_length = 5
np.linspace(step * np.pi, (step+1)*np.pi, seq_length + 1)
143/117:
step = 1
seq_length = 5
plt.plot(np.linspace(step * np.pi, (step+1)*np.pi, seq_length + 1))
143/118:
step = 1
seq_length = 5
import matplotlib.pyplot as plt
plt.plot(np.linspace(step * np.pi, (step+1)*np.pi, seq_length + 1))
143/119:
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=100):
    
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter[0][0].shape[0])    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        y_hat, state = model(X.to(device).double(), state.to(device).double())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c, y_c = y_hat.argmax(1), y.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Step:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(model, epochs=10):
    
    model = GRUD().double()
    
    
    for epoch in range(1, epochs+1):
        avg_loss, cf_matrix, loss_steps = train_epoch(model, train_iter, criterion, optimizer, 2, 500)
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{}Charge Acc:{}" for l,s in zip(["Low", "Medium", "High"], scores)]
        
        plt.plot(loss_steps)
        plt.xlabel("Steps")
        plt.ylabel("Loss")
        plt.ylim((0,0.2))
        print(f"\nEPOCH {epoch},   {scores}")
143/120:
l=[0,1,2]
l[4]=9
143/121:
l=[0,1,2]
m=[1,2,3]
m.append(l)
143/122:
l=[0,1,2]
m=[1,2,3]
m.append(l)
m
143/123:
l=[0,1,2]
m=[1,2,3]
m.append(*l)
m
143/124: ?(np.randint)
143/125: help(np.randint)
143/126: help(np.random.randint)
143/127: isinstance([1,2], 'list')
143/128: type([1,2])
143/129: isinstance([1,2], type([1,2]))
143/130: isinstance([1,2], 'list')
143/131: isinstance([1,2], list)
143/132:
def chk(*a):
    for aa in a:
        print(aa)
        
chk(1,2,3)
chk([1,2,3])
143/133:
def chk(*a):
    print(type(a))
    for aa in a:
        print(aa)
        
chk(1,2,3)
chk([1,2,3])
143/134:
def chk(*a):
    print(type(a[0]))
    for aa in a:
        print(aa)
        
chk(1,2,3)
chk([1,2,3])
143/135:
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=100):
    
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter[0][0].shape[0])    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).double())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c, y_c = y_hat.argmax(1), y.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Step:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(model, train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 2, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))


def load_random_patients(n=100):
    fp = './pt_data/'
    patients = os.listdir(fp+'inputs/')
    indices = np.random.randint(0, len(patients), n)
    
    X, y = torch.Tensor(), torch.Tensor()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        y = torch.cat((y, y_), axis=0)
    
    return TensorDataset(X, y)
143/136:
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=100):
    
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter[0][0].shape[0])    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).double())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c, y_c = y_hat.argmax(1), y.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Step:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(model, train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 2, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))


def load_random_patients(n=100):
    fp = './pt_data/'
    patients = os.listdir(fp+'inputs/')
    indices = np.random.randint(0, len(patients), n)
    
    X, y = torch.Tensor(), torch.Tensor()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        y = torch.cat((y, y_), axis=0)
    
    return TensorDataset(X, y)
143/137: dataset = load_random_patients(20)
143/138:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.autograd import Variable
import numpy as np
import data_utils as du
import math
import config as cfg
import pickle
143/139: dataset = load_random_patients(20)
143/140:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.autograd import Variable
import numpy as np
import data_utils as du
import math
import config as cfg
import pickle
import os
143/141:
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=100):
    
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter[0][0].shape[0])    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).double())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c, y_c = y_hat.argmax(1), y.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Step:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(model, train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 2, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))


def load_random_patients(n=100):
    fp = './pt_data/'
    patients = os.listdir(fp+'inputs/')
    indices = np.random.randint(0, len(patients), n)
    
    X, y = torch.Tensor(), torch.Tensor()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        y = torch.cat((y, y_), axis=0)
    
    return TensorDataset(X, y)
143/142: dataset = load_random_patients(20)
143/143: np.random.randint(0, 20, n)
143/144: np.random.randint(0, 20, 10)
143/145:
fp = './pt_data/'
os.listdir(fp+'inputs/')
143/146:
fp = './pt_data/'
os.listdir(fp+'inputs/')[0,1,2]
143/147:
fp = './pt_data/'
os.listdir(fp+'inputs/')[[0,1,2]]
143/148:
fp = './pt_data/'
np.array(os.listdir(fp+'inputs/'))[0,1,2]
143/149:
fp = './pt_data/'
np.array(os.listdir(fp+'inputs/'))[[0,1,2]]
143/150:
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=100):
    
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter[0][0].shape[0])    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).double())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c, y_c = y_hat.argmax(1), y.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Step:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(model, train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 2, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))


def load_random_patients(n=100):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    indices = np.random.randint(0, len(patients), n)
    
    X, y = torch.Tensor(), torch.Tensor()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        y = torch.cat((y, y_), axis=0)
    
    return TensorDataset(X, y)
143/151: dataset = load_random_patients(20)
143/152:
# Saved in the d2l package for later use
class Accumulator(object):
    """Sum a list of numbers over time."""

    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a+float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0] * len(self.data)

    def __getitem__(self, i):
        return self.data[i]

def try_gpu():
    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
    is_cuda = torch.cuda.is_available()
    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    return device

def pkl_dump(obj, f):
    with open(f, 'wb') as fo:
        pickle.dump(obj, fo)

def pkl_load(file):
    with open(file, 'rb') as file:
        return pickle.load(file)
143/153:
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=100):
    
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter[0][0].shape[0])    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).double())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c, y_c = y_hat.argmax(1), y.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Step:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(model, train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 2, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))


def load_random_patients(n=100):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    indices = np.random.randint(0, len(patients), n)
    
    X, y = torch.Tensor(), torch.Tensor()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        y = torch.cat((y, y_), axis=0)
    
    return TensorDataset(X, y)
143/154: dataset = load_random_patients(20)
143/155:
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=100):
    
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter[0][0].shape[0])    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).double())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c, y_c = y_hat.argmax(1), y.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Step:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(model, train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 2, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))


def load_random_patients(n=100):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    indices = np.random.randint(0, len(patients), n)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        y = torch.cat((y, y_), axis=0)
    
    return TensorDataset(X, y)
143/156: dataset = load_random_patients(20)
143/157:
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=100):
    
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter[0][0].shape[0])    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).double())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c, y_c = y_hat.argmax(1), y.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Step:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(model, train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 2, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))


def load_random_patients(n=100):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    indices = np.random.randint(0, len(patients), n)
    
    X, y = torch.Tensor().double(), torch.Tensor().double()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        y = torch.cat((y, y_), axis=0)
    
    return TensorDataset(X, y)
143/158: dataset = load_random_patients(20)
143/159:
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=100):
    
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter[0][0].shape[0])    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).double())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c, y_c = y_hat.argmax(1), y.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Step:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(model, train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 2, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))


def load_random_patients(n=100):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    indices = np.random.randint(0, len(patients), n)
    
    X, y = torch.Tensor().double(), torch.Tensor().double()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        y = torch.cat((y, y_.double()), axis=0)
    
    return TensorDataset(X, y)
143/160: dataset = load_random_patients(20)
143/161: dataset
143/162: dataset.tensors
143/163: dataset.size()
143/164: dataset[0]
143/165:
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=100):
    
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter[0][0].shape[0])    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c, y_c = y_hat.argmax(1), y.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Step:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(model, train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 2, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))


def load_random_patients(n=100):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    indices = np.random.randint(0, len(patients), n)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y)
143/166: load_random_patients(1)
143/167:
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=100):
    
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter[0][0].shape[0])    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c, y_c = y_hat.argmax(1), y.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Step:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(model, train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 2, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))


def load_random_patients(n=100):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    indices = np.random.randint(0, len(patients), n)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y)
143/168: load_random_patients(1)
143/169:
dataset = load_random_patients(500)
trained = train_model(dataset, 3)
143/170:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter[0][0].shape[0])    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c, y_c = y_hat.argmax(1), y.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Step:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 2, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_random_patients(n=100):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    indices = np.random.randint(0, len(patients), n)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y)
143/171:
dataset = load_random_patients(100)
trained = train_model(dataset, 1)
143/172:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter[0][0].shape[0])    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c, y_c = y_hat.argmax(1), y.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Step:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 2, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_random_patients(n=100):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    indices = np.random.randint(0, len(patients), n)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y)
143/173:
dataset = load_random_patients(100)
trained = train_model(dataset, 1)
143/174:
X = pkl_load('pt')
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
143/175:
X = pkl_load('pt_data/inputs/0.npy')
y = pkl_load('pt_data/labels/0.npy')
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X.float()), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
143/176:
X = pkl_load('pt_data/inputs/0.npy')
y = pkl_load('pt_data/labels/0.npy')
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X).float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
143/177: X.size()
143/178: X.shape
143/179:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True, op_activation=None):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        # Output layer
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        self.op_activation = op_activation if op_activation is not None else F.softmax
        
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, hidden):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute mean on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
        
        outputs = torch.unsqueeze(self.op_activation(self.fc1(hidden)), 0)
        
        return outputs, hidden

    
    def step(self, inputs, h, xmean):
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:]), \
                            torch.squeeze(inputs[:,1,:]), \
                            torch.squeeze(inputs[:,2,:]), \
                            torch.squeeze(inputs[:,3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask))
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask))
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/180:
X = pkl_load('pt_data/inputs/0.npy')
y = pkl_load('pt_data/labels/0.npy')
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X).float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
143/181:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True, op_activation=None):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        # Output layer
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        self.op_activation = op_activation if op_activation is not None else F.softmax
        
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, hidden):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute mean on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
        
        outputs = torch.unsqueeze(self.op_activation(self.fc1(hidden)), 0)
        
        return outputs, hidden

    
    def step(self, inputs, h, xmean): # inputs = (batch_size x type x dim)
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:]), \ # x = (batch_size x dim)
                            torch.squeeze(inputs[:,1,:]), \
                            torch.squeeze(inputs[:,2,:]), \
                            torch.squeeze(inputs[:,3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/182:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True, op_activation=None):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        # Output layer
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        self.op_activation = op_activation if op_activation is not None else F.softmax
        
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, hidden):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute mean on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
        
        outputs = torch.unsqueeze(self.op_activation(self.fc1(hidden)), 0)
        
        return outputs, hidden

    
    def step(self, inputs, h, xmean): # inputs = (batch_size x type x dim)
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:]), \ 
                            torch.squeeze(inputs[:,1,:]), \
                            torch.squeeze(inputs[:,2,:]), \
                            torch.squeeze(inputs[:,3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/183:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True, op_activation=None):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        # Output layer
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        self.op_activation = op_activation if op_activation is not None else F.softmax
        
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, hidden):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute mean on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
        
        outputs = torch.unsqueeze(self.op_activation(self.fc1(hidden)), 0)
        
        return outputs, hidden

    
    def step(self, inputs, h, xmean): # inputs = (batch_size x type x dim)
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:]), \
                            torch.squeeze(inputs[:,1,:]), \
                            torch.squeeze(inputs[:,2,:]), \
                            torch.squeeze(inputs[:,3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/184:
X = pkl_load('pt_data/inputs/0.npy')
y = pkl_load('pt_data/labels/0.npy')
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X).float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
143/185:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True, op_activation=None):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        # Output layer
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        self.op_activation = op_activation if op_activation is not None else F.softmax
        
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, hidden):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute mean on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
        
        outputs = self.op_activation(self.fc1(hidden))
        
        return outputs, hidden

    
    def step(self, inputs, h, xmean): # inputs = (batch_size x type x dim)
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:]), \
                            torch.squeeze(inputs[:,1,:]), \
                            torch.squeeze(inputs[:,2,:]), \
                            torch.squeeze(inputs[:,3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/186:
X = pkl_load('pt_data/inputs/0.npy')
y = pkl_load('pt_data/labels/0.npy')
model = GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X).float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
143/187:
# dataset = load_random_patients(100)
# trained = train_model(dataset, 1)
x,y = dataset[0]
x.size(), y.size()
143/188:
# dataset = load_random_patients(100)
# trained = train_model(dataset, 1)
x,y = dataset[12]
x.size(), y.size()
len(dataset)
143/189:
# dataset = load_random_patients(100)
# trained = train_model(dataset, 1)
x,y = dataset[12]
x.size(), y.size()
143/190:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter[0][0].shape[0])    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c, y_c = y_hat.argmax(1), y.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Step:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 2, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_random_patients(n):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y)
143/191:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter[0][0].shape[0])    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c, y_c = y_hat.argmax(1), y.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Step:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 2, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y)
143/192:
# dataset = load_patients(100)
loader = DataLoader(dataset, shuffle=True, batch_size=64)
trained = train_model(loader, 1)
143/193:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter.batch_size)    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c, y_c = y_hat.argmax(1), y.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Step:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 2, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y)
143/194:
# dataset = load_patients(100)
loader = DataLoader(dataset, shuffle=True, batch_size=64)
trained = train_model(loader, 1)
143/195:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter.batch_size)    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        print(y_hat.size(), y.size())
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c, y_c = y_hat.argmax(1), y.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Minibatch:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 2, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y)
143/196:
# dataset = load_patients(100)
loader = DataLoader(dataset, shuffle=True, batch_size=64)
trained = train_model(loader, 1)
143/197:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter.batch_size)    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        print(y_hat, y)
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c, y_c = y_hat.argmax(1), y.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Minibatch:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 2, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y)
143/198:
# dataset = load_patients(100)
loader = DataLoader(dataset, shuffle=True, batch_size=64)
trained = train_model(loader, 1)
143/199:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True, op_activation=None):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        # Output layer
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        self.op_activation = op_activation if op_activation is not None else lambda x:x #do-nothing
        
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, hidden):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute mean on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
        
        outputs = self.op_activation(self.fc1(hidden))
        
        return outputs, hidden

    
    def step(self, inputs, h, xmean): # inputs = (batch_size x type x dim)
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:]), \
                            torch.squeeze(inputs[:,1,:]), \
                            torch.squeeze(inputs[:,2,:]), \
                            torch.squeeze(inputs[:,3,:])
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/200:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter.batch_size)    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c, y_c = y_hat.argmax(1), y.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Minibatch:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 2, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y)
143/201:
# dataset = load_patients(100)
loader = DataLoader(dataset, shuffle=True, batch_size=64)
trained = train_model(loader, 1)
143/202:
input = torch.randn(3, 5, requires_grad=True)
target = torch.randint(5, (3,), dtype=torch.int64)
input.size(), target.size()
143/203:
input = torch.randn(3, 5, requires_grad=True)
target = torch.randint(5, (3,), dtype=torch.int64)
target
143/204:
input = torch.randn(3, 5, requires_grad=True)
target = torch.randint(5, (3,), dtype=torch.int64)
input
143/205:
input = torch.randn(3, 5, requires_grad=True)
target = torch.randint(5, (3,), dtype=torch.int64)
input.argamx(1)
143/206:
input = torch.randn(3, 5, requires_grad=True)
target = torch.randint(5, (3,), dtype=torch.int64)
input.argmax(1)
143/207:
input = torch.randn(3, 5, requires_grad=True)
target = torch.randint(5, (3,), dtype=torch.int64)
input, input.argmax(1)
143/208:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter.batch_size)    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, torch.squeeze(y))
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c, y_c = y_hat.argmax(1), y.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Minibatch:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 2, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: y_ = y_.argmax(1)
        y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y)
143/209:
# dataset = load_patients(100)
loader = DataLoader(dataset, shuffle=True, batch_size=64)
trained = train_model(loader, 1)
143/210:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter.batch_size)    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        print(y_hat.size(), y.size())
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c, y_c = y_hat.argmax(1), y.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Minibatch:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 2, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: y_ = y_.argmax(1)
        y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y)
143/211:
# dataset = load_patients(100)
loader = DataLoader(dataset, shuffle=True, batch_size=64)
trained = train_model(loader, 1)
143/212:
dataset = load_patients(10)
loader = DataLoader(dataset, shuffle=True, batch_size=64)
trained = train_model(loader, 1)
143/213:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter.batch_size)    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c, y_c = y_hat.argmax(1), y.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Minibatch:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 2, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y)
143/214:
dataset = load_patients(10)
loader = DataLoader(dataset, shuffle=True, batch_size=64)
trained = train_model(loader, 1)
143/215:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter.batch_size)    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c, y_c = y_hat.argmax(1), y.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Minibatch:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 2, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y)
143/216:
dataset = load_patients(10)
loader = DataLoader(dataset, shuffle=True, batch_size=64)
trained = train_model(loader, 1)
143/217:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter.batch_size)    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c = y_hat.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Minibatch:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 2, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y)
143/218:
dataset = load_patients(10)
loader = DataLoader(dataset, shuffle=True, batch_size=64)
trained = train_model(loader, 1)
143/219:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter.batch_size)    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c = y_hat.argmax(1)
            cf_matrix[yh_c][y_c] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Minibatch:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 3, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y)
143/220:
dataset = load_patients(10)
loader = DataLoader(dataset, shuffle=True, batch_size=64)
trained = train_model(loader, 1)
143/221:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter.batch_size)    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c = y_hat.argmax(1)
            cf_matrix[yh_c][y] += 1
        
        metrics.add(y.size, loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Minibatch:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 3, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y)
143/222:
dataset = load_patients(10)
loader = DataLoader(dataset, shuffle=True, batch_size=64)
trained = train_model(loader, 1)
143/223:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual

    state = model.init_hidden(train_iter.batch_size)    
    for i, (X, y) in enumerate(train_iter): # iterate through batches
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c = y_hat.argmax(1)
            cf_matrix[yh_c][y] += 1
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Minibatch:{i}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 3, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y)
143/224:
dataset = load_patients(10)
loader = DataLoader(dataset, shuffle=True, batch_size=64)
trained = train_model(loader, 1)
143/225:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual
        
    for batch, (X, y) in enumerate(train_iter): 
        state = model.init_hidden(train_iter.batch_size)
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c = y_hat.argmax(1)
            cf_matrix[yh_c][y] += 1
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        
        if i%print_every == print_every-1:
            print(f"Minibatch:{batch}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 3, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y)
143/226:
dataset = load_patients(10)
loader = DataLoader(dataset, shuffle=True, batch_size=64)
trained = train_model(loader, 1)
143/227:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual
        
    for batch, (X, y) in enumerate(train_iter): 
        state = model.init_hidden(train_iter.batch_size)
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c = y_hat.argmax(1)
            cf_matrix[yh_c][y] += 1
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        
        if batch%print_every == print_every-1:
            print(f"Minibatch:{batch}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 3, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y)
143/228:
dataset = load_patients(10)
loader = DataLoader(dataset, shuffle=True, batch_size=64)
trained = train_model(loader, 1)
143/229:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True, op_activation=None):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        # Output layer
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        self.op_activation = op_activation if op_activation is not None else lambda x:x #do-nothing
        
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, hidden):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute mean on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
        
        outputs = self.op_activation(self.fc1(hidden))
        
        return outputs, hidden

    
    def step(self, inputs, h, xmean): # inputs = (batch_size x type x dim)
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:]), \
                            torch.squeeze(inputs[:,1,:]), \
                            torch.squeeze(inputs[:,2,:]), \
                            torch.squeeze(inputs[:,3,:])
        
        print(inputs.size(), x.size(), h.size())
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/230:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual
        
    for batch, (X, y) in enumerate(train_iter): 
        state = model.init_hidden(train_iter.batch_size)
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c = y_hat.argmax(1)
            cf_matrix[yh_c][y] += 1
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        
        if batch%print_every == print_every-1:
            print(f"Minibatch:{batch}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 3, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in cf_matrix.shape[0]]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y)
143/231:
# dataset = load_patients(10)
# loader = DataLoader(dataset, shuffle=True, batch_size=64)
trained = train_model(loader, 1)
143/232:
class GRUD(nn.Module):
    
    def __init__(self, input_dim=47, hidden_dim=300, output_dim=3, use_decay=True, op_activation=None):
        super(GRUD, self).__init__()
        self.ctx = try_gpu()
        
        # Assign input and hidden dim
        mask_dim = delta_dim = input_dim
        self.hidden_dim = hidden_dim
        self.use_decay = use_decay
        self.zeros = lambda x: Variable(torch.zeros(x))
        
        # Output layer
        self.fc1 = nn.Linear(hidden_dim, output_dim)
        self.op_activation = op_activation if op_activation is not None else lambda x:x #do-nothing
        
        
        
        # Combinators
        if use_decay:
            self.R_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim + mask_dim, hidden_dim)
            self.gamma_x_lin = nn.Linear(delta_dim, delta_dim)
            self.gamma_h_lin = nn.Linear(delta_dim, hidden_dim)
        else:
            self.R_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.Z_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            self.tilde_lin = nn.Linear(input_dim + hidden_dim, hidden_dim)
            
            
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_dim, device=self.ctx)

    def forward(self, inputs, hidden):
        # forward fn. receives a batch of inputs. init hidden before processing inputs. return outputs
        # we can direcly compute mean on the batch because we're shuffle-sampling
        batch_size = inputs.size(0)
        step_size = inputs.size(2)
        outputs = None
        
        xmean = Variable(torch.mean(torch.squeeze(inputs[:,0,:,:]), [0,1]))

        for t in range(step_size):
            hidden = self.step(torch.squeeze(inputs[:,:,t:t+1,:]), hidden, xmean)
        
        outputs = self.op_activation(self.fc1(hidden))
        
        return outputs, hidden

    
    def step(self, inputs, h, xmean): # inputs = (batch_size x type x dim)
        x, obs_mask, delta, x_tm1 = torch.squeeze(inputs[:,0,:]), \
                            torch.squeeze(inputs[:,1,:]), \
                            torch.squeeze(inputs[:,2,:]), \
                            torch.squeeze(inputs[:,3,:])
        
#         print(inputs.size(), x.size(), h.size())
        
        if self.use_decay:
            gamma_x = torch.exp(-torch.max(self.zeros(delta.size()), self.gamma_x_lin(delta)))
            gamma_h = torch.exp(-torch.max(self.zeros(h.size()), self.gamma_h_lin(delta)))
            x = (obs_mask * x) + (1-obs_mask)*( (gamma_x*x_tm1) + (1-gamma_x)*xmean ) # Empirical mean at batch-level
            h = torch.squeeze(gamma_h*h)

        gate_in = torch.cat((x,h,obs_mask), axis=1)
        z = F.sigmoid(self.Z_lin(gate_in))
        r = F.sigmoid(self.R_lin(gate_in))
        tilde_in = torch.cat((x, r*h, obs_mask), axis=1)
        tilde = F.tanh(self.tilde_lin(tilde_in))
        h = (1-z)*h + z*tilde
        return h
143/233:
# dataset, patients = load_patients(10)
loader = DataLoader(dataset, shuffle=True, batch_size=64, drop_last=True)
trained = train_model(loader, 1)
143/234:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual
        
    for batch, (X, y) in enumerate(train_iter): 
        state = model.init_hidden(train_iter.batch_size)
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c = y_hat.argmax(1)
            cf_matrix[yh_c][y] += 1
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        
        if batch%print_every == print_every-1:
            print(f"Minibatch:{batch}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 3, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in range(cf_matrix.shape[0])]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y), patients
143/235:
# dataset, patients = load_patients(10)
loader = DataLoader(dataset, shuffle=True, batch_size=64, drop_last=True)
trained = train_model(loader, 1)
143/236:
# dataset, patients = load_patients(10)
# loader = DataLoader(dataset, shuffle=True, batch_size=64, drop_last=True)
# trained = train_model(loader, 1)
len(loader)
143/237:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=1):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual
        
    for batch, (X, y) in enumerate(train_iter): 
        state = model.init_hidden(train_iter.batch_size)
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c = y_hat.argmax(1)
            cf_matrix[yh_c][y] += 1
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        
        if batch%print_every == print_every-1:
            print(f"Minibatch:{batch}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 3, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in range(cf_matrix.shape[0])]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y), patients
143/238:
# dataset, patients = load_patients(10)
# loader = DataLoader(dataset, shuffle=True, batch_size=64, drop_last=True)
# trained = train_model(loader, 1)
143/239:
# dataset, patients = load_patients(10)
# loader = DataLoader(dataset, shuffle=True, batch_size=64, drop_last=True)
trained = train_model(loader, 1)
143/240:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=1):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual
        
    for batch, (X, y) in enumerate(train_iter): 
        state = model.init_hidden(train_iter.batch_size)
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c = y_hat.argmax(1)
            cf_matrix[yh_c][y] += 1
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        
        if batch%print_every == print_every-1:
            print(f"Minibatch:{batch}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 3, 500)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in range(cf_matrix.shape[0])]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y), patients
143/241:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual
        
    for batch, (X, y) in enumerate(train_iter): 
        state = model.init_hidden(train_iter.batch_size)
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c = y_hat.argmax(1)
            cf_matrix[yh_c][y] += 1
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        
        if batch%print_every == print_every-1:
            print(f"Minibatch:{batch}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
            
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 3, 1)

        loss_steps+=l
        scores = [cf_matrix[i,i]/cf_matrix[:,i].sum() for i in range(cf_matrix.shape[0])]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y), patients
143/242:
# dataset, patients = load_patients(10)
# loader = DataLoader(dataset, shuffle=True, batch_size=64, drop_last=True)
trained = train_model(loader, 1)
143/243: cf = torch.rand()
143/244: cf = torch.rand((5,5))
143/245:
cf = torch.rand((5,5))
cf
143/246:
cf = torch.rand((5,5))
cf[0,0]
143/247:
# cf = torch.rand((5,5))
cf, cf[0,0]
143/248:
# cf = torch.rand((5,5))
cf[0,0]/cf[:,0]
143/249:
# cf = torch.rand((5,5))
cf[0,0]/cf[:,0].sum()
143/250:
# cf = torch.rand((5,5))
cf[:,0]
143/251:
# cf = torch.rand((5,5))

cf[0,0]/cf[:,0].sum(), 0.2363/sum([0.9164, 0.0417, 0.8764, 0.6574])
143/252:
# cf = torch.rand((5,5))

cf[0,0]/(cf[:,0].sum()), 0.2363/sum([0.9164, 0.0417, 0.8764, 0.6574])
143/253:
# cf = torch.rand((5,5))

cf[0,0]#/(cf[:,0].sum()), 0.2363/sum([0.9164, 0.0417, 0.8764, 0.6574])
143/254:
# cf = torch.rand((5,5))

cf[0,0]/(cf[:,0].sum()), 0.2363/sum([0.9164, 0.0417, 0.8764, 0.6574])
143/255:
# cf = torch.rand((5,5))

cf[0,0].item()/(cf[:,0].sum()), 0.2363/sum([0.9164, 0.0417, 0.8764, 0.6574])
143/256:
# cf = torch.rand((5,5))
cf[:,0].sum()
# cf[0,0].item()/(cf[:,0].sum()), 0.2363/sum([0.9164, 0.0417, 0.8764, 0.6574])
143/257:
# cf = torch.rand((5,5))
cf[:,0].sum(), sum([0.9164, 0.0417, 0.8764, 0.6574])

# cf[0,0].item()/(cf[:,0].sum()), 0.2363/sum([0.9164, 0.0417, 0.8764, 0.6574])
143/258:
# cf = torch.rand((5,5))
cf[:,0], sum([0.9164, 0.0417, 0.8764, 0.6574])

# cf[0,0].item()/(cf[:,0].sum()), 0.2363/sum([0.9164, 0.0417, 0.8764, 0.6574])
143/259:
# cf = torch.rand((5,5))
# cf[:,0], sum([0.9164, 0.0417, 0.8764, 0.6574])

cf[0,0].item()/(cf[:,0].sum()), 0.2363/sum([0.2363, 0.9164, 0.0417, 0.8764, 0.6574])
143/260:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual
        
    for batch, (X, y) in enumerate(train_iter): 
        state = model.init_hidden(train_iter.batch_size)
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c = y_hat.argmax(1)
            cf_matrix[yh_c][y] += 1
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        scores = [(cf_matrix[i,i]/cf_matrix[:,i].sum()).item() for i in range(cf_matrix.size(0))]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        
        if batch%print_every == print_every-1:
            print(f"Minibatch:{batch}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
        
    return metrics[1]/metrics[0], cf_matrix, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, cf_matrix, l = train_epoch(model, train_iter, criterion, optimizer, 3, 1)

        loss_steps+=l
        print(f"\nEPOCH {epoch},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y), patients
143/261:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual
        
    for batch, (X, y) in enumerate(train_iter): 
        state = model.init_hidden(train_iter.batch_size)
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c = y_hat.argmax(1)
            cf_matrix[yh_c][y] += 1
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        scores = [(cf_matrix[i,i]/cf_matrix[:,i].sum()).item() for i in range(cf_matrix.size(0))]
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        
        if batch%print_every == print_every-1:
            print(f"Minibatch:{batch}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
        
    return metrics[1]/metrics[0], scores, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, scores, l = train_epoch(model, train_iter, criterion, optimizer, 3, 1)
        loss_steps+=l
        print(f"\nEPOCH {epoch}, AvgLoss: {avg_loss},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y), patients
143/262:
# dataset, patients = load_patients(10)
# loader = DataLoader(dataset, shuffle=True, batch_size=64, drop_last=True)
trained = train_model(loader, 1)
143/263:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual
        
    for batch, (X, y) in enumerate(train_iter): 
        state = model.init_hidden(train_iter.batch_size)
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c = y_hat.argmax(1)
            cf_matrix[yh_c][y] += 1
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        scores = [(cf_matrix[i,i]/cf_matrix[:,i].sum()).item() for i in range(cf_matrix.size(0))]
        print(scores)
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        
        if batch%print_every == print_every-1:
            print(f"Minibatch:{batch}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
        
    return metrics[1]/metrics[0], scores, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, scores, l = train_epoch(model, train_iter, criterion, optimizer, 3, 1)
        loss_steps+=l
        print(f"\nEPOCH {epoch}, AvgLoss: {avg_loss},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y), patients
143/264:
# dataset, patients = load_patients(10)
# loader = DataLoader(dataset, shuffle=True, batch_size=64, drop_last=True)
trained = train_model(loader, 1)
143/265:
# cf = torch.rand((5,5))
# cf[:,0], sum([0.9164, 0.0417, 0.8764, 0.6574])

cf[0][0] +=1
cf
143/266:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual
        
    for batch, (X, y) in enumerate(train_iter): 
        state = model.init_hidden(train_iter.batch_size)
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c = y_hat.argmax(1)
            cf_matrix[yh_c][y] += 1
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        scores = [(cf_matrix[i,i]/cf_matrix[:,i].sum()).item() for i in range(cf_matrix.size(0))]
        print(cf_matrix)
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        
        if batch%print_every == print_every-1:
            print(f"Minibatch:{batch}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
        
    return metrics[1]/metrics[0], scores, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, scores, l = train_epoch(model, train_iter, criterion, optimizer, 3, 1)
        loss_steps+=l
        print(f"\nEPOCH {epoch}, AvgLoss: {avg_loss},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y), patients
143/267:
# dataset, patients = load_patients(10)
# loader = DataLoader(dataset, shuffle=True, batch_size=64, drop_last=True)
trained = train_model(loader, 1)
143/268:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual
        
    for batch, (X, y) in enumerate(train_iter): 
        state = model.init_hidden(train_iter.batch_size)
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        if nb_classes:
            yh_c = y_hat.argmax(1)
            print(yh_c)
            cf_matrix[yh_c][y] += 1
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        scores = [(cf_matrix[i,i]/cf_matrix[:,i].sum()).item() for i in range(cf_matrix.size(0))]
        print(cf_matrix)
        scores = [f"{l}Charge Acc:{s}" for l,s in zip(["Low", "Medium", "High"], scores)]
        
        if batch%print_every == print_every-1:
            print(f"Minibatch:{batch}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
        
    return metrics[1]/metrics[0], scores, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, scores, l = train_epoch(model, train_iter, criterion, optimizer, 3, 1)
        loss_steps+=l
        print(f"\nEPOCH {epoch}, AvgLoss: {avg_loss},   {scores}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y), patients
143/269:
# dataset, patients = load_patients(10)
# loader = DataLoader(dataset, shuffle=True, batch_size=64, drop_last=True)
trained = train_model(loader, 1)
143/270:
def train_epoch(model, train_iter, criterion, optimizer, nb_classes=None, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
    if nb_classes: cf_matrix = torch.zeros(nb_classes, nb_classes) # pred x actual
        
    for batch, (X, y) in enumerate(train_iter): 
        state = model.init_hidden(train_iter.batch_size)
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        
        if batch%print_every == print_every-1:
            print(f"Minibatch:{batch}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
        
    return metrics[1]/metrics[0], scores, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, l = train_epoch(model, train_iter, criterion, optimizer, 3, 1)
        loss_steps+=l
        print(f"\nEPOCH {epoch}, AvgLoss: {avg_loss}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y), patients
143/271:
def train_epoch(model, train_iter, criterion, optimizer, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
        
    for batch, (X, y) in enumerate(train_iter): 
        state = model.init_hidden(train_iter.batch_size)
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        
        if batch%print_every == print_every-1:
            print(f"Minibatch:{batch}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
        
    return metrics[1]/metrics[0], scores, loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, l = train_epoch(model, train_iter, criterion, optimizer, 10)
        loss_steps+=l
        print(f"\nEPOCH {epoch}, AvgLoss: {avg_loss}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y), patients
143/272:
# dataset, patients = load_patients(10)
# loader = DataLoader(dataset, shuffle=True, batch_size=64, drop_last=True)
trained = train_model(loader, 1)
143/273:
def train_epoch(model, train_iter, criterion, optimizer, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
        
    for batch, (X, y) in enumerate(train_iter): 
        state = model.init_hidden(train_iter.batch_size)
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        
        if batch%print_every == print_every-1:
            print(f"Minibatch:{batch}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
        
    return metrics[1]/metrics[0], loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, l = train_epoch(model, train_iter, criterion, optimizer, 10)
        loss_steps+=l
        print(f"\nEPOCH {epoch}, AvgLoss: {avg_loss}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y), patients
143/274:
dataset, patients = load_patients(1000)
loader = DataLoader(dataset, shuffle=True, batch_size=64, drop_last=True)
trained = train_model(loader, 1)
143/275:
dataset, patients = load_patients(500)
loader = DataLoader(dataset, shuffle=True, batch_size=64, drop_last=True)
trained = train_model(loader, 1)
143/276:
def train_epoch(model, train_iter, criterion, optimizer, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
        
    for batch, (X, y) in enumerate(train_iter): 
        state = model.init_hidden(train_iter.batch_size)
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        
        if batch%print_every == 0:
            print(f"Minibatch:{batch}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
        
    return metrics[1]/metrics[0], loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, l = train_epoch(model, train_iter, criterion, optimizer, 10)
        loss_steps+=l
        print(f"\nEPOCH {epoch}, AvgLoss: {avg_loss}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = np.random.randint(0, len(patients), 100)
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y), patients
143/277:
def train_epoch(model, train_iter, criterion, optimizer, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
        
    for batch, (X, y) in enumerate(train_iter): 
        state = model.init_hidden(train_iter.batch_size)
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        
        if batch%print_every == 0:
            print(f"Minibatch:{batch}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
        
    return metrics[1]/metrics[0], loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, l = train_epoch(model, train_iter, criterion, optimizer, 10)
        loss_steps+=l
        print(f"\nEPOCH {epoch}, AvgLoss: {avg_loss}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.ylim((0,0.2))
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = range(len(patients))
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y), patients
143/278:
def train_epoch(model, train_iter, criterion, optimizer, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
        
    for batch, (X, y) in enumerate(train_iter): 
        state = model.init_hidden(train_iter.batch_size)
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        
        if batch%print_every == 0:
            print(f"Minibatch:{batch}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
        
    return metrics[1]/metrics[0], loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, l = train_epoch(model, train_iter, criterion, optimizer, 10)
        loss_steps+=l
        print(f"\nEPOCH {epoch}, AvgLoss: {avg_loss}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    
    return model


def load_patients(n, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = range(len(patients))
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y), patients
143/279:
import time
t0 = time.time()

dataset, patients = load_patients()
loader = DataLoader(dataset, shuffle=True, batch_size=200, drop_last=True)
trained = train_model(loader, 1)

print(time.time()-t0)
143/280:
def train_epoch(model, train_iter, criterion, optimizer, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
        
    for batch, (X, y) in enumerate(train_iter): 
        state = model.init_hidden(train_iter.batch_size)
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        
        if batch%print_every == 0:
            print(f"Minibatch:{batch}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
        
    return metrics[1]/metrics[0], loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, l = train_epoch(model, train_iter, criterion, optimizer, 10)
        loss_steps+=l
        print(f"\nEPOCH {epoch}, AvgLoss: {avg_loss}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    
    return model


def load_patients(n=None, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = range(len(patients))
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y), patients
143/281:
import time
t0 = time.time()

dataset, patients = load_patients()
loader = DataLoader(dataset, shuffle=True, batch_size=200, drop_last=True)
trained = train_model(loader, 1)

print(time.time()-t0)
143/282: torch.load('pt_data/inputs/0.npy')
143/283: pkl_load('pt_data/inputs/0.npy')
143/284: torch.load('pt_data/inputs/0.npy')
143/285:
# Saved in the d2l package for later use
class Accumulator(object):
    """Sum a list of numbers over time."""

    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a+float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0] * len(self.data)

    def __getitem__(self, i):
        return self.data[i]

def try_gpu():
    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
    is_cuda = torch.cuda.is_available()
    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    return device

def pkl_dump(obj, f):
    with open(f, 'wb') as fo:
        pickle.dump(obj, fo)

def pkl_load(file):
    with open(file, 'rb') as file:
        return pickle.load(file)
    

import torch
from torch.utils import data

class Dataset(data.Dataset):  
    def __init__(self, list_IDs):
        self.fp = './pt_data/'
        if isinstance(list_IDs, int):
            patients = np.array(os.listdir(self.fp+'inputs/'))
            self.list_IDs = np.random.randint(0, len(patients), list_IDs)
        self.list_IDs = list_IDs

    def __len__(self):
        return len(self.list_IDs)

    def __getitem__(self, index):
        # Select sample
        ID = self.list_IDs[index]
        # Load data and get label
        X = torch.load('data/' + ID + '.pt')
        y = self.labels[ID]
        return X, y
143/286:
# Saved in the d2l package for later use
class Accumulator(object):
    """Sum a list of numbers over time."""

    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a+float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0] * len(self.data)

    def __getitem__(self, i):
        return self.data[i]

def try_gpu():
    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
    is_cuda = torch.cuda.is_available()
    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    return device

def pkl_dump(obj, f):
    with open(f, 'wb') as fo:
        pickle.dump(obj, fo)

def pkl_load(file):
    with open(file, 'rb') as file:
        return pickle.load(file)
    

import torch
from torch.utils import data

class PatientDataset(data.Dataset):  
    def __init__(self, list_IDs, onehot=False):
        self.fp = './pt_data/'
        if isinstance(list_IDs, int):
            patients = np.array(os.listdir(self.fp+'inputs/'))
            ix = np.random.randint(0, len(patients), list_IDs)
            self.list_IDs = [patients[i].replace('.npy','') for i in ix]
        self.list_IDs = list_IDs

    def __len__(self):
        return len(self.list_IDs)

    def __getitem__(self, index):
        # Select sample
        ID = self.list_IDs[index]
        # Load data and get label
        X = pkl_load(os.path.join(self.fp, 'inputs', ID, '.npy'))
        y = pkl_load(os.path.join(self.fp, 'labels', ID, '.npy'))
        if not onehot: 
            y = y.argmax(1)
        return X, y
143/287:
import time
t0 = time.time()

dataset = PatientDataset(10)
loader = DataLoader(dataset, shuffle=True, batch_size=10, drop_last=True)
print(time.time()-t0)
143/288:
# Saved in the d2l package for later use
class Accumulator(object):
    """Sum a list of numbers over time."""

    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a+float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0] * len(self.data)

    def __getitem__(self, i):
        return self.data[i]

def try_gpu():
    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
    is_cuda = torch.cuda.is_available()
    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    return device

def pkl_dump(obj, f):
    with open(f, 'wb') as fo:
        pickle.dump(obj, fo)

def pkl_load(file):
    with open(file, 'rb') as file:
        return pickle.load(file)
    

import torch
from torch.utils import data

class PatientDataset(data.Dataset):  
    def __init__(self, list_IDs, onehot=False):
        self.fp = './pt_data/'
        if isinstance(list_IDs, int):
            patients = np.array(os.listdir(self.fp+'inputs/'))
            ix = np.random.randint(0, len(patients), list_IDs)
            self.list_IDs = [patients[i].replace('.npy','') for i in ix]
            print(self.list_IDs)
        self.list_IDs = list_IDs

    def __len__(self):
        return len(self.list_IDs)

    def __getitem__(self, index):
        # Select sample
        ID = self.list_IDs[index]
        # Load data and get label
        X = pkl_load(os.path.join(self.fp, 'inputs', ID, '.npy'))
        y = pkl_load(os.path.join(self.fp, 'labels', ID, '.npy'))
        if not onehot: 
            y = y.argmax(1)
        return X, y
143/289:
import time
t0 = time.time()

dataset = PatientDataset(10)
loader = DataLoader(dataset, shuffle=True, batch_size=10, drop_last=True)
print(time.time()-t0)
143/290:
# Saved in the d2l package for later use
class Accumulator(object):
    """Sum a list of numbers over time."""

    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a+float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0] * len(self.data)

    def __getitem__(self, i):
        return self.data[i]

def try_gpu():
    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
    is_cuda = torch.cuda.is_available()
    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    return device

def pkl_dump(obj, f):
    with open(f, 'wb') as fo:
        pickle.dump(obj, fo)

def pkl_load(file):
    with open(file, 'rb') as file:
        return pickle.load(file)
    

import torch
from torch.utils import data

class PatientDataset(data.Dataset):  
    def __init__(self, list_IDs, onehot=False):
        self.fp = './pt_data/'
        if isinstance(list_IDs, int):
            patients = np.array(os.listdir(self.fp+'inputs/'))
            ix = np.random.randint(0, len(patients), list_IDs)
            self.list_IDs = [patients[i].replace('.npy','') for i in ix]
            print(self.list_IDs)
        else:
            self.list_IDs = list_IDs

    def __len__(self):
        return len(self.list_IDs)

    def __getitem__(self, index):
        # Select sample
        ID = self.list_IDs[index]
        # Load data and get label
        X = pkl_load(os.path.join(self.fp, 'inputs', ID, '.npy'))
        y = pkl_load(os.path.join(self.fp, 'labels', ID, '.npy'))
        if not onehot: 
            y = y.argmax(1)
        return X, y
143/291:
import time
t0 = time.time()

dataset = PatientDataset(10)
loader = DataLoader(dataset, shuffle=True, batch_size=10, drop_last=True)
print(time.time()-t0)
143/292:
import time
t0 = time.time()

dataset = PatientDataset(100)
loader = DataLoader(dataset, shuffle=True, batch_size=10, drop_last=True)
print(time.time()-t0)
143/293:
import time
t0 = time.time()

dataset = PatientDataset(1000)
loader = DataLoader(dataset, shuffle=True, batch_size=10, drop_last=True)
print(time.time()-t0)
143/294:
import time
t0 = time.time()

%timeit dataset = PatientDataset(100)
%timeit d = load_patients(100)
loader = DataLoader(dataset, shuffle=True, batch_size=10, drop_last=True)
143/295:
import time
t0 = time.time()

%timeit dataset = PatientDataset(10)
%timeit d = load_patients(10)
loader = DataLoader(dataset, shuffle=True, batch_size=10, drop_last=True)
143/296:
def train_epoch(model, train_iter, criterion, optimizer, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
        
    for batch, (X, y) in enumerate(train_iter): 
        state = model.init_hidden(train_iter.batch_size)
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        
        if batch%print_every == 0:
            print(f"Minibatch:{batch}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
        
    return metrics[1]/metrics[0], loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, l = train_epoch(model, train_iter, criterion, optimizer, 10)
        loss_steps+=l
        print(f"\nEPOCH {epoch}, AvgLoss: {avg_loss}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    
    return model


def load_patients(n=None, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = range(len(patients))
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y), patients
143/297:
def train_epoch(model, train_iter, criterion, optimizer, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
        
    for batch, (X, y) in enumerate(train_iter): 
        state = model.init_hidden(train_iter.batch_size)
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        
        if batch%print_every == 0:
            print(f"Minibatch:{batch}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
        
    return metrics[1]/metrics[0], loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, l = train_epoch(model, train_iter, criterion, optimizer, 10)
        loss_steps+=l
        print(f"\nEPOCH {epoch}, AvgLoss: {avg_loss}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    
    return model


def load_patients(n=None, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = range(len(patients))
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y), patients
143/298:
def train_epoch(model, train_iter, criterion, optimizer, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
        
    for batch, (X, y) in enumerate(train_iter): 
        state = model.init_hidden(train_iter.batch_size)
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        
        if batch%print_every == 0:
            print(f"Minibatch:{batch}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
        
    return metrics[1]/metrics[0], loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, l = train_epoch(model, train_iter, criterion, optimizer, 10)
        loss_steps+=l
        print(f"\nEPOCH {epoch}, AvgLoss: {avg_loss}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    
    return model


def load_patients(n=None, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = range(len(patients))
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y), patients
143/299:
def train_epoch(model, train_iter, criterion, optimizer, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
        
    for batch, (X, y) in enumerate(train_iter): 
        state = model.init_hidden(train_iter.batch_size)
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        
        if batch%print_every == 0:
            print(f"Minibatch:{batch}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
        
    return metrics[1]/metrics[0], loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, l = train_epoch(model, train_iter, criterion, optimizer, 10)
        loss_steps+=l
        print(f"\nEPOCH {epoch}, AvgLoss: {avg_loss}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    
    return model


def load_patients(n=None, onehot=False):
    fp = './pt_data/'
    patients = np.array(os.listdir(fp+'inputs/'))
    
    if isinstance(n, list):
        indices = n
    elif isinstance(n, int):
        indices = np.random.randint(0, len(patients), n)
    else:
        indices = range(len(patients))
    
    X, y = torch.Tensor().float(), torch.Tensor().float()
    for filename in patients[indices]:
        ipx = os.path.join(fp, 'inputs', filename)
        ipy = os.path.join(fp, 'labels', filename)
        
        x_ = torch.from_numpy(pkl_load(ipx))
        X = torch.cat((X, x_.float()), axis=0)
        
        y_ = torch.from_numpy(pkl_load(ipy))
        if not onehot: 
            y = y.long()
            y_ = y_.argmax(1)
            y = torch.cat((y, y_), axis=0)
        else:
            y = torch.cat((y, y_.float()), axis=0)
    
    return TensorDataset(X, y), patients
143/300:
import time
t0 = time.time()

%timeit dataset = PatientDataset(10)
%timeit d = load_patients(10)
loader = DataLoader(dataset, shuffle=True, batch_size=10, drop_last=True)
143/301:
# Saved in the d2l package for later use
class Accumulator(object):
    """Sum a list of numbers over time."""

    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a+float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0] * len(self.data)

    def __getitem__(self, i):
        return self.data[i]

def try_gpu():
    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
    is_cuda = torch.cuda.is_available()
    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    return device

def pkl_dump(obj, f):
    with open(f, 'wb') as fo:
        pickle.dump(obj, fo)

def pkl_load(file):
    with open(file, 'rb') as file:
        return pickle.load(file)
    

import torch
from torch.utils import data

class PatientDataset(data.Dataset):  
    def __init__(self, list_IDs, onehot=False):
        self.fp = './pt_data/'
        if isinstance(list_IDs, int):
            patients = np.array(os.listdir(self.fp+'inputs/'))
            ix = np.random.randint(0, len(patients), list_IDs)
            self.list_IDs = [patients[i].replace('.npy','') for i in ix]
        else:
            self.list_IDs = list_IDs

    def __len__(self):
        return len(self.list_IDs)

    def __getitem__(self, index):
        # Select sample
        ID = self.list_IDs[index]
        # Load data and get label
        X = pkl_load(os.path.join(self.fp, 'inputs', ID, '.npy'))
        y = pkl_load(os.path.join(self.fp, 'labels', ID, '.npy'))
        if not onehot: 
            y = y.argmax(1)
        return X, y
143/302:
# Saved in the d2l package for later use
class Accumulator(object):
    """Sum a list of numbers over time."""

    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a+float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0] * len(self.data)

    def __getitem__(self, i):
        return self.data[i]

def try_gpu():
    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
    is_cuda = torch.cuda.is_available()
    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    return device

def pkl_dump(obj, f):
    with open(f, 'wb') as fo:
        pickle.dump(obj, fo)

def pkl_load(file):
    with open(file, 'rb') as file:
        return pickle.load(file)
    

import torch
from torch.utils import data

class PatientDataset(data.Dataset):  
    def __init__(self, list_IDs, onehot=False):
        self.fp = './pt_data/'
        if isinstance(list_IDs, int):
            patients = np.array(os.listdir(self.fp+'inputs/'))
            ix = np.random.randint(0, len(patients), list_IDs)
            self.list_IDs = [patients[i].replace('.npy','') for i in ix]
        else:
            self.list_IDs = list_IDs

    def __len__(self):
        return len(self.list_IDs)

    def __getitem__(self, index):
        # Select sample
        ID = self.list_IDs[index]
        # Load data and get label
        X = pkl_load(os.path.join(self.fp, 'inputs', ID, '.npy'))
        y = pkl_load(os.path.join(self.fp, 'labels', ID, '.npy'))
        if not onehot: 
            y = y.argmax(1)
        return X, y
143/303:
import time
t0 = time.time()

%timeit dataset = PatientDataset(10)
# %timeit d,p = load_patients(10)
loader = DataLoader(dataset, shuffle=True, batch_size=10, drop_last=True)
143/304:
import time
t0 = time.time()

# %timeit dataset = PatientDataset(10)
%timeit d,p = load_patients(10)
loader = DataLoader(dataset, shuffle=True, batch_size=10, drop_last=True)
143/305:
import time
t0 = time.time()

%timeit dataset = PatientDataset(100)
# %timeit d,p = load_patients(10)
loader = DataLoader(dataset, shuffle=True, batch_size=10, drop_last=True)
143/306:
import time
t0 = time.time()

# %timeit dataset = PatientDataset(100)
%timeit d,p = load_patients(100)
loader = DataLoader(dataset, shuffle=True, batch_size=10, drop_last=True)
143/307:
# Saved in the d2l package for later use
class Accumulator(object):
    """Sum a list of numbers over time."""

    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a+float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0] * len(self.data)

    def __getitem__(self, i):
        return self.data[i]

def try_gpu():
    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
    is_cuda = torch.cuda.is_available()
    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    return device

def pkl_dump(obj, f):
    with open(f, 'wb') as fo:
        pickle.dump(obj, fo)

def pkl_load(file):
    with open(file, 'rb') as file:
        return pickle.load(file)
    

import torch
from torch.utils import data

class PatientDataset(data.Dataset):  
    def __init__(self, list_IDs, onehot=False):
        self.fp = './pt_data/'
        patients = np.array(os.listdir(self.fp+'inputs/'))
        if isinstance(list_IDs, list):
            self.list_IDs = list_IDs
        elif isinstance(list_IDs, int):
            ix = np.random.randint(0, len(patients), list_IDs)
            self.list_IDs = [patients[i].replace('.npy','') for i in ix]
        else:
            self.list_IDs = [p.replace('.npy','') for p in patients]
            

    def __len__(self):
        return len(self.list_IDs)

    def __getitem__(self, index):
        # Select sample
        ID = self.list_IDs[index]
        # Load data and get label
        X = pkl_load(os.path.join(self.fp, 'inputs', ID, '.npy'))
        y = pkl_load(os.path.join(self.fp, 'labels', ID, '.npy'))
        if not onehot: 
            y = y.argmax(1)
        return X, y
143/308:
import time
t0 = time.time()

dataset = PatientDataset()
loader = DataLoader(dataset, shuffle=True, batch_size=120, drop_last=True)
model = train_model(loader, 1)
143/309:
# Saved in the d2l package for later use
class Accumulator(object):
    """Sum a list of numbers over time."""

    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a+float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0] * len(self.data)

    def __getitem__(self, i):
        return self.data[i]

def try_gpu():
    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
    is_cuda = torch.cuda.is_available()
    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    return device

def pkl_dump(obj, f):
    with open(f, 'wb') as fo:
        pickle.dump(obj, fo)

def pkl_load(file):
    with open(file, 'rb') as file:
        return pickle.load(file)
    

import torch
from torch.utils import data

class PatientDataset(data.Dataset):  
    def __init__(self, list_IDs=None, onehot=False):
        self.fp = './pt_data/'
        patients = np.array(os.listdir(self.fp+'inputs/'))
        if isinstance(list_IDs, list):
            self.list_IDs = list_IDs
        elif isinstance(list_IDs, int):
            ix = np.random.randint(0, len(patients), list_IDs)
            self.list_IDs = [patients[i].replace('.npy','') for i in ix]
        else:
            self.list_IDs = [p.replace('.npy','') for p in patients]
            

    def __len__(self):
        return len(self.list_IDs)

    def __getitem__(self, index):
        # Select sample
        ID = self.list_IDs[index]
        # Load data and get label
        X = pkl_load(os.path.join(self.fp, 'inputs', ID, '.npy'))
        y = pkl_load(os.path.join(self.fp, 'labels', ID, '.npy'))
        if not onehot: 
            y = y.argmax(1)
        return X, y
143/310:
# Saved in the d2l package for later use
class Accumulator(object):
    """Sum a list of numbers over time."""

    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a+float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0] * len(self.data)

    def __getitem__(self, i):
        return self.data[i]

def try_gpu():
    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
    is_cuda = torch.cuda.is_available()
    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    return device

def pkl_dump(obj, f):
    with open(f, 'wb') as fo:
        pickle.dump(obj, fo)

def pkl_load(file):
    with open(file, 'rb') as file:
        return pickle.load(file)
    

import torch
from torch.utils import data

class PatientDataset(data.Dataset):  
    def __init__(self, list_IDs=None, onehot=False):
        self.fp = './pt_data/'
        patients = np.array(os.listdir(self.fp+'inputs/'))
        if isinstance(list_IDs, list):
            self.list_IDs = list_IDs
        elif isinstance(list_IDs, int):
            ix = np.random.randint(0, len(patients), list_IDs)
            self.list_IDs = [patients[i].replace('.npy','') for i in ix]
        else:
            self.list_IDs = [p.replace('.npy','') for p in patients]
            

    def __len__(self):
        return len(self.list_IDs)

    def __getitem__(self, index):
        # Select sample
        ID = self.list_IDs[index]
        # Load data and get label
        X = pkl_load(os.path.join(self.fp, 'inputs', ID, '.npy'))
        y = pkl_load(os.path.join(self.fp, 'labels', ID, '.npy'))
        if not onehot: 
            y = y.argmax(1)
        return X, y
143/311:
import time
t0 = time.time()

dataset = PatientDataset()
loader = DataLoader(dataset, shuffle=True, batch_size=120, drop_last=True)
model = train_model(loader, 1)
143/312:
# Saved in the d2l package for later use
class Accumulator(object):
    """Sum a list of numbers over time."""

    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a+float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0] * len(self.data)

    def __getitem__(self, i):
        return self.data[i]

def try_gpu():
    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
    is_cuda = torch.cuda.is_available()
    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    return device

def pkl_dump(obj, f):
    with open(f, 'wb') as fo:
        pickle.dump(obj, fo)

def pkl_load(file):
    with open(file, 'rb') as file:
        return pickle.load(file)
    

import torch
from torch.utils import data

class PatientDataset(data.Dataset):  
    def __init__(self, list_IDs=None, onehot=False):
        self.fp = './pt_data/'
        patients = np.array(os.listdir(self.fp+'inputs/'))
        if isinstance(list_IDs, list):
            self.list_IDs = list(map(lambda aid: f'{aid}.npy', list_IDs))
        elif isinstance(list_IDs, int):
            ix = np.random.randint(0, len(patients), list_IDs)
            self.list_IDs = patients[ix]
        else:
            self.list_IDs = patients
            

    def __len__(self):
        return len(self.list_IDs)

    def __getitem__(self, index):
        # Select sample
        filename = self.list_IDs[index]
        # Load data and get label
        X = pkl_load(os.path.join(self.fp, 'inputs', filename)
        y = pkl_load(os.path.join(self.fp, 'labels', filename)
        if not onehot: 
            y = y.argmax(1)
        return X, y
143/313:
# Saved in the d2l package for later use
class Accumulator(object):
    """Sum a list of numbers over time."""

    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a+float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0] * len(self.data)

    def __getitem__(self, i):
        return self.data[i]

def try_gpu():
    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
    is_cuda = torch.cuda.is_available()
    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    return device

def pkl_dump(obj, f):
    with open(f, 'wb') as fo:
        pickle.dump(obj, fo)

def pkl_load(file):
    with open(file, 'rb') as file:
        return pickle.load(file)
    

import torch
from torch.utils import data

class PatientDataset(data.Dataset):  
    def __init__(self, list_IDs=None, onehot=False):
        self.fp = './pt_data/'
        patients = np.array(os.listdir(self.fp+'inputs/'))
        if isinstance(list_IDs, list):
            self.list_IDs = list(map(lambda aid: f'{aid}.npy', list_IDs))
        elif isinstance(list_IDs, int):
            ix = np.random.randint(0, len(patients), list_IDs)
            self.list_IDs = patients[ix]
        else:
            self.list_IDs = patients
            

    def __len__(self):
        return len(self.list_IDs)

    def __getitem__(self, index):
        # Select sample
        filename = self.list_IDs[index]
        # Load data and get label
        X = pkl_load(os.path.join(self.fp, 'inputs', filename))
        y = pkl_load(os.path.join(self.fp, 'labels', filename))
        if not onehot: 
            y = y.argmax(1)
        return X, y
143/314:
import time
t0 = time.time()

dataset = PatientDataset()
loader = DataLoader(dataset, shuffle=True, batch_size=120, drop_last=True)
model = train_model(loader, 1)
143/315:
# Saved in the d2l package for later use
class Accumulator(object):
    """Sum a list of numbers over time."""

    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a+float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0] * len(self.data)

    def __getitem__(self, i):
        return self.data[i]

def try_gpu():
    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
    is_cuda = torch.cuda.is_available()
    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    return device

def pkl_dump(obj, f):
    with open(f, 'wb') as fo:
        pickle.dump(obj, fo)

def pkl_load(file):
    with open(file, 'rb') as file:
        return pickle.load(file)
    

import torch
from torch.utils import data

class PatientDataset(data.Dataset):  
    def __init__(self, list_IDs=None, onehot=False):
        self.onehot = onehot
        self.fp = './pt_data/'
        patients = np.array(os.listdir(self.fp+'inputs/'))
        
        if isinstance(list_IDs, list):
            self.list_IDs = list(map(lambda aid: f'{aid}.npy', list_IDs))
        elif isinstance(list_IDs, int):
            ix = np.random.randint(0, len(patients), list_IDs)
            self.list_IDs = patients[ix]
        else:
            self.list_IDs = patients

    def __len__(self):
        return len(self.list_IDs)

    def __getitem__(self, index):
        # Select sample
        filename = self.list_IDs[index]
        # Load data and get label
        X = pkl_load(os.path.join(self.fp, 'inputs', filename))
        y = pkl_load(os.path.join(self.fp, 'labels', filename))
        if not self.onehot: 
            y = y.argmax(1)
        return X, y
143/316:
def train_epoch(model, train_iter, criterion, optimizer, print_every=10):
    device = try_gpu()
    metrics = Accumulator(2) #nb_examples, loss,  
    loss_steps = []
        
    for batch, (X, y) in enumerate(train_iter): 
        state = model.init_hidden(train_iter.batch_size)
        
        y_hat, state = model(X.float(), state.to(device).float())
        
        optimizer.zero_grad()
        loss = criterion(y_hat, y)
        loss.backward()
        optimizer.step()
        
        metrics.add(y.size(0), loss.item())
        loss_steps.append(loss.item())
        
        if batch%print_every == 0:
            print(f"Minibatch:{batch}  Loss:{metrics[1]/metrics[0]} Examples seen: {metrics[0]}")
        
    return metrics[1]/metrics[0], loss_steps


def train_model(train_iter, epochs=10):
    device = try_gpu()
    model = GRUD().float().to(device)
    criterion = F.cross_entropy
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_steps = []
    
    for epoch in range(1, epochs+1):
        
        avg_loss, l = train_epoch(model, train_iter, criterion, optimizer, 10)
        loss_steps+=l
        print(f"\nEPOCH {epoch}, AvgLoss: {avg_loss}")
        
    plt.plot(loss_steps)
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    
    return model
143/317:
import time
t0 = time.time()

dataset = PatientDataset()
loader = DataLoader(dataset, shuffle=True, batch_size=120, drop_last=True)
model = train_model(loader, 1)
143/318: loader[0]
143/319: loader.dataset[0]
143/320:
x= pkl_load('pt_data/inputs/0.npy')
x.shape
143/321: len(dataset)
143/322:
fp = './pt_data/'
patients = np.array(os.listdir(self.fp+'inputs/'))[:10]

for p in patients:
    xarr = pkl_load(os.path.join(fp, 'inputs', p))
    yarr = pkl_load(os.path.join(fp, 'labels', p))
    
    autoid = p.replace('.npy','')
    patient_dir = os.path.join(fp, autoid)
    os.makedirs(patient_dir)
    input_dir = os.path.join(fp, autoid, 'inputs')
    os.makedirs(input_dir)
    
    for c,x in enumerate(xarr):
        pkl_dump(x, os.path.join(input_dir, f'{c}.npy'))
    
    pkl_dump(yarr, os.path.join(patient_dir, 'labels.npy'))
143/323:
fp = './pt_data/'
patients = np.array(os.listdir(fp+'inputs/'))[:10]

for p in patients:
    xarr = pkl_load(os.path.join(fp, 'inputs', p))
    yarr = pkl_load(os.path.join(fp, 'labels', p))
    
    autoid = p.replace('.npy','')
    patient_dir = os.path.join(fp, autoid)
    os.makedirs(patient_dir)
    input_dir = os.path.join(fp, autoid, 'inputs')
    os.makedirs(input_dir)
    
    for c,x in enumerate(xarr):
        pkl_dump(x, os.path.join(input_dir, f'{c}.npy'))
    
    pkl_dump(yarr, os.path.join(patient_dir, 'labels.npy'))
143/324:
x= pkl_load('pt_data/inputs/0.npy')
y= pkl_load('pt_data/labels/0.npy')
x.shape, y.shape
143/325:
x= pkl_load('pt_data/inputs/0.npy')
y= pkl_load('pt_data/labels/0.npy')
x.shape, y.shape
y[0]
143/326:
x= pkl_load('pt_data/inputs/0.npy')
y= pkl_load('pt_data/labels/0.npy')
x.shape, y.shape
y[58]
143/327:
x= pkl_load('pt_data/inputs/0.npy')
y= pkl_load('pt_data/labels/0.npy')
x.shape, y.shape
y[57]
143/328:
x= pkl_load('pt_data/inputs/0.npy')
y= pkl_load('pt_data/labels/0.npy')
x.shape, y.shape
y[57].argmax()
143/329:
x= pkl_load('pt_data/inputs/0.npy')
y= pkl_load('pt_data/labels/0.npy')
x.shape, y.shape
y[57].argmax(0)
143/330:
x= pkl_load('pt_data/inputs/0.npy')
y= pkl_load('pt_data/labels/0.npy')
x.shape, y.shape
y[57].argmax(1)
143/331:
x= pkl_load('pt_data/inputs/0.npy')
y= pkl_load('pt_data/labels/0.npy')
x.shape, y.shape
y[57].argmax()
143/332:
fp = './pt_data/'
patients = np.array(os.listdir(fp+'inputs/'))[:5]
seq_dir = os.makedirs(os.path.join(fp, 'sequences'))
labels = dict()

for p in patients:
    xarr = pkl_load(os.path.join(fp, 'inputs', p))
    yarr = pkl_load(os.path.join(fp, 'labels', p))
    autoid = p.replace('.npy','')
    for c,x in enumerate(xarr):
        ID = f'{autoid}_{c}'
        pkl_dump(x, os.path.join(seq_dir, ID+'.npy'))
        labels[ID] = yarr[c].argmax()
        
pkl_dump(yarr, os.path.join(fp, 'label_dict.pkl'))
143/333:
fp = './pt_data/'
patients = np.array(os.listdir(fp+'inputs/'))[:5]
seq_dir = os.path.join(fp, 'sequences')
os.makedirs(seq_dir)
labels = dict()

for p in patients:
    xarr = pkl_load(os.path.join(fp, 'inputs', p))
    yarr = pkl_load(os.path.join(fp, 'labels', p))
    autoid = p.replace('.npy','')
    for c,x in enumerate(xarr):
        ID = f'{autoid}_{c}'
        pkl_dump(x, os.path.join(seq_dir, ID+'.npy'))
        labels[ID] = yarr[c].argmax()
        
pkl_dump(yarr, os.path.join(fp, 'label_dict.pkl'))
143/334:
fp = './pt_data/'
patients = np.array(os.listdir(fp+'inputs/'))[:5]
seq_dir = os.path.join(fp, 'sequences')
os.makedirs(seq_dir)
labels = dict()

for p in patients:
    xarr = pkl_load(os.path.join(fp, 'inputs', p))
    yarr = pkl_load(os.path.join(fp, 'labels', p))
    autoid = p.replace('.npy','')
    for c,x in enumerate(xarr):
        ID = f'{autoid}_{c}'
        pkl_dump(x, os.path.join(seq_dir, ID+'.npy'))
        labels[ID] = yarr[c].argmax()
        
pkl_dump(yarr, os.path.join(fp, 'label_dict.pkl'))
143/335:
x= pkl_load('pt_data/label_dict.pkl')
len(x)
143/336:
x= pkl_load('pt_data/label_dict.pkl')
print(x.items())
143/337:
fp = './pt_data/'
patients = np.array(os.listdir(fp+'inputs/'))[:5]
seq_dir = os.path.join(fp, 'sequences')
os.makedirs(seq_dir)
labels = dict()

for p in patients:
    xarr = pkl_load(os.path.join(fp, 'inputs', p))
    yarr = pkl_load(os.path.join(fp, 'labels', p))
    autoid = p.replace('.npy','')
    for c,x in enumerate(xarr):
        ID = f'{autoid}_{c}'
        pkl_dump(x, os.path.join(seq_dir, ID+'.npy'))
        labels[ID] = yarr[c].argmax()
        
pkl_dump(labels, os.path.join(fp, 'label_dict.pkl'))
143/338:
x= pkl_load('pt_data/label_dict.pkl')
print(x.items())
143/339:
x= pkl_load('pt_data/label_dict.pkl')
len(x)
143/340:
x= pkl_load('pt_data/label_dict.pkl')
x.items()[0]
143/341:
x= pkl_load('pt_data/label_dict.pkl')
x.items()
143/342:
fp = './pt_data/'
patients = np.array(os.listdir(fp+'inputs/'))
seq_dir = os.path.join(fp, 'sequences')
os.makedirs(seq_dir)
labels = dict()

for p in patients:
    xarr = pkl_load(os.path.join(fp, 'inputs', p))
    yarr = pkl_load(os.path.join(fp, 'labels', p))
    autoid = p.replace('.npy','')
    for c,x in enumerate(xarr):
        ID = f'{autoid}_{c}'
        pkl_dump(x, os.path.join(seq_dir, ID+'.npy'))
        labels[ID] = yarr[c].argmax()
        
pkl_dump(labels, os.path.join(fp, 'label_dict.pkl'))
143/343:
# Saved in the d2l package for later use
class Accumulator(object):
    """Sum a list of numbers over time."""

    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a+float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0] * len(self.data)

    def __getitem__(self, i):
        return self.data[i]

def try_gpu():
    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
    is_cuda = torch.cuda.is_available()
    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    return device

def pkl_dump(obj, f):
    with open(f, 'wb') as fo:
        pickle.dump(obj, fo)

def pkl_load(file):
    with open(file, 'rb') as file:
        return pickle.load(file)
    

import torch
from torch.utils import data

class PatientDataset(data.Dataset):  
    def __init__(self, list_IDs=None, labels): # accept auto_ids or number of sequences
        self.fp = './pt_data/sequences'
        self.list_IDs = list_IDs
        
       
    def __len__(self):
        return len(self.list_IDs)

    def __getitem__(self, index):
        # Select sample
        filename = self.list_IDs[index]
        # Load data and get label
        X = pkl_load(os.path.join(self.fp, 'inputs', filename))
        y = pkl_load(os.path.join(self.fp, 'labels', filename))
        if not self.onehot: 
            y = y.argmax(1)
        return X, y
143/344:
x= pkl_load('pt_data/label_dict.pkl')
x.items()
143/345:
# Saved in the d2l package for later use
class Accumulator(object):
    """Sum a list of numbers over time."""

    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a+float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0] * len(self.data)

    def __getitem__(self, i):
        return self.data[i]

def try_gpu():
    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
    is_cuda = torch.cuda.is_available()
    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    return device

def pkl_dump(obj, f):
    with open(f, 'wb') as fo:
        pickle.dump(obj, fo)

def pkl_load(file):
    with open(file, 'rb') as file:
        return pickle.load(file)
    

import torch
from torch.utils import data

class PatientDataset(data.Dataset):  
    def __init__(self, list_IDs=None): # accept auto_ids or number of sequences
        self.fp = './pt_data/sequences'
        self.list_IDs = list_IDs
        self.labels = pkl_load('./pt_data/label_dict.pkl')
       
    def __len__(self):
        return len(self.list_IDs)

    def __getitem__(self, index):
        # Select sample
        filename = self.list_IDs[index]
        # Load data and get label
        X = pkl_load(os.path.join(self.fp, filename))
        y = self.labels[filename.replace('.npy','')]
        return X, y
143/346:
import time
t0 = time.time()

dataset = PatientDataset()
loader = DataLoader(dataset, shuffle=True, batch_size=120, drop_last=True)
# model = train_model(loader, 1)
143/347:
# Saved in the d2l package for later use
class Accumulator(object):
    """Sum a list of numbers over time."""

    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a+float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0] * len(self.data)

    def __getitem__(self, i):
        return self.data[i]

def try_gpu():
    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
    is_cuda = torch.cuda.is_available()
    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    return device

def pkl_dump(obj, f):
    with open(f, 'wb') as fo:
        pickle.dump(obj, fo)

def pkl_load(file):
    with open(file, 'rb') as file:
        return pickle.load(file)
    

import torch
from torch.utils import data

class PatientDataset(data.Dataset):  
    def __init__(self, list_IDs): # accept auto_ids or number of sequences
        self.fp = './pt_data/sequences'
        if isinstance(list_IDs, int):
            list_IDs = random.sample(os.listdir(self.fp), list_IDs)
        self.list_IDs = list_IDs
        self.labels = pkl_load('./pt_data/label_dict.pkl')
       
    def __len__(self):
        return len(self.list_IDs)

    def __getitem__(self, index):
        # Select sample
        filename = self.list_IDs[index]
        # Load data and get label
        X = pkl_load(os.path.join(self.fp, filename))
        y = self.labels[filename.replace('.npy','')]
        return X, y
143/348:
import time
t0 = time.time()

dataset = PatientDataset(10)
loader = DataLoader(dataset, shuffle=True, batch_size=120, drop_last=True)
# model = train_model(loader, 1)
143/349:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.autograd import Variable
import numpy as np
import data_utils as du
import math
import config as cfg
import pickle
import os
import random
143/350:
# Saved in the d2l package for later use
class Accumulator(object):
    """Sum a list of numbers over time."""

    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a+float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0] * len(self.data)

    def __getitem__(self, i):
        return self.data[i]

def try_gpu():
    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
    is_cuda = torch.cuda.is_available()
    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    return device

def pkl_dump(obj, f):
    with open(f, 'wb') as fo:
        pickle.dump(obj, fo)

def pkl_load(file):
    with open(file, 'rb') as file:
        return pickle.load(file)
    

class PatientDataset(data.Dataset):  
    def __init__(self, list_IDs): # accept auto_ids or number of sequences
        self.fp = './pt_data/sequences'
        if isinstance(list_IDs, int):
            list_IDs = random.sample(os.listdir(self.fp), list_IDs)
        self.list_IDs = list_IDs
        self.labels = pkl_load('./pt_data/label_dict.pkl')
       
    def __len__(self):
        return len(self.list_IDs)

    def __getitem__(self, index):
        # Select sample
        filename = self.list_IDs[index]
        # Load data and get label
        X = pkl_load(os.path.join(self.fp, filename))
        y = self.labels[filename.replace('.npy','')]
        return X, y
143/351:
import time
t0 = time.time()

dataset = PatientDataset(10)
loader = DataLoader(dataset, shuffle=True, batch_size=120, drop_last=True)
# model = train_model(loader, 1)
143/352: len(dataset)
143/353: dataset[0]
143/354: dataset[1][1]
143/355: dataset[2][1]
143/356: dataset[:, 1]
143/357: dataset[3,1]
143/358: dataset[3][1]
143/359:
import time
t0 = time.time()

%timeit dataset = PatientDataset(120000)
loader = DataLoader(dataset, shuffle=True, batch_size=120, drop_last=True)
# model = train_model(loader, 1)
143/360:
for x, y in loader:
    print(x.shape, y)
    breal
143/361:
for x, y in loader:
    print(x.shape, y)
    break
143/362:
for x, y in loader:
    print(x.shape, y)
    break
143/363:
import time
t0 = time.time()

# %timeit dataset = PatientDataset(120000)
loader = DataLoader(dataset, shuffle=True, batch_size=120, drop_last=True)
# model = train_model(loader, 1)
143/364:
for x, y in loader:
    print(x.shape, y)
    break
143/365:
for x, y in loader:
    print(x.shape, y)
143/366: loader.dataset
143/367: dataset[1340]
143/368: len(dataset)
143/369:
import time
t0 = time.time()

dataset = PatientDataset(120000)
loader = DataLoader(dataset, shuffle=True, batch_size=120, drop_last=True)
# model = train_model(loader, 1)
143/370: len(dataset)
143/371:
for x,y in loader:
    print (x.shape, y)
143/372:
# Saved in the d2l package for later use
class Accumulator(object):
    """Sum a list of numbers over time."""

    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a+float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0] * len(self.data)

    def __getitem__(self, i):
        return self.data[i]

def try_gpu():
    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False
    is_cuda = torch.cuda.is_available()
    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    return device

def pkl_dump(obj, f):
    with open(f, 'wb') as fo:
        pickle.dump(obj, fo)

def pkl_load(file):
    with open(file, 'rb') as file:
        return pickle.load(file)
    

class PatientDataset(data.Dataset):  
    def __init__(self, list_IDs): # accept auto_ids or number of sequences
        self.fp = './pt_data/sequences'
        if isinstance(list_IDs, int):
            if list_IDs>0: 
                list_IDs = random.sample(os.listdir(self.fp), list_IDs)
            else:
                list_IDs = os.listdir(self.fp)
        self.list_IDs = list_IDs
        self.labels = pkl_load('./pt_data/label_dict.pkl')
       
    def __len__(self):
        return len(self.list_IDs)

    def __getitem__(self, index):
        # Select sample
        filename = self.list_IDs[index]
        # Load data and get label
        X = pkl_load(os.path.join(self.fp, filename))
        y = self.labels[filename.replace('.npy','')]
        return X, y
143/373:
import time
t0 = time.time()

dataset = PatientDataset(0)
loader = DataLoader(dataset, shuffle=True, batch_size=120, drop_last=True)
143/374: len
143/375: len(dataset)
143/376:
import time
t0 = time.time()

dataset = PatientDataset(0)
loader = DataLoader(dataset, shuffle=True, batch_size=120, drop_last=True)
model = train_model(loader, 1)
143/377: torch.zeros((3,2))
143/378: torch.zeros((3,2,3))
143/379:
dataset = du.PatientDataset(1)
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X).float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
149/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.autograd import Variable
import numpy as np
import data_utils as du
import math
import config as cfg
import pickle
import os
import random
import gru
149/2:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.autograd import Variable
import numpy as np
import data_utils as du
import math
import config as cfg
import pickle
import os
import random
import gru
149/3:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.autograd import Variable
import numpy as np
import data_utils as du
import math
import config as cfg
import pickle
import os
import random
import gru
149/4:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.autograd import Variable
import numpy as np
import data_utils as du
import math
import config as cfg
import pickle
import os
import random
import gru
149/5:
from torch.utils.data import DataLoader
import data_utils as du
import gru
149/6:
dataset = du.PatientDataset(1)
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X).float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
150/1:
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
150/2:
dataset = du.PatientDataset(1)
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X).float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
150/3: import utils
150/4:
dataset = du.PatientDataset(1)
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X).float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
151/1:
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
151/2:
dataset = du.PatientDataset(1)
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X).float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
152/1:
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
152/2:
dataset = du.PatientDataset(1)
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X).float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
152/3:
dataset = du.PatientDataset(1)
X, y = dataset[0]
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X).float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
152/4:
import torch 
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
152/5:
dataset = du.PatientDataset(1)
X, y = dataset[0]
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X).float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
153/1:
import torch 
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
153/2:
dataset = du.PatientDataset(1)
X, y = dataset[0]
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X).float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
153/3:
dataset = du.PatientDataset(1)
X, y = dataset[0]
X = torch.unsqueeze(X, 0)
y = torch.unsqueeze(y, 0)

model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(torch.tensor(X).float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
153/4:
dataset = du.PatientDataset(1)
X, y = dataset[0]
X = torch.unsqueeze(torch.tensor(X), 0)

model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(X.float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
153/5:
dataset = du.PatientDataset(1)
X, y = dataset[0]
X = torch.unsqueeze(torch.tensor(X), 0)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}")
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(X.float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
154/1:
import torch 
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
154/2:
dataset = du.PatientDataset(1)
X, y = dataset[:2]
X = torch.unsqueeze(torch.tensor(X), 0)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}")
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(X.float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
154/3:
dataset = du.PatientDataset(1)
X, y = dataset[0]
X = torch.unsqueeze(torch.tensor(X), 0)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}")
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(X.float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
155/1:
import torch 
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
155/2:
dataset = du.PatientDataset(1)
X, y = dataset[0]
X = torch.unsqueeze(torch.tensor(X), 0)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}")
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(X.float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
155/3:
import time
t0 = time.time()

# dataset = du.PatientDataset(-1)
# loader = DataLoader(dataset, shuffle=True, batch_size=120, drop_last=True)
# model = train_model(loader, 1)
156/1:
import torch 
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
156/2:
dataset = du.PatientDataset(1)
X, y = dataset[0]
X = torch.unsqueeze(torch.tensor(X), 0)
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(X.float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
156/3:
import time
t0 = time.time()

dataset = du.PatientDataset(30000)
loader = DataLoader(dataset, shuffle=True, batch_size=64, drop_last=True)
model = train_model(loader, 1)
157/1:
import torch 
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
157/2:
dataset = du.PatientDataset(1)
X, y = dataset[0]
X = torch.unsqueeze(torch.tensor(X), 0)
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(X.float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y}")
157/3:
import time
t0 = time.time()
dataset = du.PatientDataset(30000)
loader = DataLoader(dataset, shuffle=True, batch_size=64, drop_last=True)
model = gru.train_model(loader, 1)
print(f'Time taken: {time.time()-t0}')
158/1:
data = pd.DataFrame(index=range(20))
data['ID'] = [100]*10 + [200]*10
data['TS'] = [sorted(np.random.randint(0,15, size=10))]*2
data = pd.DataFrame(np.random.randint(0, 100, size=(20, 3)), columns=['x1','x2','y'])

data
158/2:
import gru
import data_utils
import utils
import pandas as pd
158/3:
import gru
import data_utils
import utils
import pandas as pd
158/4:
data = pd.DataFrame(index=range(20))
data['ID'] = [100]*10 + [200]*10
data['TS'] = [sorted(np.random.randint(0,15, size=10))]*2
data = pd.DataFrame(np.random.randint(0, 100, size=(20, 3)), columns=['x1','x2','y'])

data
158/5: [sorted(np.random.randint(0,15, size=10))]*2
158/6: sum([sorted(np.random.randint(0,15, size=10))]*2)
158/7: [sorted(np.random.randint(0,15, size=10))]*2
158/8:
l=[sorted(np.random.randint(0,15, size=10))]*2
sum(l)
158/9:
l=[sorted(np.random.randint(0,15, size=10))]*2
l.sum()
158/10:
l=[sorted(np.random.randint(0,15, size=10))]*2
sum(*l)
158/11:
# l=[sorted(np.random.randint(0,15, size=10))]*2
l, sum(*l)
158/12: np.sample(0,15,10)
158/13: random.sample(0,15,10)
158/14:
import random
random.sample(0,15,10)
158/15:
import random
random.sample(range(15),10)
158/16: sorted(random.sample(range(15),10)) + sorted(random.sample(range(15),10))
158/17:
data = pd.DataFrame(index=range(20))
data['ID'] = [100]*10 + [200]*10
data['TS'] = sorted(random.sample(range(15),10)) + sorted(random.sample(range(15),10))
data = pd.DataFrame(np.random.randint(0, 100, size=(20, 3)), columns=['x1','x2','y'])

data
158/18:
data = pd.DataFrame(index=range(20))
data['ID'] = [100]*10 + [200]*10
data['TS'] = sorted(random.sample(range(15),10)) + sorted(random.sample(range(15),10))
data = data + pd.DataFrame(np.random.randint(0, 100, size=(20, 3)), columns=['x1','x2','y'])

data
158/19:
data = pd.DataFrame(index=range(20))
data['ID'] = [100]*10 + [200]*10
data['TS'] = sorted(random.sample(range(15),10)) + sorted(random.sample(range(15),10))
data = pd.concat(data, pd.DataFrame(np.random.randint(0, 100, size=(20, 3)), columns=['x1','x2','y']), axis=1)

data
158/20:
data = pd.DataFrame(index=range(20))
data['ID'] = [100]*10 + [200]*10
data['TS'] = sorted(random.sample(range(15),10)) + sorted(random.sample(range(15),10))
data = pd.concat([data, pd.DataFrame(np.random.randint(0, 100, size=(20, 3)), columns=['x1','x2','y'])], axis=1)

data
158/21:
data = pd.DataFrame(index=range(10))
data['ID'] = [100]*5 + [200]*5
data['TS'] = sorted(random.sample(range(10),5)) + sorted(random.sample(range(10),5))
data = pd.concat([data, pd.DataFrame(np.random.randint(0, 100, size=(10, 3)), columns=['x1','x2','y'])], axis=1)

data
159/1:
import gru
import data_utils
import utils
import pandas as pd
import random
159/2:
data = pd.DataFrame(index=range(10))
data['ID'] = [100]*5 + [200]*5
data['TS'] = sorted(random.sample(range(10),5)) + sorted(random.sample(range(10),5))
data = pd.concat([data, pd.DataFrame(np.random.randint(0, 100, size=(10, 3)), columns=['x1','x2','y'])], axis=1)

data
159/3:
import random
random.randint((0, 100, size=(10, 3))
159/4:
import random
random.randint(0, 100, size=(10, 3))
159/5:
import random
random.randint(0, 100)
159/6:
import random
random.randint(0, 100, shape=(10, 3))
159/7:
import random
random.randint()
159/8:
data = pd.DataFrame(index=range(10))
data['ID'] = [100]*5 + [200]*5
data['TS'] = sorted(random.sample(range(10),5)) + sorted(random.sample(range(10),5))
data = pd.concat([data, pd.DataFrame(pd.np.random.randint(0, 100, size=(10, 3)), columns=['x1','x2','y'])], axis=1)

data
159/9:
padded_data = data_utils.pad_missing_months(data)

padded_data
159/10:
m, t = data_utils.missingness_indicators(padded_data)

m, t
159/11:
m, t = data_utils.missingness_indicators(padded_data)

print(m)
print(t)
159/12:
m, t = data_utils.missingness_indicators(padded_data)

print(pd.concat([m,t], axis=1))
159/13:
m, t = data_utils.missingness_indicators(padded_data)

pd.concat([m,t], axis=1)
159/14:
imputer = data_utils.Imputer()
imputed_data = imputer.grp_forward(padded_data, ['ID'], [-1,-1,-1])

imputed_data
160/1:
import gru
import data_utils
import utils
import pandas as pd
import random
160/2:
data = pd.DataFrame(index=range(10))
data['ID'] = [100]*5 + [200]*5
data['TS'] = sorted(random.sample(range(10),5)) + sorted(random.sample(range(10),5))
data = pd.concat([data, pd.DataFrame(pd.np.random.randint(0, 100, size=(10, 3)), columns=['x1','x2','y'])], axis=1)

data
160/3:
padded_data = data_utils.pad_missing_months(data)

padded_data
160/4:
m, t = data_utils.missingness_indicators(padded_data, prefixes=['m','t'])

pd.concat([m,t], axis=1)
160/5:
padded_data = data_utils.pad_missing_months(data)

padded_data.reset_index()
161/1:
import gru
import data_utils
import utils
import pandas as pd
import random
161/2:
data = pd.DataFrame(index=range(10))
data['ID'] = [100]*5 + [200]*5
data['TS'] = sorted(random.sample(range(10),5)) + sorted(random.sample(range(10),5))
data = pd.concat([data, pd.DataFrame(pd.np.random.randint(0, 100, size=(10, 3)), columns=['x1','x2','y'])], axis=1)

data
161/3:
padded_data = data_utils.pad_missing_months(data)

padded_data.reset_index()
162/1:
import gru
import data_utils
import utils
import pandas as pd
import random
162/2:
data = pd.DataFrame(index=range(10))
data['ID'] = [100]*5 + [200]*5
data['TS'] = sorted(random.sample(range(10),5)) + sorted(random.sample(range(10),5))
data = pd.concat([data, pd.DataFrame(pd.np.random.randint(0, 100, size=(10, 3)), columns=['x1','x2','y'])], axis=1)

data
162/3:
padded_data = data_utils.pad_missing_months(data)

padded_data.reset_index()
162/4:
m, t = data_utils.missingness_indicators(padded_data, prefixes=['m','t'])

pd.concat([m,t], axis=1)
162/5:
imputer = data_utils.Imputer()
imputed_data = imputer.grp_forward(padded_data, ['ID'], [-1,-1,-1])

imputed_data
162/6:
padded_data = data_utils.pad_missing_months(data)

padded_data
163/1:
import gru
import data_utils
import utils
import pandas as pd
import random
163/2:
data = pd.DataFrame(index=range(10))
data['ID'] = [100]*5 + [200]*5
data['TS'] = sorted(random.sample(range(10),5)) + sorted(random.sample(range(10),5))
data = pd.concat([data, pd.DataFrame(pd.np.random.randint(0, 100, size=(10, 3)), columns=['x1','x2','y'])], axis=1)

data
163/3:
padded_data = data_utils.pad_missing_months(data)

padded_data
163/4:
m, t = data_utils.missingness_indicators(padded_data, prefixes=['m','t'])

pd.concat([m,t], axis=1)
163/5:
imputer = data_utils.Imputer()
imputed_data = imputer.grp_forward(padded_data, ['ID'], [-1,-1,-1])

imputed_data
163/6:
# padded_data = data_utils.pad_missing_months(data)

padded_data.reset_index()
164/1:
import gru
import data_utils
import utils
import pandas as pd
import random
164/2:
data = pd.DataFrame(index=range(10))
data['ID'] = [100]*5 + [200]*5
data['TS'] = sorted(random.sample(range(10),5)) + sorted(random.sample(range(10),5))
data = pd.concat([data, pd.DataFrame(pd.np.random.randint(0, 100, size=(10, 3)), columns=['x1','x2','y'])], axis=1)

data
164/3:
# padded_data = data_utils.pad_missing_months(data)

padded_data.reset_index()
164/4:
padded_data = data_utils.pad_missing_months(data)

padded_data.reset_index()
164/5:
data = pd.DataFrame(index=range(10))
data['ID'] = [100]*5 + [200]*5
data['TS'] = sorted(random.sample(range(10),5)) + sorted(random.sample(range(10),5))
data = pd.concat([data, pd.DataFrame(pd.np.random.randint(0, 100, size=(10, 3)), columns=['x1','x2','y'])], axis=1)

data.columns
164/6:
data = pd.DataFrame(index=range(10))
data['ID'] = [100]*5 + [200]*5
data['TS'] = sorted(random.sample(range(10),5)) + sorted(random.sample(range(10),5))
data = pd.concat([data, pd.DataFrame(pd.np.random.randint(0, 100, size=(10, 3)), columns=['x1','x2','y'])], axis=1)

data
164/7:
padded_data = data_utils.pad_missing_months(data)

padded_data#.reset_index()
165/1:
import gru
import data_utils
import utils
import pandas as pd
import random
165/2:
data = pd.DataFrame(index=range(10))
data['ID'] = [100]*5 + [200]*5
data['TS'] = sorted(random.sample(range(10),5)) + sorted(random.sample(range(10),5))
data = pd.concat([data, pd.DataFrame(pd.np.random.randint(0, 100, size=(10, 3)), columns=['x1','x2','y'])], axis=1)

data
165/3:
padded_data = data_utils.pad_missing_months(data)

padded_data#.reset_index()
165/4:
m, t = data_utils.missingness_indicators(padded_data, prefixes=['m','t'])

pd.concat([m,t], axis=1)
165/5:
imputer = data_utils.Imputer()
imputed_data = imputer.grp_forward(padded_data, ['ID'], [-1,-1,-1])

imputed_data
165/6:
# padded_data = data_utils.pad_missing_months(data)

padded_data.reset_index()
166/1:
import gru
import data_utils
import utils
import pandas as pd
import random
166/2:
data = pd.DataFrame(index=range(10))
data['ID'] = [100]*5 + [200]*5
data['TS'] = sorted(random.sample(range(10),5)) + sorted(random.sample(range(10),5))
data = pd.concat([data, pd.DataFrame(pd.np.random.randint(0, 100, size=(10, 3)), columns=['x1','x2','y'])], axis=1)

data
166/3:
# padded_data = data_utils.pad_missing_months(data)

padded_data.reset_index()
166/4:
padded_data = data_utils.pad_missing_months(data)

padded_data.reset_index()
167/1:
import gru
import data_utils
import utils
import pandas as pd
import random
167/2:
data = pd.DataFrame(index=range(10))
data['ID'] = [100]*5 + [200]*5
data['TS'] = sorted(random.sample(range(10),5)) + sorted(random.sample(range(10),5))
data = pd.concat([data, pd.DataFrame(pd.np.random.randint(0, 100, size=(10, 3)), columns=['x1','x2','y'])], axis=1)

data
167/3:
padded_data = data_utils.pad_missing_months(data)

padded_data.reset_index()
168/1:
import gru
import data_utils
import utils
import pandas as pd
import random
168/2:
data = pd.DataFrame(index=range(10))
data['ID'] = [100]*5 + [200]*5
data['TS'] = sorted(random.sample(range(10),5)) + sorted(random.sample(range(10),5))
data = pd.concat([data, pd.DataFrame(pd.np.random.randint(0, 100, size=(10, 3)), columns=['x1','x2','y'])], axis=1)

data
168/3:
padded_data = data_utils.pad_missing_months(data)

padded_data
168/4:
m, t = data_utils.missingness_indicators(padded_data, prefixes=['m','t'])

pd.concat([m,t], axis=1)
168/5:
imputer = data_utils.Imputer()
imputed_data = imputer.grp_forward(padded_data, ['ID'], [-1,-1,-1])

imputed_data
168/6:
imputer = data_utils.Imputer()
default_values = pd.Series([-1,-1,-1], index=['x1','x2','y'])
imputed_data = imputer.grp_forward(padded_data, groupby_cols=['ID'], default_values)

imputed_data
168/7:
imputer = data_utils.Imputer()
default_values = pd.Series([-1,-1,-1], index=['x1','x2','y'])
imputed_data = imputer.grp_forward(padded_data, groupby_cols=['ID'], default_values=default_values)

imputed_data
168/8:
imputer = data_utils.Imputer()

imputed_data = padded_data.groupby('ID').apply(imputer.forward)
imputed_data
169/1:
import gru
import data_utils
import utils
import pandas as pd
import random
169/2:
data = pd.DataFrame(index=range(10))
data['ID'] = [100]*5 + [200]*5
data['TS'] = sorted(random.sample(range(10),5)) + sorted(random.sample(range(10),5))
data = pd.concat([data, pd.DataFrame(pd.np.random.randint(0, 100, size=(10, 3)), columns=['x1','x2','y'])], axis=1)

data
169/3:
padded_data = data_utils.pad_missing_months(data)

padded_data
169/4:
m, t = data_utils.missingness_indicators(padded_data, prefixes=['m','t'])

pd.concat([m,t], axis=1)
169/5:
imputer = data_utils.Imputer()
imputed_data = padded_data.groupby('ID').apply(imputer.forward)
imputed_data
170/1:
import gru
import data_utils
import utils
import pandas as pd
import random
170/2:
data = pd.DataFrame(index=range(10))
data['ID'] = [100]*5 + [200]*5
data['TS'] = sorted(random.sample(range(10),5)) + sorted(random.sample(range(10),5))
data = pd.concat([data, pd.DataFrame(pd.np.random.randint(0, 100, size=(10, 3)), columns=['x1','x2','y'])], axis=1)

data
170/3:
padded_data = data_utils.pad_missing_months(data)

padded_data
170/4:
m, t = data_utils.missingness_indicators(padded_data, prefixes=['m','t'])

pd.concat([m,t], axis=1)
170/5:
imputer = data_utils.Imputer()
imputed_data = padded_data.groupby('ID').apply(imputer.forward)
imputed_data
170/6:
input = pd.concat([imputed_data, m, t, x_obs])
input
170/7:
x_obs = data_utils.x_last_observed(imputed_data)

x_obs
170/8: list(range(0,10,4))
170/9:
X=[]
y=[]

input = pd.concat([imputed_data, m, t, x_obs])
step = 4

for i in range(0, 10, step):
    start, end = i, i+step
    X.append(input.iloc[start:end, :-1].values)
    y.append(input.iloc[end, -1].values)

X, y
172/1:
import gru
import data_utils
import utils
import pandas as pd
import random
172/2:
data = pd.DataFrame(index=range(10))
data['ID'] = [100]*5 + [200]*5
data['TS'] = sorted(random.sample(range(10),5)) + sorted(random.sample(range(10),5))
data = pd.concat([data, pd.DataFrame(pd.np.random.randint(0, 100, size=(10, 3)), columns=['x1','x2','y'])], axis=1)

data
172/3: padded_data = data_utils.pad_missing_months(data)
172/4:
m, t = data_utils.missingness_indicators(padded_data, prefixes=['m','t'])
pd.concat([m,t], axis=1)
172/5:
imputed_data = padded_data.groupby('ID').apply(data_utils.Imputer().forward)
imputed_data
172/6:
x_obs = data_utils.x_last_observed(imputed_data)
x_obs
172/7:

input
172/8:
X=[]
y=[]

input = pd.concat([imputed_data, m, t, x_obs])
step = 4

for i in range(0, 10, step):
    start, end = i, i+step
    X.append(input.iloc[start:end, :-1].values)
    y.append(input.iloc[end, -1].values)

X, y
172/9:
X=[]
y=[]

input = pd.concat([imputed_data, m, t, x_obs])
step = 4

for i in range(0, 10, step):
    start, end = i, i+step
    X.append(input.iloc[start:end, :-1])
    y.append(input.iloc[end, -1])

X, y
172/10:
X=[]
y=[]

input = pd.concat([imputed_data, m, t, x_obs])
step = 4

for i in range(0, 10, step):
    start, end = i, i+step
    X.append(input.iloc[start:end, :-1].values)
    y.append(input.iloc[end, -1])

X, y
172/11:
X=[]
y=[]

input = pd.concat([imputed_data, m, t, x_obs])
step = 4

for i in range(0, 10, step):
    start, end = i, i+step
    X.append(input.iloc[start:end, :-1].values)
    y.append(input.iloc[end, -1].values)

X, y
172/12:
X=[]
y=[]

input = pd.concat([imputed_data, m, t, x_obs])
step = 4

for i in range(0, 10, step):
    start, end = i, i+step
    X.append(input.iloc[start:end, :-1].values)
    y.append(input.iloc[end, -1])

X, y
172/13:
X=[]
y=[]

input = pd.concat([imputed_data, m, t, x_obs])
input.head()
step = 4

for i in range(0, 10, step):
    start, end = i, i+step
    X.append(input.iloc[start:end, :-1].values)
    y.append(input.iloc[end, -1])

X, y
172/14:
X=[]
y=[]

input = pd.concat([imputed_data, m, t, x_obs])
input.head()
step = 4

for i in range(0, 10, step):
    start, end = i, i+step
    X.append(input.iloc[start:end, :-1].values)
    y.append(input.iloc[end, -1])

# X, y
172/15:
X=[]
y=[]

input = pd.concat([imputed_data, m, t, x_obs])

step = 4

for i in range(0, 10, step):
    start, end = i, i+step
    X.append(input.iloc[start:end, :-1].values)
    y.append(input.iloc[end, -1])

input.head()
# X, y
172/16: 'abc'.index(a)
172/17: 'abc'.index('a')
172/18: 'abc'.index('D')
172/19: bool('abc'.index('D'))
172/20:
X=[]
y=[]

input = pd.concat([imputed_data, m, t, x_obs], axis=1)
tgt = input['y'].copy()
input = input[[x for x in input.columns if x[-2:]=='_y']]
del input['y']

step = 4

for i in range(0, 10, step):
    start, end = i, i+step
    X.append(input.iloc[start:end, ].values)
    y.append(tgt.iloc[end])


print(X[:2], y[:2])
172/21: input = pd.concat([imputed_data, m, t, x_obs], axis=1)
172/22:
input = pd.concat([imputed_data, m, t, x_obs], axis=1)
input
172/23:
input = pd.concat([imputed_data, m, t, x_obs], axis=1)
[x for x in input.columns if x[-2:]=='_y']
172/24:
X=[]
y=[]

input = pd.concat([imputed_data, m, t, x_obs], axis=1)
tgt = input['y'].copy()
input = input[[x for x in input.columns if x[-2:]!='_y']]
del input['y']

step = 4

for i in range(0, 10, step):
    start, end = i, i+step
    X.append(input.iloc[start:end, ].values)
    y.append(tgt.iloc[end])


print(X[:2], y[:2])
172/25:
X=[]
y=[]

input = pd.concat([imputed_data, m, t, x_obs], axis=1)
tgt = input['y'].copy()
input = input[[x for x in input.columns if x[-2:]!='_y']]
del input['y']

step = 4

for i in range(0, 10, step):
    start, end = i, i+step
    X.append(input.iloc[start:end, ].values)
    y.append(tgt.iloc[end])


print(zip(X[:2], y[:2]))
172/26:
X=[]
y=[]

input = pd.concat([imputed_data, m, t, x_obs], axis=1)
tgt = input['y'].copy()
input = input[[x for x in input.columns if x[-2:]!='_y']]
del input['y']

step = 4

for i in range(0, 10, step):
    start, end = i, i+step
    X.append(input.iloc[start:end, ].values)
    y.append(tgt.iloc[end])


print(list(zip(X[:2], y[:2])))
172/27:
input = pd.concat([imputed_data, m, t, x_obs], axis=1)
input
172/28:
input = pd.concat([imputed_data, m, t, x_obs], axis=1)
input = input[[x for x in input.columns if x[-2:]!='_y']]
input
172/29:
input = pd.concat([imputed_data, m, t, x_obs], axis=1)
input = input[[x for x in input.columns if x[-2:]!='_y']]
tgt = input['y']
del input['y']

input['y'] = tgt
172/30:
input = pd.concat([imputed_data, m, t, x_obs], axis=1)
input = input[[x for x in input.columns if x[-2:]!='_y']]
tgt = input['y']
del input['y']

input['y'] = tgt
input
173/1:
import torch 
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
173/2:
import torch 
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
173/3:
dataset = du.PatientDataset(5)
X, y = dataset[0]
X = torch.unsqueeze(torch.tensor(X), 0)
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(X.float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y}")
173/4:
dataset = du.PatientDataset(5)
X, y = dataset[:5]
# X = torch.unsqueeze(torch.tensor(X), 0)
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(X.float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y}")
173/5:
dataset = du.PatientDataset(5)
loader = DataLoader(dataset, shuffle=True, batch_size=5, drop_last=True)
for X,y in loader:
    pass
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, state = model(X.float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y}")
173/6:
# dataset = du.PatientDataset(5)
# loader = DataLoader(dataset, shuffle=True, batch_size=5, drop_last=True)
# for X,y in loader:
#     pass
# model = gru.GRUD().float()
# state = model.init_hidden(X.shape[0])
# yhat, state = model(X.float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
173/7:
a = utils.pkl_load('pt_data\archive\labels\0.npy')
a
173/8:
a = utils.pkl_load('pt_data/archive/labels/0.npy')
a
173/9:
a = utils.pkl_load('pt_data/archive/labels/0.npy')
a.shape
173/10: X
173/11: X[0].size()
173/12: X[0][0].size()
173/13: X[0][0]#.size()
173/14:
df = pd.read_pickle('pickles/inputs.pkl')
df.columns
173/15:
import pandas as pd
df = pd.read_pickle('pickles/inputs.pkl')
df.columns
173/16:
import pandas as pd
df = pd.read_pickle('pickles/inputs.pkl')
df.columns[:37]
173/17:
import pandas as pd
df = pd.read_pickle('pickles/inputs.pkl')
df.columns[:50]
173/18:
import pandas as pd
df = pd.read_pickle('pickles/inputs.pkl')
df.columns[:48]
173/19:
import pandas as pd
df = pd.read_pickle('pickles/inputs.pkl')
dic = utils.pkl_load('pickles/col_dict.pkl')
# df.columns[:48]
dic['input']
173/20:
import pandas as pd
df = pd.read_pickle('pickles/inputs.pkl')
dic = utils.pkl_load('pickles/col_dict.pkl')
# df.columns[:48]
len(dic['input'])
173/21:
import pandas as pd
df = pd.read_pickle('pickles/inputs.pkl')
dic = utils.pkl_load('pickles/col_dict.pkl')
# df.columns[:48]
dic['input']
173/22:
import pandas as pd
df = pd.read_pickle('pickles/inputs.pkl')
dic = utils.pkl_load('pickles/col_dict.pkl')
df.columns[:48]
173/23:
import pandas as pd
df = pd.read_pickle('pickles/inputs.pkl')
dic = utils.pkl_load('pickles/col_dict.pkl')
df.columns[1:48]
175/1:
import torch
torch.zeros(1,2,0)
175/2:
import utils
d = utils.pkl_load('pickles/col_dict.pkl')
d['input']
175/3:
import utils
d = utils.pkl_load('pickles/col_dict.pkl')
print(list(zip(range(47),d['input'][1:])))
175/4:
import pandas as pd
df = pd.read_csv('pickles/inputs.pkl')
df['ENC_TEL_Related'].value_counts()
175/5:
import pandas as pd
df = pd.read_pickle('pickles/inputs.pkl')
df['ENC_TEL_Related'].value_counts()
175/6:
import pandas as pd
df = pd.read_pickle('pickles/inputs.pkl')
diags = [x for x in df.columns if x[:4]=='DIAG']
df[diags].value_counts()
175/7:
import pandas as pd
df = pd.read_pickle('pickles/inputs.pkl')
diags = [x for x in df.columns if x[:4]=='DIAG']
df[diags].describe()
175/8:
import pandas as pd
# df = pd.read_pickle('pickles/inputs.pkl')
diags = [x for x in df.columns if x[:4]=='DIAG']
# df[diags].describe()
df['MED_ANTI_TNF'].value_counts()
175/9:
import pandas as pd
# df = pd.read_pickle('pickles/inputs.pkl')
diags = [x for x in df.columns if x[:4]=='DIAG']
# df[diags].describe()
df['MED_ANTI_TNF'].fillna(0).value_counts()
175/10:
import pandas as pd
# df = pd.read_pickle('pickles/inputs.pkl')
diags = [x for x in df.columns if x[:4]=='DIAG']
df[diags].fillna(0).describe()
# df['MED_ANTI_TNF'].fillna(0).value_counts()
175/11:
import torch
a = torch.randint(2, 5, 2)
a
175/12:
import torch
a = torch.randint(0,10,(2, 5, 2))
a
175/13:
import torch
a = torch.randint(0,10,(2, 5, 2))
a_dash = a[:,1:,:]
print(a, a_dash)
175/14: a
175/15: a[:,:-1,:]
175/16: a[:,:,:]
175/17: a[:,:-2,:]
175/18: a[:,:,:]
175/19: a[:,[0,1,2],:]
175/20:
import torch 
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
175/21:
dataset = du.PatientDataset(5)
loader = DataLoader(dataset, shuffle=True, batch_size=5, drop_last=True)
for X,y in loader:
    pass
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, y_tr, state = model(X.float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y_TR: {y_tr.shape}  Y: {y.shape}")
177/1:
import torch 
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
177/2:
dataset = du.PatientDataset(5)
loader = DataLoader(dataset, shuffle=True, batch_size=5, drop_last=True)
for X,y in loader:
    pass
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, y_tr, state = model(X.float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y_TR: {y_tr.shape}  Y: {y.shape}")
178/1:
import torch 
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
178/2:
dataset = du.PatientDataset(5)
loader = DataLoader(dataset, shuffle=True, batch_size=5, drop_last=True)
for X,y in loader:
    pass
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, y_tr, state = model(X.float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y_TR: {y_tr.shape}  Y: {y.shape}")
179/1:
import torch 
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
179/2:
dataset = du.PatientDataset(5)
loader = DataLoader(dataset, shuffle=True, batch_size=5, drop_last=True)
for X,y in loader:
    pass
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, y_tr, state = model(X.float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y_TR: {y_tr.shape}  Y: {y.shape}")
179/3:
import time
t0 = time.time()
dataset = du.PatientDataset(30)
loader = DataLoader(dataset, shuffle=True, batch_size=64, drop_last=True)
model = gru.train_model(loader, 1)
print(f'Time taken: {time.time()-t0}')
179/4:
a = torch.randint(100, 1000,(3,1,3))
b = torch.randint(0,10,(3,5,3))
print(a, b)
179/5:
# a = torch.randint(100, 1000,(3,1,3))
# b = torch.randint(0,10,(3,4,3))

torch.cat(b, a, 1)
179/6:
# a = torch.randint(100, 1000,(3,1,3))
# b = torch.randint(0,10,(3,4,3))

torch.cat((b, a), 1)
179/7:
# a = torch.randint(100, 1000,(3,1,3))
# b = torch.randint(0,10,(3,4,3))
print(b)
print(a)
print(torch.cat((b, a), 1))
179/8:
# a = torch.randint(100, 1000,(3,1,3))
# b = torch.randint(0,10,(3,4,3))

a = torch.Tensor([1,2,3])
a
179/9:
# a = torch.randint(100, 1000,(3,1,3))
# b = torch.randint(0,10,(3,4,3))

a = torch.Tensor([1,2,3])
a.size()
179/10:
# a = torch.randint(100, 1000,(3,1,3))
# b = torch.randint(0,10,(3,4,3))

a = torch.Tensor([1,2,3])
a.repeat(5)
179/11:
# a = torch.randint(100, 1000,(3,1,3))
# b = torch.randint(0,10,(3,4,3))

a = torch.Tensor([1,2,3])
a.repeat(5,1)
179/12:
# a = torch.randint(100, 1000,(3,1,3))
# b = torch.randint(0,10,(3,4,3))

a = torch.Tensor([1,2,3])
a.repeat(5,2)
179/13:
# a = torch.randint(100, 1000,(3,1,3))
# b = torch.randint(0,10,(3,4,3))

a = torch.Tensor([1,2,3])
torch.unsqueeze(a, 0)
179/14:
# a = torch.randint(100, 1000,(3,1,3))
# b = torch.randint(0,10,(3,4,3))

a = torch.Tensor([1,2,3])
torch.unsqueeze(a, 1)
179/15:
# a = torch.randint(100, 1000,(3,1,3))
# b = torch.randint(0,10,(3,4,3))

a = torch.Tensor([1,2,3])
torch.unsqueeze(a, 1).repeat(1,5)
179/16:
# a = torch.randint(100, 1000,(3,1,3))
# b = torch.randint(0,10,(3,4,3))

a = torch.Tensor([1,2,3])
torch.unsqueeze(a, 1).repeat(1,5).view(1,-1)
179/17:
# a = torch.randint(100, 1000,(3,1,3))
# b = torch.randint(0,10,(3,4,3))

a = torch.Tensor([1,2,3])
torch.unsqueeze(a, 1).repeat(1,5)#.view(1,-1)
179/18:
# a = torch.randint(100, 1000,(3,1,3))
# b = torch.randint(0,10,(3,4,3))

a = torch.Tensor([1,2,3])
torch.unsqueeze(a, 1).repeat(1,5).view(1)
179/19:
# a = torch.randint(100, 1000,(3,1,3))
# b = torch.randint(0,10,(3,4,3))

a = torch.Tensor([1,2,3])
torch.unsqueeze(a, 1).repeat(1,5).view(0)
179/20:
# a = torch.randint(100, 1000,(3,1,3))
# b = torch.randint(0,10,(3,4,3))

a = torch.Tensor([1,2,3])
torch.unsqueeze(a, 1).repeat(1,5).view(0,-1)
179/21:
# a = torch.randint(100, 1000,(3,1,3))
# b = torch.randint(0,10,(3,4,3))

a = torch.Tensor([1,2,3])
torch.unsqueeze(a, 1).repeat(1,5).view(0,1)
179/22:
# a = torch.randint(100, 1000,(3,1,3))
# b = torch.randint(0,10,(3,4,3))

a = torch.Tensor([1,2,3])
torch.unsqueeze(a, 1).repeat(1,5).view(1,-1)
179/23:
# a = torch.randint(100, 1000,(3,1,3))
b = torch.randint(0,1,(3,4,3))
b
179/24:
# a = torch.randint(100, 1000,(3,1,3))
b = torch.randint(0,5,(3,4,3))
b
179/25:
# a = torch.randint(100, 1000,(3,1,3))
b = torch.randint(0,5,(3,4,3))
b.view(12,-1)
179/26:
# a = torch.randint(100, 1000,(3,1,3))
# b = torch.randint(0,5,(3,4,3))
print(b)
print("\n\n")
print(b.view(12,-1))
180/1:
import torch 
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
180/2:
dataset = du.PatientDataset(5)
loader = DataLoader(dataset, shuffle=True, batch_size=5, drop_last=True)
for X,y in loader:
    pass
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, y_tr, state = model(X.float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y_TR: {y_tr.shape}  Y: {y.shape}")
180/3:
import time
t0 = time.time()
dataset = du.PatientDataset(300)
loader = DataLoader(dataset, shuffle=True, batch_size=64, drop_last=True)
model = gru.train_model(loader, 1)
print(f'Time taken: {time.time()-t0}')
180/4:
y = torch.tensor([1,2,3,4,5])
y2 = torch.unsqueeze(y, 1).repeat(1, 3).view(1,-1)
print(y.size(), y2.size())
180/5:
y = torch.tensor([1,2,3,4,5])
y2 = torch.unsqueeze(y, 1).repeat(1, 3).view(0,-1)
print(y.size(), y2.size())
180/6:
y = torch.tensor([1,2,3,4,5])
y2 = torch.unsqueeze(y, 1).repeat(1, 3).view(15,-1)
print(y.size(), y2.size())
180/7:
y = torch.tensor([1,2,3,4,5])
y2 = torch.unsqueeze(y, 1).repeat(1, 3).view(15,0)
print(y.size(), y2.size())
180/8:
y = torch.tensor([1,2,3,4,5])
y2 = torch.unsqueeze(y, 1).repeat(1, 3).view(15)
print(y.size(), y2.size())
181/1:
import torch 
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
181/2:
dataset = du.PatientDataset(5)
loader = DataLoader(dataset, shuffle=True, batch_size=5, drop_last=True)
for X,y in loader:
    pass
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, y_tr, state = model(X.float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y_TR: {y_tr.shape}  Y: {y.shape}")
181/3:
import time
t0 = time.time()
dataset = du.PatientDataset(300)
loader = DataLoader(dataset, shuffle=True, batch_size=64, drop_last=True)
model = gru.train_model(loader, 1)
print(f'Time taken: {time.time()-t0}')
181/4:
import time
t0 = time.time()
dataset = du.PatientDataset(3000)
loader = DataLoader(dataset, shuffle=True, batch_size=64, drop_last=True)
model = gru.train_model(loader, epochs=1)
print(f'Time taken: {time.time()-t0}')
181/5: int(0.9*1)
183/1:
import utils
d=utils.pkl_load('pt_data/label_dict.pkl')
d
183/2:
import torch 
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
183/3: tr, v, te = du.get_tt_datasets()
184/1: tr, v, te = du.get_tt_datasets()
184/2:
import torch 
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
184/3: tr, v, te = du.get_tt_datasets()
184/4: tr.labels
184/5: tr.list_IDs
184/6: tr, v, te = du.get_tt_datasets(split_on_patients=False)
184/7: tr.list_IDs
184/8: tr.labels
184/9: tr, v, te = du.get_tt_datasets(split_on_patients=True)
184/10: v.labels
184/11: v.list_IDs
184/12: te.list_IDs
184/13: tr.list_IDs
184/14: te.list_IDs
185/1:
import torch 
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
185/2: tr, v, te = du.get_tt_datasets(split_on_patients=True)
186/1:
import du
tr, v, te = du.get_tt_datasets(split_on_patients=True)
186/2:
import data_utils as du
tr, v, te = du.get_tt_datasets(split_on_patients=True)
186/3:
import data_utils as du
tr, v, te = du.get_tt_datasets(split_on_patients=True)
187/1:
import data_utils as du
tr, v, te = du.get_tt_datasets(split_on_patients=True)
187/2: te.list_IDs
187/3: tr.list_IDs
187/4: [x.split('_')[0] for x in tr.list_IDs]
187/5: set([x.split('_')[0] for x in tr.list_IDs])
187/6: sorted(list(set([x.split('_')[0] for x in tr.list_IDs]))
187/7: sorted(list(set([x.split('_')[0] for x in tr.list_IDs])))
187/8: v.list_IDs
187/9: set([x.split('_')[0] for x in tr.list_IDs]).intersection(set([x.split('_')[0] for x in v.list_IDs]))
187/10: set([x.split('_')[0] for x in tr.list_IDs]).intersection(set([x.split('_')[0] for x in te.list_IDs]))
187/11: set([x.split('_')[0] for x in te.list_IDs]).intersection(set([x.split('_')[0] for x in v.list_IDs]))
188/1:
import data_utils as du
tr, v, te = du.get_tt_datasets(split_on_patients=True)
189/1:
import data_utils as du
tr, v, te = du.get_tt_datasets(split_on_patients=True)
189/2: set([x.split('_')[0] for x in te.list_IDs]).intersection(set([x.split('_')[0] for x in v.list_IDs]))
189/3: set([x.split('_')[0] for x in tr.list_IDs]).intersection(set([x.split('_')[0] for x in v.list_IDs]))
189/4: tr
189/5: tr.list_IDs
189/6: set([x.split('_')[0] for x in tr.list_IDs])#.intersection(set([x.split('_')[0] for x in v.list_IDs]))
189/7:
import time
t0 = time.time()
loader = DataLoader(te, shuffle=True, batch_size=64, drop_last=True)
model = gru.train_model(loader, epochs=1)
print(f'Time taken: {time.time()-t0}')
189/8:
import torch 
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
189/9:
import time
t0 = time.time()
loader = DataLoader(te, shuffle=True, batch_size=64, drop_last=True)
model = gru.train_model(loader, epochs=1)
print(f'Time taken: {time.time()-t0}')
189/10:
import importlib
importlib.reload(gru)
189/11:
import time
t0 = time.time()
loader = DataLoader(te, shuffle=True, batch_size=64, drop_last=True)
model = gru.train_model(loader, epochs=1)
print(f'Time taken: {time.time()-t0}')
189/12:
import importlib
importlib.reload(gru)
189/13:
import time
t0 = time.time()
loader = DataLoader(te, shuffle=True, batch_size=64, drop_last=True)
model = gru.train_model(loader, epochs=1)
print(f'Time taken: {time.time()-t0}')
189/14:
dataset = du.PatientDataset(5)
loader = DataLoader(dataset, shuffle=True, batch_size=5, drop_last=True)
for X,y in loader:
    pass
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, y_tr, state = model(X.float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y_TR: {y_tr.shape}  Y: {y.shape}")
189/15: yhat
189/16: torch.exp(yhat)
189/17: torch.sigmoid(yhat)
189/18: torch.softmax(yhat)
189/19: torch.softmax(yhat,1)
189/20: Y
189/21: y
189/22: torch.softmax(yhat,0)
189/23: torch.softmax(yhat,1)
189/24: y.data
189/25: ps=torch.softmax(yhat,1)
189/26:
ps=torch.softmax(yhat,1)
ps
189/27:
ps=torch.softmax(yhat,1)
ps, y
189/28: y==ps.max(dim=1)
189/29: ps.max(dim=1)
189/30: ps.max(dim=1)[1]
189/31: y==ps.max(dim=1)[1]
189/32: y==ps.max(dim=1)[1].sum()
189/33: (y==ps.max(dim=1)[1]).sum()
189/34: (y==ps.max(dim=1)[1]).mean()
189/35: (y==ps.max(dim=1)[1]).type(torch.Float()).mean()
189/36: (y==ps.max(dim=1)[1]).type(torch.FloatTensor).mean()
189/37:
import importlib
importlib.reload(gru)
189/38: gru.eval_model(model, v)
189/39:
import importlib
importlib.reload(gru)
189/40: gru.eval_model(model, v)
189/41:
import importlib
importlib.reload(gru)
189/42: gru.eval_model(model, v)
189/43: gru.eval_model(model, DataLoader(v))
189/44:
import importlib
importlib.reload(gru)
189/45: gru.eval_model(model, DataLoader(v))
189/46:
import importlib
importlib.reload(gru)
189/47: gru.eval_model(model, DataLoader(v))
189/48:
import importlib
importlib.reload(gru)
189/49: gru.eval_model(model, DataLoader(v))
189/50: gru.eval_model(model.eval(), DataLoader(v))
189/51:
model.eval()
model.training
189/52: gru.eval_model(model.eval(), DataLoader(v))
189/53: gru.eval_model(model, DataLoader(v))
189/54: gru.eval_model(model, DataLoader(v, batch_size=2))
189/55:
import importlib
importlib.reload(gru)
189/56: gru.eval_model(model, DataLoader(v, batch_size=2))
189/57: gru.eval_model(model, DataLoader(v, batch_size=len(v)))
190/1:
dataset = du.PatientDataset(5)
loader = DataLoader(dataset, shuffle=True, batch_size=5, drop_last=True)
for X,y in loader:
    pass
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, y_tr, y_aux, state = model(X.float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y_TR: {y_tr.shape}  Y_Aux:{y_aux.shape}  Y: {y.shape}")
190/2:
import torch 
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
190/3:
dataset = du.PatientDataset(5)
loader = DataLoader(dataset, shuffle=True, batch_size=5, drop_last=True)
for X,y in loader:
    pass
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, y_tr, y_aux, state = model(X.float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y_TR: {y_tr.shape}  Y_Aux:{y_aux.shape}  Y: {y.shape}")
191/1:
import torch 
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
191/2:
dataset = du.PatientDataset(5)
loader = DataLoader(dataset, shuffle=True, batch_size=5, drop_last=True)
for X,y in loader:
    pass
model = gru.GRUD().float()
state = model.init_hidden(X.shape[0])
yhat, y_tr, y_aux, state = model(X.float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y_TR: {y_tr.shape}  Y_Aux:{y_aux.shape}  Y: {y.shape}")
191/3:
dataset = du.PatientDataset(5)
loader = DataLoader(dataset, shuffle=True, batch_size=5, drop_last=True)
for X,y in loader:
    pass
model = gru.GRUD(aux_loss_cols=['diag', 'enc', 'hilab', 'lolab']).float()
state = model.init_hidden(X.shape[0])
yhat, y_tr, y_aux, state = model(X.float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y_TR: {y_tr.shape}  Y_Aux:{y_aux.shape}  Y: {y.shape}")
191/4: Y_Aux
191/5: y_aux
193/1: import pickle
193/2: di = pickle.load('data/label_dict.pkl')
193/3: di = pickle.load(open('data/label_dict.pkl','rb'))
193/4: aut = list(set([x.split('_')[0] for x in di.keys()]))
193/5: len(aut)
193/6: l = [2, 3, 4, 7, 11, 14, 23, 26, 27, 28, 30, 31, 33, 38, 39, 53, 55, 57, 59, 61, 65, 66, 70, 71, 73, 82, 101, 104, 105, 106, 109, 119, 131, 144, 146, 149, 150, 157, 159, 160, 163, 164, 165, 171, 172, 176, 180, 183, 185, 187, 188, 193, 201, 204, 205, 219, 223, 226, 229, 230, 233, 237, 242, 243, 245, 251, 252, 256, 258, 261, 267, 270, 272, 274, 276, 278, 279, 282, 283, 285, 289, 290, 293, 300, 301, 304, 305, 306, 314, 315, 321, 326, 327, 330, 331, 335, 337, 338, 342, 345, 347, 350, 358, 363, 367, 368, 373, 375, 376, 380, 381, 382, 383, 384, 386, 392, 393, 397, 399, 406, 407, 410, 414, 421, 422, 429, 435, 437, 446, 449, 453, 454, 458, 460, 467, 474, 484, 487, 493, 494, 498, 501, 502, 503, 512, 518, 523, 524, 526, 527, 530, 536, 537, 541, 545, 546, 551, 553, 555, 556, 560, 561, 563, 564, 567, 573, 575, 580, 581, 585, 590, 592, 595, 599, 600, 601, 608, 616, 620, 625, 627, 628, 632, 644, 646, 649, 653, 654, 660, 661, 664, 667, 668, 672, 679, 681, 682, 683, 697, 700, 701, 703, 704, 706, 707, 708, 710, 711, 715, 720, 728, 730, 733, 734, 737, 746, 747, 749, 750, 751, 756, 767, 779, 781, 784, 786, 788, 789, 799, 800, 806, 807, 811, 812, 820, 821, 825, 826, 827, 828, 835, 837, 840, 841, 844, 846, 848, 851, 852, 860, 863, 864, 865, 866, 868, 870, 872, 878, 881, 884, 887, 888, 889, 890, 894, 896, 902, 903, 904, 909, 911, 914, 917, 920, 923, 924, 935, 942, 946, 948, 950, 953, 954, 963, 966, 968, 971, 980, 987, 990, 991, 992, 993, 995, 997, 999, 1001, 1005, 1006, 1007, 1009, 1011, 1012, 1015, 1018, 1019, 1024, 1028, 1040, 1041, 1043, 1052, 1053, 1057, 1060, 1070, 1074, 1079, 1083, 1086, 1087, 1088, 1090, 1092, 1093, 1095, 1096, 1097, 1099, 1100, 1102, 1103, 1104, 1105, 1106, 1112, 1115, 1120, 1123, 1124, 1126, 1130, 1134, 1136, 1138, 1140, 1142, 1145, 1148, 1151, 1152, 1174, 1178, 1183, 1185, 1186, 1187, 1195, 1206, 1209, 1210, 1214, 1216, 1223, 1233, 1235, 1236, 1237, 1243, 1250, 1251, 1259, 1264, 1269, 1273, 1278, 1279, 1280, 1293, 1295, 1298, 1302, 1303, 1304, 1307, 1309, 1312, 1317, 1318, 1322, 1328, 1333, 1337, 1342, 1344, 1350, 1352, 1359, 1371, 1376, 1378, 1379, 1383, 1392, 1393, 1397, 1399, 1400, 1401, 1404, 1406, 1407, 1408, 1409, 1411, 1426, 1429, 1441, 1442, 1443, 1444, 1447, 1454, 1462, 1466, 1469, 1473, 1474, 1479, 1485, 1486, 1489, 1493, 1495, 1500, 1503, 1505, 1509, 1512, 1517, 1522, 1530, 1532, 1533, 1535, 1540, 1546, 1547, 1549, 1552, 1556, 1557, 1560, 1562, 1569, 1571, 1577, 1583, 1584, 1589, 1590, 1592, 1598, 1599, 1601, 1604, 1605, 1613, 1616, 1617, 1623, 1626, 1628, 1629, 1631, 1635, 1636, 1643, 1650, 1651, 1652, 1654, 1656, 1657, 1667, 1674, 1677, 1680, 1681, 1689, 1695, 1696, 1698, 1700, 1707, 1714, 1715, 1716, 1717, 1718, 1720, 1721, 1724, 1726, 1728, 1734, 1737, 1738, 1741, 1744, 1746, 1749, 1750, 1753, 1757, 1763, 1766, 1767, 1768, 1769, 1770, 1781, 1785, 1789, 1794, 1797, 1799, 1805, 1808, 1809, 1814, 1816, 1818, 1820, 1832, 1835, 1837, 1839, 1851, 1854, 1859, 1864, 1869, 1870, 1871, 1876, 1877, 1881, 1885, 1888, 1891, 1893, 1894, 1901, 1902, 1907, 1916, 1917, 1919, 1920, 1922, 1923, 1928, 1929, 1930, 1931, 1935, 1938, 1939, 1940, 1944, 1946, 1949, 1950, 1959, 1960, 1962, 1963, 1966, 1967, 1968, 1970, 1980, 1987, 1991, 1994, 1995, 2000, 2004, 2009, 2010, 2013, 2015, 2018, 2019, 2025, 2028, 2031, 2032, 2037, 2041, 2043, 2044, 2047, 2056, 2063, 2064, 2067, 2072, 2073, 2077, 2080, 2083, 2084, 2087, 2089, 2090, 2091, 2096, 2099, 2111, 2113, 2114, 2123, 2128, 2135, 2139, 2140, 2142, 2143, 2146, 2147, 2149, 2164, 2166, 2170, 2178, 2180, 2183, 2189, 2190, 2191, 2192, 2195, 2199, 2201, 2203, 2210, 2212, 2215, 2220, 2228, 2231, 2232, 2237, 2240, 2241, 2248, 2249, 2253, 2257, 2258, 2261, 2263, 2276, 2277, 2279, 2280, 2282, 2292, 2295, 2298, 2300, 2318, 2319, 2322, 2326, 2327, 2328, 2329, 2331, 2333, 2334, 2335, 2336, 2337, 2348, 2350, 2351, 2355, 2356, 2368, 2371, 2372, 2374, 2375, 2378, 2379, 2381, 2384, 2386, 2387, 2389, 2395, 2397, 2401, 2402, 2403, 2406, 2414, 2417, 2424, 2425, 2431, 2433, 2434, 2437, 2439, 2440, 2441, 2446, 2451, 2457, 2467, 2468, 2471, 2472, 2473, 2475, 2485, 2487, 2491, 2493, 2496, 2502, 2503, 2510, 2514, 2517, 2521, 2522, 2527, 2528, 2538, 2542, 2544, 2550, 2551, 2552, 2554, 2557, 2560, 2565, 2569, 2571, 2576, 2580, 2585, 2587, 2591, 2592, 2594, 2599, 2603, 2604, 2606, 2610, 2616, 2617, 2619, 2632, 2633, 2634, 2638, 2645, 2647, 2650, 2664, 2665, 2668, 2669, 2670, 2673, 2675, 2676, 2677, 2678, 2687, 2690, 2691, 2692, 2695, 2697, 2700, 2705, 2706, 2709, 2716, 2719, 2724, 2726, 2727, 2728, 2731, 2733, 2734, 2739, 2743, 2744, 2750, 2764, 2765, 2768, 2769, 2770, 2773, 2775, 2776, 2778, 2779, 2789, 2792, 2794, 2795, 2808, 2813, 2819, 2820, 2829, 2831, 2833, 2834, 2835, 2836, 2838, 2839, 2841, 2843, 2845, 2848, 2854, 2856, 2861, 2867, 2870, 2872, 2875, 2878, 2881, 2883, 2887, 2892, 2895, 2896, 2897, 2898, 2909, 2910, 2912, 2913, 2914, 2917, 2923, 2928, 2934, 2937, 2939, 2949, 2950, 2952, 2953, 2955, 2959, 2966, 2967, 2968, 2970, 2971, 2972, 2974, 2982, 2991, 3001, 3008, 3018, 3020, 3028, 3033, 3034, 3043, 3046, 3048, 3050, 3055, 3056, 3059, 3060, 3068, 3070, 3075, 3079, 3087, 3088, 3096, 3101, 3106, 3110, 3111, 3114, 3115, 3116, 3117, 3119, 3122, 3124, 3125, 3128, 3133, 3135, 3136]
193/7: aut = list(set([x if x.split('_')[0] in l for x in di.keys()]))
193/8: aut = list(set([x if l.index(int(x.split('_')[0]))>0 for x in di.keys()]))
193/9: aut=[]
193/10:
for x in di.keys():
    if int(x.split('_')[0]) in l:
      aut.append(x)
193/11: len(aut)
193/12: len(di.keys())
193/13: l
193/14: au
193/15: taut
193/16: aut
193/17: pprint off
194/1:
auto_ids = [2, 3, 4, 7, 11, 14, 23, 26, 27, 28, 30, 31, 33, 38, 39, 53, 55, 57, 59, 61, 65, 66, 70, 71, 73, 82, 101, 104, 105, 106, 109, 119, 131, 144, 146, 149, 150, 157, 159, 160, 163, 164, 165, 171, 172, 176, 180, 183, 185, 187, 188, 193, 201, 204, 205, 219, 223, 226, 229, 230, 233, 237, 242, 243, 245, 251, 252, 256, 258, 261, 267, 270, 272, 274, 276, 278, 279, 282, 283, 285, 289, 290, 293, 300, 301, 304, 305, 306, 314, 315, 321, 326, 327, 330, 331, 335, 337, 338, 342, 345, 347, 350, 358, 363, 367, 368, 373, 375, 376, 380, 381, 382, 383, 384, 386, 392, 393, 397, 399, 406, 407, 410, 414, 421, 422, 429, 435, 437, 446, 449, 453, 454, 458, 460, 467, 474, 484, 487, 493, 494, 498, 501, 502, 503, 512, 518, 523, 524, 526, 527, 530, 536, 537, 541, 545, 546, 551, 553, 555, 556, 560, 561, 563, 564, 567, 573, 575, 580, 581, 585, 590, 592, 595, 599, 600, 601, 608, 616, 620, 625, 627, 628, 632, 644, 646, 649, 653, 654, 660, 661, 664, 667, 668, 672, 679, 681, 682, 683, 697, 700, 701, 703, 704, 706, 707, 708, 710, 711, 715, 720, 728, 730, 733, 734, 737, 746, 747, 749, 750, 751, 756, 767, 779, 781, 784, 786, 788, 789, 799, 800, 806, 807, 811, 812, 820, 821, 825, 826, 827, 828, 835, 837, 840, 841, 844, 846, 848, 851, 852, 860, 863, 864, 865, 866, 868, 870, 872, 878, 881, 884, 887, 888, 889, 890, 894, 896, 902, 903, 904, 909, 911, 914, 917, 920, 923, 924, 935, 942, 946, 948, 950, 953, 954, 963, 966, 968, 971, 980, 987, 990, 991, 992, 993, 995, 997, 999, 1001, 1005, 1006, 1007, 1009, 1011, 1012, 1015, 1018, 1019, 1024, 1028, 1040, 1041, 1043, 1052, 1053, 1057, 1060, 1070, 1074, 1079, 1083, 1086, 1087, 1088, 1090, 1092, 1093, 1095, 1096, 1097, 1099, 1100, 1102, 1103, 1104, 1105, 1106, 1112, 1115, 1120, 1123, 1124, 1126, 1130, 1134, 1136, 1138, 1140, 1142, 1145, 1148, 1151, 1152, 1174, 1178, 1183, 1185, 1186, 1187, 1195, 1206, 1209, 1210, 1214, 1216, 1223, 1233, 1235, 1236, 1237, 1243, 1250, 1251, 1259, 1264, 1269, 1273, 1278, 1279, 1280, 1293, 1295, 1298, 1302, 1303, 1304, 1307, 1309, 1312, 1317, 1318, 1322, 1328, 1333, 1337, 1342, 1344, 1350, 1352, 1359, 1371, 1376, 1378, 1379, 1383, 1392, 1393, 1397, 1399, 1400, 1401, 1404, 1406, 1407, 1408, 1409, 1411, 1426, 1429, 1441, 1442, 1443, 1444, 1447, 1454, 1462, 1466, 1469, 1473, 1474, 1479, 1485, 1486, 1489, 1493, 1495, 1500, 1503, 1505, 1509, 1512, 1517, 1522, 1530, 1532, 1533, 1535, 1540, 1546, 1547, 1549, 1552, 1556, 1557, 1560, 1562, 1569, 1571, 1577, 1583, 1584, 1589, 1590, 1592, 1598, 1599, 1601, 1604, 1605, 1613, 1616, 1617, 1623, 1626, 1628, 1629, 1631, 1635, 1636, 1643, 1650, 1651, 1652, 1654, 1656, 1657, 1667, 1674, 1677, 1680, 1681, 1689, 1695, 1696, 1698, 1700, 1707, 1714, 1715, 1716, 1717, 1718, 1720, 1721, 1724, 1726, 1728, 1734, 1737, 1738, 1741, 1744, 1746, 1749, 1750, 1753, 1757, 1763, 1766, 1767, 1768, 1769, 1770, 1781, 1785, 1789, 1794, 1797, 1799, 1805, 1808, 1809, 1814, 1816, 1818, 1820, 1832, 1835, 1837, 1839, 1851, 1854, 1859, 1864, 1869, 1870, 1871, 1876, 1877, 1881, 1885, 1888, 1891, 1893, 1894, 1901, 1902, 1907, 1916, 1917, 1919, 1920, 1922, 1923, 1928, 1929, 1930, 1931, 1935, 1938, 1939, 1940, 1944, 1946, 1949, 1950, 1959, 1960, 1962, 1963, 1966, 1967, 1968, 1970, 1980, 1987, 1991, 1994, 1995, 2000, 2004, 2009, 2010, 2013, 2015, 2018, 2019, 2025, 2028, 2031, 2032, 2037, 2041, 2043, 2044, 2047, 2056, 2063, 2064, 2067, 2072, 2073, 2077, 2080, 2083, 2084, 2087, 2089, 2090, 2091, 2096, 2099, 2111, 2113, 2114, 2123, 2128, 2135, 2139, 2140, 2142, 2143, 2146, 2147, 2149, 2164, 2166, 2170, 2178, 2180, 2183, 2189, 2190, 2191, 2192, 2195, 2199, 2201, 2203, 2210, 2212, 2215, 2220, 2228, 2231, 2232, 2237, 2240, 2241, 2248, 2249, 2253, 2257, 2258, 2261, 2263, 2276, 2277, 2279, 2280, 2282, 2292, 2295, 2298, 2300, 2318, 2319, 2322, 2326, 2327, 2328, 2329, 2331, 2333, 2334, 2335, 2336, 2337, 2348, 2350, 2351, 2355, 2356, 2368, 2371, 2372, 2374, 2375, 2378, 2379, 2381, 2384, 2386, 2387, 2389, 2395, 2397, 2401, 2402, 2403, 2406, 2414, 2417, 2424, 2425, 2431, 2433, 2434, 2437, 2439, 2440, 2441, 2446, 2451, 2457, 2467, 2468, 2471, 2472, 2473, 2475, 2485, 2487, 2491, 2493, 2496, 2502, 2503, 2510, 2514, 2517, 2521, 2522, 2527, 2528, 2538, 2542, 2544, 2550, 2551, 2552, 2554, 2557, 2560, 2565, 2569, 2571, 2576, 2580, 2585, 2587, 2591, 2592, 2594, 2599, 2603, 2604, 2606, 2610, 2616, 2617, 2619, 2632, 2633, 2634, 2638, 2645, 2647, 2650, 2664, 2665, 2668, 2669, 2670, 2673, 2675, 2676, 2677, 2678, 2687, 2690, 2691, 2692, 2695, 2697, 2700, 2705, 2706, 2709, 2716, 2719, 2724, 2726, 2727, 2728, 2731, 2733, 2734, 2739, 2743, 2744, 2750, 2764, 2765, 2768, 2769, 2770, 2773, 2775, 2776, 2778, 2779, 2789, 2792, 2794, 2795, 2808, 2813, 2819, 2820, 2829, 2831, 2833, 2834, 2835, 2836, 2838, 2839, 2841, 2843, 2845, 2848, 2854, 2856, 2861, 2867, 2870, 2872, 2875, 2878, 2881, 2883, 2887, 2892, 2895, 2896, 2897, 2898, 2909, 2910, 2912, 2913, 2914, 2917, 2923, 2928, 2934, 2937, 2939, 2949, 2950, 2952, 2953, 2955, 2959, 2966, 2967, 2968, 2970, 2971, 2972, 2974, 2982, 2991, 3001, 3008, 3018, 3020, 3028, 3033, 3034, 3043, 3046, 3048, 3050, 3055, 3056, 3059, 3060, 3068, 3070, 3075, 3079, 3087, 3088, 3096, 3101, 3106, 3110, 3111, 3114, 3115, 3116, 3117, 3119, 3122, 3124, 3125, 3128, 3133, 3135, 3136]
sequences = [x for x in all]
194/2:
auto_ids = [2, 3, 4, 7, 11, 14, 23, 26, 27, 28, 30, 31, 33, 38, 39, 53, 55, 57, 59, 61, 65, 66, 70, 71, 73, 82, 101, 104, 105, 106, 109, 119, 131, 144, 146, 149, 150, 157, 159, 160, 163, 164, 165, 171, 172, 176, 180, 183, 185, 187, 188, 193, 201, 204, 205, 219, 223, 226, 229, 230, 233, 237, 242, 243, 245, 251, 252, 256, 258, 261, 267, 270, 272, 274, 276, 278, 279, 282, 283, 285, 289, 290, 293, 300, 301, 304, 305, 306, 314, 315, 321, 326, 327, 330, 331, 335, 337, 338, 342, 345, 347, 350, 358, 363, 367, 368, 373, 375, 376, 380, 381, 382, 383, 384, 386, 392, 393, 397, 399, 406, 407, 410, 414, 421, 422, 429, 435, 437, 446, 449, 453, 454, 458, 460, 467, 474, 484, 487, 493, 494, 498, 501, 502, 503, 512, 518, 523, 524, 526, 527, 530, 536, 537, 541, 545, 546, 551, 553, 555, 556, 560, 561, 563, 564, 567, 573, 575, 580, 581, 585, 590, 592, 595, 599, 600, 601, 608, 616, 620, 625, 627, 628, 632, 644, 646, 649, 653, 654, 660, 661, 664, 667, 668, 672, 679, 681, 682, 683, 697, 700, 701, 703, 704, 706, 707, 708, 710, 711, 715, 720, 728, 730, 733, 734, 737, 746, 747, 749, 750, 751, 756, 767, 779, 781, 784, 786, 788, 789, 799, 800, 806, 807, 811, 812, 820, 821, 825, 826, 827, 828, 835, 837, 840, 841, 844, 846, 848, 851, 852, 860, 863, 864, 865, 866, 868, 870, 872, 878, 881, 884, 887, 888, 889, 890, 894, 896, 902, 903, 904, 909, 911, 914, 917, 920, 923, 924, 935, 942, 946, 948, 950, 953, 954, 963, 966, 968, 971, 980, 987, 990, 991, 992, 993, 995, 997, 999, 1001, 1005, 1006, 1007, 1009, 1011, 1012, 1015, 1018, 1019, 1024, 1028, 1040, 1041, 1043, 1052, 1053, 1057, 1060, 1070, 1074, 1079, 1083, 1086, 1087, 1088, 1090, 1092, 1093, 1095, 1096, 1097, 1099, 1100, 1102, 1103, 1104, 1105, 1106, 1112, 1115, 1120, 1123, 1124, 1126, 1130, 1134, 1136, 1138, 1140, 1142, 1145, 1148, 1151, 1152, 1174, 1178, 1183, 1185, 1186, 1187, 1195, 1206, 1209, 1210, 1214, 1216, 1223, 1233, 1235, 1236, 1237, 1243, 1250, 1251, 1259, 1264, 1269, 1273, 1278, 1279, 1280, 1293, 1295, 1298, 1302, 1303, 1304, 1307, 1309, 1312, 1317, 1318, 1322, 1328, 1333, 1337, 1342, 1344, 1350, 1352, 1359, 1371, 1376, 1378, 1379, 1383, 1392, 1393, 1397, 1399, 1400, 1401, 1404, 1406, 1407, 1408, 1409, 1411, 1426, 1429, 1441, 1442, 1443, 1444, 1447, 1454, 1462, 1466, 1469, 1473, 1474, 1479, 1485, 1486, 1489, 1493, 1495, 1500, 1503, 1505, 1509, 1512, 1517, 1522, 1530, 1532, 1533, 1535, 1540, 1546, 1547, 1549, 1552, 1556, 1557, 1560, 1562, 1569, 1571, 1577, 1583, 1584, 1589, 1590, 1592, 1598, 1599, 1601, 1604, 1605, 1613, 1616, 1617, 1623, 1626, 1628, 1629, 1631, 1635, 1636, 1643, 1650, 1651, 1652, 1654, 1656, 1657, 1667, 1674, 1677, 1680, 1681, 1689, 1695, 1696, 1698, 1700, 1707, 1714, 1715, 1716, 1717, 1718, 1720, 1721, 1724, 1726, 1728, 1734, 1737, 1738, 1741, 1744, 1746, 1749, 1750, 1753, 1757, 1763, 1766, 1767, 1768, 1769, 1770, 1781, 1785, 1789, 1794, 1797, 1799, 1805, 1808, 1809, 1814, 1816, 1818, 1820, 1832, 1835, 1837, 1839, 1851, 1854, 1859, 1864, 1869, 1870, 1871, 1876, 1877, 1881, 1885, 1888, 1891, 1893, 1894, 1901, 1902, 1907, 1916, 1917, 1919, 1920, 1922, 1923, 1928, 1929, 1930, 1931, 1935, 1938, 1939, 1940, 1944, 1946, 1949, 1950, 1959, 1960, 1962, 1963, 1966, 1967, 1968, 1970, 1980, 1987, 1991, 1994, 1995, 2000, 2004, 2009, 2010, 2013, 2015, 2018, 2019, 2025, 2028, 2031, 2032, 2037, 2041, 2043, 2044, 2047, 2056, 2063, 2064, 2067, 2072, 2073, 2077, 2080, 2083, 2084, 2087, 2089, 2090, 2091, 2096, 2099, 2111, 2113, 2114, 2123, 2128, 2135, 2139, 2140, 2142, 2143, 2146, 2147, 2149, 2164, 2166, 2170, 2178, 2180, 2183, 2189, 2190, 2191, 2192, 2195, 2199, 2201, 2203, 2210, 2212, 2215, 2220, 2228, 2231, 2232, 2237, 2240, 2241, 2248, 2249, 2253, 2257, 2258, 2261, 2263, 2276, 2277, 2279, 2280, 2282, 2292, 2295, 2298, 2300, 2318, 2319, 2322, 2326, 2327, 2328, 2329, 2331, 2333, 2334, 2335, 2336, 2337, 2348, 2350, 2351, 2355, 2356, 2368, 2371, 2372, 2374, 2375, 2378, 2379, 2381, 2384, 2386, 2387, 2389, 2395, 2397, 2401, 2402, 2403, 2406, 2414, 2417, 2424, 2425, 2431, 2433, 2434, 2437, 2439, 2440, 2441, 2446, 2451, 2457, 2467, 2468, 2471, 2472, 2473, 2475, 2485, 2487, 2491, 2493, 2496, 2502, 2503, 2510, 2514, 2517, 2521, 2522, 2527, 2528, 2538, 2542, 2544, 2550, 2551, 2552, 2554, 2557, 2560, 2565, 2569, 2571, 2576, 2580, 2585, 2587, 2591, 2592, 2594, 2599, 2603, 2604, 2606, 2610, 2616, 2617, 2619, 2632, 2633, 2634, 2638, 2645, 2647, 2650, 2664, 2665, 2668, 2669, 2670, 2673, 2675, 2676, 2677, 2678, 2687, 2690, 2691, 2692, 2695, 2697, 2700, 2705, 2706, 2709, 2716, 2719, 2724, 2726, 2727, 2728, 2731, 2733, 2734, 2739, 2743, 2744, 2750, 2764, 2765, 2768, 2769, 2770, 2773, 2775, 2776, 2778, 2779, 2789, 2792, 2794, 2795, 2808, 2813, 2819, 2820, 2829, 2831, 2833, 2834, 2835, 2836, 2838, 2839, 2841, 2843, 2845, 2848, 2854, 2856, 2861, 2867, 2870, 2872, 2875, 2878, 2881, 2883, 2887, 2892, 2895, 2896, 2897, 2898, 2909, 2910, 2912, 2913, 2914, 2917, 2923, 2928, 2934, 2937, 2939, 2949, 2950, 2952, 2953, 2955, 2959, 2966, 2967, 2968, 2970, 2971, 2972, 2974, 2982, 2991, 3001, 3008, 3018, 3020, 3028, 3033, 3034, 3043, 3046, 3048, 3050, 3055, 3056, 3059, 3060, 3068, 3070, 3075, 3079, 3087, 3088, 3096, 3101, 3106, 3110, 3111, 3114, 3115, 3116, 3117, 3119, 3122, 3124, 3125, 3128, 3133, 3135, 3136]
sequences = [x for x in all_data_dict.keys() if int(x.split('_')[0]) in auto_ids]
tr,v,te = du.get_tt_datasets(sequences)
194/3:
import utils
import data_utils as du
194/4: all_data_dict = utils.pkl_load('data/label_dict.pkl')
194/5:
auto_ids = [2, 3, 4, 7, 11, 14, 23, 26, 27, 28, 30, 31, 33, 38, 39, 53, 55, 57, 59, 61, 65, 66, 70, 71, 73, 82, 101, 104, 105, 106, 109, 119, 131, 144, 146, 149, 150, 157, 159, 160, 163, 164, 165, 171, 172, 176, 180, 183, 185, 187, 188, 193, 201, 204, 205, 219, 223, 226, 229, 230, 233, 237, 242, 243, 245, 251, 252, 256, 258, 261, 267, 270, 272, 274, 276, 278, 279, 282, 283, 285, 289, 290, 293, 300, 301, 304, 305, 306, 314, 315, 321, 326, 327, 330, 331, 335, 337, 338, 342, 345, 347, 350, 358, 363, 367, 368, 373, 375, 376, 380, 381, 382, 383, 384, 386, 392, 393, 397, 399, 406, 407, 410, 414, 421, 422, 429, 435, 437, 446, 449, 453, 454, 458, 460, 467, 474, 484, 487, 493, 494, 498, 501, 502, 503, 512, 518, 523, 524, 526, 527, 530, 536, 537, 541, 545, 546, 551, 553, 555, 556, 560, 561, 563, 564, 567, 573, 575, 580, 581, 585, 590, 592, 595, 599, 600, 601, 608, 616, 620, 625, 627, 628, 632, 644, 646, 649, 653, 654, 660, 661, 664, 667, 668, 672, 679, 681, 682, 683, 697, 700, 701, 703, 704, 706, 707, 708, 710, 711, 715, 720, 728, 730, 733, 734, 737, 746, 747, 749, 750, 751, 756, 767, 779, 781, 784, 786, 788, 789, 799, 800, 806, 807, 811, 812, 820, 821, 825, 826, 827, 828, 835, 837, 840, 841, 844, 846, 848, 851, 852, 860, 863, 864, 865, 866, 868, 870, 872, 878, 881, 884, 887, 888, 889, 890, 894, 896, 902, 903, 904, 909, 911, 914, 917, 920, 923, 924, 935, 942, 946, 948, 950, 953, 954, 963, 966, 968, 971, 980, 987, 990, 991, 992, 993, 995, 997, 999, 1001, 1005, 1006, 1007, 1009, 1011, 1012, 1015, 1018, 1019, 1024, 1028, 1040, 1041, 1043, 1052, 1053, 1057, 1060, 1070, 1074, 1079, 1083, 1086, 1087, 1088, 1090, 1092, 1093, 1095, 1096, 1097, 1099, 1100, 1102, 1103, 1104, 1105, 1106, 1112, 1115, 1120, 1123, 1124, 1126, 1130, 1134, 1136, 1138, 1140, 1142, 1145, 1148, 1151, 1152, 1174, 1178, 1183, 1185, 1186, 1187, 1195, 1206, 1209, 1210, 1214, 1216, 1223, 1233, 1235, 1236, 1237, 1243, 1250, 1251, 1259, 1264, 1269, 1273, 1278, 1279, 1280, 1293, 1295, 1298, 1302, 1303, 1304, 1307, 1309, 1312, 1317, 1318, 1322, 1328, 1333, 1337, 1342, 1344, 1350, 1352, 1359, 1371, 1376, 1378, 1379, 1383, 1392, 1393, 1397, 1399, 1400, 1401, 1404, 1406, 1407, 1408, 1409, 1411, 1426, 1429, 1441, 1442, 1443, 1444, 1447, 1454, 1462, 1466, 1469, 1473, 1474, 1479, 1485, 1486, 1489, 1493, 1495, 1500, 1503, 1505, 1509, 1512, 1517, 1522, 1530, 1532, 1533, 1535, 1540, 1546, 1547, 1549, 1552, 1556, 1557, 1560, 1562, 1569, 1571, 1577, 1583, 1584, 1589, 1590, 1592, 1598, 1599, 1601, 1604, 1605, 1613, 1616, 1617, 1623, 1626, 1628, 1629, 1631, 1635, 1636, 1643, 1650, 1651, 1652, 1654, 1656, 1657, 1667, 1674, 1677, 1680, 1681, 1689, 1695, 1696, 1698, 1700, 1707, 1714, 1715, 1716, 1717, 1718, 1720, 1721, 1724, 1726, 1728, 1734, 1737, 1738, 1741, 1744, 1746, 1749, 1750, 1753, 1757, 1763, 1766, 1767, 1768, 1769, 1770, 1781, 1785, 1789, 1794, 1797, 1799, 1805, 1808, 1809, 1814, 1816, 1818, 1820, 1832, 1835, 1837, 1839, 1851, 1854, 1859, 1864, 1869, 1870, 1871, 1876, 1877, 1881, 1885, 1888, 1891, 1893, 1894, 1901, 1902, 1907, 1916, 1917, 1919, 1920, 1922, 1923, 1928, 1929, 1930, 1931, 1935, 1938, 1939, 1940, 1944, 1946, 1949, 1950, 1959, 1960, 1962, 1963, 1966, 1967, 1968, 1970, 1980, 1987, 1991, 1994, 1995, 2000, 2004, 2009, 2010, 2013, 2015, 2018, 2019, 2025, 2028, 2031, 2032, 2037, 2041, 2043, 2044, 2047, 2056, 2063, 2064, 2067, 2072, 2073, 2077, 2080, 2083, 2084, 2087, 2089, 2090, 2091, 2096, 2099, 2111, 2113, 2114, 2123, 2128, 2135, 2139, 2140, 2142, 2143, 2146, 2147, 2149, 2164, 2166, 2170, 2178, 2180, 2183, 2189, 2190, 2191, 2192, 2195, 2199, 2201, 2203, 2210, 2212, 2215, 2220, 2228, 2231, 2232, 2237, 2240, 2241, 2248, 2249, 2253, 2257, 2258, 2261, 2263, 2276, 2277, 2279, 2280, 2282, 2292, 2295, 2298, 2300, 2318, 2319, 2322, 2326, 2327, 2328, 2329, 2331, 2333, 2334, 2335, 2336, 2337, 2348, 2350, 2351, 2355, 2356, 2368, 2371, 2372, 2374, 2375, 2378, 2379, 2381, 2384, 2386, 2387, 2389, 2395, 2397, 2401, 2402, 2403, 2406, 2414, 2417, 2424, 2425, 2431, 2433, 2434, 2437, 2439, 2440, 2441, 2446, 2451, 2457, 2467, 2468, 2471, 2472, 2473, 2475, 2485, 2487, 2491, 2493, 2496, 2502, 2503, 2510, 2514, 2517, 2521, 2522, 2527, 2528, 2538, 2542, 2544, 2550, 2551, 2552, 2554, 2557, 2560, 2565, 2569, 2571, 2576, 2580, 2585, 2587, 2591, 2592, 2594, 2599, 2603, 2604, 2606, 2610, 2616, 2617, 2619, 2632, 2633, 2634, 2638, 2645, 2647, 2650, 2664, 2665, 2668, 2669, 2670, 2673, 2675, 2676, 2677, 2678, 2687, 2690, 2691, 2692, 2695, 2697, 2700, 2705, 2706, 2709, 2716, 2719, 2724, 2726, 2727, 2728, 2731, 2733, 2734, 2739, 2743, 2744, 2750, 2764, 2765, 2768, 2769, 2770, 2773, 2775, 2776, 2778, 2779, 2789, 2792, 2794, 2795, 2808, 2813, 2819, 2820, 2829, 2831, 2833, 2834, 2835, 2836, 2838, 2839, 2841, 2843, 2845, 2848, 2854, 2856, 2861, 2867, 2870, 2872, 2875, 2878, 2881, 2883, 2887, 2892, 2895, 2896, 2897, 2898, 2909, 2910, 2912, 2913, 2914, 2917, 2923, 2928, 2934, 2937, 2939, 2949, 2950, 2952, 2953, 2955, 2959, 2966, 2967, 2968, 2970, 2971, 2972, 2974, 2982, 2991, 3001, 3008, 3018, 3020, 3028, 3033, 3034, 3043, 3046, 3048, 3050, 3055, 3056, 3059, 3060, 3068, 3070, 3075, 3079, 3087, 3088, 3096, 3101, 3106, 3110, 3111, 3114, 3115, 3116, 3117, 3119, 3122, 3124, 3125, 3128, 3133, 3135, 3136]
sequences = [x for x in all_data_dict.keys() if int(x.split('_')[0]) in auto_ids]
tr,v,te = du.get_tt_datasets(sequences)
194/6: print(all_data_dict)
194/7:
auto_ids = [2, 3, 4, 7, 11, 14, 23, 26, 27, 28, 30, 31, 33, 38, 39, 53, 55, 57, 59, 61, 65, 66, 70, 71, 73, 82, 101, 104, 105, 106, 109, 119, 131, 144, 146, 149, 150, 157, 159, 160, 163, 164, 165, 171, 172, 176, 180, 183, 185, 187, 188, 193, 201, 204, 205, 219, 223, 226, 229, 230, 233, 237, 242, 243, 245, 251, 252, 256, 258, 261, 267, 270, 272, 274, 276, 278, 279, 282, 283, 285, 289, 290, 293, 300, 301, 304, 305, 306, 314, 315, 321, 326, 327, 330, 331, 335, 337, 338, 342, 345, 347, 350, 358, 363, 367, 368, 373, 375, 376, 380, 381, 382, 383, 384, 386, 392, 393, 397, 399, 406, 407, 410, 414, 421, 422, 429, 435, 437, 446, 449, 453, 454, 458, 460, 467, 474, 484, 487, 493, 494, 498, 501, 502, 503, 512, 518, 523, 524, 526, 527, 530, 536, 537, 541, 545, 546, 551, 553, 555, 556, 560, 561, 563, 564, 567, 573, 575, 580, 581, 585, 590, 592, 595, 599, 600, 601, 608, 616, 620, 625, 627, 628, 632, 644, 646, 649, 653, 654, 660, 661, 664, 667, 668, 672, 679, 681, 682, 683, 697, 700, 701, 703, 704, 706, 707, 708, 710, 711, 715, 720, 728, 730, 733, 734, 737, 746, 747, 749, 750, 751, 756, 767, 779, 781, 784, 786, 788, 789, 799, 800, 806, 807, 811, 812, 820, 821, 825, 826, 827, 828, 835, 837, 840, 841, 844, 846, 848, 851, 852, 860, 863, 864, 865, 866, 868, 870, 872, 878, 881, 884, 887, 888, 889, 890, 894, 896, 902, 903, 904, 909, 911, 914, 917, 920, 923, 924, 935, 942, 946, 948, 950, 953, 954, 963, 966, 968, 971, 980, 987, 990, 991, 992, 993, 995, 997, 999, 1001, 1005, 1006, 1007, 1009, 1011, 1012, 1015, 1018, 1019, 1024, 1028, 1040, 1041, 1043, 1052, 1053, 1057, 1060, 1070, 1074, 1079, 1083, 1086, 1087, 1088, 1090, 1092, 1093, 1095, 1096, 1097, 1099, 1100, 1102, 1103, 1104, 1105, 1106, 1112, 1115, 1120, 1123, 1124, 1126, 1130, 1134, 1136, 1138, 1140, 1142, 1145, 1148, 1151, 1152, 1174, 1178, 1183, 1185, 1186, 1187, 1195, 1206, 1209, 1210, 1214, 1216, 1223, 1233, 1235, 1236, 1237, 1243, 1250, 1251, 1259, 1264, 1269, 1273, 1278, 1279, 1280, 1293, 1295, 1298, 1302, 1303, 1304, 1307, 1309, 1312, 1317, 1318, 1322, 1328, 1333, 1337, 1342, 1344, 1350, 1352, 1359, 1371, 1376, 1378, 1379, 1383, 1392, 1393, 1397, 1399, 1400, 1401, 1404, 1406, 1407, 1408, 1409, 1411, 1426, 1429, 1441, 1442, 1443, 1444, 1447, 1454, 1462, 1466, 1469, 1473, 1474, 1479, 1485, 1486, 1489, 1493, 1495, 1500, 1503, 1505, 1509, 1512, 1517, 1522, 1530, 1532, 1533, 1535, 1540, 1546, 1547, 1549, 1552, 1556, 1557, 1560, 1562, 1569, 1571, 1577, 1583, 1584, 1589, 1590, 1592, 1598, 1599, 1601, 1604, 1605, 1613, 1616, 1617, 1623, 1626, 1628, 1629, 1631, 1635, 1636, 1643, 1650, 1651, 1652, 1654, 1656, 1657, 1667, 1674, 1677, 1680, 1681, 1689, 1695, 1696, 1698, 1700, 1707, 1714, 1715, 1716, 1717, 1718, 1720, 1721, 1724, 1726, 1728, 1734, 1737, 1738, 1741, 1744, 1746, 1749, 1750, 1753, 1757, 1763, 1766, 1767, 1768, 1769, 1770, 1781, 1785, 1789, 1794, 1797, 1799, 1805, 1808, 1809, 1814, 1816, 1818, 1820, 1832, 1835, 1837, 1839, 1851, 1854, 1859, 1864, 1869, 1870, 1871, 1876, 1877, 1881, 1885, 1888, 1891, 1893, 1894, 1901, 1902, 1907, 1916, 1917, 1919, 1920, 1922, 1923, 1928, 1929, 1930, 1931, 1935, 1938, 1939, 1940, 1944, 1946, 1949, 1950, 1959, 1960, 1962, 1963, 1966, 1967, 1968, 1970, 1980, 1987, 1991, 1994, 1995, 2000, 2004, 2009, 2010, 2013, 2015, 2018, 2019, 2025, 2028, 2031, 2032, 2037, 2041, 2043, 2044, 2047, 2056, 2063, 2064, 2067, 2072, 2073, 2077, 2080, 2083, 2084, 2087, 2089, 2090, 2091, 2096, 2099, 2111, 2113, 2114, 2123, 2128, 2135, 2139, 2140, 2142, 2143, 2146, 2147, 2149, 2164, 2166, 2170, 2178, 2180, 2183, 2189, 2190, 2191, 2192, 2195, 2199, 2201, 2203, 2210, 2212, 2215, 2220, 2228, 2231, 2232, 2237, 2240, 2241, 2248, 2249, 2253, 2257, 2258, 2261, 2263, 2276, 2277, 2279, 2280, 2282, 2292, 2295, 2298, 2300, 2318, 2319, 2322, 2326, 2327, 2328, 2329, 2331, 2333, 2334, 2335, 2336, 2337, 2348, 2350, 2351, 2355, 2356, 2368, 2371, 2372, 2374, 2375, 2378, 2379, 2381, 2384, 2386, 2387, 2389, 2395, 2397, 2401, 2402, 2403, 2406, 2414, 2417, 2424, 2425, 2431, 2433, 2434, 2437, 2439, 2440, 2441, 2446, 2451, 2457, 2467, 2468, 2471, 2472, 2473, 2475, 2485, 2487, 2491, 2493, 2496, 2502, 2503, 2510, 2514, 2517, 2521, 2522, 2527, 2528, 2538, 2542, 2544, 2550, 2551, 2552, 2554, 2557, 2560, 2565, 2569, 2571, 2576, 2580, 2585, 2587, 2591, 2592, 2594, 2599, 2603, 2604, 2606, 2610, 2616, 2617, 2619, 2632, 2633, 2634, 2638, 2645, 2647, 2650, 2664, 2665, 2668, 2669, 2670, 2673, 2675, 2676, 2677, 2678, 2687, 2690, 2691, 2692, 2695, 2697, 2700, 2705, 2706, 2709, 2716, 2719, 2724, 2726, 2727, 2728, 2731, 2733, 2734, 2739, 2743, 2744, 2750, 2764, 2765, 2768, 2769, 2770, 2773, 2775, 2776, 2778, 2779, 2789, 2792, 2794, 2795, 2808, 2813, 2819, 2820, 2829, 2831, 2833, 2834, 2835, 2836, 2838, 2839, 2841, 2843, 2845, 2848, 2854, 2856, 2861, 2867, 2870, 2872, 2875, 2878, 2881, 2883, 2887, 2892, 2895, 2896, 2897, 2898, 2909, 2910, 2912, 2913, 2914, 2917, 2923, 2928, 2934, 2937, 2939, 2949, 2950, 2952, 2953, 2955, 2959, 2966, 2967, 2968, 2970, 2971, 2972, 2974, 2982, 2991, 3001, 3008, 3018, 3020, 3028, 3033, 3034, 3043, 3046, 3048, 3050, 3055, 3056, 3059, 3060, 3068, 3070, 3075, 3079, 3087, 3088, 3096, 3101, 3106, 3110, 3111, 3114, 3115, 3116, 3117, 3119, 3122, 3124, 3125, 3128, 3133, 3135, 3136]
sequences = [x+'.npy' for x in all_data_dict.keys() if int(x.split('_')[0]) in auto_ids]
tr,v,te = du.get_tt_datasets(sequences)
194/8:
auto_ids = [2, 3, 4, 7, 11, 14, 23, 26, 27, 28, 30, 31, 33, 38, 39, 53, 55, 57, 59, 61, 65, 66, 70, 71, 73, 82, 101, 104, 105, 106, 109, 119, 131, 144, 146, 149, 150, 157, 159, 160, 163, 164, 165, 171, 172, 176, 180, 183, 185, 187, 188, 193, 201, 204, 205, 219, 223, 226, 229, 230, 233, 237, 242, 243, 245, 251, 252, 256, 258, 261, 267, 270, 272, 274, 276, 278, 279, 282, 283, 285, 289, 290, 293, 300, 301, 304, 305, 306, 314, 315, 321, 326, 327, 330, 331, 335, 337, 338, 342, 345, 347, 350, 358, 363, 367, 368, 373, 375, 376, 380, 381, 382, 383, 384, 386, 392, 393, 397, 399, 406, 407, 410, 414, 421, 422, 429, 435, 437, 446, 449, 453, 454, 458, 460, 467, 474, 484, 487, 493, 494, 498, 501, 502, 503, 512, 518, 523, 524, 526, 527, 530, 536, 537, 541, 545, 546, 551, 553, 555, 556, 560, 561, 563, 564, 567, 573, 575, 580, 581, 585, 590, 592, 595, 599, 600, 601, 608, 616, 620, 625, 627, 628, 632, 644, 646, 649, 653, 654, 660, 661, 664, 667, 668, 672, 679, 681, 682, 683, 697, 700, 701, 703, 704, 706, 707, 708, 710, 711, 715, 720, 728, 730, 733, 734, 737, 746, 747, 749, 750, 751, 756, 767, 779, 781, 784, 786, 788, 789, 799, 800, 806, 807, 811, 812, 820, 821, 825, 826, 827, 828, 835, 837, 840, 841, 844, 846, 848, 851, 852, 860, 863, 864, 865, 866, 868, 870, 872, 878, 881, 884, 887, 888, 889, 890, 894, 896, 902, 903, 904, 909, 911, 914, 917, 920, 923, 924, 935, 942, 946, 948, 950, 953, 954, 963, 966, 968, 971, 980, 987, 990, 991, 992, 993, 995, 997, 999, 1001, 1005, 1006, 1007, 1009, 1011, 1012, 1015, 1018, 1019, 1024, 1028, 1040, 1041, 1043, 1052, 1053, 1057, 1060, 1070, 1074, 1079, 1083, 1086, 1087, 1088, 1090, 1092, 1093, 1095, 1096, 1097, 1099, 1100, 1102, 1103, 1104, 1105, 1106, 1112, 1115, 1120, 1123, 1124, 1126, 1130, 1134, 1136, 1138, 1140, 1142, 1145, 1148, 1151, 1152, 1174, 1178, 1183, 1185, 1186, 1187, 1195, 1206, 1209, 1210, 1214, 1216, 1223, 1233, 1235, 1236, 1237, 1243, 1250, 1251, 1259, 1264, 1269, 1273, 1278, 1279, 1280, 1293, 1295, 1298, 1302, 1303, 1304, 1307, 1309, 1312, 1317, 1318, 1322, 1328, 1333, 1337, 1342, 1344, 1350, 1352, 1359, 1371, 1376, 1378, 1379, 1383, 1392, 1393, 1397, 1399, 1400, 1401, 1404, 1406, 1407, 1408, 1409, 1411, 1426, 1429, 1441, 1442, 1443, 1444, 1447, 1454, 1462, 1466, 1469, 1473, 1474, 1479, 1485, 1486, 1489, 1493, 1495, 1500, 1503, 1505, 1509, 1512, 1517, 1522, 1530, 1532, 1533, 1535, 1540, 1546, 1547, 1549, 1552, 1556, 1557, 1560, 1562, 1569, 1571, 1577, 1583, 1584, 1589, 1590, 1592, 1598, 1599, 1601, 1604, 1605, 1613, 1616, 1617, 1623, 1626, 1628, 1629, 1631, 1635, 1636, 1643, 1650, 1651, 1652, 1654, 1656, 1657, 1667, 1674, 1677, 1680, 1681, 1689, 1695, 1696, 1698, 1700, 1707, 1714, 1715, 1716, 1717, 1718, 1720, 1721, 1724, 1726, 1728, 1734, 1737, 1738, 1741, 1744, 1746, 1749, 1750, 1753, 1757, 1763, 1766, 1767, 1768, 1769, 1770, 1781, 1785, 1789, 1794, 1797, 1799, 1805, 1808, 1809, 1814, 1816, 1818, 1820, 1832, 1835, 1837, 1839, 1851, 1854, 1859, 1864, 1869, 1870, 1871, 1876, 1877, 1881, 1885, 1888, 1891, 1893, 1894, 1901, 1902, 1907, 1916, 1917, 1919, 1920, 1922, 1923, 1928, 1929, 1930, 1931, 1935, 1938, 1939, 1940, 1944, 1946, 1949, 1950, 1959, 1960, 1962, 1963, 1966, 1967, 1968, 1970, 1980, 1987, 1991, 1994, 1995, 2000, 2004, 2009, 2010, 2013, 2015, 2018, 2019, 2025, 2028, 2031, 2032, 2037, 2041, 2043, 2044, 2047, 2056, 2063, 2064, 2067, 2072, 2073, 2077, 2080, 2083, 2084, 2087, 2089, 2090, 2091, 2096, 2099, 2111, 2113, 2114, 2123, 2128, 2135, 2139, 2140, 2142, 2143, 2146, 2147, 2149, 2164, 2166, 2170, 2178, 2180, 2183, 2189, 2190, 2191, 2192, 2195, 2199, 2201, 2203, 2210, 2212, 2215, 2220, 2228, 2231, 2232, 2237, 2240, 2241, 2248, 2249, 2253, 2257, 2258, 2261, 2263, 2276, 2277, 2279, 2280, 2282, 2292, 2295, 2298, 2300, 2318, 2319, 2322, 2326, 2327, 2328, 2329, 2331, 2333, 2334, 2335, 2336, 2337, 2348, 2350, 2351, 2355, 2356, 2368, 2371, 2372, 2374, 2375, 2378, 2379, 2381, 2384, 2386, 2387, 2389, 2395, 2397, 2401, 2402, 2403, 2406, 2414, 2417, 2424, 2425, 2431, 2433, 2434, 2437, 2439, 2440, 2441, 2446, 2451, 2457, 2467, 2468, 2471, 2472, 2473, 2475, 2485, 2487, 2491, 2493, 2496, 2502, 2503, 2510, 2514, 2517, 2521, 2522, 2527, 2528, 2538, 2542, 2544, 2550, 2551, 2552, 2554, 2557, 2560, 2565, 2569, 2571, 2576, 2580, 2585, 2587, 2591, 2592, 2594, 2599, 2603, 2604, 2606, 2610, 2616, 2617, 2619, 2632, 2633, 2634, 2638, 2645, 2647, 2650, 2664, 2665, 2668, 2669, 2670, 2673, 2675, 2676, 2677, 2678, 2687, 2690, 2691, 2692, 2695, 2697, 2700, 2705, 2706, 2709, 2716, 2719, 2724, 2726, 2727, 2728, 2731, 2733, 2734, 2739, 2743, 2744, 2750, 2764, 2765, 2768, 2769, 2770, 2773, 2775, 2776, 2778, 2779, 2789, 2792, 2794, 2795, 2808, 2813, 2819, 2820, 2829, 2831, 2833, 2834, 2835, 2836, 2838, 2839, 2841, 2843, 2845, 2848, 2854, 2856, 2861, 2867, 2870, 2872, 2875, 2878, 2881, 2883, 2887, 2892, 2895, 2896, 2897, 2898, 2909, 2910, 2912, 2913, 2914, 2917, 2923, 2928, 2934, 2937, 2939, 2949, 2950, 2952, 2953, 2955, 2959, 2966, 2967, 2968, 2970, 2971, 2972, 2974, 2982, 2991, 3001, 3008, 3018, 3020, 3028, 3033, 3034, 3043, 3046, 3048, 3050, 3055, 3056, 3059, 3060, 3068, 3070, 3075, 3079, 3087, 3088, 3096, 3101, 3106, 3110, 3111, 3114, 3115, 3116, 3117, 3119, 3122, 3124, 3125, 3128, 3133, 3135, 3136]
sequences = [x+'.npy' for x in all_data_dict.keys() if int(x.split('_')[0]) in auto_ids]
sequences
# tr,v,te = du.get_tt_datasets(sequences)
195/1:
import utils
import data_utils as du
195/2: all_data_dict = utils.pkl_load('data/label_dict.pkl')
195/3:
auto_ids = [2, 3, 4, 7, 11, 14, 23, 26, 27, 28, 30, 31, 33, 38, 39, 53, 55, 57, 59, 61, 65, 66, 70, 71, 73, 82, 101, 104, 105, 106, 109, 119, 131, 144, 146, 149, 150, 157, 159, 160, 163, 164, 165, 171, 172, 176, 180, 183, 185, 187, 188, 193, 201, 204, 205, 219, 223, 226, 229, 230, 233, 237, 242, 243, 245, 251, 252, 256, 258, 261, 267, 270, 272, 274, 276, 278, 279, 282, 283, 285, 289, 290, 293, 300, 301, 304, 305, 306, 314, 315, 321, 326, 327, 330, 331, 335, 337, 338, 342, 345, 347, 350, 358, 363, 367, 368, 373, 375, 376, 380, 381, 382, 383, 384, 386, 392, 393, 397, 399, 406, 407, 410, 414, 421, 422, 429, 435, 437, 446, 449, 453, 454, 458, 460, 467, 474, 484, 487, 493, 494, 498, 501, 502, 503, 512, 518, 523, 524, 526, 527, 530, 536, 537, 541, 545, 546, 551, 553, 555, 556, 560, 561, 563, 564, 567, 573, 575, 580, 581, 585, 590, 592, 595, 599, 600, 601, 608, 616, 620, 625, 627, 628, 632, 644, 646, 649, 653, 654, 660, 661, 664, 667, 668, 672, 679, 681, 682, 683, 697, 700, 701, 703, 704, 706, 707, 708, 710, 711, 715, 720, 728, 730, 733, 734, 737, 746, 747, 749, 750, 751, 756, 767, 779, 781, 784, 786, 788, 789, 799, 800, 806, 807, 811, 812, 820, 821, 825, 826, 827, 828, 835, 837, 840, 841, 844, 846, 848, 851, 852, 860, 863, 864, 865, 866, 868, 870, 872, 878, 881, 884, 887, 888, 889, 890, 894, 896, 902, 903, 904, 909, 911, 914, 917, 920, 923, 924, 935, 942, 946, 948, 950, 953, 954, 963, 966, 968, 971, 980, 987, 990, 991, 992, 993, 995, 997, 999, 1001, 1005, 1006, 1007, 1009, 1011, 1012, 1015, 1018, 1019, 1024, 1028, 1040, 1041, 1043, 1052, 1053, 1057, 1060, 1070, 1074, 1079, 1083, 1086, 1087, 1088, 1090, 1092, 1093, 1095, 1096, 1097, 1099, 1100, 1102, 1103, 1104, 1105, 1106, 1112, 1115, 1120, 1123, 1124, 1126, 1130, 1134, 1136, 1138, 1140, 1142, 1145, 1148, 1151, 1152, 1174, 1178, 1183, 1185, 1186, 1187, 1195, 1206, 1209, 1210, 1214, 1216, 1223, 1233, 1235, 1236, 1237, 1243, 1250, 1251, 1259, 1264, 1269, 1273, 1278, 1279, 1280, 1293, 1295, 1298, 1302, 1303, 1304, 1307, 1309, 1312, 1317, 1318, 1322, 1328, 1333, 1337, 1342, 1344, 1350, 1352, 1359, 1371, 1376, 1378, 1379, 1383, 1392, 1393, 1397, 1399, 1400, 1401, 1404, 1406, 1407, 1408, 1409, 1411, 1426, 1429, 1441, 1442, 1443, 1444, 1447, 1454, 1462, 1466, 1469, 1473, 1474, 1479, 1485, 1486, 1489, 1493, 1495, 1500, 1503, 1505, 1509, 1512, 1517, 1522, 1530, 1532, 1533, 1535, 1540, 1546, 1547, 1549, 1552, 1556, 1557, 1560, 1562, 1569, 1571, 1577, 1583, 1584, 1589, 1590, 1592, 1598, 1599, 1601, 1604, 1605, 1613, 1616, 1617, 1623, 1626, 1628, 1629, 1631, 1635, 1636, 1643, 1650, 1651, 1652, 1654, 1656, 1657, 1667, 1674, 1677, 1680, 1681, 1689, 1695, 1696, 1698, 1700, 1707, 1714, 1715, 1716, 1717, 1718, 1720, 1721, 1724, 1726, 1728, 1734, 1737, 1738, 1741, 1744, 1746, 1749, 1750, 1753, 1757, 1763, 1766, 1767, 1768, 1769, 1770, 1781, 1785, 1789, 1794, 1797, 1799, 1805, 1808, 1809, 1814, 1816, 1818, 1820, 1832, 1835, 1837, 1839, 1851, 1854, 1859, 1864, 1869, 1870, 1871, 1876, 1877, 1881, 1885, 1888, 1891, 1893, 1894, 1901, 1902, 1907, 1916, 1917, 1919, 1920, 1922, 1923, 1928, 1929, 1930, 1931, 1935, 1938, 1939, 1940, 1944, 1946, 1949, 1950, 1959, 1960, 1962, 1963, 1966, 1967, 1968, 1970, 1980, 1987, 1991, 1994, 1995, 2000, 2004, 2009, 2010, 2013, 2015, 2018, 2019, 2025, 2028, 2031, 2032, 2037, 2041, 2043, 2044, 2047, 2056, 2063, 2064, 2067, 2072, 2073, 2077, 2080, 2083, 2084, 2087, 2089, 2090, 2091, 2096, 2099, 2111, 2113, 2114, 2123, 2128, 2135, 2139, 2140, 2142, 2143, 2146, 2147, 2149, 2164, 2166, 2170, 2178, 2180, 2183, 2189, 2190, 2191, 2192, 2195, 2199, 2201, 2203, 2210, 2212, 2215, 2220, 2228, 2231, 2232, 2237, 2240, 2241, 2248, 2249, 2253, 2257, 2258, 2261, 2263, 2276, 2277, 2279, 2280, 2282, 2292, 2295, 2298, 2300, 2318, 2319, 2322, 2326, 2327, 2328, 2329, 2331, 2333, 2334, 2335, 2336, 2337, 2348, 2350, 2351, 2355, 2356, 2368, 2371, 2372, 2374, 2375, 2378, 2379, 2381, 2384, 2386, 2387, 2389, 2395, 2397, 2401, 2402, 2403, 2406, 2414, 2417, 2424, 2425, 2431, 2433, 2434, 2437, 2439, 2440, 2441, 2446, 2451, 2457, 2467, 2468, 2471, 2472, 2473, 2475, 2485, 2487, 2491, 2493, 2496, 2502, 2503, 2510, 2514, 2517, 2521, 2522, 2527, 2528, 2538, 2542, 2544, 2550, 2551, 2552, 2554, 2557, 2560, 2565, 2569, 2571, 2576, 2580, 2585, 2587, 2591, 2592, 2594, 2599, 2603, 2604, 2606, 2610, 2616, 2617, 2619, 2632, 2633, 2634, 2638, 2645, 2647, 2650, 2664, 2665, 2668, 2669, 2670, 2673, 2675, 2676, 2677, 2678, 2687, 2690, 2691, 2692, 2695, 2697, 2700, 2705, 2706, 2709, 2716, 2719, 2724, 2726, 2727, 2728, 2731, 2733, 2734, 2739, 2743, 2744, 2750, 2764, 2765, 2768, 2769, 2770, 2773, 2775, 2776, 2778, 2779, 2789, 2792, 2794, 2795, 2808, 2813, 2819, 2820, 2829, 2831, 2833, 2834, 2835, 2836, 2838, 2839, 2841, 2843, 2845, 2848, 2854, 2856, 2861, 2867, 2870, 2872, 2875, 2878, 2881, 2883, 2887, 2892, 2895, 2896, 2897, 2898, 2909, 2910, 2912, 2913, 2914, 2917, 2923, 2928, 2934, 2937, 2939, 2949, 2950, 2952, 2953, 2955, 2959, 2966, 2967, 2968, 2970, 2971, 2972, 2974, 2982, 2991, 3001, 3008, 3018, 3020, 3028, 3033, 3034, 3043, 3046, 3048, 3050, 3055, 3056, 3059, 3060, 3068, 3070, 3075, 3079, 3087, 3088, 3096, 3101, 3106, 3110, 3111, 3114, 3115, 3116, 3117, 3119, 3122, 3124, 3125, 3128, 3133, 3135, 3136]
sequences = [x+'.npy' for x in all_data_dict.keys() if int(x.split('_')[0]) in auto_ids]
tr,v,te = du.get_tt_datasets(sequences)
196/1:
import utils
import data_utils as du
196/2: all_data_dict = utils.pkl_load('data/label_dict.pkl')
196/3:
auto_ids = [2, 3, 4, 7, 11, 14, 23, 26, 27, 28, 30, 31, 33, 38, 39, 53, 55, 57, 59, 61, 65, 66, 70, 71, 73, 82, 101, 104, 105, 106, 109, 119, 131, 144, 146, 149, 150, 157, 159, 160, 163, 164, 165, 171, 172, 176, 180, 183, 185, 187, 188, 193, 201, 204, 205, 219, 223, 226, 229, 230, 233, 237, 242, 243, 245, 251, 252, 256, 258, 261, 267, 270, 272, 274, 276, 278, 279, 282, 283, 285, 289, 290, 293, 300, 301, 304, 305, 306, 314, 315, 321, 326, 327, 330, 331, 335, 337, 338, 342, 345, 347, 350, 358, 363, 367, 368, 373, 375, 376, 380, 381, 382, 383, 384, 386, 392, 393, 397, 399, 406, 407, 410, 414, 421, 422, 429, 435, 437, 446, 449, 453, 454, 458, 460, 467, 474, 484, 487, 493, 494, 498, 501, 502, 503, 512, 518, 523, 524, 526, 527, 530, 536, 537, 541, 545, 546, 551, 553, 555, 556, 560, 561, 563, 564, 567, 573, 575, 580, 581, 585, 590, 592, 595, 599, 600, 601, 608, 616, 620, 625, 627, 628, 632, 644, 646, 649, 653, 654, 660, 661, 664, 667, 668, 672, 679, 681, 682, 683, 697, 700, 701, 703, 704, 706, 707, 708, 710, 711, 715, 720, 728, 730, 733, 734, 737, 746, 747, 749, 750, 751, 756, 767, 779, 781, 784, 786, 788, 789, 799, 800, 806, 807, 811, 812, 820, 821, 825, 826, 827, 828, 835, 837, 840, 841, 844, 846, 848, 851, 852, 860, 863, 864, 865, 866, 868, 870, 872, 878, 881, 884, 887, 888, 889, 890, 894, 896, 902, 903, 904, 909, 911, 914, 917, 920, 923, 924, 935, 942, 946, 948, 950, 953, 954, 963, 966, 968, 971, 980, 987, 990, 991, 992, 993, 995, 997, 999, 1001, 1005, 1006, 1007, 1009, 1011, 1012, 1015, 1018, 1019, 1024, 1028, 1040, 1041, 1043, 1052, 1053, 1057, 1060, 1070, 1074, 1079, 1083, 1086, 1087, 1088, 1090, 1092, 1093, 1095, 1096, 1097, 1099, 1100, 1102, 1103, 1104, 1105, 1106, 1112, 1115, 1120, 1123, 1124, 1126, 1130, 1134, 1136, 1138, 1140, 1142, 1145, 1148, 1151, 1152, 1174, 1178, 1183, 1185, 1186, 1187, 1195, 1206, 1209, 1210, 1214, 1216, 1223, 1233, 1235, 1236, 1237, 1243, 1250, 1251, 1259, 1264, 1269, 1273, 1278, 1279, 1280, 1293, 1295, 1298, 1302, 1303, 1304, 1307, 1309, 1312, 1317, 1318, 1322, 1328, 1333, 1337, 1342, 1344, 1350, 1352, 1359, 1371, 1376, 1378, 1379, 1383, 1392, 1393, 1397, 1399, 1400, 1401, 1404, 1406, 1407, 1408, 1409, 1411, 1426, 1429, 1441, 1442, 1443, 1444, 1447, 1454, 1462, 1466, 1469, 1473, 1474, 1479, 1485, 1486, 1489, 1493, 1495, 1500, 1503, 1505, 1509, 1512, 1517, 1522, 1530, 1532, 1533, 1535, 1540, 1546, 1547, 1549, 1552, 1556, 1557, 1560, 1562, 1569, 1571, 1577, 1583, 1584, 1589, 1590, 1592, 1598, 1599, 1601, 1604, 1605, 1613, 1616, 1617, 1623, 1626, 1628, 1629, 1631, 1635, 1636, 1643, 1650, 1651, 1652, 1654, 1656, 1657, 1667, 1674, 1677, 1680, 1681, 1689, 1695, 1696, 1698, 1700, 1707, 1714, 1715, 1716, 1717, 1718, 1720, 1721, 1724, 1726, 1728, 1734, 1737, 1738, 1741, 1744, 1746, 1749, 1750, 1753, 1757, 1763, 1766, 1767, 1768, 1769, 1770, 1781, 1785, 1789, 1794, 1797, 1799, 1805, 1808, 1809, 1814, 1816, 1818, 1820, 1832, 1835, 1837, 1839, 1851, 1854, 1859, 1864, 1869, 1870, 1871, 1876, 1877, 1881, 1885, 1888, 1891, 1893, 1894, 1901, 1902, 1907, 1916, 1917, 1919, 1920, 1922, 1923, 1928, 1929, 1930, 1931, 1935, 1938, 1939, 1940, 1944, 1946, 1949, 1950, 1959, 1960, 1962, 1963, 1966, 1967, 1968, 1970, 1980, 1987, 1991, 1994, 1995, 2000, 2004, 2009, 2010, 2013, 2015, 2018, 2019, 2025, 2028, 2031, 2032, 2037, 2041, 2043, 2044, 2047, 2056, 2063, 2064, 2067, 2072, 2073, 2077, 2080, 2083, 2084, 2087, 2089, 2090, 2091, 2096, 2099, 2111, 2113, 2114, 2123, 2128, 2135, 2139, 2140, 2142, 2143, 2146, 2147, 2149, 2164, 2166, 2170, 2178, 2180, 2183, 2189, 2190, 2191, 2192, 2195, 2199, 2201, 2203, 2210, 2212, 2215, 2220, 2228, 2231, 2232, 2237, 2240, 2241, 2248, 2249, 2253, 2257, 2258, 2261, 2263, 2276, 2277, 2279, 2280, 2282, 2292, 2295, 2298, 2300, 2318, 2319, 2322, 2326, 2327, 2328, 2329, 2331, 2333, 2334, 2335, 2336, 2337, 2348, 2350, 2351, 2355, 2356, 2368, 2371, 2372, 2374, 2375, 2378, 2379, 2381, 2384, 2386, 2387, 2389, 2395, 2397, 2401, 2402, 2403, 2406, 2414, 2417, 2424, 2425, 2431, 2433, 2434, 2437, 2439, 2440, 2441, 2446, 2451, 2457, 2467, 2468, 2471, 2472, 2473, 2475, 2485, 2487, 2491, 2493, 2496, 2502, 2503, 2510, 2514, 2517, 2521, 2522, 2527, 2528, 2538, 2542, 2544, 2550, 2551, 2552, 2554, 2557, 2560, 2565, 2569, 2571, 2576, 2580, 2585, 2587, 2591, 2592, 2594, 2599, 2603, 2604, 2606, 2610, 2616, 2617, 2619, 2632, 2633, 2634, 2638, 2645, 2647, 2650, 2664, 2665, 2668, 2669, 2670, 2673, 2675, 2676, 2677, 2678, 2687, 2690, 2691, 2692, 2695, 2697, 2700, 2705, 2706, 2709, 2716, 2719, 2724, 2726, 2727, 2728, 2731, 2733, 2734, 2739, 2743, 2744, 2750, 2764, 2765, 2768, 2769, 2770, 2773, 2775, 2776, 2778, 2779, 2789, 2792, 2794, 2795, 2808, 2813, 2819, 2820, 2829, 2831, 2833, 2834, 2835, 2836, 2838, 2839, 2841, 2843, 2845, 2848, 2854, 2856, 2861, 2867, 2870, 2872, 2875, 2878, 2881, 2883, 2887, 2892, 2895, 2896, 2897, 2898, 2909, 2910, 2912, 2913, 2914, 2917, 2923, 2928, 2934, 2937, 2939, 2949, 2950, 2952, 2953, 2955, 2959, 2966, 2967, 2968, 2970, 2971, 2972, 2974, 2982, 2991, 3001, 3008, 3018, 3020, 3028, 3033, 3034, 3043, 3046, 3048, 3050, 3055, 3056, 3059, 3060, 3068, 3070, 3075, 3079, 3087, 3088, 3096, 3101, 3106, 3110, 3111, 3114, 3115, 3116, 3117, 3119, 3122, 3124, 3125, 3128, 3133, 3135, 3136]
sequences = [x+'.npy' for x in all_data_dict.keys() if int(x.split('_')[0]) in auto_ids]
tr,v,te = du.get_tt_datasets(sequences)
196/4: tr
196/5:
auto_ids = [2, 3, 4, 7, 11, 14, 23, 26, 27, 28, 30, 31, 33, 38, 39, 53, 55, 57, 59, 61, 65, 66, 70, 71, 73, 82, 101, 104, 105, 106, 109, 119, 131, 144, 146, 149, 150, 157, 159, 160, 163, 164, 165, 171, 172, 176, 180, 183, 185, 187, 188, 193, 201, 204, 205, 219, 223, 226, 229, 230, 233, 237, 242, 243, 245, 251, 252, 256, 258, 261, 267, 270, 272, 274, 276, 278, 279, 282, 283, 285, 289, 290, 293, 300, 301, 304, 305, 306, 314, 315, 321, 326, 327, 330, 331, 335, 337, 338, 342, 345, 347, 350, 358, 363, 367, 368, 373, 375, 376, 380, 381, 382, 383, 384, 386, 392, 393, 397, 399, 406, 407, 410, 414, 421, 422, 429, 435, 437, 446, 449, 453, 454, 458, 460, 467, 474, 484, 487, 493, 494, 498, 501, 502, 503, 512, 518, 523, 524, 526, 527, 530, 536, 537, 541, 545, 546, 551, 553, 555, 556, 560, 561, 563, 564, 567, 573, 575, 580, 581, 585, 590, 592, 595, 599, 600, 601, 608, 616, 620, 625, 627, 628, 632, 644, 646, 649, 653, 654, 660, 661, 664, 667, 668, 672, 679, 681, 682, 683, 697, 700, 701, 703, 704, 706, 707, 708, 710, 711, 715, 720, 728, 730, 733, 734, 737, 746, 747, 749, 750, 751, 756, 767, 779, 781, 784, 786, 788, 789, 799, 800, 806, 807, 811, 812, 820, 821, 825, 826, 827, 828, 835, 837, 840, 841, 844, 846, 848, 851, 852, 860, 863, 864, 865, 866, 868, 870, 872, 878, 881, 884, 887, 888, 889, 890, 894, 896, 902, 903, 904, 909, 911, 914, 917, 920, 923, 924, 935, 942, 946, 948, 950, 953, 954, 963, 966, 968, 971, 980, 987, 990, 991, 992, 993, 995, 997, 999, 1001, 1005, 1006, 1007, 1009, 1011, 1012, 1015, 1018, 1019, 1024, 1028, 1040, 1041, 1043, 1052, 1053, 1057, 1060, 1070, 1074, 1079, 1083, 1086, 1087, 1088, 1090, 1092, 1093, 1095, 1096, 1097, 1099, 1100, 1102, 1103, 1104, 1105, 1106, 1112, 1115, 1120, 1123, 1124, 1126, 1130, 1134, 1136, 1138, 1140, 1142, 1145, 1148, 1151, 1152, 1174, 1178, 1183, 1185, 1186, 1187, 1195, 1206, 1209, 1210, 1214, 1216, 1223, 1233, 1235, 1236, 1237, 1243, 1250, 1251, 1259, 1264, 1269, 1273, 1278, 1279, 1280, 1293, 1295, 1298, 1302, 1303, 1304, 1307, 1309, 1312, 1317, 1318, 1322, 1328, 1333, 1337, 1342, 1344, 1350, 1352, 1359, 1371, 1376, 1378, 1379, 1383, 1392, 1393, 1397, 1399, 1400, 1401, 1404, 1406, 1407, 1408, 1409, 1411, 1426, 1429, 1441, 1442, 1443, 1444, 1447, 1454, 1462, 1466, 1469, 1473, 1474, 1479, 1485, 1486, 1489, 1493, 1495, 1500, 1503, 1505, 1509, 1512, 1517, 1522, 1530, 1532, 1533, 1535, 1540, 1546, 1547, 1549, 1552, 1556, 1557, 1560, 1562, 1569, 1571, 1577, 1583, 1584, 1589, 1590, 1592, 1598, 1599, 1601, 1604, 1605, 1613, 1616, 1617, 1623, 1626, 1628, 1629, 1631, 1635, 1636, 1643, 1650, 1651, 1652, 1654, 1656, 1657, 1667, 1674, 1677, 1680, 1681, 1689, 1695, 1696, 1698, 1700, 1707, 1714, 1715, 1716, 1717, 1718, 1720, 1721, 1724, 1726, 1728, 1734, 1737, 1738, 1741, 1744, 1746, 1749, 1750, 1753, 1757, 1763, 1766, 1767, 1768, 1769, 1770, 1781, 1785, 1789, 1794, 1797, 1799, 1805, 1808, 1809, 1814, 1816, 1818, 1820, 1832, 1835, 1837, 1839, 1851, 1854, 1859, 1864, 1869, 1870, 1871, 1876, 1877, 1881, 1885, 1888, 1891, 1893, 1894, 1901, 1902, 1907, 1916, 1917, 1919, 1920, 1922, 1923, 1928, 1929, 1930, 1931, 1935, 1938, 1939, 1940, 1944, 1946, 1949, 1950, 1959, 1960, 1962, 1963, 1966, 1967, 1968, 1970, 1980, 1987, 1991, 1994, 1995, 2000, 2004, 2009, 2010, 2013, 2015, 2018, 2019, 2025, 2028, 2031, 2032, 2037, 2041, 2043, 2044, 2047, 2056, 2063, 2064, 2067, 2072, 2073, 2077, 2080, 2083, 2084, 2087, 2089, 2090, 2091, 2096, 2099, 2111, 2113, 2114, 2123, 2128, 2135, 2139, 2140, 2142, 2143, 2146, 2147, 2149, 2164, 2166, 2170, 2178, 2180, 2183, 2189, 2190, 2191, 2192, 2195, 2199, 2201, 2203, 2210, 2212, 2215, 2220, 2228, 2231, 2232, 2237, 2240, 2241, 2248, 2249, 2253, 2257, 2258, 2261, 2263, 2276, 2277, 2279, 2280, 2282, 2292, 2295, 2298, 2300, 2318, 2319, 2322, 2326, 2327, 2328, 2329, 2331, 2333, 2334, 2335, 2336, 2337, 2348, 2350, 2351, 2355, 2356, 2368, 2371, 2372, 2374, 2375, 2378, 2379, 2381, 2384, 2386, 2387, 2389, 2395, 2397, 2401, 2402, 2403, 2406, 2414, 2417, 2424, 2425, 2431, 2433, 2434, 2437, 2439, 2440, 2441, 2446, 2451, 2457, 2467, 2468, 2471, 2472, 2473, 2475, 2485, 2487, 2491, 2493, 2496, 2502, 2503, 2510, 2514, 2517, 2521, 2522, 2527, 2528, 2538, 2542, 2544, 2550, 2551, 2552, 2554, 2557, 2560, 2565, 2569, 2571, 2576, 2580, 2585, 2587, 2591, 2592, 2594, 2599, 2603, 2604, 2606, 2610, 2616, 2617, 2619, 2632, 2633, 2634, 2638, 2645, 2647, 2650, 2664, 2665, 2668, 2669, 2670, 2673, 2675, 2676, 2677, 2678, 2687, 2690, 2691, 2692, 2695, 2697, 2700, 2705, 2706, 2709, 2716, 2719, 2724, 2726, 2727, 2728, 2731, 2733, 2734, 2739, 2743, 2744, 2750, 2764, 2765, 2768, 2769, 2770, 2773, 2775, 2776, 2778, 2779, 2789, 2792, 2794, 2795, 2808, 2813, 2819, 2820, 2829, 2831, 2833, 2834, 2835, 2836, 2838, 2839, 2841, 2843, 2845, 2848, 2854, 2856, 2861, 2867, 2870, 2872, 2875, 2878, 2881, 2883, 2887, 2892, 2895, 2896, 2897, 2898, 2909, 2910, 2912, 2913, 2914, 2917, 2923, 2928, 2934, 2937, 2939, 2949, 2950, 2952, 2953, 2955, 2959, 2966, 2967, 2968, 2970, 2971, 2972, 2974, 2982, 2991, 3001, 3008, 3018, 3020, 3028, 3033, 3034, 3043, 3046, 3048, 3050, 3055, 3056, 3059, 3060, 3068, 3070, 3075, 3079, 3087, 3088, 3096, 3101, 3106, 3110, 3111, 3114, 3115, 3116, 3117, 3119, 3122, 3124, 3125, 3128, 3133, 3135, 3136]
sequences = [x+'.npy' for x in all_data_dict.keys() if int(x.split('_')[0]) in auto_ids]
tr,v,te = du.get_tt_datasets(sequences)
fp = utils.get_filepath('datasets', 'xianling')
utils.pkl_dump(tr, os.path.join(fp,'train.dataset')), utils.pkl_dump(v, os.path.join(fp,'valid.dataset')), utils.pkl_dump(te, os.path.join(fp,'test.dataset'))
196/6:
auto_ids = [2, 3, 4, 7, 11, 14, 23, 26, 27, 28, 30, 31, 33, 38, 39, 53, 55, 57, 59, 61, 65, 66, 70, 71, 73, 82, 101, 104, 105, 106, 109, 119, 131, 144, 146, 149, 150, 157, 159, 160, 163, 164, 165, 171, 172, 176, 180, 183, 185, 187, 188, 193, 201, 204, 205, 219, 223, 226, 229, 230, 233, 237, 242, 243, 245, 251, 252, 256, 258, 261, 267, 270, 272, 274, 276, 278, 279, 282, 283, 285, 289, 290, 293, 300, 301, 304, 305, 306, 314, 315, 321, 326, 327, 330, 331, 335, 337, 338, 342, 345, 347, 350, 358, 363, 367, 368, 373, 375, 376, 380, 381, 382, 383, 384, 386, 392, 393, 397, 399, 406, 407, 410, 414, 421, 422, 429, 435, 437, 446, 449, 453, 454, 458, 460, 467, 474, 484, 487, 493, 494, 498, 501, 502, 503, 512, 518, 523, 524, 526, 527, 530, 536, 537, 541, 545, 546, 551, 553, 555, 556, 560, 561, 563, 564, 567, 573, 575, 580, 581, 585, 590, 592, 595, 599, 600, 601, 608, 616, 620, 625, 627, 628, 632, 644, 646, 649, 653, 654, 660, 661, 664, 667, 668, 672, 679, 681, 682, 683, 697, 700, 701, 703, 704, 706, 707, 708, 710, 711, 715, 720, 728, 730, 733, 734, 737, 746, 747, 749, 750, 751, 756, 767, 779, 781, 784, 786, 788, 789, 799, 800, 806, 807, 811, 812, 820, 821, 825, 826, 827, 828, 835, 837, 840, 841, 844, 846, 848, 851, 852, 860, 863, 864, 865, 866, 868, 870, 872, 878, 881, 884, 887, 888, 889, 890, 894, 896, 902, 903, 904, 909, 911, 914, 917, 920, 923, 924, 935, 942, 946, 948, 950, 953, 954, 963, 966, 968, 971, 980, 987, 990, 991, 992, 993, 995, 997, 999, 1001, 1005, 1006, 1007, 1009, 1011, 1012, 1015, 1018, 1019, 1024, 1028, 1040, 1041, 1043, 1052, 1053, 1057, 1060, 1070, 1074, 1079, 1083, 1086, 1087, 1088, 1090, 1092, 1093, 1095, 1096, 1097, 1099, 1100, 1102, 1103, 1104, 1105, 1106, 1112, 1115, 1120, 1123, 1124, 1126, 1130, 1134, 1136, 1138, 1140, 1142, 1145, 1148, 1151, 1152, 1174, 1178, 1183, 1185, 1186, 1187, 1195, 1206, 1209, 1210, 1214, 1216, 1223, 1233, 1235, 1236, 1237, 1243, 1250, 1251, 1259, 1264, 1269, 1273, 1278, 1279, 1280, 1293, 1295, 1298, 1302, 1303, 1304, 1307, 1309, 1312, 1317, 1318, 1322, 1328, 1333, 1337, 1342, 1344, 1350, 1352, 1359, 1371, 1376, 1378, 1379, 1383, 1392, 1393, 1397, 1399, 1400, 1401, 1404, 1406, 1407, 1408, 1409, 1411, 1426, 1429, 1441, 1442, 1443, 1444, 1447, 1454, 1462, 1466, 1469, 1473, 1474, 1479, 1485, 1486, 1489, 1493, 1495, 1500, 1503, 1505, 1509, 1512, 1517, 1522, 1530, 1532, 1533, 1535, 1540, 1546, 1547, 1549, 1552, 1556, 1557, 1560, 1562, 1569, 1571, 1577, 1583, 1584, 1589, 1590, 1592, 1598, 1599, 1601, 1604, 1605, 1613, 1616, 1617, 1623, 1626, 1628, 1629, 1631, 1635, 1636, 1643, 1650, 1651, 1652, 1654, 1656, 1657, 1667, 1674, 1677, 1680, 1681, 1689, 1695, 1696, 1698, 1700, 1707, 1714, 1715, 1716, 1717, 1718, 1720, 1721, 1724, 1726, 1728, 1734, 1737, 1738, 1741, 1744, 1746, 1749, 1750, 1753, 1757, 1763, 1766, 1767, 1768, 1769, 1770, 1781, 1785, 1789, 1794, 1797, 1799, 1805, 1808, 1809, 1814, 1816, 1818, 1820, 1832, 1835, 1837, 1839, 1851, 1854, 1859, 1864, 1869, 1870, 1871, 1876, 1877, 1881, 1885, 1888, 1891, 1893, 1894, 1901, 1902, 1907, 1916, 1917, 1919, 1920, 1922, 1923, 1928, 1929, 1930, 1931, 1935, 1938, 1939, 1940, 1944, 1946, 1949, 1950, 1959, 1960, 1962, 1963, 1966, 1967, 1968, 1970, 1980, 1987, 1991, 1994, 1995, 2000, 2004, 2009, 2010, 2013, 2015, 2018, 2019, 2025, 2028, 2031, 2032, 2037, 2041, 2043, 2044, 2047, 2056, 2063, 2064, 2067, 2072, 2073, 2077, 2080, 2083, 2084, 2087, 2089, 2090, 2091, 2096, 2099, 2111, 2113, 2114, 2123, 2128, 2135, 2139, 2140, 2142, 2143, 2146, 2147, 2149, 2164, 2166, 2170, 2178, 2180, 2183, 2189, 2190, 2191, 2192, 2195, 2199, 2201, 2203, 2210, 2212, 2215, 2220, 2228, 2231, 2232, 2237, 2240, 2241, 2248, 2249, 2253, 2257, 2258, 2261, 2263, 2276, 2277, 2279, 2280, 2282, 2292, 2295, 2298, 2300, 2318, 2319, 2322, 2326, 2327, 2328, 2329, 2331, 2333, 2334, 2335, 2336, 2337, 2348, 2350, 2351, 2355, 2356, 2368, 2371, 2372, 2374, 2375, 2378, 2379, 2381, 2384, 2386, 2387, 2389, 2395, 2397, 2401, 2402, 2403, 2406, 2414, 2417, 2424, 2425, 2431, 2433, 2434, 2437, 2439, 2440, 2441, 2446, 2451, 2457, 2467, 2468, 2471, 2472, 2473, 2475, 2485, 2487, 2491, 2493, 2496, 2502, 2503, 2510, 2514, 2517, 2521, 2522, 2527, 2528, 2538, 2542, 2544, 2550, 2551, 2552, 2554, 2557, 2560, 2565, 2569, 2571, 2576, 2580, 2585, 2587, 2591, 2592, 2594, 2599, 2603, 2604, 2606, 2610, 2616, 2617, 2619, 2632, 2633, 2634, 2638, 2645, 2647, 2650, 2664, 2665, 2668, 2669, 2670, 2673, 2675, 2676, 2677, 2678, 2687, 2690, 2691, 2692, 2695, 2697, 2700, 2705, 2706, 2709, 2716, 2719, 2724, 2726, 2727, 2728, 2731, 2733, 2734, 2739, 2743, 2744, 2750, 2764, 2765, 2768, 2769, 2770, 2773, 2775, 2776, 2778, 2779, 2789, 2792, 2794, 2795, 2808, 2813, 2819, 2820, 2829, 2831, 2833, 2834, 2835, 2836, 2838, 2839, 2841, 2843, 2845, 2848, 2854, 2856, 2861, 2867, 2870, 2872, 2875, 2878, 2881, 2883, 2887, 2892, 2895, 2896, 2897, 2898, 2909, 2910, 2912, 2913, 2914, 2917, 2923, 2928, 2934, 2937, 2939, 2949, 2950, 2952, 2953, 2955, 2959, 2966, 2967, 2968, 2970, 2971, 2972, 2974, 2982, 2991, 3001, 3008, 3018, 3020, 3028, 3033, 3034, 3043, 3046, 3048, 3050, 3055, 3056, 3059, 3060, 3068, 3070, 3075, 3079, 3087, 3088, 3096, 3101, 3106, 3110, 3111, 3114, 3115, 3116, 3117, 3119, 3122, 3124, 3125, 3128, 3133, 3135, 3136]
sequences = [x+'.npy' for x in all_data_dict.keys() if int(x.split('_')[0]) in auto_ids]
tr,v,te = du.get_tt_datasets(sequences)
fp = utils.get_filepath('datasets', 'xianling')
utils.pkl_dump(tr, os.path.join(fp,'train.dataset')), utils.pkl_dump(v, os.path.join(fp,'valid.dataset')), utils.pkl_dump(te, os.path.join(fp,'test.dataset'))
196/7:
import utils
import data_utils as du
import os
196/8:
# auto_ids = [2, 3, 4, 7, 11, 14, 23, 26, 27, 28, 30, 31, 33, 38, 39, 53, 55, 57, 59, 61, 65, 66, 70, 71, 73, 82, 101, 104, 105, 106, 109, 119, 131, 144, 146, 149, 150, 157, 159, 160, 163, 164, 165, 171, 172, 176, 180, 183, 185, 187, 188, 193, 201, 204, 205, 219, 223, 226, 229, 230, 233, 237, 242, 243, 245, 251, 252, 256, 258, 261, 267, 270, 272, 274, 276, 278, 279, 282, 283, 285, 289, 290, 293, 300, 301, 304, 305, 306, 314, 315, 321, 326, 327, 330, 331, 335, 337, 338, 342, 345, 347, 350, 358, 363, 367, 368, 373, 375, 376, 380, 381, 382, 383, 384, 386, 392, 393, 397, 399, 406, 407, 410, 414, 421, 422, 429, 435, 437, 446, 449, 453, 454, 458, 460, 467, 474, 484, 487, 493, 494, 498, 501, 502, 503, 512, 518, 523, 524, 526, 527, 530, 536, 537, 541, 545, 546, 551, 553, 555, 556, 560, 561, 563, 564, 567, 573, 575, 580, 581, 585, 590, 592, 595, 599, 600, 601, 608, 616, 620, 625, 627, 628, 632, 644, 646, 649, 653, 654, 660, 661, 664, 667, 668, 672, 679, 681, 682, 683, 697, 700, 701, 703, 704, 706, 707, 708, 710, 711, 715, 720, 728, 730, 733, 734, 737, 746, 747, 749, 750, 751, 756, 767, 779, 781, 784, 786, 788, 789, 799, 800, 806, 807, 811, 812, 820, 821, 825, 826, 827, 828, 835, 837, 840, 841, 844, 846, 848, 851, 852, 860, 863, 864, 865, 866, 868, 870, 872, 878, 881, 884, 887, 888, 889, 890, 894, 896, 902, 903, 904, 909, 911, 914, 917, 920, 923, 924, 935, 942, 946, 948, 950, 953, 954, 963, 966, 968, 971, 980, 987, 990, 991, 992, 993, 995, 997, 999, 1001, 1005, 1006, 1007, 1009, 1011, 1012, 1015, 1018, 1019, 1024, 1028, 1040, 1041, 1043, 1052, 1053, 1057, 1060, 1070, 1074, 1079, 1083, 1086, 1087, 1088, 1090, 1092, 1093, 1095, 1096, 1097, 1099, 1100, 1102, 1103, 1104, 1105, 1106, 1112, 1115, 1120, 1123, 1124, 1126, 1130, 1134, 1136, 1138, 1140, 1142, 1145, 1148, 1151, 1152, 1174, 1178, 1183, 1185, 1186, 1187, 1195, 1206, 1209, 1210, 1214, 1216, 1223, 1233, 1235, 1236, 1237, 1243, 1250, 1251, 1259, 1264, 1269, 1273, 1278, 1279, 1280, 1293, 1295, 1298, 1302, 1303, 1304, 1307, 1309, 1312, 1317, 1318, 1322, 1328, 1333, 1337, 1342, 1344, 1350, 1352, 1359, 1371, 1376, 1378, 1379, 1383, 1392, 1393, 1397, 1399, 1400, 1401, 1404, 1406, 1407, 1408, 1409, 1411, 1426, 1429, 1441, 1442, 1443, 1444, 1447, 1454, 1462, 1466, 1469, 1473, 1474, 1479, 1485, 1486, 1489, 1493, 1495, 1500, 1503, 1505, 1509, 1512, 1517, 1522, 1530, 1532, 1533, 1535, 1540, 1546, 1547, 1549, 1552, 1556, 1557, 1560, 1562, 1569, 1571, 1577, 1583, 1584, 1589, 1590, 1592, 1598, 1599, 1601, 1604, 1605, 1613, 1616, 1617, 1623, 1626, 1628, 1629, 1631, 1635, 1636, 1643, 1650, 1651, 1652, 1654, 1656, 1657, 1667, 1674, 1677, 1680, 1681, 1689, 1695, 1696, 1698, 1700, 1707, 1714, 1715, 1716, 1717, 1718, 1720, 1721, 1724, 1726, 1728, 1734, 1737, 1738, 1741, 1744, 1746, 1749, 1750, 1753, 1757, 1763, 1766, 1767, 1768, 1769, 1770, 1781, 1785, 1789, 1794, 1797, 1799, 1805, 1808, 1809, 1814, 1816, 1818, 1820, 1832, 1835, 1837, 1839, 1851, 1854, 1859, 1864, 1869, 1870, 1871, 1876, 1877, 1881, 1885, 1888, 1891, 1893, 1894, 1901, 1902, 1907, 1916, 1917, 1919, 1920, 1922, 1923, 1928, 1929, 1930, 1931, 1935, 1938, 1939, 1940, 1944, 1946, 1949, 1950, 1959, 1960, 1962, 1963, 1966, 1967, 1968, 1970, 1980, 1987, 1991, 1994, 1995, 2000, 2004, 2009, 2010, 2013, 2015, 2018, 2019, 2025, 2028, 2031, 2032, 2037, 2041, 2043, 2044, 2047, 2056, 2063, 2064, 2067, 2072, 2073, 2077, 2080, 2083, 2084, 2087, 2089, 2090, 2091, 2096, 2099, 2111, 2113, 2114, 2123, 2128, 2135, 2139, 2140, 2142, 2143, 2146, 2147, 2149, 2164, 2166, 2170, 2178, 2180, 2183, 2189, 2190, 2191, 2192, 2195, 2199, 2201, 2203, 2210, 2212, 2215, 2220, 2228, 2231, 2232, 2237, 2240, 2241, 2248, 2249, 2253, 2257, 2258, 2261, 2263, 2276, 2277, 2279, 2280, 2282, 2292, 2295, 2298, 2300, 2318, 2319, 2322, 2326, 2327, 2328, 2329, 2331, 2333, 2334, 2335, 2336, 2337, 2348, 2350, 2351, 2355, 2356, 2368, 2371, 2372, 2374, 2375, 2378, 2379, 2381, 2384, 2386, 2387, 2389, 2395, 2397, 2401, 2402, 2403, 2406, 2414, 2417, 2424, 2425, 2431, 2433, 2434, 2437, 2439, 2440, 2441, 2446, 2451, 2457, 2467, 2468, 2471, 2472, 2473, 2475, 2485, 2487, 2491, 2493, 2496, 2502, 2503, 2510, 2514, 2517, 2521, 2522, 2527, 2528, 2538, 2542, 2544, 2550, 2551, 2552, 2554, 2557, 2560, 2565, 2569, 2571, 2576, 2580, 2585, 2587, 2591, 2592, 2594, 2599, 2603, 2604, 2606, 2610, 2616, 2617, 2619, 2632, 2633, 2634, 2638, 2645, 2647, 2650, 2664, 2665, 2668, 2669, 2670, 2673, 2675, 2676, 2677, 2678, 2687, 2690, 2691, 2692, 2695, 2697, 2700, 2705, 2706, 2709, 2716, 2719, 2724, 2726, 2727, 2728, 2731, 2733, 2734, 2739, 2743, 2744, 2750, 2764, 2765, 2768, 2769, 2770, 2773, 2775, 2776, 2778, 2779, 2789, 2792, 2794, 2795, 2808, 2813, 2819, 2820, 2829, 2831, 2833, 2834, 2835, 2836, 2838, 2839, 2841, 2843, 2845, 2848, 2854, 2856, 2861, 2867, 2870, 2872, 2875, 2878, 2881, 2883, 2887, 2892, 2895, 2896, 2897, 2898, 2909, 2910, 2912, 2913, 2914, 2917, 2923, 2928, 2934, 2937, 2939, 2949, 2950, 2952, 2953, 2955, 2959, 2966, 2967, 2968, 2970, 2971, 2972, 2974, 2982, 2991, 3001, 3008, 3018, 3020, 3028, 3033, 3034, 3043, 3046, 3048, 3050, 3055, 3056, 3059, 3060, 3068, 3070, 3075, 3079, 3087, 3088, 3096, 3101, 3106, 3110, 3111, 3114, 3115, 3116, 3117, 3119, 3122, 3124, 3125, 3128, 3133, 3135, 3136]
# sequences = [x+'.npy' for x in all_data_dict.keys() if int(x.split('_')[0]) in auto_ids]
# tr,v,te = du.get_tt_datasets(sequences)
# fp = utils.get_filepath('datasets', 'xianling')
utils.pkl_dump(tr, os.path.join(fp,'train.dataset')), utils.pkl_dump(v, os.path.join(fp,'valid.dataset')), utils.pkl_dump(te, os.path.join(fp,'test.dataset'))
196/9:
# auto_ids = [2, 3, 4, 7, 11, 14, 23, 26, 27, 28, 30, 31, 33, 38, 39, 53, 55, 57, 59, 61, 65, 66, 70, 71, 73, 82, 101, 104, 105, 106, 109, 119, 131, 144, 146, 149, 150, 157, 159, 160, 163, 164, 165, 171, 172, 176, 180, 183, 185, 187, 188, 193, 201, 204, 205, 219, 223, 226, 229, 230, 233, 237, 242, 243, 245, 251, 252, 256, 258, 261, 267, 270, 272, 274, 276, 278, 279, 282, 283, 285, 289, 290, 293, 300, 301, 304, 305, 306, 314, 315, 321, 326, 327, 330, 331, 335, 337, 338, 342, 345, 347, 350, 358, 363, 367, 368, 373, 375, 376, 380, 381, 382, 383, 384, 386, 392, 393, 397, 399, 406, 407, 410, 414, 421, 422, 429, 435, 437, 446, 449, 453, 454, 458, 460, 467, 474, 484, 487, 493, 494, 498, 501, 502, 503, 512, 518, 523, 524, 526, 527, 530, 536, 537, 541, 545, 546, 551, 553, 555, 556, 560, 561, 563, 564, 567, 573, 575, 580, 581, 585, 590, 592, 595, 599, 600, 601, 608, 616, 620, 625, 627, 628, 632, 644, 646, 649, 653, 654, 660, 661, 664, 667, 668, 672, 679, 681, 682, 683, 697, 700, 701, 703, 704, 706, 707, 708, 710, 711, 715, 720, 728, 730, 733, 734, 737, 746, 747, 749, 750, 751, 756, 767, 779, 781, 784, 786, 788, 789, 799, 800, 806, 807, 811, 812, 820, 821, 825, 826, 827, 828, 835, 837, 840, 841, 844, 846, 848, 851, 852, 860, 863, 864, 865, 866, 868, 870, 872, 878, 881, 884, 887, 888, 889, 890, 894, 896, 902, 903, 904, 909, 911, 914, 917, 920, 923, 924, 935, 942, 946, 948, 950, 953, 954, 963, 966, 968, 971, 980, 987, 990, 991, 992, 993, 995, 997, 999, 1001, 1005, 1006, 1007, 1009, 1011, 1012, 1015, 1018, 1019, 1024, 1028, 1040, 1041, 1043, 1052, 1053, 1057, 1060, 1070, 1074, 1079, 1083, 1086, 1087, 1088, 1090, 1092, 1093, 1095, 1096, 1097, 1099, 1100, 1102, 1103, 1104, 1105, 1106, 1112, 1115, 1120, 1123, 1124, 1126, 1130, 1134, 1136, 1138, 1140, 1142, 1145, 1148, 1151, 1152, 1174, 1178, 1183, 1185, 1186, 1187, 1195, 1206, 1209, 1210, 1214, 1216, 1223, 1233, 1235, 1236, 1237, 1243, 1250, 1251, 1259, 1264, 1269, 1273, 1278, 1279, 1280, 1293, 1295, 1298, 1302, 1303, 1304, 1307, 1309, 1312, 1317, 1318, 1322, 1328, 1333, 1337, 1342, 1344, 1350, 1352, 1359, 1371, 1376, 1378, 1379, 1383, 1392, 1393, 1397, 1399, 1400, 1401, 1404, 1406, 1407, 1408, 1409, 1411, 1426, 1429, 1441, 1442, 1443, 1444, 1447, 1454, 1462, 1466, 1469, 1473, 1474, 1479, 1485, 1486, 1489, 1493, 1495, 1500, 1503, 1505, 1509, 1512, 1517, 1522, 1530, 1532, 1533, 1535, 1540, 1546, 1547, 1549, 1552, 1556, 1557, 1560, 1562, 1569, 1571, 1577, 1583, 1584, 1589, 1590, 1592, 1598, 1599, 1601, 1604, 1605, 1613, 1616, 1617, 1623, 1626, 1628, 1629, 1631, 1635, 1636, 1643, 1650, 1651, 1652, 1654, 1656, 1657, 1667, 1674, 1677, 1680, 1681, 1689, 1695, 1696, 1698, 1700, 1707, 1714, 1715, 1716, 1717, 1718, 1720, 1721, 1724, 1726, 1728, 1734, 1737, 1738, 1741, 1744, 1746, 1749, 1750, 1753, 1757, 1763, 1766, 1767, 1768, 1769, 1770, 1781, 1785, 1789, 1794, 1797, 1799, 1805, 1808, 1809, 1814, 1816, 1818, 1820, 1832, 1835, 1837, 1839, 1851, 1854, 1859, 1864, 1869, 1870, 1871, 1876, 1877, 1881, 1885, 1888, 1891, 1893, 1894, 1901, 1902, 1907, 1916, 1917, 1919, 1920, 1922, 1923, 1928, 1929, 1930, 1931, 1935, 1938, 1939, 1940, 1944, 1946, 1949, 1950, 1959, 1960, 1962, 1963, 1966, 1967, 1968, 1970, 1980, 1987, 1991, 1994, 1995, 2000, 2004, 2009, 2010, 2013, 2015, 2018, 2019, 2025, 2028, 2031, 2032, 2037, 2041, 2043, 2044, 2047, 2056, 2063, 2064, 2067, 2072, 2073, 2077, 2080, 2083, 2084, 2087, 2089, 2090, 2091, 2096, 2099, 2111, 2113, 2114, 2123, 2128, 2135, 2139, 2140, 2142, 2143, 2146, 2147, 2149, 2164, 2166, 2170, 2178, 2180, 2183, 2189, 2190, 2191, 2192, 2195, 2199, 2201, 2203, 2210, 2212, 2215, 2220, 2228, 2231, 2232, 2237, 2240, 2241, 2248, 2249, 2253, 2257, 2258, 2261, 2263, 2276, 2277, 2279, 2280, 2282, 2292, 2295, 2298, 2300, 2318, 2319, 2322, 2326, 2327, 2328, 2329, 2331, 2333, 2334, 2335, 2336, 2337, 2348, 2350, 2351, 2355, 2356, 2368, 2371, 2372, 2374, 2375, 2378, 2379, 2381, 2384, 2386, 2387, 2389, 2395, 2397, 2401, 2402, 2403, 2406, 2414, 2417, 2424, 2425, 2431, 2433, 2434, 2437, 2439, 2440, 2441, 2446, 2451, 2457, 2467, 2468, 2471, 2472, 2473, 2475, 2485, 2487, 2491, 2493, 2496, 2502, 2503, 2510, 2514, 2517, 2521, 2522, 2527, 2528, 2538, 2542, 2544, 2550, 2551, 2552, 2554, 2557, 2560, 2565, 2569, 2571, 2576, 2580, 2585, 2587, 2591, 2592, 2594, 2599, 2603, 2604, 2606, 2610, 2616, 2617, 2619, 2632, 2633, 2634, 2638, 2645, 2647, 2650, 2664, 2665, 2668, 2669, 2670, 2673, 2675, 2676, 2677, 2678, 2687, 2690, 2691, 2692, 2695, 2697, 2700, 2705, 2706, 2709, 2716, 2719, 2724, 2726, 2727, 2728, 2731, 2733, 2734, 2739, 2743, 2744, 2750, 2764, 2765, 2768, 2769, 2770, 2773, 2775, 2776, 2778, 2779, 2789, 2792, 2794, 2795, 2808, 2813, 2819, 2820, 2829, 2831, 2833, 2834, 2835, 2836, 2838, 2839, 2841, 2843, 2845, 2848, 2854, 2856, 2861, 2867, 2870, 2872, 2875, 2878, 2881, 2883, 2887, 2892, 2895, 2896, 2897, 2898, 2909, 2910, 2912, 2913, 2914, 2917, 2923, 2928, 2934, 2937, 2939, 2949, 2950, 2952, 2953, 2955, 2959, 2966, 2967, 2968, 2970, 2971, 2972, 2974, 2982, 2991, 3001, 3008, 3018, 3020, 3028, 3033, 3034, 3043, 3046, 3048, 3050, 3055, 3056, 3059, 3060, 3068, 3070, 3075, 3079, 3087, 3088, 3096, 3101, 3106, 3110, 3111, 3114, 3115, 3116, 3117, 3119, 3122, 3124, 3125, 3128, 3133, 3135, 3136]
# sequences = [x+'.npy' for x in all_data_dict.keys() if int(x.split('_')[0]) in auto_ids]
# tr,v,te = du.get_tt_datasets(sequences)
fp = utils.get_filepath('datasets', 'xianling')
utils.pkl_dump(tr, os.path.join(fp,'train.dataset')), utils.pkl_dump(v, os.path.join(fp,'valid.dataset')), utils.pkl_dump(te, os.path.join(fp,'test.dataset'))
196/10:
# auto_ids = [2, 3, 4, 7, 11, 14, 23, 26, 27, 28, 30, 31, 33, 38, 39, 53, 55, 57, 59, 61, 65, 66, 70, 71, 73, 82, 101, 104, 105, 106, 109, 119, 131, 144, 146, 149, 150, 157, 159, 160, 163, 164, 165, 171, 172, 176, 180, 183, 185, 187, 188, 193, 201, 204, 205, 219, 223, 226, 229, 230, 233, 237, 242, 243, 245, 251, 252, 256, 258, 261, 267, 270, 272, 274, 276, 278, 279, 282, 283, 285, 289, 290, 293, 300, 301, 304, 305, 306, 314, 315, 321, 326, 327, 330, 331, 335, 337, 338, 342, 345, 347, 350, 358, 363, 367, 368, 373, 375, 376, 380, 381, 382, 383, 384, 386, 392, 393, 397, 399, 406, 407, 410, 414, 421, 422, 429, 435, 437, 446, 449, 453, 454, 458, 460, 467, 474, 484, 487, 493, 494, 498, 501, 502, 503, 512, 518, 523, 524, 526, 527, 530, 536, 537, 541, 545, 546, 551, 553, 555, 556, 560, 561, 563, 564, 567, 573, 575, 580, 581, 585, 590, 592, 595, 599, 600, 601, 608, 616, 620, 625, 627, 628, 632, 644, 646, 649, 653, 654, 660, 661, 664, 667, 668, 672, 679, 681, 682, 683, 697, 700, 701, 703, 704, 706, 707, 708, 710, 711, 715, 720, 728, 730, 733, 734, 737, 746, 747, 749, 750, 751, 756, 767, 779, 781, 784, 786, 788, 789, 799, 800, 806, 807, 811, 812, 820, 821, 825, 826, 827, 828, 835, 837, 840, 841, 844, 846, 848, 851, 852, 860, 863, 864, 865, 866, 868, 870, 872, 878, 881, 884, 887, 888, 889, 890, 894, 896, 902, 903, 904, 909, 911, 914, 917, 920, 923, 924, 935, 942, 946, 948, 950, 953, 954, 963, 966, 968, 971, 980, 987, 990, 991, 992, 993, 995, 997, 999, 1001, 1005, 1006, 1007, 1009, 1011, 1012, 1015, 1018, 1019, 1024, 1028, 1040, 1041, 1043, 1052, 1053, 1057, 1060, 1070, 1074, 1079, 1083, 1086, 1087, 1088, 1090, 1092, 1093, 1095, 1096, 1097, 1099, 1100, 1102, 1103, 1104, 1105, 1106, 1112, 1115, 1120, 1123, 1124, 1126, 1130, 1134, 1136, 1138, 1140, 1142, 1145, 1148, 1151, 1152, 1174, 1178, 1183, 1185, 1186, 1187, 1195, 1206, 1209, 1210, 1214, 1216, 1223, 1233, 1235, 1236, 1237, 1243, 1250, 1251, 1259, 1264, 1269, 1273, 1278, 1279, 1280, 1293, 1295, 1298, 1302, 1303, 1304, 1307, 1309, 1312, 1317, 1318, 1322, 1328, 1333, 1337, 1342, 1344, 1350, 1352, 1359, 1371, 1376, 1378, 1379, 1383, 1392, 1393, 1397, 1399, 1400, 1401, 1404, 1406, 1407, 1408, 1409, 1411, 1426, 1429, 1441, 1442, 1443, 1444, 1447, 1454, 1462, 1466, 1469, 1473, 1474, 1479, 1485, 1486, 1489, 1493, 1495, 1500, 1503, 1505, 1509, 1512, 1517, 1522, 1530, 1532, 1533, 1535, 1540, 1546, 1547, 1549, 1552, 1556, 1557, 1560, 1562, 1569, 1571, 1577, 1583, 1584, 1589, 1590, 1592, 1598, 1599, 1601, 1604, 1605, 1613, 1616, 1617, 1623, 1626, 1628, 1629, 1631, 1635, 1636, 1643, 1650, 1651, 1652, 1654, 1656, 1657, 1667, 1674, 1677, 1680, 1681, 1689, 1695, 1696, 1698, 1700, 1707, 1714, 1715, 1716, 1717, 1718, 1720, 1721, 1724, 1726, 1728, 1734, 1737, 1738, 1741, 1744, 1746, 1749, 1750, 1753, 1757, 1763, 1766, 1767, 1768, 1769, 1770, 1781, 1785, 1789, 1794, 1797, 1799, 1805, 1808, 1809, 1814, 1816, 1818, 1820, 1832, 1835, 1837, 1839, 1851, 1854, 1859, 1864, 1869, 1870, 1871, 1876, 1877, 1881, 1885, 1888, 1891, 1893, 1894, 1901, 1902, 1907, 1916, 1917, 1919, 1920, 1922, 1923, 1928, 1929, 1930, 1931, 1935, 1938, 1939, 1940, 1944, 1946, 1949, 1950, 1959, 1960, 1962, 1963, 1966, 1967, 1968, 1970, 1980, 1987, 1991, 1994, 1995, 2000, 2004, 2009, 2010, 2013, 2015, 2018, 2019, 2025, 2028, 2031, 2032, 2037, 2041, 2043, 2044, 2047, 2056, 2063, 2064, 2067, 2072, 2073, 2077, 2080, 2083, 2084, 2087, 2089, 2090, 2091, 2096, 2099, 2111, 2113, 2114, 2123, 2128, 2135, 2139, 2140, 2142, 2143, 2146, 2147, 2149, 2164, 2166, 2170, 2178, 2180, 2183, 2189, 2190, 2191, 2192, 2195, 2199, 2201, 2203, 2210, 2212, 2215, 2220, 2228, 2231, 2232, 2237, 2240, 2241, 2248, 2249, 2253, 2257, 2258, 2261, 2263, 2276, 2277, 2279, 2280, 2282, 2292, 2295, 2298, 2300, 2318, 2319, 2322, 2326, 2327, 2328, 2329, 2331, 2333, 2334, 2335, 2336, 2337, 2348, 2350, 2351, 2355, 2356, 2368, 2371, 2372, 2374, 2375, 2378, 2379, 2381, 2384, 2386, 2387, 2389, 2395, 2397, 2401, 2402, 2403, 2406, 2414, 2417, 2424, 2425, 2431, 2433, 2434, 2437, 2439, 2440, 2441, 2446, 2451, 2457, 2467, 2468, 2471, 2472, 2473, 2475, 2485, 2487, 2491, 2493, 2496, 2502, 2503, 2510, 2514, 2517, 2521, 2522, 2527, 2528, 2538, 2542, 2544, 2550, 2551, 2552, 2554, 2557, 2560, 2565, 2569, 2571, 2576, 2580, 2585, 2587, 2591, 2592, 2594, 2599, 2603, 2604, 2606, 2610, 2616, 2617, 2619, 2632, 2633, 2634, 2638, 2645, 2647, 2650, 2664, 2665, 2668, 2669, 2670, 2673, 2675, 2676, 2677, 2678, 2687, 2690, 2691, 2692, 2695, 2697, 2700, 2705, 2706, 2709, 2716, 2719, 2724, 2726, 2727, 2728, 2731, 2733, 2734, 2739, 2743, 2744, 2750, 2764, 2765, 2768, 2769, 2770, 2773, 2775, 2776, 2778, 2779, 2789, 2792, 2794, 2795, 2808, 2813, 2819, 2820, 2829, 2831, 2833, 2834, 2835, 2836, 2838, 2839, 2841, 2843, 2845, 2848, 2854, 2856, 2861, 2867, 2870, 2872, 2875, 2878, 2881, 2883, 2887, 2892, 2895, 2896, 2897, 2898, 2909, 2910, 2912, 2913, 2914, 2917, 2923, 2928, 2934, 2937, 2939, 2949, 2950, 2952, 2953, 2955, 2959, 2966, 2967, 2968, 2970, 2971, 2972, 2974, 2982, 2991, 3001, 3008, 3018, 3020, 3028, 3033, 3034, 3043, 3046, 3048, 3050, 3055, 3056, 3059, 3060, 3068, 3070, 3075, 3079, 3087, 3088, 3096, 3101, 3106, 3110, 3111, 3114, 3115, 3116, 3117, 3119, 3122, 3124, 3125, 3128, 3133, 3135, 3136]
# sequences = [x+'.npy' for x in all_data_dict.keys() if int(x.split('_')[0]) in auto_ids]
# tr,v,te = du.get_tt_datasets(sequences)
utils.pkl_dump(tr, os.path.join('datasets', 'xianling','train.dataset'))
utils.pkl_dump(v, os.path.join('datasets', 'xianling','valid.dataset'))
utils.pkl_dump(te, os.path.join('datasets', 'xianling','test.dataset'))
196/11:
# auto_ids = [2, 3, 4, 7, 11, 14, 23, 26, 27, 28, 30, 31, 33, 38, 39, 53, 55, 57, 59, 61, 65, 66, 70, 71, 73, 82, 101, 104, 105, 106, 109, 119, 131, 144, 146, 149, 150, 157, 159, 160, 163, 164, 165, 171, 172, 176, 180, 183, 185, 187, 188, 193, 201, 204, 205, 219, 223, 226, 229, 230, 233, 237, 242, 243, 245, 251, 252, 256, 258, 261, 267, 270, 272, 274, 276, 278, 279, 282, 283, 285, 289, 290, 293, 300, 301, 304, 305, 306, 314, 315, 321, 326, 327, 330, 331, 335, 337, 338, 342, 345, 347, 350, 358, 363, 367, 368, 373, 375, 376, 380, 381, 382, 383, 384, 386, 392, 393, 397, 399, 406, 407, 410, 414, 421, 422, 429, 435, 437, 446, 449, 453, 454, 458, 460, 467, 474, 484, 487, 493, 494, 498, 501, 502, 503, 512, 518, 523, 524, 526, 527, 530, 536, 537, 541, 545, 546, 551, 553, 555, 556, 560, 561, 563, 564, 567, 573, 575, 580, 581, 585, 590, 592, 595, 599, 600, 601, 608, 616, 620, 625, 627, 628, 632, 644, 646, 649, 653, 654, 660, 661, 664, 667, 668, 672, 679, 681, 682, 683, 697, 700, 701, 703, 704, 706, 707, 708, 710, 711, 715, 720, 728, 730, 733, 734, 737, 746, 747, 749, 750, 751, 756, 767, 779, 781, 784, 786, 788, 789, 799, 800, 806, 807, 811, 812, 820, 821, 825, 826, 827, 828, 835, 837, 840, 841, 844, 846, 848, 851, 852, 860, 863, 864, 865, 866, 868, 870, 872, 878, 881, 884, 887, 888, 889, 890, 894, 896, 902, 903, 904, 909, 911, 914, 917, 920, 923, 924, 935, 942, 946, 948, 950, 953, 954, 963, 966, 968, 971, 980, 987, 990, 991, 992, 993, 995, 997, 999, 1001, 1005, 1006, 1007, 1009, 1011, 1012, 1015, 1018, 1019, 1024, 1028, 1040, 1041, 1043, 1052, 1053, 1057, 1060, 1070, 1074, 1079, 1083, 1086, 1087, 1088, 1090, 1092, 1093, 1095, 1096, 1097, 1099, 1100, 1102, 1103, 1104, 1105, 1106, 1112, 1115, 1120, 1123, 1124, 1126, 1130, 1134, 1136, 1138, 1140, 1142, 1145, 1148, 1151, 1152, 1174, 1178, 1183, 1185, 1186, 1187, 1195, 1206, 1209, 1210, 1214, 1216, 1223, 1233, 1235, 1236, 1237, 1243, 1250, 1251, 1259, 1264, 1269, 1273, 1278, 1279, 1280, 1293, 1295, 1298, 1302, 1303, 1304, 1307, 1309, 1312, 1317, 1318, 1322, 1328, 1333, 1337, 1342, 1344, 1350, 1352, 1359, 1371, 1376, 1378, 1379, 1383, 1392, 1393, 1397, 1399, 1400, 1401, 1404, 1406, 1407, 1408, 1409, 1411, 1426, 1429, 1441, 1442, 1443, 1444, 1447, 1454, 1462, 1466, 1469, 1473, 1474, 1479, 1485, 1486, 1489, 1493, 1495, 1500, 1503, 1505, 1509, 1512, 1517, 1522, 1530, 1532, 1533, 1535, 1540, 1546, 1547, 1549, 1552, 1556, 1557, 1560, 1562, 1569, 1571, 1577, 1583, 1584, 1589, 1590, 1592, 1598, 1599, 1601, 1604, 1605, 1613, 1616, 1617, 1623, 1626, 1628, 1629, 1631, 1635, 1636, 1643, 1650, 1651, 1652, 1654, 1656, 1657, 1667, 1674, 1677, 1680, 1681, 1689, 1695, 1696, 1698, 1700, 1707, 1714, 1715, 1716, 1717, 1718, 1720, 1721, 1724, 1726, 1728, 1734, 1737, 1738, 1741, 1744, 1746, 1749, 1750, 1753, 1757, 1763, 1766, 1767, 1768, 1769, 1770, 1781, 1785, 1789, 1794, 1797, 1799, 1805, 1808, 1809, 1814, 1816, 1818, 1820, 1832, 1835, 1837, 1839, 1851, 1854, 1859, 1864, 1869, 1870, 1871, 1876, 1877, 1881, 1885, 1888, 1891, 1893, 1894, 1901, 1902, 1907, 1916, 1917, 1919, 1920, 1922, 1923, 1928, 1929, 1930, 1931, 1935, 1938, 1939, 1940, 1944, 1946, 1949, 1950, 1959, 1960, 1962, 1963, 1966, 1967, 1968, 1970, 1980, 1987, 1991, 1994, 1995, 2000, 2004, 2009, 2010, 2013, 2015, 2018, 2019, 2025, 2028, 2031, 2032, 2037, 2041, 2043, 2044, 2047, 2056, 2063, 2064, 2067, 2072, 2073, 2077, 2080, 2083, 2084, 2087, 2089, 2090, 2091, 2096, 2099, 2111, 2113, 2114, 2123, 2128, 2135, 2139, 2140, 2142, 2143, 2146, 2147, 2149, 2164, 2166, 2170, 2178, 2180, 2183, 2189, 2190, 2191, 2192, 2195, 2199, 2201, 2203, 2210, 2212, 2215, 2220, 2228, 2231, 2232, 2237, 2240, 2241, 2248, 2249, 2253, 2257, 2258, 2261, 2263, 2276, 2277, 2279, 2280, 2282, 2292, 2295, 2298, 2300, 2318, 2319, 2322, 2326, 2327, 2328, 2329, 2331, 2333, 2334, 2335, 2336, 2337, 2348, 2350, 2351, 2355, 2356, 2368, 2371, 2372, 2374, 2375, 2378, 2379, 2381, 2384, 2386, 2387, 2389, 2395, 2397, 2401, 2402, 2403, 2406, 2414, 2417, 2424, 2425, 2431, 2433, 2434, 2437, 2439, 2440, 2441, 2446, 2451, 2457, 2467, 2468, 2471, 2472, 2473, 2475, 2485, 2487, 2491, 2493, 2496, 2502, 2503, 2510, 2514, 2517, 2521, 2522, 2527, 2528, 2538, 2542, 2544, 2550, 2551, 2552, 2554, 2557, 2560, 2565, 2569, 2571, 2576, 2580, 2585, 2587, 2591, 2592, 2594, 2599, 2603, 2604, 2606, 2610, 2616, 2617, 2619, 2632, 2633, 2634, 2638, 2645, 2647, 2650, 2664, 2665, 2668, 2669, 2670, 2673, 2675, 2676, 2677, 2678, 2687, 2690, 2691, 2692, 2695, 2697, 2700, 2705, 2706, 2709, 2716, 2719, 2724, 2726, 2727, 2728, 2731, 2733, 2734, 2739, 2743, 2744, 2750, 2764, 2765, 2768, 2769, 2770, 2773, 2775, 2776, 2778, 2779, 2789, 2792, 2794, 2795, 2808, 2813, 2819, 2820, 2829, 2831, 2833, 2834, 2835, 2836, 2838, 2839, 2841, 2843, 2845, 2848, 2854, 2856, 2861, 2867, 2870, 2872, 2875, 2878, 2881, 2883, 2887, 2892, 2895, 2896, 2897, 2898, 2909, 2910, 2912, 2913, 2914, 2917, 2923, 2928, 2934, 2937, 2939, 2949, 2950, 2952, 2953, 2955, 2959, 2966, 2967, 2968, 2970, 2971, 2972, 2974, 2982, 2991, 3001, 3008, 3018, 3020, 3028, 3033, 3034, 3043, 3046, 3048, 3050, 3055, 3056, 3059, 3060, 3068, 3070, 3075, 3079, 3087, 3088, 3096, 3101, 3106, 3110, 3111, 3114, 3115, 3116, 3117, 3119, 3122, 3124, 3125, 3128, 3133, 3135, 3136]
# sequences = [x+'.npy' for x in all_data_dict.keys() if int(x.split('_')[0]) in auto_ids]
# tr,v,te = du.get_tt_datasets(sequences)
os.makedirs(os.path.join('datasets', 'xianling'))
utils.pkl_dump(tr, os.path.join('datasets', 'xianling','train.dataset'))
utils.pkl_dump(v, os.path.join('datasets', 'xianling','valid.dataset'))
utils.pkl_dump(te, os.path.join('datasets', 'xianling','test.dataset'))
196/12:
all_data_dict = utils.pkl_load('data/label_dict.pkl')

def dump(name, tr, v, te):
    os.makedirs(os.path.join('datasets', name))
    utils.pkl_dump(tr, os.path.join('datasets', name,'train.dataset'))
    utils.pkl_dump(v, os.path.join('datasets', name,'valid.dataset'))
    utils.pkl_dump(te, os.path.join('datasets', name,'test.dataset'))
196/13:
auto_ids = [2, 3, 4, 7, 11, 14, 23, 26, 27, 28, 30, 31, 33, 38, 39, 53, 55, 57, 59, 61, 65, 66, 70, 71, 73, 82, 101, 104, 105, 106, 109, 119, 131, 144, 146, 149, 150, 157, 159, 160, 163, 164, 165, 171, 172, 176, 180, 183, 185, 187, 188, 193, 201, 204, 205, 219, 223, 226, 229, 230, 233, 237, 242, 243, 245, 251, 252, 256, 258, 261, 267, 270, 272, 274, 276, 278, 279, 282, 283, 285, 289, 290, 293, 300, 301, 304, 305, 306, 314, 315, 321, 326, 327, 330, 331, 335, 337, 338, 342, 345, 347, 350, 358, 363, 367, 368, 373, 375, 376, 380, 381, 382, 383, 384, 386, 392, 393, 397, 399, 406, 407, 410, 414, 421, 422, 429, 435, 437, 446, 449, 453, 454, 458, 460, 467, 474, 484, 487, 493, 494, 498, 501, 502, 503, 512, 518, 523, 524, 526, 527, 530, 536, 537, 541, 545, 546, 551, 553, 555, 556, 560, 561, 563, 564, 567, 573, 575, 580, 581, 585, 590, 592, 595, 599, 600, 601, 608, 616, 620, 625, 627, 628, 632, 644, 646, 649, 653, 654, 660, 661, 664, 667, 668, 672, 679, 681, 682, 683, 697, 700, 701, 703, 704, 706, 707, 708, 710, 711, 715, 720, 728, 730, 733, 734, 737, 746, 747, 749, 750, 751, 756, 767, 779, 781, 784, 786, 788, 789, 799, 800, 806, 807, 811, 812, 820, 821, 825, 826, 827, 828, 835, 837, 840, 841, 844, 846, 848, 851, 852, 860, 863, 864, 865, 866, 868, 870, 872, 878, 881, 884, 887, 888, 889, 890, 894, 896, 902, 903, 904, 909, 911, 914, 917, 920, 923, 924, 935, 942, 946, 948, 950, 953, 954, 963, 966, 968, 971, 980, 987, 990, 991, 992, 993, 995, 997, 999, 1001, 1005, 1006, 1007, 1009, 1011, 1012, 1015, 1018, 1019, 1024, 1028, 1040, 1041, 1043, 1052, 1053, 1057, 1060, 1070, 1074, 1079, 1083, 1086, 1087, 1088, 1090, 1092, 1093, 1095, 1096, 1097, 1099, 1100, 1102, 1103, 1104, 1105, 1106, 1112, 1115, 1120, 1123, 1124, 1126, 1130, 1134, 1136, 1138, 1140, 1142, 1145, 1148, 1151, 1152, 1174, 1178, 1183, 1185, 1186, 1187, 1195, 1206, 1209, 1210, 1214, 1216, 1223, 1233, 1235, 1236, 1237, 1243, 1250, 1251, 1259, 1264, 1269, 1273, 1278, 1279, 1280, 1293, 1295, 1298, 1302, 1303, 1304, 1307, 1309, 1312, 1317, 1318, 1322, 1328, 1333, 1337, 1342, 1344, 1350, 1352, 1359, 1371, 1376, 1378, 1379, 1383, 1392, 1393, 1397, 1399, 1400, 1401, 1404, 1406, 1407, 1408, 1409, 1411, 1426, 1429, 1441, 1442, 1443, 1444, 1447, 1454, 1462, 1466, 1469, 1473, 1474, 1479, 1485, 1486, 1489, 1493, 1495, 1500, 1503, 1505, 1509, 1512, 1517, 1522, 1530, 1532, 1533, 1535, 1540, 1546, 1547, 1549, 1552, 1556, 1557, 1560, 1562, 1569, 1571, 1577, 1583, 1584, 1589, 1590, 1592, 1598, 1599, 1601, 1604, 1605, 1613, 1616, 1617, 1623, 1626, 1628, 1629, 1631, 1635, 1636, 1643, 1650, 1651, 1652, 1654, 1656, 1657, 1667, 1674, 1677, 1680, 1681, 1689, 1695, 1696, 1698, 1700, 1707, 1714, 1715, 1716, 1717, 1718, 1720, 1721, 1724, 1726, 1728, 1734, 1737, 1738, 1741, 1744, 1746, 1749, 1750, 1753, 1757, 1763, 1766, 1767, 1768, 1769, 1770, 1781, 1785, 1789, 1794, 1797, 1799, 1805, 1808, 1809, 1814, 1816, 1818, 1820, 1832, 1835, 1837, 1839, 1851, 1854, 1859, 1864, 1869, 1870, 1871, 1876, 1877, 1881, 1885, 1888, 1891, 1893, 1894, 1901, 1902, 1907, 1916, 1917, 1919, 1920, 1922, 1923, 1928, 1929, 1930, 1931, 1935, 1938, 1939, 1940, 1944, 1946, 1949, 1950, 1959, 1960, 1962, 1963, 1966, 1967, 1968, 1970, 1980, 1987, 1991, 1994, 1995, 2000, 2004, 2009, 2010, 2013, 2015, 2018, 2019, 2025, 2028, 2031, 2032, 2037, 2041, 2043, 2044, 2047, 2056, 2063, 2064, 2067, 2072, 2073, 2077, 2080, 2083, 2084, 2087, 2089, 2090, 2091, 2096, 2099, 2111, 2113, 2114, 2123, 2128, 2135, 2139, 2140, 2142, 2143, 2146, 2147, 2149, 2164, 2166, 2170, 2178, 2180, 2183, 2189, 2190, 2191, 2192, 2195, 2199, 2201, 2203, 2210, 2212, 2215, 2220, 2228, 2231, 2232, 2237, 2240, 2241, 2248, 2249, 2253, 2257, 2258, 2261, 2263, 2276, 2277, 2279, 2280, 2282, 2292, 2295, 2298, 2300, 2318, 2319, 2322, 2326, 2327, 2328, 2329, 2331, 2333, 2334, 2335, 2336, 2337, 2348, 2350, 2351, 2355, 2356, 2368, 2371, 2372, 2374, 2375, 2378, 2379, 2381, 2384, 2386, 2387, 2389, 2395, 2397, 2401, 2402, 2403, 2406, 2414, 2417, 2424, 2425, 2431, 2433, 2434, 2437, 2439, 2440, 2441, 2446, 2451, 2457, 2467, 2468, 2471, 2472, 2473, 2475, 2485, 2487, 2491, 2493, 2496, 2502, 2503, 2510, 2514, 2517, 2521, 2522, 2527, 2528, 2538, 2542, 2544, 2550, 2551, 2552, 2554, 2557, 2560, 2565, 2569, 2571, 2576, 2580, 2585, 2587, 2591, 2592, 2594, 2599, 2603, 2604, 2606, 2610, 2616, 2617, 2619, 2632, 2633, 2634, 2638, 2645, 2647, 2650, 2664, 2665, 2668, 2669, 2670, 2673, 2675, 2676, 2677, 2678, 2687, 2690, 2691, 2692, 2695, 2697, 2700, 2705, 2706, 2709, 2716, 2719, 2724, 2726, 2727, 2728, 2731, 2733, 2734, 2739, 2743, 2744, 2750, 2764, 2765, 2768, 2769, 2770, 2773, 2775, 2776, 2778, 2779, 2789, 2792, 2794, 2795, 2808, 2813, 2819, 2820, 2829, 2831, 2833, 2834, 2835, 2836, 2838, 2839, 2841, 2843, 2845, 2848, 2854, 2856, 2861, 2867, 2870, 2872, 2875, 2878, 2881, 2883, 2887, 2892, 2895, 2896, 2897, 2898, 2909, 2910, 2912, 2913, 2914, 2917, 2923, 2928, 2934, 2937, 2939, 2949, 2950, 2952, 2953, 2955, 2959, 2966, 2967, 2968, 2970, 2971, 2972, 2974, 2982, 2991, 3001, 3008, 3018, 3020, 3028, 3033, 3034, 3043, 3046, 3048, 3050, 3055, 3056, 3059, 3060, 3068, 3070, 3075, 3079, 3087, 3088, 3096, 3101, 3106, 3110, 3111, 3114, 3115, 3116, 3117, 3119, 3122, 3124, 3125, 3128, 3133, 3135, 3136]
sequences = [x+'.npy' for x in all_data_dict.keys() if int(x.split('_')[0]) in auto_ids]
tr,v,te = du.get_tt_datasets(sequences)
dunp('xianling', tr, v, te)
196/14:
# auto_ids = [2, 3, 4, 7, 11, 14, 23, 26, 27, 28, 30, 31, 33, 38, 39, 53, 55, 57, 59, 61, 65, 66, 70, 71, 73, 82, 101, 104, 105, 106, 109, 119, 131, 144, 146, 149, 150, 157, 159, 160, 163, 164, 165, 171, 172, 176, 180, 183, 185, 187, 188, 193, 201, 204, 205, 219, 223, 226, 229, 230, 233, 237, 242, 243, 245, 251, 252, 256, 258, 261, 267, 270, 272, 274, 276, 278, 279, 282, 283, 285, 289, 290, 293, 300, 301, 304, 305, 306, 314, 315, 321, 326, 327, 330, 331, 335, 337, 338, 342, 345, 347, 350, 358, 363, 367, 368, 373, 375, 376, 380, 381, 382, 383, 384, 386, 392, 393, 397, 399, 406, 407, 410, 414, 421, 422, 429, 435, 437, 446, 449, 453, 454, 458, 460, 467, 474, 484, 487, 493, 494, 498, 501, 502, 503, 512, 518, 523, 524, 526, 527, 530, 536, 537, 541, 545, 546, 551, 553, 555, 556, 560, 561, 563, 564, 567, 573, 575, 580, 581, 585, 590, 592, 595, 599, 600, 601, 608, 616, 620, 625, 627, 628, 632, 644, 646, 649, 653, 654, 660, 661, 664, 667, 668, 672, 679, 681, 682, 683, 697, 700, 701, 703, 704, 706, 707, 708, 710, 711, 715, 720, 728, 730, 733, 734, 737, 746, 747, 749, 750, 751, 756, 767, 779, 781, 784, 786, 788, 789, 799, 800, 806, 807, 811, 812, 820, 821, 825, 826, 827, 828, 835, 837, 840, 841, 844, 846, 848, 851, 852, 860, 863, 864, 865, 866, 868, 870, 872, 878, 881, 884, 887, 888, 889, 890, 894, 896, 902, 903, 904, 909, 911, 914, 917, 920, 923, 924, 935, 942, 946, 948, 950, 953, 954, 963, 966, 968, 971, 980, 987, 990, 991, 992, 993, 995, 997, 999, 1001, 1005, 1006, 1007, 1009, 1011, 1012, 1015, 1018, 1019, 1024, 1028, 1040, 1041, 1043, 1052, 1053, 1057, 1060, 1070, 1074, 1079, 1083, 1086, 1087, 1088, 1090, 1092, 1093, 1095, 1096, 1097, 1099, 1100, 1102, 1103, 1104, 1105, 1106, 1112, 1115, 1120, 1123, 1124, 1126, 1130, 1134, 1136, 1138, 1140, 1142, 1145, 1148, 1151, 1152, 1174, 1178, 1183, 1185, 1186, 1187, 1195, 1206, 1209, 1210, 1214, 1216, 1223, 1233, 1235, 1236, 1237, 1243, 1250, 1251, 1259, 1264, 1269, 1273, 1278, 1279, 1280, 1293, 1295, 1298, 1302, 1303, 1304, 1307, 1309, 1312, 1317, 1318, 1322, 1328, 1333, 1337, 1342, 1344, 1350, 1352, 1359, 1371, 1376, 1378, 1379, 1383, 1392, 1393, 1397, 1399, 1400, 1401, 1404, 1406, 1407, 1408, 1409, 1411, 1426, 1429, 1441, 1442, 1443, 1444, 1447, 1454, 1462, 1466, 1469, 1473, 1474, 1479, 1485, 1486, 1489, 1493, 1495, 1500, 1503, 1505, 1509, 1512, 1517, 1522, 1530, 1532, 1533, 1535, 1540, 1546, 1547, 1549, 1552, 1556, 1557, 1560, 1562, 1569, 1571, 1577, 1583, 1584, 1589, 1590, 1592, 1598, 1599, 1601, 1604, 1605, 1613, 1616, 1617, 1623, 1626, 1628, 1629, 1631, 1635, 1636, 1643, 1650, 1651, 1652, 1654, 1656, 1657, 1667, 1674, 1677, 1680, 1681, 1689, 1695, 1696, 1698, 1700, 1707, 1714, 1715, 1716, 1717, 1718, 1720, 1721, 1724, 1726, 1728, 1734, 1737, 1738, 1741, 1744, 1746, 1749, 1750, 1753, 1757, 1763, 1766, 1767, 1768, 1769, 1770, 1781, 1785, 1789, 1794, 1797, 1799, 1805, 1808, 1809, 1814, 1816, 1818, 1820, 1832, 1835, 1837, 1839, 1851, 1854, 1859, 1864, 1869, 1870, 1871, 1876, 1877, 1881, 1885, 1888, 1891, 1893, 1894, 1901, 1902, 1907, 1916, 1917, 1919, 1920, 1922, 1923, 1928, 1929, 1930, 1931, 1935, 1938, 1939, 1940, 1944, 1946, 1949, 1950, 1959, 1960, 1962, 1963, 1966, 1967, 1968, 1970, 1980, 1987, 1991, 1994, 1995, 2000, 2004, 2009, 2010, 2013, 2015, 2018, 2019, 2025, 2028, 2031, 2032, 2037, 2041, 2043, 2044, 2047, 2056, 2063, 2064, 2067, 2072, 2073, 2077, 2080, 2083, 2084, 2087, 2089, 2090, 2091, 2096, 2099, 2111, 2113, 2114, 2123, 2128, 2135, 2139, 2140, 2142, 2143, 2146, 2147, 2149, 2164, 2166, 2170, 2178, 2180, 2183, 2189, 2190, 2191, 2192, 2195, 2199, 2201, 2203, 2210, 2212, 2215, 2220, 2228, 2231, 2232, 2237, 2240, 2241, 2248, 2249, 2253, 2257, 2258, 2261, 2263, 2276, 2277, 2279, 2280, 2282, 2292, 2295, 2298, 2300, 2318, 2319, 2322, 2326, 2327, 2328, 2329, 2331, 2333, 2334, 2335, 2336, 2337, 2348, 2350, 2351, 2355, 2356, 2368, 2371, 2372, 2374, 2375, 2378, 2379, 2381, 2384, 2386, 2387, 2389, 2395, 2397, 2401, 2402, 2403, 2406, 2414, 2417, 2424, 2425, 2431, 2433, 2434, 2437, 2439, 2440, 2441, 2446, 2451, 2457, 2467, 2468, 2471, 2472, 2473, 2475, 2485, 2487, 2491, 2493, 2496, 2502, 2503, 2510, 2514, 2517, 2521, 2522, 2527, 2528, 2538, 2542, 2544, 2550, 2551, 2552, 2554, 2557, 2560, 2565, 2569, 2571, 2576, 2580, 2585, 2587, 2591, 2592, 2594, 2599, 2603, 2604, 2606, 2610, 2616, 2617, 2619, 2632, 2633, 2634, 2638, 2645, 2647, 2650, 2664, 2665, 2668, 2669, 2670, 2673, 2675, 2676, 2677, 2678, 2687, 2690, 2691, 2692, 2695, 2697, 2700, 2705, 2706, 2709, 2716, 2719, 2724, 2726, 2727, 2728, 2731, 2733, 2734, 2739, 2743, 2744, 2750, 2764, 2765, 2768, 2769, 2770, 2773, 2775, 2776, 2778, 2779, 2789, 2792, 2794, 2795, 2808, 2813, 2819, 2820, 2829, 2831, 2833, 2834, 2835, 2836, 2838, 2839, 2841, 2843, 2845, 2848, 2854, 2856, 2861, 2867, 2870, 2872, 2875, 2878, 2881, 2883, 2887, 2892, 2895, 2896, 2897, 2898, 2909, 2910, 2912, 2913, 2914, 2917, 2923, 2928, 2934, 2937, 2939, 2949, 2950, 2952, 2953, 2955, 2959, 2966, 2967, 2968, 2970, 2971, 2972, 2974, 2982, 2991, 3001, 3008, 3018, 3020, 3028, 3033, 3034, 3043, 3046, 3048, 3050, 3055, 3056, 3059, 3060, 3068, 3070, 3075, 3079, 3087, 3088, 3096, 3101, 3106, 3110, 3111, 3114, 3115, 3116, 3117, 3119, 3122, 3124, 3125, 3128, 3133, 3135, 3136]
# sequences = [x+'.npy' for x in all_data_dict.keys() if int(x.split('_')[0]) in auto_ids]
# tr,v,te = du.get_tt_datasets(sequences)
dump('xianling', tr, v, te)
196/15:
tr,v,te = du.get_tt_datasets()
dump('all', tr, v, te)
197/1:
import utils
import data_utils as du
import os
197/2:
all_data_dict = utils.pkl_load('data/label_dict.pkl')

def dump(name, tr, v, te):
    os.makedirs(os.path.join('datasets', name))
    utils.pkl_dump(tr, os.path.join('datasets', name,'train.dataset'))
    utils.pkl_dump(v, os.path.join('datasets', name,'valid.dataset'))
    utils.pkl_dump(te, os.path.join('datasets', name,'test.dataset'))
197/3:
tr,v,te = du.get_tt_datasets()
dump('all', tr, v, te)
193/18: m = gru.GRUD()
193/19: import gru
193/20: m = gru.GRUD()
193/21: torch.randint(1,10,(10,3,))
193/22: import torch
193/23: a=torch.randint(1,10,(10,3,))
193/24: a
193/25: a=torch.randint(-10,10,(10,3,))
193/26: a
193/27: torch.softmax(a,1)
193/28: a=torch.random(-1,1,(10,3,))
193/29: a=torch.rand(-1,1,(10,3,))
193/30: a=torch.rand((10,3,))
193/31: a
193/32: torch.softmax(a,1)
198/1:
import torch 
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
198/2:
dataset = du.PatientDataset(5)
loader = DataLoader(dataset, shuffle=True, batch_size=5, drop_last=True)
for X,y in loader:
    pass
model = gru.GRUD(aux_loss_cols=['diag', 'enc', 'hilab', 'lolab']).float()
state = model.init_hidden(X.shape[0])
yhat, y_tr, y_aux, state = model(X.float(), state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y_TR: {y_tr.shape}  Y_Aux:{y_aux.shape}  Y: {y.shape}")
198/3:
dataset = du.PatientDataset(5)
loader = DataLoader(dataset, shuffle=True, batch_size=5, drop_last=True)
for X,y in loader:
    pass
model = gru.GRUD(aux_loss_cols=['diag', 'enc', 'hilab', 'lolab']).float()
state = model.init_hidden(X.shape[0])

# yhat, y_tr, y_aux, state = model(X.float(), state)
# print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y_TR: {y_tr.shape}  Y_Aux:{y_aux.shape}  Y: {y.shape}")

yhat, state = gru.predict(model, X,y, state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
200/1:
import torch 
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
200/2:
dataset = du.PatientDataset(5)
loader = DataLoader(dataset, shuffle=True, batch_size=5, drop_last=True)
for X,y in loader:
    pass
model = gru.GRUD(aux_loss_cols=['diag', 'enc', 'hilab', 'lolab']).float()
state = model.init_hidden(X.shape[0])

# yhat, y_tr, y_aux, state = model(X.float(), state)
# print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y_TR: {y_tr.shape}  Y_Aux:{y_aux.shape}  Y: {y.shape}")

yhat, state = gru.predict(model, X,y, state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
200/3: yhat
200/4: torch.softmax(yhat, 1)
193/33: a>1
193/34: a>0.5
193/35: (a>0.5).mean()
193/36: (a>0.5).type(torch.FloatTensor()).mean()
193/37: (a>0.5).astype(torch.FloatTensor()).mean()
193/38: (a>0.5).cast(torch.FloatTensor()).mean()
193/39: (a>0.5).float().mean()
201/1: import pandas as pd
201/2: df = pd.read_pickle('pickles/inputs.pkl')
201/3: df.columns
201/4: df['charges'] = df.RELATED_OP.fillna(0) + df.RELATED_IP.fillna(0)
201/5: df['cumsum'] = df.groupby('AUTO_ID').charges.cumsum()
201/6: xdf = df.sort_values(['AUTO_ID','MONTH_TS'], ascending=False)
201/7: xdf = xdf.drop_duplicates(['AUTO_ID','MONTH_TS'])
201/8: xdf.shape
201/9: xdf = xdf.drop_duplicates(['AUTO_ID'])
201/10: xdf.shape
201/11: xdf.cumsum.describe()
201/12: xdf['cumsum'].describe()
201/13:
def delta_median(s):
    median = s.quantile(0.5)
    return s-median
201/14: xdf['del_median'] = xdf.groupby('AUTO_ID').charges.apply(delta_median)
201/15: xdf['del_median']
201/16: df['del_median'] = df.groupby('AUTO_ID').charges.apply(delta_median)
201/17: df.del_median
201/18: a = df[df.AUTO_ID==1133]
201/19: a
201/20: a1133 = a
201/21: b532 = df[df.AUTO_ID==532]
201/22: c2318 = df.df[AUTO_ID==2318]
201/23: c2318 = df[df.AUTO_ID==2318]
201/24: d1170 = df[dfAUTO_ID==1170]
201/25: d1170 = df[df.AUTO_ID==1170]
201/26: os.makedirs('human_intel')
201/27: import os
201/28: os.makedirs('human_intel')
201/29: a1133.to_csv('human_intel/A.csv')
201/30: b532.to_csv('human_intel/B.csv')
201/31: c2318.to_csv('human_intel/C.csv')
201/32: d1170.to_csv('human_intel/D.csv')
201/33: df.charges.head.median()
201/34: df.charges.head()
201/35: df.charges.head().quantile(0.5)
201/36:
def delta_median(s):
    median = s[s>0].quantile(0.5)
    return s-median
201/37: df['del_median'] = df.groupby('AUTO_ID').charges.apply(delta_median)
201/38: df.del_median
201/39: a1133 = df[df.AUTO_ID==1133]
201/40: a1133
201/41: a1133.charges
201/42: mi = a1133.del_median.min()
201/43: ma = a1133.del_median.max()
201/44: a1133['scaled'] = (a1133.del_median-mi)/(ma-mi)
201/45: a1133.scaled
201/46: a1133.scaled.values
201/47: a1133.to_csv('B.csv')
201/48: a1133.to_csv('B.csv')
201/49: s=a1133.scaled
201/50: s[s>0].quantile(0.5)
201/51: s[s>0].stdev()
201/52: s[s>0].std()
201/53: s[s>0].std()+s[s>0].quantile(0.5)
201/54:
def delta_median(s):
    median = s-s[s>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (s-mi)/(mx-mi)
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std()
    spikes = np.where(s > (scmed+scstd), 1, 0)
    return spikes
201/55: a1133 = df[df.AUTO_ID==1133]
201/56: a1133.columns
201/57: del a1133[['cumsum','del_median']]
201/58: del a1133['cumsum']
201/59: del a1133['del_median']
201/60: a1133['spikes'] = delta_median(a1133.charges)
201/61: import numpy as np
201/62: a1133['spikes'] = delta_median(a1133.charges)
201/63: a1133[a1133.spikes==0]
201/64: a1133[a1133.spikes==1]
201/65:
def delta_median(s):
    median = s-s[s>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (s-mi)/(mx-mi)
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std()
    spikes = np.where(scaled > (scmed+scstd), 1, 0)
    return spikes
201/66: a1133['spikes'] = delta_median(a1133.charges)
201/67: a1133[a1133.spikes==1]
201/68: b532['spikes'] = delta_median(b532.charges)
201/69: b532[b532.spikes==1]
201/70: b532
201/71: df['spikes'] = df.groupby('AUTO_ID').charges.apply(delta_median)
201/72: spk = lambda auto: df[(df.AUTO_ID==auto) & (df.spikes==1)]
201/73: spk(1170)
201/74: spk = lambda auto: df[(df.AUTO_ID==auto) + (df.spikes==1)]
201/75: spk(1170)
201/76: spk = lambda auto: df[(df.AUTO_ID==auto) && (df.spikes==1)]
201/77: spk = lambda auto: df[np.logical_and((df.AUTO_ID==auto), (df.spikes==1))]
201/78: spk(1170)
201/79: spk = lambda auto: df[np.logical_and(df.AUTO_ID==auto, df.spikes==1)]
201/80: spk(1170)
201/81: spk = lambda auto: df.query(f'AUTO_ID=={auto} & spikes==1')
201/82: spk(1170)
201/83: spk = lambda auto: df[(df['AUTO_ID']==auto) & (df['spikes']==1)]
201/84: spk(1170)
201/85: spk = lambda auto: df.loc[(df['AUTO_ID']==auto) & (df['spikes']==1)]
201/86: spk(1170)
201/87: spk = lambda df,auto: df.loc[(df['AUTO_ID']==auto) & (df['spikes']==1)]
201/88: spk(df,1170)
201/89: df.loc[(df['AUTO_ID']==1170) & (df['spikes']==1)]
201/90: df.loc[(df['AUTO_ID']==1170) & (df['spikes']==1), ]
201/91: (df['AUTO_ID']==1170) & (df['spikes']==1)
201/92: (df['AUTO_ID']==1170) #& (df['spikes']==1)
201/93: (df['AUTO_ID']==1170) #& (df['spikes']==1)
201/94: (df['AUTO_ID']==1170) and (df['spikes']==1)
201/95: (df['AUTO_ID']==1170).bool() & (df['spikes']==1).bool()
201/96: np.logical_and(df['AUTO_ID']==1170, df['spikes']==1)
201/97: np.logical_and((df['AUTO_ID']==1170).values, (df['spikes']==1).values)
201/98: a = df[df.AUTO_ID==1170]
201/99: a[a.spikes==1]
201/100: a.columns
201/101: a.spikes
201/102: df.spikes
201/103:
def delta_median(s):
    median = s-s[s>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (s-mi)/(mx-mi)
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std()
    spikes = np.where(scaled > (scmed+scstd), 1, 0)
    return pd.Series(spikes, index=s.index)
201/104: df['spikes'] = df.groupby('AUTO_ID').charges.apply(delta_median)
201/105: spk = lambda auto: df[(df['AUTO_ID']==auto) & (df['spikes']==1)]
201/106: spk(1170)
201/107: spk(2690)
201/108: spk(2481)
201/109: df['avg_charge'] = df['cumsum']*12/(df['MONTH_TS']+1)
201/110: spk(2481)
201/111: df.groupby('AUTO_ID').spikes.sum().describe()
201/112: df.groupby('AUTO_ID').spikes.sum().idxmax()
201/113: df.groupby('AUTO_ID').spikes.sum()
201/114: spk(871)
202/1:
tot = pd.read_csv('datasets/clean_registry_data/total_charges.csv')

aids = tot.AUTO_ID.unique()
fig, axes = plt.subplots(3,3, figsize = (12,12))

for r in range(3):
    for c in range(3):
        if r==0 and c==0:
            aid = 871
        else:
            aid = aids[np.random.randint(0,len(aids))]
        df=tot[tot.AUTO_ID==aid]
        d = pd.to_datetime(df['ADMISSION_DATE'])
        x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
        y = df['CUMULATIVE']
        titl = (d.max()-d.min()).days//30
        axes[r,c].plot(x, y)
        axes[r,c].set_title(f"AID: {aid}, Months: {titl}")
plt.show()
202/2:
import time



import data_utils as du
import pandas as pd
df = pd.read_pickle('pickles/inputs.pkl')
202/3:
import time

import pandas as pd
df = pd.read_pickle('pickles/inputs.pkl')
202/4:
tot = pd.read_csv('datasets/clean_registry_data/total_charges.csv')

aids = tot.AUTO_ID.unique()
fig, axes = plt.subplots(3,3, figsize = (12,12))

for r in range(3):
    for c in range(3):
        if r==0 and c==0:
            aid = 871
        else:
            aid = aids[np.random.randint(0,len(aids))]
        df=tot[tot.AUTO_ID==aid]
        d = pd.to_datetime(df['ADMISSION_DATE'])
        x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
        y = df['CUMULATIVE']
        titl = (d.max()-d.min()).days//30
        axes[r,c].plot(x, y)
        axes[r,c].set_title(f"AID: {aid}, Months: {titl}")
plt.show()
202/5:
tot = pd.read_csv('preprocessing/datasets/clean_registry_data/total_charges.csv')

aids = tot.AUTO_ID.unique()
fig, axes = plt.subplots(3,3, figsize = (12,12))

for r in range(3):
    for c in range(3):
        if r==0 and c==0:
            aid = 871
        else:
            aid = aids[np.random.randint(0,len(aids))]
        df=tot[tot.AUTO_ID==aid]
        d = pd.to_datetime(df['ADMISSION_DATE'])
        x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
        y = df['CUMULATIVE']
        titl = (d.max()-d.min()).days//30
        axes[r,c].plot(x, y)
        axes[r,c].set_title(f"AID: {aid}, Months: {titl}")
plt.show()
202/6:
tot = pd.read_csv('../preprocessing/datasets/clean_registry_data/total_charges.csv')

aids = tot.AUTO_ID.unique()
fig, axes = plt.subplots(3,3, figsize = (12,12))

for r in range(3):
    for c in range(3):
        if r==0 and c==0:
            aid = 871
        else:
            aid = aids[np.random.randint(0,len(aids))]
        df=tot[tot.AUTO_ID==aid]
        d = pd.to_datetime(df['ADMISSION_DATE'])
        x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
        y = df['CUMULATIVE']
        titl = (d.max()-d.min()).days//30
        axes[r,c].plot(x, y)
        axes[r,c].set_title(f"AID: {aid}, Months: {titl}")
plt.show()
202/7:
from matplolib import pyplot as plt 
tot = pd.read_csv('../preprocessing/datasets/clean_registry_data/total_charges.csv')

aids = tot.AUTO_ID.unique()
fig, axes = plt.subplots(3,3, figsize = (12,12))

for r in range(3):
    for c in range(3):
        if r==0 and c==0:
            aid = 871
        else:
            aid = aids[np.random.randint(0,len(aids))]
        df=tot[tot.AUTO_ID==aid]
        d = pd.to_datetime(df['ADMISSION_DATE'])
        x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
        y = df['CUMULATIVE']
        titl = (d.max()-d.min()).days//30
        axes[r,c].plot(x, y)
        axes[r,c].set_title(f"AID: {aid}, Months: {titl}")
plt.show()
202/8:
from matplotlib import pyplot as plt 
tot = pd.read_csv('../preprocessing/datasets/clean_registry_data/total_charges.csv')

aids = tot.AUTO_ID.unique()
fig, axes = plt.subplots(3,3, figsize = (12,12))

for r in range(3):
    for c in range(3):
        if r==0 and c==0:
            aid = 871
        else:
            aid = aids[np.random.randint(0,len(aids))]
        df=tot[tot.AUTO_ID==aid]
        d = pd.to_datetime(df['ADMISSION_DATE'])
        x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
        y = df['CUMULATIVE']
        titl = (d.max()-d.min()).days//30
        axes[r,c].plot(x, y)
        axes[r,c].set_title(f"AID: {aid}, Months: {titl}")
plt.show()
202/9: tot.columns
202/10:
from matplotlib import pyplot as plt 
tot = pd.read_csv('../preprocessing/datasets/clean_registry_data/total_charges.csv')

aids = tot.AUTO_ID.unique()
fig, axes = plt.subplots(3,3, figsize = (12,12))

for r in range(3):
    for c in range(3):
        if r==0 and c==0:
            aid = 871
        else:
            aid = aids[np.random.randint(0,len(aids))]
        df=tot[tot.AUTO_ID==aid]
        d = pd.to_datetime(df['ADMISSION_DATE'])
        x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
        y = df['CUMSUM']
        titl = (d.max()-d.min()).days//30
        axes[r,c].plot(x, y)
        axes[r,c].set_title(f"AID: {aid}, Months: {titl}")
plt.show()
201/115: spk(871).to_clipboard()
201/116: df[df.AUTO_ID==871].to_clpiboard()
201/117: df[df.AUTO_ID==871].to_clipboard()
201/118:
def delta_median(s):
    median = s-s[s>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (s-mi)/(mx-mi)
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std()
    spikes = np.where(scaled > (scmed+2*scstd), 1, 0)
    return pd.Series(spikes, index=s.index)
201/119: df['spikes'] = df.groupby('AUTO_ID').charges.apply(delta_median)
201/120: spk(871)
201/121:
def delta_median(s):
    median = s-s[s>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (s-mi)/(mx-mi)
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std()
    spikes = np.where(scaled > (scmed+1.5*scstd), 1, 0)
    return pd.Series(spikes, index=s.index)
201/122: df['spikes'] = df.groupby('AUTO_ID').charges.apply(delta_median)
201/123: spk(871)
201/124: df.sort_values(['AUTO_ID','MONTH_TS'], ascending=False).drop_duplicates(['AUTO_ID']).describe(cumsum)
201/125: df.sort_values(['AUTO_ID','MONTH_TS'], ascending=False).drop_duplicates(['AUTO_ID'])['cumsum'].describe()
201/126: df.sort_values(['AUTO_ID','MONTH_TS'], ascending=False).drop_duplicates(['AUTO_ID'])['cumsum']argmin()
201/127: df.sort_values(['AUTO_ID','MONTH_TS'], ascending=False).drop_duplicates(['AUTO_ID'])['cumsum'].argmin()
201/128: df[-1]
201/129: df.loc[-1]
201/130: df.tail(1)
201/131: df.sort_values(['AUTO_ID','MONTH_TS'], ascending=False).drop_duplicates(['AUTO_ID'])['cumsum'].max()
201/132: df.sort_values(['AUTO_ID','MONTH_TS'], ascending=False).drop_duplicates(['AUTO_ID'])['cumsum'].argmax()
202/11:
from matplotlib import pyplot as plt 
tot = pd.read_csv('../preprocessing/datasets/clean_registry_data/total_charges.csv')

aids = tot.AUTO_ID.unique()
fig, axes = plt.subplots(3,3, figsize = (12,12))

for r in range(3):
    for c in range(3):
        if r==0 and c==0:
            aid = 2821
        else:
            aid = aids[np.random.randint(0,len(aids))]
        df=tot[tot.AUTO_ID==aid]
        d = pd.to_datetime(df['ADMISSION_DATE'])
        x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
        y = df['CUMSUM']
        titl = (d.max()-d.min()).days//30
        axes[r,c].plot(x, y)
        axes[r,c].set_title(f"AID: {aid}, Months: {titl}")
plt.show()
201/133: df[df.AUTO_ID==2821].to_clipboard()
201/134: chk-df.sort_values(['AUTO_ID','MONTH_TS'], ascending=False).drop_duplicates(['AUTO_ID'])
201/135: chk=df.sort_values(['AUTO_ID','MONTH_TS'], ascending=False).drop_duplicates(['AUTO_ID'])
201/136: chk[chk.AUTO_ID=2821]
201/137: chk[chk.AUTO_ID==2821]
201/138: chk['cumsum'].describe()
201/139: chk.sort_values('cumsum',ascending=False)
201/140: df[df.AUTO_ID==34].to_clipboard()
202/12:
from matplotlib import pyplot as plt 
tot = pd.read_csv('../preprocessing/datasets/clean_registry_data/total_charges.csv')

aids = tot.AUTO_ID.unique()
fig, axes = plt.subplots(3,3, figsize = (12,12))

for r in range(3):
    for c in range(3):
        if r==0 and c==0:
            aid = 34
        else:
            aid = aids[np.random.randint(0,len(aids))]
        df=tot[tot.AUTO_ID==aid]
        d = pd.to_datetime(df['ADMISSION_DATE'])
        x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
        y = df['CUMSUM']
        titl = (d.max()-d.min()).days//30
        axes[r,c].plot(x, y)
        axes[r,c].set_title(f"AID: {aid}, Months: {titl}")
plt.show()
202/13:
from matplotlib import pyplot as plt 
tot = pd.read_csv('../preprocessing/datasets/clean_registry_data/total_charges.csv')

aids = tot.AUTO_ID.unique()
fig, axes = plt.subplots(3,3, figsize = (12,12))

for r in range(3):
    for c in range(3):
        if r==0 and c==0:
            aid = 1699
        else:
            aid = aids[np.random.randint(0,len(aids))]
        df=tot[tot.AUTO_ID==aid]
        d = pd.to_datetime(df['ADMISSION_DATE'])
        x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
        y = df['CUMSUM']
        titl = (d.max()-d.min()).days//30
        axes[r,c].plot(x, y)
        axes[r,c].set_title(f"AID: {aid}, Months: {titl}")
plt.show()
201/141: df[df.AUTO_ID==34].spikes.sum()
201/142:
def delta_median(s):
    median = s-s[s>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (s-mi)/(mx-mi)
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std()
    spikes = np.where(scaled > (scmed+1.2*scstd), 1, 0)
    return pd.Series(spikes, index=s.index)
201/143: df['spikes'] = df.groupby('AUTO_ID').charges.apply(delta_median)
201/144: df[df.AUTO_ID==34].spikes.sum()
201/145:
def delta_median(charge):
    median = charge-charge[charge>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (charge-mi)/(mx-mi)
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std()
    spikes = np.where(charge > (scmed+scstd), 1, 0)
    return spikes
201/146: df['spikes'] = df.groupby('AUTO_ID').charges.apply(delta_median)
201/147: df.groupby('AUTO_ID').spikes.describe()
201/148:
def delta_median(charge):
    median = charge-charge[charge>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (charge-mi)/(mx-mi)
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std()
    spikes = np.where(charge > (scmed+scstd), 1, 0)
    return pd.Series(spikes, index=charge.index)
201/149: df['spikes'] = df.groupby('AUTO_ID').charges.apply(delta_median)
201/150: df.groupby('AUTO_ID').spikes.sum().describe()
201/151:
def delta_median(charge,std_diff):
    median = charge-charge[charge>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (charge-mi)/(mx-mi)
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std()
    spikes = pd.Series(np.where(charge > std_diff*(scmed+scstd), 1, 0), index=charge.index)
    return spikes
201/152: df['spikes'] = df.groupby('AUTO_ID').charges.apply(delta_median, std_diff=1.5)
201/153: df.groupby('AUTO_ID').spikes.sum().describe()
201/154: df.spikes
201/155: df.groupby('AUTO_ID').spikes.mean().describe()
201/156: spk(871)
201/157:
def delta_median(charge,std_diff):
    median = charge-(charge[charge>0].quantile(0.5))
    mi, mx = median.min(), median.max()
    scaled = (charge-mi)/(mx-mi)
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std()
    spikes = pd.Series(np.where(charge > std_diff*(scmed+scstd), 1, 0), index=charge.index)
    return spikes
201/158:
def delta_median(charge,std_diff):
    median_diff = charge-(charge[charge>0].quantile(0.5))
    mi, mx = median.min(), median.max()
    scaled = (median_diff-mi)/(mx-mi)
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std()
    spikes = pd.Series(np.where(charge > std_diff*(scmed+scstd), 1, 0), index=charge.index)
    return spikes
201/159: df['spikes'] = df.groupby('AUTO_ID').charges.apply(delta_median, std_diff=1.5)
201/160:
def delta_median(charge,std_diff):
    median_diff = charge-(charge[charge>0].quantile(0.5))
    mi, mx = median_diff.min(), median_diff.max()
    scaled = (median_diff-mi)/(mx-mi)
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std()
    spikes = pd.Series(np.where(charge > std_diff*(scmed+scstd), 1, 0), index=charge.index)
    return spikes
201/161: df['spikes'] = df.groupby('AUTO_ID').charges.apply(delta_median, std_diff=1.5)
201/162: df.groupby('AUTO_ID').spikes.sum().describe()
201/163: spk(1170)
202/14:
from matplotlib import pyplot as plt 
tot = pd.read_csv('../preprocessing/datasets/clean_registry_data/total_charges.csv')

aids = tot.AUTO_ID.unique()
fig, axes = plt.subplots(3,3, figsize = (12,12))

for r in range(3):
    for c in range(3):
        if r==0 and c==0:
            aid = 1170
        else:
            aid = aids[np.random.randint(0,len(aids))]
        df=tot[tot.AUTO_ID==aid]
        d = pd.to_datetime(df['ADMISSION_DATE'])
        x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
        y = df['CUMSUM']
        titl = (d.max()-d.min()).days//30
        axes[r,c].plot(x, y)
        axes[r,c].set_title(f"AID: {aid}, Months: {titl}")
plt.show()
202/15:
from matplotlib import pyplot as plt 
tot = pd.read_csv('../preprocessing/datasets/clean_registry_data/total_charges.csv')

aids = tot.AUTO_ID.unique()
fig, axes = plt.subplots(3,3, figsize = (12,12))

for r in range(3):
    for c in range(3):
        if r==0 and c==0:
            aid = 1133
            
        else:
            aid = aids[np.random.randint(0,len(aids))]
        df=tot[tot.AUTO_ID==aid]
        d = pd.to_datetime(df['ADMISSION_DATE'])
        x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
        y = df['CUMSUM']
        titl = (d.max()-d.min()).days//30
        axes[r,c].plot(x, y)
        axes[r,c].set_title(f"AID: {aid}, Months: {titl}")
plt.show()
201/164:
def delta_median(s):
    median = s-s[s>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (median-mi)/(mx-mi)
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std()
    spikes = np.where(scaled > (scmed+1.5*scstd), 1, 0)
    return pd.Series(spikes, index=s.index)
201/165: df['spikes'] = df.groupby('AUTO_ID').charges.apply(delta_median)
201/166: spk(1877)
201/167: spk(1133)
201/168:
def delta_median(s):
    median = s-s[s>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (median-mi)/(mx-mi)
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std()
    spikes = np.where(scaled > (scmed + std_diff*scstd), 1, 0)
    return pd.Series(spikes, index=s.index)

df['spikes'] = df.groupby('AUTO_ID').charges.apply(delta_median, std_diff=1)
201/169:
def delta_median(s, std_diff):
    median = s-s[s>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (median-mi)/(mx-mi)
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std()
    spikes = np.where(scaled > (scmed + std_diff*scstd), 1, 0)
    return pd.Series(spikes, index=s.index)

df['spikes'] = df.groupby('AUTO_ID').charges.apply(delta_median, std_diff=1)
201/170: spk(1133)
201/171: spk(1877)
201/172: df[df.AUTO_ID==1877].to_clipboard()
201/173: len(df)
201/174: df[df.charges>(100000/12)].shape
201/175: df[df.charges>(100000/12)].AUTO_ID.unique()
201/176: len(df[df.charges>(100000/12)].AUTO_ID.unique())
203/1:
import data_utils as du

i = du.IBDSequenceGenerator()
i.write_to_disk('annual_avg', True)
204/1:
import data_utils as du

i = du.IBDSequenceGenerator()
i.write_to_disk('annual_avg', True)
205/1:
import data_utils as du

i = du.IBDSequenceGenerator()
i.write_to_disk('annual_avg', True)
206/1:
import data_utils as du

i = du.IBDSequenceGenerator()
i.write_to_disk('annual_avg', True)
207/1:
import data_utils as du

i = du.IBDSequenceGenerator()
i.write_to_disk('annual_avg', True)
208/1:
import data_utils as du

i = du.IBDSequenceGenerator()
i.write_to_disk('annual_avg', True)
208/2:
import utils
d = utils.pkl_load('data/annual_avg_label_dict.pkl')
208/3: d
209/1:
import utils
import data_utils as du
import os
209/2:
tr,v,te = du.get_tt_datasets(label_dict_file='annual_avg_label_dict')
utils.dump_dataset('all_annual_avg', tr, v, te)
210/1:
import utils
import data_utils as du
import os
210/2:
# tr,v,te = du.get_tt_datasets()
# utils.dump_dataset('all', tr, v, te)
210/3:
# all_data_dict = utils.pkl_load('data/3class_label_dict.pkl')
# auto_ids = [2, 3, 4, 7, 11, 14, 23, 26, 27, 28, 30, 31, 33, 38, 39, 53, 55, 57, 59, 61, 65, 66, 70, 71, 73, 82, 101, 104, 105, 106, 109, 119, 131, 144, 146, 149, 150, 157, 159, 160, 163, 164, 165, 171, 172, 176, 180, 183, 185, 187, 188, 193, 201, 204, 205, 219, 223, 226, 229, 230, 233, 237, 242, 243, 245, 251, 252, 256, 258, 261, 267, 270, 272, 274, 276, 278, 279, 282, 283, 285, 289, 290, 293, 300, 301, 304, 305, 306, 314, 315, 321, 326, 327, 330, 331, 335, 337, 338, 342, 345, 347, 350, 358, 363, 367, 368, 373, 375, 376, 380, 381, 382, 383, 384, 386, 392, 393, 397, 399, 406, 407, 410, 414, 421, 422, 429, 435, 437, 446, 449, 453, 454, 458, 460, 467, 474, 484, 487, 493, 494, 498, 501, 502, 503, 512, 518, 523, 524, 526, 527, 530, 536, 537, 541, 545, 546, 551, 553, 555, 556, 560, 561, 563, 564, 567, 573, 575, 580, 581, 585, 590, 592, 595, 599, 600, 601, 608, 616, 620, 625, 627, 628, 632, 644, 646, 649, 653, 654, 660, 661, 664, 667, 668, 672, 679, 681, 682, 683, 697, 700, 701, 703, 704, 706, 707, 708, 710, 711, 715, 720, 728, 730, 733, 734, 737, 746, 747, 749, 750, 751, 756, 767, 779, 781, 784, 786, 788, 789, 799, 800, 806, 807, 811, 812, 820, 821, 825, 826, 827, 828, 835, 837, 840, 841, 844, 846, 848, 851, 852, 860, 863, 864, 865, 866, 868, 870, 872, 878, 881, 884, 887, 888, 889, 890, 894, 896, 902, 903, 904, 909, 911, 914, 917, 920, 923, 924, 935, 942, 946, 948, 950, 953, 954, 963, 966, 968, 971, 980, 987, 990, 991, 992, 993, 995, 997, 999, 1001, 1005, 1006, 1007, 1009, 1011, 1012, 1015, 1018, 1019, 1024, 1028, 1040, 1041, 1043, 1052, 1053, 1057, 1060, 1070, 1074, 1079, 1083, 1086, 1087, 1088, 1090, 1092, 1093, 1095, 1096, 1097, 1099, 1100, 1102, 1103, 1104, 1105, 1106, 1112, 1115, 1120, 1123, 1124, 1126, 1130, 1134, 1136, 1138, 1140, 1142, 1145, 1148, 1151, 1152, 1174, 1178, 1183, 1185, 1186, 1187, 1195, 1206, 1209, 1210, 1214, 1216, 1223, 1233, 1235, 1236, 1237, 1243, 1250, 1251, 1259, 1264, 1269, 1273, 1278, 1279, 1280, 1293, 1295, 1298, 1302, 1303, 1304, 1307, 1309, 1312, 1317, 1318, 1322, 1328, 1333, 1337, 1342, 1344, 1350, 1352, 1359, 1371, 1376, 1378, 1379, 1383, 1392, 1393, 1397, 1399, 1400, 1401, 1404, 1406, 1407, 1408, 1409, 1411, 1426, 1429, 1441, 1442, 1443, 1444, 1447, 1454, 1462, 1466, 1469, 1473, 1474, 1479, 1485, 1486, 1489, 1493, 1495, 1500, 1503, 1505, 1509, 1512, 1517, 1522, 1530, 1532, 1533, 1535, 1540, 1546, 1547, 1549, 1552, 1556, 1557, 1560, 1562, 1569, 1571, 1577, 1583, 1584, 1589, 1590, 1592, 1598, 1599, 1601, 1604, 1605, 1613, 1616, 1617, 1623, 1626, 1628, 1629, 1631, 1635, 1636, 1643, 1650, 1651, 1652, 1654, 1656, 1657, 1667, 1674, 1677, 1680, 1681, 1689, 1695, 1696, 1698, 1700, 1707, 1714, 1715, 1716, 1717, 1718, 1720, 1721, 1724, 1726, 1728, 1734, 1737, 1738, 1741, 1744, 1746, 1749, 1750, 1753, 1757, 1763, 1766, 1767, 1768, 1769, 1770, 1781, 1785, 1789, 1794, 1797, 1799, 1805, 1808, 1809, 1814, 1816, 1818, 1820, 1832, 1835, 1837, 1839, 1851, 1854, 1859, 1864, 1869, 1870, 1871, 1876, 1877, 1881, 1885, 1888, 1891, 1893, 1894, 1901, 1902, 1907, 1916, 1917, 1919, 1920, 1922, 1923, 1928, 1929, 1930, 1931, 1935, 1938, 1939, 1940, 1944, 1946, 1949, 1950, 1959, 1960, 1962, 1963, 1966, 1967, 1968, 1970, 1980, 1987, 1991, 1994, 1995, 2000, 2004, 2009, 2010, 2013, 2015, 2018, 2019, 2025, 2028, 2031, 2032, 2037, 2041, 2043, 2044, 2047, 2056, 2063, 2064, 2067, 2072, 2073, 2077, 2080, 2083, 2084, 2087, 2089, 2090, 2091, 2096, 2099, 2111, 2113, 2114, 2123, 2128, 2135, 2139, 2140, 2142, 2143, 2146, 2147, 2149, 2164, 2166, 2170, 2178, 2180, 2183, 2189, 2190, 2191, 2192, 2195, 2199, 2201, 2203, 2210, 2212, 2215, 2220, 2228, 2231, 2232, 2237, 2240, 2241, 2248, 2249, 2253, 2257, 2258, 2261, 2263, 2276, 2277, 2279, 2280, 2282, 2292, 2295, 2298, 2300, 2318, 2319, 2322, 2326, 2327, 2328, 2329, 2331, 2333, 2334, 2335, 2336, 2337, 2348, 2350, 2351, 2355, 2356, 2368, 2371, 2372, 2374, 2375, 2378, 2379, 2381, 2384, 2386, 2387, 2389, 2395, 2397, 2401, 2402, 2403, 2406, 2414, 2417, 2424, 2425, 2431, 2433, 2434, 2437, 2439, 2440, 2441, 2446, 2451, 2457, 2467, 2468, 2471, 2472, 2473, 2475, 2485, 2487, 2491, 2493, 2496, 2502, 2503, 2510, 2514, 2517, 2521, 2522, 2527, 2528, 2538, 2542, 2544, 2550, 2551, 2552, 2554, 2557, 2560, 2565, 2569, 2571, 2576, 2580, 2585, 2587, 2591, 2592, 2594, 2599, 2603, 2604, 2606, 2610, 2616, 2617, 2619, 2632, 2633, 2634, 2638, 2645, 2647, 2650, 2664, 2665, 2668, 2669, 2670, 2673, 2675, 2676, 2677, 2678, 2687, 2690, 2691, 2692, 2695, 2697, 2700, 2705, 2706, 2709, 2716, 2719, 2724, 2726, 2727, 2728, 2731, 2733, 2734, 2739, 2743, 2744, 2750, 2764, 2765, 2768, 2769, 2770, 2773, 2775, 2776, 2778, 2779, 2789, 2792, 2794, 2795, 2808, 2813, 2819, 2820, 2829, 2831, 2833, 2834, 2835, 2836, 2838, 2839, 2841, 2843, 2845, 2848, 2854, 2856, 2861, 2867, 2870, 2872, 2875, 2878, 2881, 2883, 2887, 2892, 2895, 2896, 2897, 2898, 2909, 2910, 2912, 2913, 2914, 2917, 2923, 2928, 2934, 2937, 2939, 2949, 2950, 2952, 2953, 2955, 2959, 2966, 2967, 2968, 2970, 2971, 2972, 2974, 2982, 2991, 3001, 3008, 3018, 3020, 3028, 3033, 3034, 3043, 3046, 3048, 3050, 3055, 3056, 3059, 3060, 3068, 3070, 3075, 3079, 3087, 3088, 3096, 3101, 3106, 3110, 3111, 3114, 3115, 3116, 3117, 3119, 3122, 3124, 3125, 3128, 3133, 3135, 3136]
# sequences = [x+'.npy' for x in all_data_dict.keys() if int(x.split('_')[0]) in auto_ids]
# tr,v,te = du.get_tt_datasets(sequences)
# utils.dump_dataset('xianling', tr, v, te)
210/4:
tr,v,te = du.get_tt_datasets(label_dict_file='annual_avg_label_dict')
utils.dump_dataset('all_annual_avg', tr, v, te)
210/5:
tr,v,te = du.get_tt_datasets(label_dict_file='annual_avg_label_dict')
du.dump_dataset('all_annual_avg', tr, v, te)
210/6:
import utils
d = utils.pkl_load('data/annual_avg_label_dict.pkl')
d.values().unique()
210/7:
import utils
d = utils.pkl_load('data/annual_avg_label_dict.pkl')
set(d.values())
208/4:
df = pd.read_pickle('pickles/inputs.pkl')
with open('pickles/col_dict.pkl','rb') as f:
    col_dict = pickle.load(f)
return df, col_dict
208/5:
import pandas as pd
import pickle

df = pd.read_pickle('pickles/inputs.pkl')
with open('pickles/col_dict.pkl','rb') as f:
    col_dict = pickle.load(f)
return df, col_dict
208/6: df.columns
201/177: df.ix[145246]
201/178: df.ix[145245]
201/179: df[df.AUTO_ID==1875].to_clipboard()
208/7:
dforig = df.copy()
df=df[df.AUTO_ID.isin([1875,1877])]
208/8:
def missingness_indicators(padded):
    missing_mask = padded.copy()
    missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
    missing_delta = missing_mask.copy()

    time_interval = 1 #1 month with padding. For without padding, maintain PADDED col, then filter on that.
    missing_delta['tmp_delta'] = time_interval
    for observed in missing_delta.columns:
        missing_delta['obs_delays'] = missing_delta.groupby('AUTO_ID')[observed].cumsum()
        missing_delta[observed] = missing_delta.groupby(['AUTO_ID', 'obs_delays'])['tmp_delta'].cumsum()
    missing_delta = missing_delta.drop(['tmp_delta', 'obs_delays'], axis=1)

    missing_mask.columns = ['AUTO_ID']+[f'MISSING_{x}' for x in missing_mask.columns[1:]]
    missing_delta.columns = ['AUTO_ID']+[f'DELTA_{x}' for x in  missing_mask.columns[1:]]
    return missing_mask.drop(['AUTO_ID'],axis=1), missing_delta.drop(['AUTO_ID'],axis=1)
208/9: missingness_indicators(df)
208/10: _,  x = missingness_indicators(df)
208/11: x.loc[x.AUTO_ID==1877, 'DELTA_MISSING_DIAG_COLONOSCOPY']
208/12:
def missingness_indicators(padded):
    missing_mask = padded.copy()
    missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
    missing_delta = missing_mask.copy()

    time_interval = 1 #1 month with padding. For without padding, maintain PADDED col, then filter on that.
    missing_delta['tmp_delta'] = time_interval
    for observed in missing_delta.columns:
        missing_delta['obs_delays'] = missing_delta.groupby('AUTO_ID')[observed].cumsum()
        missing_delta[observed] = missing_delta.groupby(['AUTO_ID', 'obs_delays'])['tmp_delta'].cumsum()
    missing_delta = missing_delta.drop(['tmp_delta', 'obs_delays'], axis=1)

    missing_mask.columns = ['AUTO_ID']+[f'MISSING_{x}' for x in missing_mask.columns[1:]]
    missing_delta.columns = ['AUTO_ID']+[f'DELTA_{x}' for x in  missing_mask.columns[1:]]
    return missing_mask, missing_delta
208/13: _,  x = missingness_indicators(df)
208/14: x.loc[x.AUTO_ID==1877, 'DELTA_MISSING_DIAG_COLONOSCOPY']
208/15:
dforig = df.copy()
df=dforig[dforig.AUTO_ID.isin([1875,1877])]
208/16:
def missingness_indicators(padded):
    missing_mask = padded.copy()
    missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
    missing_delta = missing_mask.copy()

    time_interval = 1 #1 month with padding. For without padding, maintain PADDED col, then filter on that.
    missing_delta['tmp_delta'] = time_interval
    for observed in missing_delta.columns:
        missing_delta['obs_delays'] = missing_delta.groupby('AUTO_ID')[observed].cumsum()
        missing_delta[observed] = missing_delta.groupby(['AUTO_ID', 'obs_delays'])['tmp_delta'].cumsum()
    missing_delta = missing_delta.drop(['tmp_delta', 'obs_delays'], axis=1)

    missing_mask.columns = ['AUTO_ID']+[f'MISSING_{x}' for x in missing_mask.columns[1:]]
    missing_delta.columns = ['AUTO_ID']+[f'DELTA_{x}' for x in  missing_mask.columns[1:]]
    return missing_mask, missing_delta
208/17: _,  x = missingness_indicators(df)
208/18: x.loc[x.AUTO_ID==1877, 'DELTA_MISSING_DIAG_COLONOSCOPY']
208/19: x
208/20: x.ix[145246]
208/21:
def missingness_indicators(padded):
    missing_mask = padded.copy()
    missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
    missing_delta = missing_mask.copy()

    time_interval = 1 #1 month with padding. For without padding, maintain PADDED col, then filter on that.
    missing_delta['tmp_delta'] = time_interval
    for observed in missing_delta.columns:
        missing_delta['obs_delays'] = missing_delta.groupby('AUTO_ID')[observed].cumsum()
        missing_delta[observed] = missing_delta.groupby(['AUTO_ID', 'obs_delays'])['tmp_delta'].cumsum()
    missing_delta = missing_delta.drop(['tmp_delta', 'obs_delays'], axis=1)

    missing_mask.columns = ['AUTO_ID']+[f'MISSING_{x}' for x in missing_mask.columns[1:]]
    missing_delta.columns = ['AUTO_ID']+[f'DELTA_{x}' for x in  missing_mask.columns[1:]]
    return missing_mask.drop(['AUTO_ID'],axis=1), missing_delta.drop(['AUTO_ID'],axis=1)
208/22: _,  x = missingness_indicators(df)
208/23: x.ix[145246]
208/24: x.ix[145246]['DELTA_MISSING_DIAG_COLONOSCOPY']
208/25:
def missingness_indicators(padded):
    missing_mask = padded.copy()
    missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
    missing_delta = missing_mask.copy()

    time_interval = 1 #1 month with padding. For without padding, maintain PADDED col, then filter on that.
    missing_delta['tmp_delta'] = time_interval
    for observed in missing_delta.columns:
        missing_delta['obs_delays'] = missing_delta.groupby('AUTO_ID')[observed].cumsum()
        missing_delta[observed] = missing_delta.groupby(['AUTO_ID', 'obs_delays'])['tmp_delta'].cumsum()
    missing_delta = missing_delta.drop(['tmp_delta', 'obs_delays'], axis=1)

    missing_mask.columns = ['AUTO_ID']+[f'MISSING_{x}' for x in missing_mask.columns[1:]]
    missing_delta.columns = ['AUTO_ID']+[f'DELTA_{x}' for x in  missing_mask.columns[1:]]
    return missing_mask, missing_delta
208/26: _,  x = missingness_indicators(df)
208/27: x.ix[145246]['DELTA_MISSING_DIAG_COLONOSCOPY']
208/28:
missing_mask = df.copy()
missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
missing_delta = missing_mask.copy()

missing_delta.ix[145246]
208/29:
missing_mask = df.copy()
missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
missing_delta = missing_mask.copy()

missing_delta.ix[145246]['DELTA_MISSING_DIAG_COLONOSCOPY']
208/30: missing_mask.ix[145246]['DELTA_MISSING_DIAG_COLONOSCOPY']
208/31: df.ix[145246]['DELTA_MISSING_DIAG_COLONOSCOPY']
208/32: df.ix[145246]['DIAG_COLONOSCOPY']
208/33: df.ix[145246]['DIAG_COLONOSCOPY'].notnull()
208/34:
dforig = df.copy()
df=dforig[dforig.AUTO_ID.isin([1875,1877])]
df = df[:,:48]
208/35:
dforig = df.copy()
df=dforig[dforig.AUTO_ID.isin([1875,1877])]
df = df.iloc[:,:48]
208/36:
dforig = pd.read_pickle('pickles/inputs.pkl')
with open('pickles/col_dict.pkl','rb') as f:
    col_dict = pickle.load(f)
208/37:
dforig = pd.read_pickle('pickles/monthwise_inputs.pkl')
with open('pickles/col_dict.pkl','rb') as f:
    col_dict = pickle.load(f)
208/38:
df=dforig[dforig.AUTO_ID.isin([1875,1877])].copy()
df = df.iloc[:,:48]
df.columns
208/39:
missing_mask = df.copy()
missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
missing_delta = missing_mask.copy()

missing_mask.ix[145246]['DIAG_COLONOSCOPY']
208/40:
def missingness_indicators(padded):
    missing_mask = padded.copy()
    missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
    missing_delta = missing_mask.copy()

    time_interval = 1 #1 month with padding. For without padding, maintain PADDED col, then filter on that.
    missing_delta['tmp_delta'] = time_interval
    for observed in missing_delta.columns:
        missing_delta['obs_delays'] = missing_delta.groupby('AUTO_ID')[observed].cumsum()
        missing_delta[observed] = missing_delta.groupby(['AUTO_ID', 'obs_delays'])['tmp_delta'].cumsum()
    missing_delta = missing_delta.drop(['tmp_delta', 'obs_delays'], axis=1)

#     missing_mask.columns = ['AUTO_ID']+[f'MISSING_{x}' for x in missing_mask.columns[1:]]
#     missing_delta.columns = ['AUTO_ID']+[f'DELTA_{x}' for x in  missing_mask.columns[1:]]
    return missing_mask, missing_delta
208/41:
m, t = missingness_indicators(df)
m.ix[145246]['DIAG_COLONOSCOPY'], t.ix[145246]['DIAG_COLONOSCOPY']
208/42:
missing_mask = padded.copy()
missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
missing_delta = missing_mask.copy()


# m, t = missingness_indicators(df)
# m.ix[145246]['DIAG_COLONOSCOPY'], t.ix[145246]['DIAG_COLONOSCOPY']
208/43:
missing_mask = df.copy()
missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
missing_delta = missing_mask.copy()


# m, t = missingness_indicators(df)
# m.ix[145246]['DIAG_COLONOSCOPY'], t.ix[145246]['DIAG_COLONOSCOPY']
208/44:
missing_mask = df.copy()
missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
missing_delta = missing_mask.copy()

time_interval = 1 #1 month with padding. For without padding, maintain PADDED col, then filter on that.
missing_delta['tmp_delta'] = time_interval
208/45: missing_delta.ix[145246]['DIAG_COLONOSCOPY']
208/46: missing_delta.ix[145246:]['DIAG_COLONOSCOPY']
208/47:
observed = 'DIAG_COLONOSCOPY'
missing_delta['obs_delays'] = missing_delta.groupby('AUTO_ID')[observed].cumsum()
208/48: missing_delta.ix[145246:]['DIAG_COLONOSCOPY']
208/49:
# missing_delta.ix[145246:]['DIAG_COLONOSCOPY']
missing_delta.ix[145246:]['obs_delays']
208/50:
# missing_delta.ix[145246:]['DIAG_COLONOSCOPY']
missing_delta.ix[145243:]['obs_delays']
208/51:
# missing_delta.ix[145246:]['DIAG_COLONOSCOPY']
missing_delta['obs_delays']
208/52:
# missing_delta.ix[145246:]['DIAG_COLONOSCOPY']
missing_delta[['AUTO_ID',observed',''obs_delays']].to_clipboard()
208/53:
# missing_delta.ix[145246:]['DIAG_COLONOSCOPY']
missing_delta[['AUTO_ID',observed,'obs_delays']].to_clipboard()
208/54:
# missing_delta.ix[145246:]['DIAG_COLONOSCOPY']
missing_delta#[['AUTO_ID',observed,'obs_delays']].to_clipboard()
208/55:
missing_mask = df.copy()
missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
missing_delta = missing_mask.copy()

time_interval = 1 #1 month with padding. For without padding, maintain PADDED col, then filter on that.
missing_delta['tmp_delta'] = time_interval
208/56:
# missing_delta.ix[145246:]['DIAG_COLONOSCOPY']
missing_delta#[['AUTO_ID',observed,'obs_delays']].to_clipboard()
208/57:
dforig = pd.read_pickle('pickles/monthwise_inputs.pkl')
with open('pickles/col_dict.pkl','rb') as f:
    col_dict = pickle.load(f)
208/58:
df=dforig[dforig.AUTO_ID.isin([1875,1877])].copy()
df = df.iloc[:,:48]
df.columns
208/59: col_dict['missing']
208/60:
def missingness_indicators(padded):
    missing_mask = padded.copy()
    missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
    missing_mask['AUTO_ID'] = padded['AUTO_ID'] # restore auto_ids
    missing_delta = missing_mask.copy()

    time_interval = 1 #1 month with padding. For without padding, maintain PADDED col, then filter on that.
    missing_delta['tmp_delta'] = time_interval
    for observed in missing_delta.columns:
        missing_delta['obs_delays'] = missing_delta.groupby('AUTO_ID')[observed].cumsum()
        missing_delta[observed] = missing_delta.groupby(['AUTO_ID', 'obs_delays'])['tmp_delta'].cumsum()
    missing_delta = missing_delta.drop(['tmp_delta', 'obs_delays'], axis=1)

    missing_mask.columns = ['AUTO_ID']+[f'MISSING_{x}' for x in missing_mask.columns[1:]]
    missing_delta.columns = ['AUTO_ID']+[f'DELTA_{x}' for x in  missing_mask.columns[1:]]
    return missing_mask.drop(['AUTO_ID'],axis=1), missing_delta.drop(['AUTO_ID'],axis=1)
208/61:
def missingness_indicators(padded):
    missing_mask = padded.copy()
    missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
    missing_mask['AUTO_ID'] = padded['AUTO_ID'] # restore auto_ids
    missing_delta = missing_mask.copy()

    time_interval = 1 #1 month with padding. For without padding, maintain PADDED col, then filter on that.
    missing_delta['tmp_delta'] = time_interval
    for observed in missing_delta.columns:
        missing_delta['obs_delays'] = missing_delta.groupby('AUTO_ID')[observed].cumsum()
        missing_delta[observed] = missing_delta.groupby(['AUTO_ID', 'obs_delays'])['tmp_delta'].cumsum()
    missing_delta = missing_delta.drop(['tmp_delta', 'obs_delays'], axis=1)

    missing_mask.columns = ['AUTO_ID']+[f'MISSING_{x}' for x in padded.columns[1:]]
    missing_delta.columns = ['AUTO_ID']+[f'DELTA_{x}' for x in  padded.columns[1:]]
    return missing_mask.drop(['AUTO_ID'],axis=1), missing_delta.drop(['AUTO_ID'],axis=1)
208/62:
m, t = missingness_indicators(df)
t.ix[145246]['DELTA_DIAG_COLONOSCOPY']
208/63:
df=dforig[dforig.AUTO_ID.isin([1875,1877])].copy()
df = df.iloc[:,:48]
df.columns
208/64:
m, t = missingness_indicators(df)
t.ix[145246]['DELTA_DIAG_COLONOSCOPY']
208/65:
# missing_delta.ix[145246:]['DIAG_COLONOSCOPY']
missing_delta#[['AUTO_ID',observed,'obs_delays']].to_clipboard()
208/66:
def missingness_indicators(padded):
    missing_mask = padded.copy()
    missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
    missing_mask['AUTO_ID'] = padded['AUTO_ID'] # restore auto_ids
    missing_delta = missing_mask.copy()

#     time_interval = 1 #1 month with padding. For without padding, maintain PADDED col, then filter on that.
#     missing_delta['tmp_delta'] = time_interval
#     for observed in missing_delta.columns:
#         missing_delta['obs_delays'] = missing_delta.groupby('AUTO_ID')[observed].cumsum()
#         missing_delta[observed] = missing_delta.groupby(['AUTO_ID', 'obs_delays'])['tmp_delta'].cumsum()
#     missing_delta = missing_delta.drop(['tmp_delta', 'obs_delays'], axis=1)

#     missing_mask.columns = ['AUTO_ID']+[f'MISSING_{x}' for x in padded.columns[1:]]
#     missing_delta.columns = ['AUTO_ID']+[f'DELTA_{x}' for x in  padded.columns[1:]]
    return missing_mask.drop(['AUTO_ID'],axis=1), missing_delta.drop(['AUTO_ID'],axis=1)
208/67:
m, t = missingness_indicators(df)
t
# t.ix[145246]['DELTA_DIAG_COLONOSCOPY']
208/68:
def missingness_indicators(padded):
    missing_mask = padded.copy()
    missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
    missing_mask['AUTO_ID'] = padded['AUTO_ID'] # restore auto_ids
    missing_delta = missing_mask.copy()

    time_interval = 1 #1 month with padding. For without padding, maintain PADDED col, then filter on that.
    missing_delta['tmp_delta'] = time_interval
    for observed in missing_delta.columns:
        missing_delta['obs_delays'] = missing_delta.groupby('AUTO_ID')[observed].cumsum()
        missing_delta[observed] = missing_delta.groupby(['AUTO_ID', 'obs_delays'])['tmp_delta'].cumsum()
    missing_delta = missing_delta.drop(['tmp_delta', 'obs_delays'], axis=1)

    missing_mask.columns = ['AUTO_ID']+[f'MISSING_{x}' for x in padded.columns[1:]]
    missing_delta.columns = ['AUTO_ID']+[f'DELTA_{x}' for x in  padded.columns[1:]]
    return missing_mask.drop(['AUTO_ID'],axis=1), missing_delta#.drop(['AUTO_ID'],axis=1)
208/69:
m, t = missingness_indicators(df)
t
# t.ix[145246]['DELTA_DIAG_COLONOSCOPY']
208/70:
def missingness_indicators(padded):
    missing_mask = padded.copy()
    missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
    missing_mask['AUTO_ID'] = padded['AUTO_ID'] # restore auto_ids
    missing_delta = missing_mask.copy()
    return missing_mask, missing_delta

    time_interval = 1 #1 month with padding. For without padding, maintain PADDED col, then filter on that.
    missing_delta['tmp_delta'] = time_interval
    for observed in missing_delta.columns:
        missing_delta['obs_delays'] = missing_delta.groupby('AUTO_ID')[observed].cumsum()
        missing_delta[observed] = missing_delta.groupby(['AUTO_ID', 'obs_delays'])['tmp_delta'].cumsum()
    missing_delta = missing_delta.drop(['tmp_delta', 'obs_delays'], axis=1)

    missing_mask.columns = ['AUTO_ID']+[f'MISSING_{x}' for x in padded.columns[1:]]
    missing_delta.columns = ['AUTO_ID']+[f'DELTA_{x}' for x in  padded.columns[1:]]
    return missing_mask.drop(['AUTO_ID'],axis=1), missing_delta#.drop(['AUTO_ID'],axis=1)
208/71:
m, t = missingness_indicators(df)
t
# t.ix[145246]['DELTA_DIAG_COLONOSCOPY']
208/72:
def missingness_indicators(padded):
    missing_mask = padded.copy()
    missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
    missing_mask['AUTO_ID'] = padded['AUTO_ID'] # restore auto_ids
    missing_delta = missing_mask.copy()
    
    time_interval = 1 #1 month with padding. For without padding, maintain PADDED col, then filter on that.
    missing_delta['tmp_delta'] = time_interval
    for observed in missing_delta.columns[1:]:
        missing_delta['obs_delays'] = missing_delta.groupby('AUTO_ID')[observed].cumsum()
        missing_delta[observed] = missing_delta.groupby(['AUTO_ID', 'obs_delays'])['tmp_delta'].cumsum()
    missing_delta = missing_delta.drop(['tmp_delta', 'obs_delays'], axis=1)

    missing_mask.columns = ['AUTO_ID']+[f'MISSING_{x}' for x in padded.columns[1:]]
    missing_delta.columns = ['AUTO_ID']+[f'DELTA_{x}' for x in  padded.columns[1:]]
    return missing_mask.drop(['AUTO_ID'],axis=1), missing_delta#.drop(['AUTO_ID'],axis=1)
208/73:
m, t = missingness_indicators(df)
t
# t.ix[145246]['DELTA_DIAG_COLONOSCOPY']
208/74:
m, t = missingness_indicators(df)
t.ix[145246]['DELTA_DIAG_COLONOSCOPY']
208/75:
def missingness_indicators(padded):
    missing_mask = padded.copy()
    missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
    missing_mask['AUTO_ID'] = padded['AUTO_ID'] # restore auto_ids
    missing_delta = missing_mask.copy()
    
    time_interval = 1 #1 month with padding. For without padding, maintain PADDED col, then filter on that.
    missing_delta['tmp_delta'] = time_interval
    for observed in missing_delta.columns[1:]:
        missing_delta['obs_delays'] = missing_delta.groupby('AUTO_ID')[observed].cumsum()
        missing_delta[observed] = missing_delta.groupby(['AUTO_ID', 'obs_delays'])['tmp_delta'].cumsum() -1
    missing_delta = missing_delta.drop(['tmp_delta', 'obs_delays'], axis=1)

    missing_mask.columns = ['AUTO_ID']+[f'MISSING_{x}' for x in padded.columns[1:]]
    missing_delta.columns = ['AUTO_ID']+[f'DELTA_{x}' for x in  padded.columns[1:]]
    return missing_mask.drop(['AUTO_ID'],axis=1), missing_delta#.drop(['AUTO_ID'],axis=1)
208/76:
m, t = missingness_indicators(df)
t.ix[145246]['DELTA_DIAG_COLONOSCOPY']
208/77:
m, t = missingness_indicators(df)
t.ix[145246:]['DELTA_DIAG_COLONOSCOPY']
208/78:
dforig = pd.read_pickle('pickles/monthwise_inputs.pkl')
with open('pickles/col_dict.pkl','rb') as f:
    col_dict = pickle.load(f)
208/79: m, t = missingness_indicators(dforig)
211/1:
import pandas as pd
import pickle
import data_utils as du
211/2:
dforig = pd.read_pickle('pickles/monthwise_inputs.pkl')
with open('pickles/col_dict.pkl','rb') as f:
    col_dict = pickle.load(f)
211/3: col_dict['delta']
211/4: col_dict['inputs']
211/5: col_dict['input']
211/6:
def missingness_indicators(padded):
    missing_mask = padded.copy()
    missing_mask.loc[:] = missing_mask.loc[:].notnull().astype(int)
    missing_mask['AUTO_ID'] = padded['AUTO_ID'] # restore auto_ids
    missing_delta = missing_mask.copy()
    
    time_interval = 1 #1 month with padding. For without padding, maintain PADDED col, then filter on that.
    missing_delta['tmp_delta'] = time_interval
    for observed in missing_delta.columns[1:]:
        missing_delta['obs_delays'] = missing_delta.groupby('AUTO_ID')[observed].cumsum()
        missing_delta[observed] = missing_delta.groupby(['AUTO_ID', 'obs_delays'])['tmp_delta'].cumsum() -1
    missing_delta = missing_delta.drop(['tmp_delta', 'obs_delays'], axis=1)

    missing_mask.columns = ['AUTO_ID']+[f'MISSING_{x}' for x in padded.columns[1:]]
    missing_delta.columns = ['AUTO_ID']+[f'DELTA_{x}' for x in  padded.columns[1:]]
    return missing_mask.drop(['AUTO_ID'],axis=1), missing_delta.drop(['AUTO_ID'],axis=1)
211/7:
df = dforig[col_dict['input']]
m, t = missingness_indicators(dforig)
col_dict = {'input':df.columns, 'missing':m.columns, 'delta':t.columns}
all_data = pd.concat([df, m, t], axis=1)#
211/8:
df.to_pickle('pickles/x_padded_inputs.pkl')
m.to_pickle('pickles/m_missing_mask.pkl')
t.to_pickle('pickles/t_missing_delta.pkl')
all_data.to_pickle('pickles/monthwise_inputs.pkl')  
with open('pickles/col_dict.pkl', 'wb') as f:
    pickle.dump(col_dict, f)
211/9:
# df.to_pickle('pickles/x_padded_inputs.pkl')
# m.to_pickle('pickles/m_missing_mask.pkl')
# t.to_pickle('pickles/t_missing_delta.pkl')
all_data.to_pickle('pickles/monthwise_inputs.pkl')  
# with open('pickles/col_dict.pkl', 'wb') as f:
#     pickle.dump(col_dict, f)
211/10: all_data.head()
211/11: all_data.tail()
210/8:
import utils
import data_utils as du
import os
212/1:
import utils
import data_utils as du
import os
212/2:
import utils
import data_utils as du
import os
212/3:
seqgen = du.IBDSequenceGenerator()
seqgen.write_to_disk('m36p12', seq_only=True)
211/12:
df = all_data.head()
def decompose_inputs(input_df):
    x = input_df[self.col_dict['input']]
    x_obs = x.shift(1).fillna(x.iloc[0]).values
    m = input_df[self.col_dict['missing']].values
    t = input_df[self.col_dict['delta']].values
    x=x.values
    return [x[:,1:], m, t, x_obs[:,1:]] 

decompose_inputs(df)
211/13:
df = all_data.head()
def decompose_inputs(input_df):
    x = input_df[col_dict['input']]
    x_obs = x.shift(1).fillna(x.iloc[0]).values
    m = input_df[col_dict['missing']].values
    t = input_df[col_dict['delta']].values
    x=x.values
    return [x[:,1:], m, t, x_obs[:,1:]] 

decompose_inputs(df)
211/14:
df = all_data.head()
def decompose_inputs(input_df):
    x = input_df[col_dict['input']]
    x_obs = x.shift(1).fillna(x.iloc[0]).values
    m = input_df[col_dict['missing']].values
    t = input_df[col_dict['delta']].values
    x=x.values
    return [x[:,1:], m, t, x_obs[:,1:]] 

y=decompose_inputs(df)
211/15: type(y)
211/16:
import numpy as np

df = all_data.head()
def decompose_inputs(input_df):
    x = input_df[col_dict['input']]
    x_obs = x.shift(1).fillna(x.iloc[0]).values
    m = input_df[col_dict['missing']].values
    t = input_df[col_dict['delta']].values
    x=x.values
    return np.hstack((x[:,1:], m, t, x_obs[:,1:],)) 

y=decompose_inputs(df)
211/17: type(y)
211/18: y.shape
211/19: len(col_dict['input'])
211/20: col_dict['input']
211/21:
import numpy as np

df = all_data.head()
def decompose_inputs(input_df):
    x = input_df[col_dict['input']]
    x_obs = x.shift(1).fillna(x.iloc[0]).values
    m = input_df[col_dict['missing']].values
    t = input_df[col_dict['delta']].values
    x=x.values
    return np.hstack((x[:,1:], m, t, x_obs[:,1:],)) 

yl = np.array(list(map(decompose_inputs, [df.head(), df.tail()])))
211/22: yl
211/23: yl.shape
213/1:
import utils
import data_utils as du
import os
213/2:
seqgen = du.IBDSequenceGenerator()
seqgen.write_to_disk('m36p12', seq_only=True)
214/1:
import utils
import data_utils as du
import os
214/2:
seqgen = du.IBDSequenceGenerator()
seqgen.write_to_disk('m36p12', seq_only=True)
215/1:
import utils
import data_utils as du
import os
215/2:
seqgen = du.IBDSequenceGenerator()
seqgen.write_to_disk('m36p12', seq_only=True)
216/1:
import utils
import data_utils as du
import os
216/2:
seqgen = du.IBDSequenceGenerator()
seqgen.write_to_disk('m36p12', seq_only=True)
217/1:
import utils
import data_utils as du
import os
217/2:
seqgen = du.IBDSequenceGenerator()
seqgen.write_to_disk('m36p12', seq_only=True)
218/1:
import utils
import data_utils as du
import os
218/2:
seqgen = du.IBDSequenceGenerator()
seqgen.write_to_disk('m36p12', seq_only=True)
218/3:
# tr,v,te = du.get_tt_datasets()
# du.dump_dataset('all', tr, v, te)
202/16:
import time

import pandas as pd
# df = pd.read_pickle('pickles/inputs.pkl')
df = pd.read_pickle('../preprocessing/datasets/long_out/padded')
202/17:
from matplotlib import pyplot as plt 
tot = pd.read_csv('../preprocessing/datasets/clean_registry_data/total_charges.csv')

aids = tot.AUTO_ID.unique()
fig, axes = plt.subplots(3,3, figsize = (12,12))

for r in range(3):
    for c in range(3):
        if r==0 and c==0:
            aid = 1133
            
        else:
            aid = aids[np.random.randint(0,len(aids))]
        df=tot[tot.AUTO_ID==aid]
        d = pd.to_datetime(df['ADMISSION_DATE'])
        x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
        y = df['CUMSUM']
        titl = (d.max()-d.min()).days//30
        axes[r,c].plot(x, y)
        axes[r,c].set_title(f"AID: {aid}, Months: {titl}")
plt.show()
202/18:
def find_spikes(s, std_diff=1):
    median = s-s[s>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (median-mi)/(mx-mi)
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std()
    spikes = np.where(scaled > (scmed + std_diff*scstd), 1, 0)
    return pd.Series(spikes, index=s.index)

from sklearn.preprocessing import StandardScaler
def find_spikes2(charge,std_diff=1):
    scaler = StandardScaler.fit(charge)
    scaled = scaler.transform(charge)
    median = scaled.quantile(0.5)
    spikes = np.where(scaled > (median+std_diff), 1, 0)
    return pd.Series(spikes, index=s.index)
202/19: df.charge
202/20: df.columns
202/21:
import time

import pandas as pd
# df = pd.read_pickle('pickles/inputs.pkl')
df = pd.read_pickle('../preprocessing/datasets/long_out/padded')
202/22: df.columns
202/23: df['charge'] = df.RELATED_OP.fillna(0) + df.RELATED_IP.fillna(0)
202/24:
a = find_spikes(df[df.AUTO_ID==2131]['charge'])
b = find_spikes2(df[df.AUTO_ID==2131]['charge'])
202/25:
def find_spikes(s, std_diff=1):
    median = s-s[s>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (median-mi)/(mx-mi)
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std()
    spikes = np.where(scaled > (scmed + std_diff*scstd), 1, 0)
    return pd.Series(spikes, index=s.index)

from sklearn.preprocessing import StandardScaler
def find_spikes2(charge,std_diff=1):
    scaler = StandardScaler().fit(charge)
    scaled = scaler.transform(charge)
    median = scaled.quantile(0.5)
    spikes = np.where(scaled > (median+std_diff), 1, 0)
    return pd.Series(spikes, index=s.index)
202/26:
a = find_spikes(df[df.AUTO_ID==2131]['charge'])
b = find_spikes2(df[df.AUTO_ID==2131]['charge'])
202/27:
def find_spikes(s, std_diff=1):
    median = s-s[s>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (median-mi)/(mx-mi)
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std()
    spikes = np.where(scaled > (scmed + std_diff*scstd), 1, 0)
    return pd.Series(spikes, index=s.index)

from sklearn.preprocessing import StandardScaler
def find_spikes2(charge,std_diff=1):
    scaler = StandardScaler().fit(charge.reshape(-1,1))
    scaled = scaler.transform(charge.reshape(-1,1))
    median = scaled.quantile(0.5)
    spikes = np.where(scaled > (median+std_diff), 1, 0)
    return pd.Series(spikes, index=s.index)
202/28:
a = find_spikes(df[df.AUTO_ID==2131]['charge'])
b = find_spikes2(df[df.AUTO_ID==2131]['charge'])
202/29:
def find_spikes(s, std_diff=1):
    median = s-s[s>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (median-mi)/(mx-mi)
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std()
    spikes = np.where(scaled > (scmed + std_diff*scstd), 1, 0)
    return pd.Series(spikes, index=s.index)

from sklearn.preprocessing import StandardScaler
def find_spikes2(charge,std_diff=1):
    scaler = StandardScaler().fit(charge.values.reshape(-1,1))
    scaled = scaler.transform(charge.values.reshape(-1,1))
    median = scaled.quantile(0.5)
    spikes = np.where(scaled > (median+std_diff), 1, 0)
    return pd.Series(spikes, index=s.index)
202/30:
a = find_spikes(df[df.AUTO_ID==2131]['charge'])
b = find_spikes2(df[df.AUTO_ID==2131]['charge'])
202/31:
def find_spikes(s, std_diff=1):
    median = s-s[s>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (median-mi)/(mx-mi)
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std()
    spikes = np.where(scaled > (scmed + std_diff*scstd), 1, 0)
    return pd.Series(spikes, index=s.index)

from sklearn.preprocessing import StandardScaler
def find_spikes2(charge,std_diff=1):
    scaler = StandardScaler().fit(charge.values.reshape(-1,1))
    scaled = scaler.transform(charge.values.reshape(-1,1))
    median = pd.Series(scaled).quantile(0.5)
    spikes = np.where(scaled > (median+std_diff), 1, 0)
    return pd.Series(spikes, index=s.index)
202/32:
a = find_spikes(df[df.AUTO_ID==2131]['charge'])
b = find_spikes2(df[df.AUTO_ID==2131]['charge'])
202/33:
def find_spikes(s, std_diff=1):
    median = s-s[s>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (median-mi)/(mx-mi)
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std()
    spikes = np.where(scaled > (scmed + std_diff*scstd), 1, 0)
    return pd.Series(spikes, index=s.index)

from sklearn.preprocessing import StandardScaler
def find_spikes2(charge,std_diff=1):
    scaler = StandardScaler().fit(charge.values.reshape(-1,1))
    scaled = scaler.transform(charge.values.reshape(-1,1))
    median = pd.Series(scaled.reshape(1,-1)).quantile(0.5)
    spikes = np.where(scaled > (median+std_diff), 1, 0)
    return pd.Series(spikes, index=s.index)
202/34:
a = find_spikes(df[df.AUTO_ID==2131]['charge'])
b = find_spikes2(df[df.AUTO_ID==2131]['charge'])
202/35:
def find_spikes(s, std_diff=1):
    median = s-s[s>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (median-mi)/(mx-mi)
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std()
    spikes = np.where(scaled > (scmed + std_diff*scstd), 1, 0)
    return pd.Series(spikes, index=s.index)

from sklearn.preprocessing import StandardScaler
def find_spikes2(charge,std_diff=1):
    mu, sig = charge.mean(), charge.std()
    scaled = (charge-mu)/sig
    median = scaled.quantile(0.5)
    spikes = np.where(scaled > (median+std_diff), 1, 0)
    return pd.Series(spikes, index=s.index)
202/36:
a = find_spikes(df[df.AUTO_ID==2131]['charge'])
b = find_spikes2(df[df.AUTO_ID==2131]['charge'])
202/37:
def find_spikes(s, std_diff=1):
    median = s-s[s>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (median-mi)/(mx-mi)
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std()
    spikes = np.where(scaled > (scmed + std_diff*scstd), 1, 0)
    return pd.Series(spikes, index=charge.index)

from sklearn.preprocessing import StandardScaler
def find_spikes2(charge,std_diff=1):
    mu, sig = charge.mean(), charge.std()
    scaled = (charge-mu)/sig
    median = scaled.quantile(0.5)
    spikes = np.where(scaled > (median+std_diff), 1, 0)
    return pd.Series(spikes, index=charge.index)
202/38:
a = find_spikes(df[df.AUTO_ID==2131]['charge'])
b = find_spikes2(df[df.AUTO_ID==2131]['charge'])
202/39:
def find_spikes(s, std_diff=1):
    median = s-s[s>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (median-mi)/(mx-mi)
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std()
    spikes = np.where(scaled > (scmed + std_diff*scstd), 1, 0)
    return pd.Series(spikes, index=s.index)

from sklearn.preprocessing import StandardScaler
def find_spikes2(charge,std_diff=1):
    mu, sig = charge.mean(), charge.std()
    scaled = (charge-mu)/sig
    median = scaled.quantile(0.5)
    spikes = np.where(scaled > (median+std_diff), 1, 0)
    return pd.Series(spikes, index=charge.index)
202/40:
a = find_spikes(df[df.AUTO_ID==2131]['charge'])
b = find_spikes2(df[df.AUTO_ID==2131]['charge'])
202/41: len(a), len(b)
202/42: a.sum(), b.sum()
202/43: a
202/44: a==1
202/45: (a==1).index
202/46: (a==1)
202/47: df[a.values]
202/48: a
202/49: a.astype(bool)
202/50: df[a.astype(bool).values]
202/51: df.loc[df.AUTO_ID==2131, a.astype(bool).values]
202/52:
aa = df[df.AUTO_ID==2131]
aa['s'] = a
aa[aa.s==1]
202/53:
def find_spikes(charge, std_diff=1):
    median = charge-charge[charge>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (median-mi)/(mx-mi) # scaled differences from median
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std() # median and stdev of differences
    spikes = np.where(scaled > (scmed + std_diff*scstd), 1, 0)
    return pd.Series(spikes, index=charge.index)

df['spikes'] = df.groupby('AUTO_ID').charge.apply(find_spikes, std_diff=1)
218/4:
tr,v,te = du.get_tt_datasets(dirname='m36p12', keep_autoids=None,)
du.dump_dataset('m36p12_ALL', tr, v, te)
219/1:
import utils
import data_utils as du
import os
219/2:
tr,v,te = du.get_tt_datasets(from_dir='m36p12', keep_autoids=None,)
du.dump_dataset('m36p12_ALL', tr, v, te)
219/3:
xian_auto_ids = [2, 3, 4, 7, 11, 14, 23, 26, 27, 28, 30, 31, 33, 38, 39, 53, 55, 57, 59, 61, 65, 66, 70, 71, 73, 82, 101, 104, 105, 106, 109, 119, 131, 144, 146, 149, 150, 157, 159, 160, 163, 164, 165, 171, 172, 176, 180, 183, 185, 187, 188, 193, 201, 204, 205, 219, 223, 226, 229, 230, 233, 237, 242, 243, 245, 251, 252, 256, 258, 261, 267, 270, 272, 274, 276, 278, 279, 282, 283, 285, 289, 290, 293, 300, 301, 304, 305, 306, 314, 315, 321, 326, 327, 330, 331, 335, 337, 338, 342, 345, 347, 350, 358, 363, 367, 368, 373, 375, 376, 380, 381, 382, 383, 384, 386, 392, 393, 397, 399, 406, 407, 410, 414, 421, 422, 429, 435, 437, 446, 449, 453, 454, 458, 460, 467, 474, 484, 487, 493, 494, 498, 501, 502, 503, 512, 518, 523, 524, 526, 527, 530, 536, 537, 541, 545, 546, 551, 553, 555, 556, 560, 561, 563, 564, 567, 573, 575, 580, 581, 585, 590, 592, 595, 599, 600, 601, 608, 616, 620, 625, 627, 628, 632, 644, 646, 649, 653, 654, 660, 661, 664, 667, 668, 672, 679, 681, 682, 683, 697, 700, 701, 703, 704, 706, 707, 708, 710, 711, 715, 720, 728, 730, 733, 734, 737, 746, 747, 749, 750, 751, 756, 767, 779, 781, 784, 786, 788, 789, 799, 800, 806, 807, 811, 812, 820, 821, 825, 826, 827, 828, 835, 837, 840, 841, 844, 846, 848, 851, 852, 860, 863, 864, 865, 866, 868, 870, 872, 878, 881, 884, 887, 888, 889, 890, 894, 896, 902, 903, 904, 909, 911, 914, 917, 920, 923, 924, 935, 942, 946, 948, 950, 953, 954, 963, 966, 968, 971, 980, 987, 990, 991, 992, 993, 995, 997, 999, 1001, 1005, 1006, 1007, 1009, 1011, 1012, 1015, 1018, 1019, 1024, 1028, 1040, 1041, 1043, 1052, 1053, 1057, 1060, 1070, 1074, 1079, 1083, 1086, 1087, 1088, 1090, 1092, 1093, 1095, 1096, 1097, 1099, 1100, 1102, 1103, 1104, 1105, 1106, 1112, 1115, 1120, 1123, 1124, 1126, 1130, 1134, 1136, 1138, 1140, 1142, 1145, 1148, 1151, 1152, 1174, 1178, 1183, 1185, 1186, 1187, 1195, 1206, 1209, 1210, 1214, 1216, 1223, 1233, 1235, 1236, 1237, 1243, 1250, 1251, 1259, 1264, 1269, 1273, 1278, 1279, 1280, 1293, 1295, 1298, 1302, 1303, 1304, 1307, 1309, 1312, 1317, 1318, 1322, 1328, 1333, 1337, 1342, 1344, 1350, 1352, 1359, 1371, 1376, 1378, 1379, 1383, 1392, 1393, 1397, 1399, 1400, 1401, 1404, 1406, 1407, 1408, 1409, 1411, 1426, 1429, 1441, 1442, 1443, 1444, 1447, 1454, 1462, 1466, 1469, 1473, 1474, 1479, 1485, 1486, 1489, 1493, 1495, 1500, 1503, 1505, 1509, 1512, 1517, 1522, 1530, 1532, 1533, 1535, 1540, 1546, 1547, 1549, 1552, 1556, 1557, 1560, 1562, 1569, 1571, 1577, 1583, 1584, 1589, 1590, 1592, 1598, 1599, 1601, 1604, 1605, 1613, 1616, 1617, 1623, 1626, 1628, 1629, 1631, 1635, 1636, 1643, 1650, 1651, 1652, 1654, 1656, 1657, 1667, 1674, 1677, 1680, 1681, 1689, 1695, 1696, 1698, 1700, 1707, 1714, 1715, 1716, 1717, 1718, 1720, 1721, 1724, 1726, 1728, 1734, 1737, 1738, 1741, 1744, 1746, 1749, 1750, 1753, 1757, 1763, 1766, 1767, 1768, 1769, 1770, 1781, 1785, 1789, 1794, 1797, 1799, 1805, 1808, 1809, 1814, 1816, 1818, 1820, 1832, 1835, 1837, 1839, 1851, 1854, 1859, 1864, 1869, 1870, 1871, 1876, 1877, 1881, 1885, 1888, 1891, 1893, 1894, 1901, 1902, 1907, 1916, 1917, 1919, 1920, 1922, 1923, 1928, 1929, 1930, 1931, 1935, 1938, 1939, 1940, 1944, 1946, 1949, 1950, 1959, 1960, 1962, 1963, 1966, 1967, 1968, 1970, 1980, 1987, 1991, 1994, 1995, 2000, 2004, 2009, 2010, 2013, 2015, 2018, 2019, 2025, 2028, 2031, 2032, 2037, 2041, 2043, 2044, 2047, 2056, 2063, 2064, 2067, 2072, 2073, 2077, 2080, 2083, 2084, 2087, 2089, 2090, 2091, 2096, 2099, 2111, 2113, 2114, 2123, 2128, 2135, 2139, 2140, 2142, 2143, 2146, 2147, 2149, 2164, 2166, 2170, 2178, 2180, 2183, 2189, 2190, 2191, 2192, 2195, 2199, 2201, 2203, 2210, 2212, 2215, 2220, 2228, 2231, 2232, 2237, 2240, 2241, 2248, 2249, 2253, 2257, 2258, 2261, 2263, 2276, 2277, 2279, 2280, 2282, 2292, 2295, 2298, 2300, 2318, 2319, 2322, 2326, 2327, 2328, 2329, 2331, 2333, 2334, 2335, 2336, 2337, 2348, 2350, 2351, 2355, 2356, 2368, 2371, 2372, 2374, 2375, 2378, 2379, 2381, 2384, 2386, 2387, 2389, 2395, 2397, 2401, 2402, 2403, 2406, 2414, 2417, 2424, 2425, 2431, 2433, 2434, 2437, 2439, 2440, 2441, 2446, 2451, 2457, 2467, 2468, 2471, 2472, 2473, 2475, 2485, 2487, 2491, 2493, 2496, 2502, 2503, 2510, 2514, 2517, 2521, 2522, 2527, 2528, 2538, 2542, 2544, 2550, 2551, 2552, 2554, 2557, 2560, 2565, 2569, 2571, 2576, 2580, 2585, 2587, 2591, 2592, 2594, 2599, 2603, 2604, 2606, 2610, 2616, 2617, 2619, 2632, 2633, 2634, 2638, 2645, 2647, 2650, 2664, 2665, 2668, 2669, 2670, 2673, 2675, 2676, 2677, 2678, 2687, 2690, 2691, 2692, 2695, 2697, 2700, 2705, 2706, 2709, 2716, 2719, 2724, 2726, 2727, 2728, 2731, 2733, 2734, 2739, 2743, 2744, 2750, 2764, 2765, 2768, 2769, 2770, 2773, 2775, 2776, 2778, 2779, 2789, 2792, 2794, 2795, 2808, 2813, 2819, 2820, 2829, 2831, 2833, 2834, 2835, 2836, 2838, 2839, 2841, 2843, 2845, 2848, 2854, 2856, 2861, 2867, 2870, 2872, 2875, 2878, 2881, 2883, 2887, 2892, 2895, 2896, 2897, 2898, 2909, 2910, 2912, 2913, 2914, 2917, 2923, 2928, 2934, 2937, 2939, 2949, 2950, 2952, 2953, 2955, 2959, 2966, 2967, 2968, 2970, 2971, 2972, 2974, 2982, 2991, 3001, 3008, 3018, 3020, 3028, 3033, 3034, 3043, 3046, 3048, 3050, 3055, 3056, 3059, 3060, 3068, 3070, 3075, 3079, 3087, 3088, 3096, 3101, 3106, 3110, 3111, 3114, 3115, 3116, 3117, 3119, 3122, 3124, 3125, 3128, 3133, 3135, 3136]
tr,v,te = du.get_tt_datasets('m36p12', xian_auto_ids)
du.dump_dataset('m36p12_XIAN', tr, v, te)
219/4:
import importlib
importlib.reload(du)
219/5:
seqgen = du.IBDSequenceGenerator()
seqgen.write_to_disk('m36p12', labels_only=True)  # target func not parametrized yet
219/6:
import importlib
importlib.reload(du)
219/7:
seqgen = du.IBDSequenceGenerator()
seqgen.write_to_disk('m36p12', labels_only=True)  # target func not parametrized yet
219/8:
seqgen = du.IBDSequenceGenerator()
seqgen.write_to_disk('m36p12', labels_only=True)  # target func not parametrized yet
219/9:
arr = utils.pkl_load('./data/m36p12/sequences/0_0.npy')
arr.shape
219/10:
# arr = utils.pkl_load('./data/m36p12/sequences/0_0.npy')
l = [arr[:,:12], arr[:,15:20], arr[:,30:40]]
m = np.array(l)
m.shape
219/11:
# arr = utils.pkl_load('./data/m36p12/sequences/0_0.npy')
l = [arr[:,:12], arr[:,15:20], arr[:,30:40]]
m = np.ndarray(l)
m.shape
219/12:
# arr = utils.pkl_load('./data/m36p12/sequences/0_0.npy')
l = [arr[:,:12], arr[:,15:20], arr[:,30:40]]
m = np.array([])
m=m.append(l)
m.shape
219/13:
# arr = utils.pkl_load('./data/m36p12/sequences/0_0.npy')
l = [arr[:,:12], arr[:,15:20], arr[:,30:40]]
m = np.array([])
m = np.append(m,l)
m.shape
219/14:
# arr = utils.pkl_load('./data/m36p12/sequences/0_0.npy')
l = [arr[:,:12], arr[:,:12], arr[:,:12]]
m = np.array([])
m = np.append(m,l)
m.shape
219/15:
# arr = utils.pkl_load('./data/m36p12/sequences/0_0.npy')
l = [arr[:,:12], arr[:,:12], arr[:,:12]]
m = np.array(l)
m.shape
219/16: df, cd = du.cleaned_longitudinal_inputs()
219/17: cd.keys()
219/18: len(cd['input'])
219/19: len(cd['missing'])
219/20: len(cd['delta'])
219/21: cd['delta']
219/22:
df = pd.read_pickle('pickles/x_padded_inputs.pkl')
m = pd.read_pickle('pickles/m_missing_mask.pkl')
t = pd.read_pickle('pickles/t_missing_delta.pkl')
219/23:
import utils
import data_utils as du
import os
import pandas as pd
219/24:
df = pd.read_pickle('pickles/x_padded_inputs.pkl')
m = pd.read_pickle('pickles/m_missing_mask.pkl')
t = pd.read_pickle('pickles/t_missing_delta.pkl')
219/25: m.columns
219/26: m.columns[:58]
219/27: m.columns[:48]
219/28: m.columns[:47]
219/29:
# m.columns[:47]
t.columns[:47]
219/30:
# m.columns[:47]
t[t.MONTH_TS==0]['DELTA_DIAG_COLONOSCOPY']
219/31:
# m.columns[:47]
t[t.DELTA_MONTH_TS==0]['DELTA_DIAG_COLONOSCOPY']
219/32:
# m.columns[:47]
t[t.DELTA_MONTH_TS==0]
219/33: t.iloc[145246]
219/34: df.iloc[145246]
219/35: t.iloc[145246]['DELTA_DIAG_COLONOSCOPY']
219/36: t.iloc[145246]['DELTA_DELTA_DIAG_COLONOSCOPY']
219/37: t.columns#.iloc[145246]['DELTA_DELTA_DIAG_COLONOSCOPY']
219/38: t.iloc[145246]['DELTA_DELTA_MISSING_DIAG_COLONOSCOPY']
219/39: t.columns[:48]
219/40: t[:, :47]
219/41: t.loc[:, :47]
219/42: t.iloc[:, :47]
219/43: m.iloc[:, :47]
219/44:
m = m.iloc[:, :47]
t = t.iloc[:, :47]
219/45:
all_data = pd.concat([df, m, t], axis=1)
m.to_pickle('pickles/m_missing_mask.pkl')
t.to_pickle('pickles/t_missing_delta.pkl')
all_data.to_pickle('pickles/monthwise_inputs.pkl')
col_dict = {'input':df.columns, 'missing':m.columns, 'delta':t.columns}
with open('pickles/col_dict.pkl', 'wb') as f:
        pickle.dump(col_dict, f)
219/46:
import utils
import data_utils as du
import os
import pandas as pd
import pickle
219/47:
all_data = pd.concat([df, m, t], axis=1)
m.to_pickle('pickles/m_missing_mask.pkl')
t.to_pickle('pickles/t_missing_delta.pkl')
all_data.to_pickle('pickles/monthwise_inputs.pkl')
col_dict = {'input':df.columns, 'missing':m.columns, 'delta':t.columns}
with open('pickles/col_dict.pkl', 'wb') as f:
        pickle.dump(col_dict, f)
219/48: arr[:48]
219/49: cd
219/50: [x for x in cd['missing'] if x[:-3]=='_TS']
219/51: cd['missing']# [x for x in cd['missing'] if x[:-3]=='_TS']
219/52:
# cd['missing']# 
[x for x in cd['missing'] if x[-1:-4]=='_TS']
219/53:
# cd['missing']# 
[x for x in cd['missing'] if x[-1:-3]=='_TS']
219/54:
a='abcdsf'
a[-1:-2]
219/55:
a='abcdsf'
a[-1:-2:-1]
219/56:
# cd['missing']# 
[x for x in cd['missing'] if x[-1:-4:-1]=='_TS']
219/57:
a='abcdsf'
a[-1:-4:-1]
219/58:
# cd['missing']# 
[x for x in cd['missing'] if x[-1:-4:-1]=='ST_']
219/59:
len(cd['missing'])# 
# [x for x in cd['missing'] if x[-1:-4:-1]=='ST_']
219/60:
len(cd['missing'])# 
cd['missing'][:len(cd[missing])/3]
219/61:
len(cd['missing'])# 
cd['missing'][:len(cd['missing'])/3]
219/62:
len(cd['missing'])# 
cd['missing'][:48]
219/63:
len(cd['missing'])# 
cd['missing'][:47]
219/64: len(cd['inputs']), len(cd['missing']), len(cd['delta'])
219/65: len(cd['input']), len(cd['missing']), len(cd['delta'])
219/66:
all_cols = cd['input']+cd['missing']+cd['delta']
all_cols[1:48], all_cols[48:95], all_cols[192:239]
219/67: cd['input']
219/68: list((cd['input'])
219/69: list(cd['input'])
219/70:
all_cols = list(cd['input'])+list(cd['missing'])+list(cd['delta'])
all_cols[1:48], all_cols[48:95], all_cols[192:239]
219/71:
def fix_array(a):
    x = a[:, 1:48]
    x_obs = np.vstack(x[0], x[1:,:])
    m = a[48:95]
    t = [192:239]
    ret = np.array(x,m,t,x_obs)
    return ret

fixed00 = fix_array(arr)
219/72:
def fix_array(a):
    x = a[:, 1:48]
    x_obs = np.vstack(x[0], x[1:,:])
    m = a[:, 48:95]
    t = a[:, 192:239]
    ret = np.array(x,m,t,x_obs)
    return ret

fixed00 = fix_array(arr)
219/73:
def fix_array(a):
    x = a[:, 1:48]
    x_obs = np.vstack((x[0], x[1:,:]))
    m = a[:, 48:95]
    t = a[:, 192:239]
    ret = np.array(x,m,t,x_obs)
    return ret

fixed00 = fix_array(arr)
219/74:
def fix_array(a):
    x = a[:, 1:48]
    x_obs = np.vstack((x[0], x[1:,:]))
    m = a[:, 48:95]
    t = a[:, 192:239]
    ret = np.array([x,m,t,x_obs])
    return ret

fixed00 = fix_array(arr)
219/75: fixed00
219/76: fixed00.shape
219/77: fixed00[1] == m[:36,:]
219/78: fixed00[1,:,:] == m[:36,:]
219/79: fixed00[1,:,:]# == m[:36,:]
219/80: fixed00[1,:,:].shape# == m[:36,:]
219/81: fixed00[1,:,:]# == m[:36,:]
219/82:
# fixed00[1,:,:]# == 
m[:36,:]
219/83:
# fixed00[1,:,:]# == 
m.shape
219/84:
# fixed00[1,:,:]# == 
m[0:36, :]
219/85:
# fixed00[1,:,:]# == 
m.iloc[0:36, :]
219/86: fixed00[1,:,:] == m.iloc[0:36, :].values
219/87: fixed00[2,:,:] == t.iloc[0:36, :].values
219/88:
inputs = pd.read_pickle('pickles/x_padded_inputs.pkl')

trial = inputs[inputs.AUTO_ID==1877]
trial
219/89:
inputs = pd.read_pickle('pickles/x_padded_inputs.pkl')
m, t = du.missingness_indicators(inputs)
t
219/90:
inputs = pd.read_pickle('pickles/x_padded_inputs.pkl')
m, t = du.missingness_indicators(trial)
t
219/91:
inputs = pd.read_pickle('pickles/x_padded_inputs.pkl')
m, t = du.missingness_indicators(inputs)
t
219/92:
# inputs = pd.read_pickle('pickles/x_padded_inputs.pkl')
# m, t = du.missingness_indicators(inputs)

col_dict = {'input':df.columns, 'missing':m.columns, 'delta':t.columns}
all_data = pd.concat([df, m, t], axis=1)

# Save
m.to_pickle('pickles/m_missing_mask.pkl')
t.to_pickle('pickles/t_missing_delta.pkl')
all_data.to_pickle('pickles/monthwise_inputs.pkl')
with open('pickles/col_dict.pkl', 'wb') as f:
    pickle.dump(col_dict, f)
219/93:
def find_spikes(charge, std_diff=1):
    median = charge-charge[charge>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (median-mi)/(mx-mi) # scaled differences from median
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std() # median and stdev of differences
    spikes = np.where(scaled > (scmed + std_diff*scstd), 1, 0)
    return pd.Series(spikes, index=charge.index)

x = df[df.AUTO_ID==532]
x['charges'] = x.RELATED_IP.fillna(0) + x.RELATED_OP.fillna(0)
x['spikes'] = x.groupby('AUTO_ID').charges.apply(find_spikes, std_diff=1)
x[['charges','spikes']]
219/94:
def find_spikes(charge, std_diff=1):
    median = charge-charge[charge>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (median-mi)/(mx-mi) # scaled differences from median
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std() # median and stdev of differences
    spikes = np.where(scaled > (scmed + std_diff*scstd), 1, 0)
    return pd.Series(spikes, index=charge.index)

x = df[df.AUTO_ID==532]
x['charges'] = x.RELATED_IP.fillna(0) + x.RELATED_OP.fillna(0)
x['spikes'] = x.groupby('AUTO_ID').charges.apply(find_spikes, std_diff=1)
x.spikes.sum(), x[['charges','spikes']]
219/95:
def find_spikes(charge, std_diff=1):
    median = charge-charge[charge>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (median-mi)/(mx-mi) # scaled differences from median
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std() # median and stdev of differences
    spikes = np.where(scaled > (scmed + std_diff*scstd), 1, 0)
    return pd.Series(spikes, index=charge.index)

x = df[df.AUTO_ID==532]
x['charges'] = x.RELATED_IP.fillna(0) + x.RELATED_OP.fillna(0)
x['spikes'] = x.groupby('AUTO_ID').charges.apply(find_spikes, std_diff=1)
x.spikes.idxmax(), x[['charges','spikes']]
219/96:
def find_spikes(charge, std_diff=1):
    median = charge-charge[charge>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (median-mi)/(mx-mi) # scaled differences from median
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std() # median and stdev of differences
    spikes = np.where(scaled > (scmed + std_diff*scstd), 1, 0)
    return pd.Series(spikes, index=charge.index)

x = df[df.AUTO_ID==532]
x['charges'] = x.RELATED_IP.fillna(0) + x.RELATED_OP.fillna(0)
x['spikes'] = x.groupby('AUTO_ID').charges.apply(find_spikes, std_diff=1)
x[40459:40479, ['charges','spikes']]
219/97:
def find_spikes(charge, std_diff=1):
    median = charge-charge[charge>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (median-mi)/(mx-mi) # scaled differences from median
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std() # median and stdev of differences
    spikes = np.where(scaled > (scmed + std_diff*scstd), 1, 0)
    return pd.Series(spikes, index=charge.index)

x = df[df.AUTO_ID==532]
x['charges'] = x.RELATED_IP.fillna(0) + x.RELATED_OP.fillna(0)
x['spikes'] = x.groupby('AUTO_ID').charges.apply(find_spikes, std_diff=1)
x[40459:40479]['charges','spikes']
219/98:
def find_spikes(charge, std_diff=1):
    median = charge-charge[charge>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (median-mi)/(mx-mi) # scaled differences from median
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std() # median and stdev of differences
    spikes = np.where(scaled > (scmed + std_diff*scstd), 1, 0)
    return pd.Series(spikes, index=charge.index)

x = df[df.AUTO_ID==532]
x['charges'] = x.RELATED_IP.fillna(0) + x.RELATED_OP.fillna(0)
x['spikes'] = x.groupby('AUTO_ID').charges.apply(find_spikes, std_diff=1)
x[40459:40479][['charges','spikes']]
219/99:
def find_spikes(charge, std_diff=1):
    median = charge-charge[charge>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (median-mi)/(mx-mi) # scaled differences from median
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std() # median and stdev of differences
    spikes = np.where(scaled > (scmed + std_diff*scstd), 1, 0)
    return pd.Series(spikes, index=charge.index)

x = df[df.AUTO_ID==532]
x['charges'] = x.RELATED_IP.fillna(0) + x.RELATED_OP.fillna(0)
x['spikes'] = x.groupby('AUTO_ID').charges.apply(find_spikes, std_diff=1)
x.loc[40459:40479][['charges','spikes']]
219/100:
def find_spikes(charge, std_diff=1):
    median = charge-charge[charge>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (median-mi)/(mx-mi) # scaled differences from median
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std() # median and stdev of differences
    spikes = np.where(scaled > (scmed + std_diff*scstd), 1, 0)
    return pd.Series(spikes, index=charge.index)

x = df[df.AUTO_ID==532]
x['charges'] = x.RELATED_IP.fillna(0) + x.RELATED_OP.fillna(0)
x['spikes'] = x.groupby('AUTO_ID').charges.apply(find_spikes, std_diff=1)
x[x.spikes==1]
219/101:
def find_spikes(charge, std_diff=1):
    median = charge-charge[charge>0].quantile(0.5)
    mi, mx = median.min(), median.max()
    scaled = (median-mi)/(mx-mi) # scaled differences from median
    scmed, scstd = scaled[scaled>0].quantile(0.5), scaled[scaled>0].std() # median and stdev of differences
    spikes = np.where(scaled > (scmed + std_diff*scstd), 1, 0)
    return pd.Series(spikes, index=charge.index)

x = df.copy()
x['charges'] = x.RELATED_IP.fillna(0) + x.RELATED_OP.fillna(0)
x['spikes'] = x.groupby('AUTO_ID').charges.apply(find_spikes, std_diff=1)
x.groupby('AUTO_ID').spikes.sum().describe()
219/102:
for a,g in x.groupby('AUTO_ID'):
    print g[['AUTO_ID','MONTH_TS']]
    break
219/103:
for a,g in x.groupby('AUTO_ID'):
    print(g[['AUTO_ID','MONTH_TS']])
    break
219/104:
for a,g in x.groupby('AUTO_ID'):
    if a==34:
        print(g[['AUTO_ID','MONTH_TS']])
        break
219/105:
for a,g in x.groupby('AUTO_ID'):
    if a==34:
        print(g.index)
        break
219/106: x.loc[2706:2711]
219/107:
for a,g in x.groupby('AUTO_ID'):
    if a==34:
        yy = g
        break
219/108: yy.index
219/109: yy.index[0]
219/110: yy.index[-1]
219/111: x.loc[2706:2710, 'AUTO_ID']
219/112: x.loc[2706:2710, 'AUTO_ID'].sum()
219/113: chk = x.groupby('AUTO_ID').spikes.sum()
219/114: chk
202/54:
from matplotlib import pyplot as plt 
tot = pd.read_csv('../preprocessing/datasets/clean_registry_data/total_charges.csv')

aids = tot.AUTO_ID.unique()
fig, axes = plt.subplots(3,3, figsize = (12,12))

for r in range(3):
    for c in range(3):
        if r==0 and c==0:
            aid = 0
            
        else:
            aid = aids[np.random.randint(0,len(aids))]
        df=tot[tot.AUTO_ID==aid]
        d = pd.to_datetime(df['ADMISSION_DATE'])
        x = d.dt.month.astype(str).str.cat(d.dt.year.astype(str), sep='-')
        y = df['CUMSUM']
        titl = (d.max()-d.min()).days//30
        axes[r,c].plot(x, y)
        axes[r,c].set_title(f"AID: {aid}, Months: {titl}")
plt.show()
219/115: chk.value_counts()
219/116:
import seaborn as sns
sns.distplot(chk)
219/117:
import seaborn as sns
sns.distplot(chk)
plt.show()
220/1:
x = du.cleaned_longitudinal_inputs()
t = du.TargetFunc()

t._init_spikes(x, liberal=True)
lib = t.spikes_df['spikes'].copy()
t._init_spikes(x)
con = t.spikes_df['spikes'].copy()


# chk = x.groupby('AUTO_ID').spikes.sum()
# import seaborn as sns
# sns.distplot(chk)
# plt.show()
220/2:
import utils
import data_utils as du
import os
import pandas as pd
import pickle
220/3:
x = du.cleaned_longitudinal_inputs()
t = du.TargetFunc()

t._init_spikes(x, liberal=True)
lib = t.spikes_df['spikes'].copy()
t._init_spikes(x)
con = t.spikes_df['spikes'].copy()


# chk = x.groupby('AUTO_ID').spikes.sum()
# import seaborn as sns
# sns.distplot(chk)
# plt.show()
220/4:
x = du.cleaned_longitudinal_inputs()[0]
t = du.TargetFunc()

t._init_spikes(x, liberal=True)
lib = t.spikes_df['spikes'].copy()
t._init_spikes(x)
con = t.spikes_df['spikes'].copy()


# chk = x.groupby('AUTO_ID').spikes.sum()
# import seaborn as sns
# sns.distplot(chk)
# plt.show()
220/5:
# x = du.cleaned_longitudinal_inputs()[0]
t = du.TargetFunc()

t._init_spikes(x, liberal=True)
lib = t.spikes_df.copy()
t._init_spikes(x)
con = t.spikes_df.copy()


# chk = x.groupby('AUTO_ID').spikes.sum()
# import seaborn as sns
# sns.distplot(chk)
# plt.show()
220/6:
chk = lib.groupby('AUTO_ID').spikes.sum()
import seaborn as sns
sns.distplot(chk)
plt.show()
220/7:
import utils
import data_utils as du
import os
import pandas as pd
import pickle
import seaborn as sns
from matplotlib.pyplot import plt
220/8:
import utils
import data_utils as du
import os
import pandas as pd
import pickle
import seaborn as sns
from matplotlib import pyplot as plt
220/9:
chk = lib.groupby('AUTO_ID').spikes.sum()
sns.distplot(chk)
plt.show()
220/10:
chk = lib.groupby('AUTO_ID').spikes.sum()
sns.distplot(chk)
plt.show()
220/11:
chk = con.groupby('AUTO_ID').spikes.sum()
sns.distplot(chk)
plt.show()
220/12:
chk = con.groupby('AUTO_ID').spikes.sum()
print(chk.value_counts.head(10))
sns.distplot(chk)
plt.show()
220/13:
chk = con.groupby('AUTO_ID').spikes.sum()
print(chk.value_counts().head(10))
sns.distplot(chk)
plt.show()
220/14:
chk = lib.groupby('AUTO_ID').spikes.sum()
print(chk.value_counts().head(10))
sns.distplot(chk)
plt.show()
220/15:
chk = lib.groupby('AUTO_ID').spikes.sum()
print(chk.value_counts())
sns.distplot(chk)
plt.show()
220/16:
chk = lib.groupby('AUTO_ID').spikes.sum()
# print(chk.value_counts())
# sns.distplot(chk)
# plt.show()
chk[chk==1]
220/17:
arr = [2,4,12,13,16,3132,3136,3139, 3140]


fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).view(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(x, y)
        axes[r,c].set_title(f"AID: {aid}, Months: {titl}")
plt.show()
220/18:
arr = [2,4,12,13,16,3132,3136,3139, 3140]

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(x, y)
        axes[r,c].set_title(f"AID: {aid}, Months: {titl}")
plt.show()
220/19:
arr = [2,4,12,13,16,3132,3136,3139, 3140]

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(x, y)
        axes[r,c].set_title(f"AID: {aid}, Months: {titl}")
plt.show()
220/20:
arr = [2,4,12,13,16,3132,3136,3139, 3140]

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {aid}, Months: {titl}")
plt.show()
220/21:
arr = [2,4,12,13,16,3132,3136,3139, 3140]

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.head(1)}, Months: {titl}")
plt.show()
220/22:
arr = [2,4,12,13,16,3132,3136,3139, 3140]

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.head(1)}, Months: {titl}")
fig.tight_layout()
plt.show()
220/23:
chk = lib.groupby('AUTO_ID').spikes.sum()
print(chk.value_counts())
sns.distplot(chk)
plt.show()
chk[chk==2]
220/24:
# arr = [2,4,12,13,16,3132,3136,3139, 3140]
arr = chk[chk==1].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.head(1)}, Months: {titl}")
fig.tight_layout()
plt.show()
220/25:
chk = lib.groupby('AUTO_ID').spikes.sum()
print(chk.value_counts())
sns.distplot(chk)
plt.show()
220/26:
chk = lib.groupby('AUTO_ID').spikes.sum()
print(chk.value_counts()[:10])
sns.distplot(chk)
plt.show()
220/27:
# arr = [2,4,12,13,16,3132,3136,3139, 3140]
arr = chk[chk==2].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.head(1)}, Months: {titl}")
fig.tight_layout()
plt.show()
220/28:
# arr = [2,4,12,13,16,3132,3136,3139, 3140]
arr = chk[chk==2].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
220/29:
# arr = [2,4,12,13,16,3132,3136,3139, 3140]
arr = chk[chk==2].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
220/30:
# arr = [2,4,12,13,16,3132,3136,3139, 3140]
arr = chk[chk==2].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
220/31:
# arr = [2,4,12,13,16,3132,3136,3139, 3140]
arr = chk[chk==1].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
220/32:
# arr = [2,4,12,13,16,3132,3136,3139, 3140]
arr = chk[chk==1].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
220/33:
# arr = [2,4,12,13,16,3132,3136,3139, 3140]
arr = chk[chk==3].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
220/34:
# arr = [2,4,12,13,16,3132,3136,3139, 3140]
arr = chk[chk==4].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
220/35: x[x.AUTO_ID==300].to_clipboard()
220/36:
chk = con.groupby('AUTO_ID').spikes.sum()
print(chk.value_counts().head(10))
sns.distplot(chk)
plt.show()
220/37:
# arr = [2,4,12,13,16,3132,3136,3139, 3140]
arr = chk[chk==4].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
220/38:
# arr = [2,4,12,13,16,3132,3136,3139, 3140]
arr = chk[chk==4].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
220/39:
# arr = [2,4,12,13,16,3132,3136,3139, 3140]
arr = chk[chk==4].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
220/40:
# arr = [2,4,12,13,16,3132,3136,3139, 3140]
arr = chk[chk>10].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
220/41: chk.loc[1170]
220/42:
arr = [2,4,12,13,16,3132,3136,3139, 3140]
# arr = chk[chk>10].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
220/43:
arr = [1170,4,12,13,16,3132,3136,3139, 3140]
# arr = chk[chk>10].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
220/44:
# arr = [1170,4,12,13,16,3132,3136,3139, 3140]
arr = chk[chk>10].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
220/45: x[x.AUTO_ID==2674].to_clipboard()
220/46:
# arr = [1170,4,12,13,16,3132,3136,3139, 3140]
arr = chk[chk==6].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
220/47:
import importlib
importlib.reload(du)
220/48:
x = du.cleaned_longitudinal_inputs()[0]
t = du.TargetFunc()
t._init_spikes(x)
con = t.spikes_df.copy()
220/49:
import importlib
importlib.reload(du)
220/50:
x = du.cleaned_longitudinal_inputs()[0]
t = du.TargetFunc()
t._init_spikes(x)
con = t.spikes_df.copy()
220/51:
x = du.cleaned_longitudinal_inputs()[0]
t = du.TargetFunc()
t._init_spikes(x)
con = t.spikes_df.copy()
220/52:
import importlib
importlib.reload(du)
220/53:
x = du.cleaned_longitudinal_inputs()[0]
t = du.TargetFunc()
t._init_spikes(x)
con = t.spikes_df.copy()
220/54:
import importlib
importlib.reload(du)
220/55:
x = du.cleaned_longitudinal_inputs()[0]
t = du.TargetFunc()
t._init_spikes(x)
con = t.spikes_df.copy()
220/56:
import importlib
importlib.reload(du)
220/57:
x = du.cleaned_longitudinal_inputs()[0]
t = du.TargetFunc()
t._init_spikes(x)
con = t.spikes_df.copy()
220/58:
chk = con.groupby('AUTO_ID').spikes.sum()
print(chk.value_counts().head(10))
sns.distplot(chk)
plt.show()
220/59:
# arr = [1170,4,12,13,16,3132,3136,3139, 3140]
arr = chk[chk==1].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
220/60:
# arr = [1170,4,12,13,16,3132,3136,3139, 3140]
arr = chk[chk==0].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
220/61:
# arr = [1170,4,12,13,16,3132,3136,3139, 3140]
arr = chk[chk==0].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
220/62:
# arr = [1170,4,12,13,16,3132,3136,3139, 3140]
arr = chk[chk==0].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
220/63:
# arr = [1170,4,12,13,16,3132,3136,3139, 3140]
arr = chk[chk==0].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
220/64:
# arr = [1170,4,12,13,16,3132,3136,3139, 3140]
arr = chk[chk==3].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
220/65:
# arr = [1170,4,12,13,16,3132,3136,3139, 3140]
arr = chk[chk==3].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
220/66:
# arr = [1170,4,12,13,16,3132,3136,3139, 3140]
arr = chk[chk==3].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
220/67: x.columns
220/68:
cols = x.columns[1:]
cols[list(range(11,20))+[5,7,9]+[23,26,29,34]+[21,32,38]]
220/69:
# cols = x.columns[1:]
x.columns[1:][list(range(11,20))+[5,7,9]+[23,26,29,34]+[21,32,38]]
220/70:
for _,df in x.head(150).groupby('AUTO_ID'):
    print x.columns[:10]
220/71:
for _,df in x.head(150).groupby('AUTO_ID'):
    print (x.columns[:10])
220/72: x[x.columns[1:][list(range(11,20))+[5,7,9]+[23,26,29,34]+[21,32,38]]].sum()
220/73: type(x[x.columns[1:][list(range(11,20))+[5,7,9]+[23,26,29,34]+[21,32,38]]].sum())
220/74: x[x.columns[1:][list(range(11,20))+[5,7,9]+[23,26,29,34]+[21,32,38]]].sum().values
220/75:
import importlib
importlib.reload(du)
220/76: y1 = du.IBDSequenceGenerator().write_to_disk('tmp', labels_only=True)
220/77:
import importlib
importlib.reload(du)
220/78: y1 = du.IBDSequenceGenerator().write_to_disk('tmp', labels_only=True)
220/79: y1
220/80: df = pd.DataFrame({k:v[0] for k,v in y1})
220/81: y1.items()
220/82: list(y1.items())[0]
220/83: {k:v[0] for k,v in y1[:5]}
220/84: {k:v[0] for k,v in y1}
220/85: {k:v[0] for k,v in list(y1.items())[:5]}
220/86: df = pd.DataFrame({k:v[0] for k,v in list(y1.items())})
220/87: df = pd.DataFrame({k:v[0] for k,v in list(y1.items())}, index=range(len(y1)))
220/88:
chg = [v[0] for k,v in y1.items()]
chg = pd.Series(chg)
220/89: chg.describe()
220/90: sns.distplot(chg)
220/91:
# chg = [v[0] for k,v in y1.items()]
# chg = pd.Series(chg)
chg.describe()
220/92: print(f"%ile of $60k {chg[chg<60000].mean()})
220/93: print(f"%ile of $60k {chg[chg<60000].mean()}"")
220/94: print(f"%ile of $60k {chg[chg<60000].mean()}")
220/95: print(f"%ile of $60k {(chg<60000).mean()}")
220/96:
print(f"%ile of $60k {(chg<60000).mean()}")
print(f" $ value at 80%ile {chg.quantile(0.8)}")
220/97: utils.pkl_dump(y1, 'pickles/1ycharges.dict')
220/98:
print(f"%ile of $60k {(chg<60000).mean()}")
print(f" $ value at 80%ile {chg.quantile(0.8)}")

print([chg.quantile(x) for x in range(0,1.01,0.1)])
220/99: range(0,1,0.1)
220/100: range(0,11)
220/101: list(range(0,11))
220/102:
print(f"%ile of $60k {(chg<60000).mean()}")
print(f" $ value at 80%ile {chg.quantile(0.8)}")
print([chg.quantile(x/10) for x in range(0,10)])
220/103:
print(f"%ile of $60k {(chg<60000).mean()}")
print(f" $ value at 80%ile {chg.quantile(0.8)}")
print([chg.quantile(x/10) for x in range(0,10)])
print([chg.quantile(x/10) for x in range(80,100,2)])
220/104:
print(f"%ile of $60k {(chg<60000).mean()}")
print(f" $ value at 80%ile {chg.quantile(0.8)}")
print([chg.quantile(x/10) for x in range(0,10)])
print([chg.quantile(x/100) for x in range(80,100,2)])
220/105:
print(f"%ile of $60k {(chg<60000).mean()}")
print(f" $ value at 80%ile {chg.quantile(0.8)}")
print([chg.quantile(x/10) for x in range(0,10)])
print([chg.quantile(x/100) for x in range(80,100,2)])
for k,v in y1.items():
    if v[0]>8000:
        print k
220/106:
print(f"%ile of $60k {(chg<60000).mean()}")
print(f" $ value at 80%ile {chg.quantile(0.8)}")
print([chg.quantile(x/10) for x in range(0,10)])
print([chg.quantile(x/100) for x in range(80,100,2)])
for k,v in y1.items():
    if v[0]>8000:
        print(k)
220/107:
arr = [1,4,9,10,16,24,27,28,51]
# arr = chk[chk==3].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges']
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
220/108:
arr = [1,4,9,10,16,24,27,28,51]
# arr = chk[chk==3].sample(9).index

fig, axes = plt.subplots(3,3, figsize = (12,12))
arr = np.array(arr).reshape(3,-1)
for r in range(3):
    for c in range(3):
        df=lib[lib.AUTO_ID==arr[r][c]]
        xax = range(len(df))
        y = df['charges'].cumsum()
        titl = len(df)
        axes[r,c].plot(xax, y)
        axes[r,c].set_title(f"AID: {df.AUTO_ID.values[0]}, Months: {titl}")
fig.tight_layout()
plt.show()
223/1: l = [1,2,3,4,5]
223/2: *l
223/3: 3*l
223/4: f = False
223/5: c = not f
223/6: c
223/7: d = not c
223/8: d
223/9: d = not f
223/10: d
223/11:
def walk(nums):
    i=0
    j=0
    dir_down=False

    R = len(nums)
    C = len(nums[0])
    res = [0]*(R+C)

    for k in range(len(res)):
        res[k] = nums[i][j]

        if dir_down:
            i+=1
            j-=1
        else:
            i-=1
            j+=1

        if i==-1:
            dir_down = not dir_down
            i=0
            continue

        if j==-1:
            dir_down = not dir_down
            j=0
            continue

        if i==R:
            dir_down = not dir_down
            j+=2
            i=R-1
            continue

        if j==C:
            dir_down = not dir_down
            i+=2
            j = C-1

    return res
223/12: walk([[1,2,3,4],[5,6,7,8],[9,10,11,12]])
223/13:
def walk(nums):
    i=0
    j=0
    dir_down=False

    R = len(nums)
    C = len(nums[0])
    res = [0]*(R*C)

    for k in range(len(res)):
        res[k] = nums[i][j]

        if dir_down:
            i+=1
            j-=1
        else:
            i-=1
            j+=1

        if i==-1:
            dir_down = not dir_down
            i=0
            continue

        if j==-1:
            dir_down = not dir_down
            j=0
            continue

        if i==R:
            dir_down = not dir_down
            j+=2
            i=R-1
            continue

        if j==C:
            dir_down = not dir_down
            i+=2
            j = C-1

    return res
223/14: walk([[1,2,3,4],[5,6,7,8],[9,10,11,12]])
223/15: walk([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])
223/16:
def walk(nums):
    i=0
    j=0
    dir_down=False

    R = len(nums)
    C = len(nums[0])
    res = [0]*(R*C)

    for k in range(len(res)):
        print(i, j)
        res[k] = nums[i][j]

        if dir_down:
            i+=1
            j-=1
        else:
            i-=1
            j+=1

        if i==-1:
            dir_down = not dir_down
            i=0
            continue

        if j==-1:
            dir_down = not dir_down
            j=0
            continue

        if i==R:
            dir_down = not dir_down
            j+=2
            i=R-1
            continue

        if j==C:
            dir_down = not dir_down
            i+=2
            j = C-1

    return res
    \
223/18:
def walk(nums):
    i=0
    j=0
    dir_down=False

    R = len(nums)
    C = len(nums[0])
    res = [0]*(R*C)

    for k in range(len(res)):
        print(i, j)
        res[k] = nums[i][j]

        if dir_down:
            i+=1
            j-=1
        else:
            i-=1
            j+=1

        if i==-1:
            dir_down = not dir_down
            i=0
            continue

        if j==-1:
            dir_down = not dir_down
            j=0
            continue

        if i==R:
            dir_down = not dir_down
            j+=2
            i=R-1
            continue

        if j==C:
            dir_down = not dir_down
            i+=2
            j = C-1

    return res
223/19: walk([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])
223/20:
def walk(nums):
    i=0
    j=0
    dir_down=False

    R = len(nums)
    C = len(nums[0])
    res = [0]*(R*C)

    for k in range(len(res)):
        print(i, j)
        res[k] = nums[i][j]

        if dir_down:
            i+=1
            j-=1
        else:
            i-=1
            j+=1

        if i==R:
            dir_down = not dir_down
            j+=2
            i=R-1
            continue

        if j==C:
            dir_down = not dir_down
            i+=2
            j = C-1
            continue

        if i==-1:
            dir_down = not dir_down
            i=0
            continue

        if j==-1:
            dir_down = not dir_down
            j=0
            continue

    return res
223/21: walk([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])
224/1:
seqgen = du.IBDSequenceGenerator(36, 12)
seqgen.write_to_disk('m36p12', seq_only=True)
224/2:
y1 = utils.pkl_load('pickles/1ycharges.dict')
chg = [v[0] for k,v in y1.items()]
chg = pd.Series(chg)
print(f"Mean: {chg.mean()}  Median:{chg.quantile(0.5)}")
print("Distribution of chg above median")
sns.distplot(chg[chg>chg.quantile(0.5)])
224/3:
import utils
import data_utils as du
import os
import pandas as pd
import pickle
import seaborn as sns
from matplotlib import pyplot as plt
224/4:
y1 = utils.pkl_load('pickles/1ycharges.dict')
chg = [v[0] for k,v in y1.items()]
chg = pd.Series(chg)
print(f"Mean: {chg.mean()}  Median:{chg.quantile(0.5)}")
print("Distribution of chg above median")
sns.distplot(chg[chg>chg.quantile(0.5)])
224/5:
import utils
import data_utils as du
import os
import pandas as pd
import pickle
import seaborn as sns
from matplotlib import pyplot as plt
import pandas as pd
225/1:
import utils
import data_utils as du
import os
import pandas as pd
import pickle
import seaborn as sns
from matplotlib import pyplot as plt
import pandas as pd
225/2:
y1 = utils.pkl_load('pickles/1ycharges.dict')
chg = [v[0] for k,v in y1.items()]
chg = pd.Series(chg)
print(f"Mean: {chg.mean()}  Median:{chg.quantile(0.5)}")
print("Distribution of chg above median")
sns.distplot(chg[chg>chg.quantile(0.5)])
226/1:
import utils
import data_utils as du
import os
import pandas as pd
import pickle
import seaborn as sns
from matplotlib import pyplot as plt
import pandas as pd
226/2:
# y1 = utils.pkl_load('pickles/1ycharges.dict')
chg = [v[0] for k,v in y1.items()]
chg = pd.Series(chg)
print(f"Mean: {chg.mean()}  Median:{chg.quantile(0.5)}")
print("Distribution of chg above median")
sns.distplot(chg[chg.between(chg.quantile(0.1), chg.quantile(0.95)])
226/3:
# y1 = utils.pkl_load('pickles/1ycharges.dict')
chg = [v[0] for k,v in y1.items()]
chg = pd.Series(chg)
print(f"Mean: {chg.mean()}  Median:{chg.quantile(0.5)}")
print("Distribution of chg above median")
sns.distplot(chg[chg.between(chg.quantile(0.1), chg.quantile(0.95))])
226/4:
y1 = utils.pkl_load('pickles/1ycharges.dict')
chg = [v[0] for k,v in y1.items()]
chg = pd.Series(chg)
print(f"Mean: {chg.mean()}  Median:{chg.quantile(0.5)}")
print("Distribution of chg above median")
sns.distplot(chg[chg.between(chg.quantile(0.1), chg.quantile(0.95))])
226/5:
# y1 = utils.pkl_load('pickles/1ycharges.dict')
# chg = [v[0] for k,v in y1.items()]
# chg = pd.Series(chg)
print(f"Mean: {chg.mean()}  Median:{chg.quantile(0.5)}")
print("Distribution of chg between the 10th and 95th %ile:")
sns.distplot(chg[chg.between(chg.quantile(0.1), chg.quantile(0.95))])

print("Distribution of chg between the $1 and $20000:")
sns.distplot(chg[chg.between(1, 20000)])
226/6:
# y1 = utils.pkl_load('pickles/1ycharges.dict')
# chg = [v[0] for k,v in y1.items()]
# chg = pd.Series(chg)
print(f"Mean: {chg.mean()}  Median:{chg.quantile(0.5)}")
print("Distribution of chg between the 10th and 95th %ile:")
sns.distplot(chg[chg.between(chg.quantile(0.1), chg.quantile(0.95))])
plt.show()
print("Distribution of chg between the $1 and $20000:")
sns.distplot(chg[chg.between(1, 20000)])
226/7:
# y1 = utils.pkl_load('pickles/1ycharges.dict')
# chg = [v[0] for k,v in y1.items()]
# chg = pd.Series(chg)
print(f"Mean: {chg.mean()}. Mean is heavily skewed by disproportionate expenses.\nMedian:{chg.quantile(0.5)}")
print("Distribution of chg between the 10th and 95th %ile:")
sns.distplot(chg[chg.between(chg.quantile(0.1), chg.quantile(0.95))])
plt.show()
print("Distribution of chg between the $1 and $20000:")
sns.distplot(chg[chg.between(1, 20000)])
plt.show()
print("Distribution of chg between the $1 and $5000:")
sns.distplot(chg[chg.between(1, 5000)])
plt.show()
226/8:
print(f" $ value at 80%ile {chg.quantile(0.8)}")
print([f"{x/10}%ile: {chg.quantile(x/10)}" for x in range(0,10)])
print([f"{x/100}%ile: {chg.quantile(x/100)}" for x in range(80,100,2)])
226/9:
print(f" $ value at 80%ile {chg.quantile(0.8)}")
print([f"{x}%ile: {chg.quantile(x/10)}" for x in range(0,10)])
print([f"{x/10}%ile: {chg.quantile(x/100)}" for x in range(80,100,2)])
226/10:
x = du.cleaned_longitudinal_inputs()[0]
t = du.TargetFunc()
t._init_spikes(x)
con = t.spikes_df.copy()
226/11:
con['spikes_val'] = con['spikes']*con['charges']

chk1 = con.groupby('AUTO_ID').spikes.sum()
sns.distplot(chk1)
plt.show()

chk2 = con.groupby('AUTO_ID').spikes_val.sum()
sns.distplot(chk2)
plt.show()
226/12:
# con['spikes_val'] = con['spikes']*con['charges']

# chk1 = con.groupby('AUTO_ID').spikes.sum()
# sns.distplot(chk1)
# plt.show()

chk2 = con.groupby('AUTO_ID').spikes_val.sum()
sns.distplot(chk2[chk2>0])
plt.show()
227/1:
import utils
import data_utils as du
import os
import pandas as pd
import pickle
import seaborn as sns
from matplotlib import pyplot as plt
import pandas as pd
227/2:
import utils
import data_utils as du
import os
import pandas as pd
import pickle
import seaborn as sns
from matplotlib import pyplot as plt
import pandas as pd
227/3:
import utils
import data_utils as du
import os
import pandas as pd
import pickle
import seaborn as sns
from matplotlib import pyplot as plt
import pandas as pd
227/4:
x = du.cleaned_longitudinal_inputs()[0]
t = du.TargetFunc()
t._init_spikes(x)
con = t.spikes_df.copy()
227/5:
# con['spikes_val'] = con['spikes']*con['charges']

# chk1 = con.groupby('AUTO_ID').spikes.sum()
# sns.distplot(chk1)
# plt.show()

chk2 = con.groupby('AUTO_ID').spikes_val.sum()
sns.distplot(chk2[chk2.between(0,100000)])
plt.show()
227/6:
con['spikes_val'] = con['spikes']*con['charges']

# chk1 = con.groupby('AUTO_ID').spikes.sum()
# sns.distplot(chk1)
# plt.show()

chk2 = con.groupby('AUTO_ID').spikes_val.sum()
sns.distplot(chk2[chk2.between(0,100000)])
plt.show()
227/7:
# con['spikes_val'] = con['spikes']*con['charges']

# chk1 = con.groupby('AUTO_ID').spikes.sum()
# sns.distplot(chk1)
# plt.show()

chk2 = con.groupby('AUTO_ID').spikes_val.sum()
sns.distplot(chk2[chk2.between(2,100000)])
plt.show()
227/8: con.spikes_val.quantile(0.5)
227/9: con.spikes_val.describe()
227/10:
yy = con.spikes_val
yy[yy>1].quantile(0.5)
229/1:
dataset = du.PatientDataset(list_IDs=5, fp='./data/m36p12/sequences', label_dict_path='./data/m36p12/annual_avg_label_dict.pkl')
loader = DataLoader(dataset, shuffle=True, batch_size=5, drop_last=True)
for X,y in loader:
    pass
print(type(y), type(y[0]))
model = gru.GRUD(aux_loss_cols=['diag', 'enc', 'hilab', 'lolab']).float()
state = model.init_hidden(X.shape[0])

# yhat, y_tr, y_aux, state = model(X.float(), state)
# print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y_TR: {y_tr.shape}  Y_Aux:{y_aux.shape}  Y: {y.shape}")

yhat, state = gru.predict(model, X,y, state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
229/2:
import torch 
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
229/3:
dataset = du.PatientDataset(list_IDs=5, fp='./data/m36p12/sequences', label_dict_path='./data/m36p12/annual_avg_label_dict.pkl')
loader = DataLoader(dataset, shuffle=True, batch_size=5, drop_last=True)
for X,y in loader:
    pass
print(type(y), type(y[0]))
model = gru.GRUD(aux_loss_cols=['diag', 'enc', 'hilab', 'lolab']).float()
state = model.init_hidden(X.shape[0])

# yhat, y_tr, y_aux, state = model(X.float(), state)
# print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y_TR: {y_tr.shape}  Y_Aux:{y_aux.shape}  Y: {y.shape}")

yhat, state = gru.predict(model, X,y, state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
223/22: 'a.npy'[:-4]
230/1:
import torch 
from torch.utils.data import DataLoader
import data_utils as du
import gru
import utils
230/2:
dataset = du.PatientDataset(list_IDs=5, fp='./data/m36p12/sequences', label_dict_path='./data/m36p12/annual_avg_label_dict.pkl')
loader = DataLoader(dataset, shuffle=True, batch_size=5, drop_last=True)
for X,y in loader:
    pass
print(type(y), type(y[0]))
model = gru.GRUD(aux_loss_cols=['diag', 'enc', 'hilab', 'lolab']).float()
state = model.init_hidden(X.shape[0])

# yhat, y_tr, y_aux, state = model(X.float(), state)
# print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y_TR: {y_tr.shape}  Y_Aux:{y_aux.shape}  Y: {y.shape}")

yhat, state = gru.predict(model, X,y, state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
230/3:
dataset = du.PatientDataset(list_IDs=5, fp='./data/m36p12/sequences', label_dict_path='./data/m36p12/annual_avg_label_dict.pkl')
loader = DataLoader(dataset, shuffle=True, batch_size=5, drop_last=True)
for X,y in loader:
    pass
print(type(y), type(y[0]))
model = gru.GRUD(aux_loss_cols=['diag', 'enc', 'hilab', 'lolab']).float()
state = model.init_hidden(X.shape[0])

# yhat, y_tr, y_aux, state = model(X.float(), state)
# print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y_TR: {y_tr.shape}  Y_Aux:{y_aux.shape}  Y: {y.shape}")

yhat, state = gru.predict(model, X,y[0], state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
230/4: y
230/5:
dataset = du.PatientDataset(list_IDs=5, fp='./data/m36p12/sequences', label_dict_path='./data/m36p12/annual_avg_label_dict.pkl')
loader = DataLoader(dataset, shuffle=True, batch_size=3, drop_last=True)
for X,y in loader:
    pass
print(type(y), type(y[0]))
model = gru.GRUD(aux_loss_cols=['diag', 'enc', 'hilab', 'lolab']).float()
state = model.init_hidden(X.shape[0])

# yhat, y_tr, y_aux, state = model(X.float(), state)
# print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y_TR: {y_tr.shape}  Y_Aux:{y_aux.shape}  Y: {y.shape}")

yhat, state = gru.predict(model, X,y[0], state)
print(f"X: {X.shape}  Xt: {X[:,0,:,:].shape}  State: {state.shape} \nYhat: {yhat.shape}  Y: {y.shape}")
230/6: y
223/23: from sklearn.utils import class_weight
223/24: class_weight.compute_class_weight('balanced', [0,1], [0]*8+[1]*2)
231/1:
import pandas as pd
import pickle
import data_utils as du
231/2: df = pd.read_pickle('pickles/monthwise_inputs.pkl')
231/3:
# df = pd.read_pickle('pickles/monthwise_inputs.pkl')
df.head()
231/4:
import pandas as pd
import pickle
import data_utils as du
231/5:
# df = pd.read_pickle('pickles/monthwise_inputs.pkl')
with open('pickles/col_dict.pkl','rb') as f:
    col_dict = pickle.load(f)
# df.head()
col_dict['input']
231/6: df.ENC_PROC_Related[df.ENC_PROC_Related.notnull()].mean()
231/7: df.DIAG_COLONOSCOPY[df.DIAG_COLONOSCOPY.notnull()].mean()
231/8: df.DIAG_COLONOSCOPY.fillna(0).mean()
231/9: df.LAB_albumin_Low.fillna(0).mean()
231/10: df.LAB_monocytes_Low.fillna(0).mean()
231/11: df.MED_ANTI_IL12.fillna(0).mean()
231/12: df.MED_ANTI_TNF.fillna(0).mean()
231/13: df.MED_Psych.fillna(0).mean()
231/14: df.LAB_vitamin_d_High.fillna(0).mean()
231/15: df.LAB_vitamin_d_Low.fillna(0).mean()
231/16: df.LAB_vitamin_d_Normal.fillna(0).mean()
231/17: df.LAB_hemoglobin_High.fillna(0).mean()
231/18:
# df.LAB_hemoglobin_High.fillna(0).mean()
s = 'LAB_hemoglobin_High'
df[s][df[s].notnull()].mean()
231/19:
# df.LAB_hemoglobin_High.fillna(0).mean()
s = 'LAB_vitamin_d_High'
df[s][df[s].notnull()].mean()
231/20:
# df.LAB_hemoglobin_High.fillna(0).mean()
s = 'LAB_vitamin_d_Normal'
df[s][df[s].notnull()].mean()
231/21:
# df.LAB_hemoglobin_High.fillna(0).mean()
s = 'MED_Psych'
df[s][df[s].notnull()].mean()
231/22:
# df.LAB_hemoglobin_High.fillna(0).mean()
s = 'MED_ANTI_TNF'
df[s][df[s].notnull()].mean()
231/23:
# df.LAB_hemoglobin_High.fillna(0).mean() - MED
s = 'LAB_eos_Normal'
df[s][df[s].notnull()].mean()
231/24: df[s].describe()
231/25:
# df.LAB_hemoglobin_High.fillna(0).mean() - MED
s = 'LAB_crp_Low'
df[s][df[s].notnull()].mean()
231/26:
# df.LAB_hemoglobin_High.fillna(0).mean() - MED
s = 'LAB_crp_High'
df[s][df[s].notnull()].mean()
231/27:
# df.LAB_hemoglobin_High.fillna(0).mean() - MED
s = 'LAB_crp_Normal'
df[s][df[s].notnull()].mean()
231/28:
# df.LAB_hemoglobin_High.fillna(0).mean() - MED
s = 'DIAG_COLONOSCOPY'
df[s][df[s].notnull()].mean()
231/29:
s = 'DIAG_COLONOSCOPY'

# df[s][df[s].notnull()].mean()

df[s].fillna(0).mean() # - MED
231/30:
s = 'DIAG_SIGMOIDOSCOPY'

# df[s][df[s].notnull()].mean()

df[s].fillna(0).mean() # - MED
231/31:
s = 'DIAG_ILEOSCOPY'

# df[s][df[s].notnull()].mean()

df[s].fillna(0).mean() # - MED
231/32:
s = 'ENC_PROC_Unrelated'

# df[s][df[s].notnull()].mean()

df[s].fillna(0).mean() # - MED, DIAG
231/33:
cols = ['ENC_OFF_Related', 'ENC_OFF_Unrelated',\
                'ENC_PROC_Related', 'ENC_PROC_Unrelated', 'ENC_TEL_Related',\
                'ENC_TEL_Unrelated', 'DIAG_COLONOSCOPY', 'DIAG_ENDOSCOPY',\
                'DIAG_SIGMOIDOSCOPY', 'DIAG_ILEOSCOPY', 'DIAG_ERCP', 'DIAG_EGD',\
                'DIAG_UPPER GI ENDO', 'DIAG_UPPER EUS', 'DIAG_GI_PROCEDURE',\
                'LAB_albumin_High', 'LAB_albumin_Low', 'LAB_albumin_Normal',\
                'LAB_crp_High', 'LAB_crp_Low', 'LAB_crp_Normal', 'LAB_eos_High',\
                'LAB_eos_Low', 'LAB_eos_Normal', 'LAB_esr_High', 'LAB_esr_Normal',\
                'LAB_hemoglobin_High', 'LAB_hemoglobin_Low', 'LAB_hemoglobin_Normal',\
                'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_monocytes_Normal',\
                'LAB_vitamin_d_High', 'LAB_vitamin_d_Low', 'LAB_vitamin_d_Normal', \
                'MED_5_ASA', 'MED_Systemic_steroids', 'MED_Immunomodulators', 'MED_Psych',\
                'MED_ANTI_TNF', 'MED_ANTI_IL12', 'MED_ANTI_INTEGRIN']

for c in cols:
    print(c, df[c].min(), df[c].max())
232/1: import pandas as pd
232/2: pd.read_pickle('padded').to_csv('padded.csv', index=False)
232/3: df = pd.read_pickle('padded')#.to_csv('padded.csv', index=False)
232/4: df[df.AUTO_ID==2870].iloc[:18]
232/5: df[df.AUTO_ID==2870].iloc[:18].sum()
232/6: df[df.AUTO_ID==2870].iloc[:18].sum().to_clipboard()
232/7: df[df.AUTO_ID==2870].iloc[:18].sum().T.to_clipboard()
232/8: df[df.AUTO_ID==2870].iloc[19:51].sum().T.to_clipboard()
232/9: df[df.AUTO_ID==2537].iloc[:28].sum().T.to_clipboard()
232/10: df[df.AUTO_ID==2537].iloc[29:46].sum().T.to_clipboard()
232/11: df[df.AUTO_ID==136].iloc[:26].sum().T.to_clipboard()
232/12: df[df.AUTO_ID==136].iloc[26:61].sum().T.to_clipboard()
232/13: df[df.AUTO_ID==2870].iloc[:18].sum().T.to_clipboard()
232/14: df[df.AUTO_ID==2870].iloc[:18].to_clipboard()
232/15: df[df.AUTO_ID==2870].iloc[:19].to_clipboard()
232/16: df[df.AUTO_ID==2870].iloc[:19].to_clipboard()
232/17: df[df.AUTO_ID==2870].iloc[19:51].sum().T.to_clipboard()
232/18: df[df.AUTO_ID==2870].iloc[19:51].to_clipboard()
236/1:
%matplotlib inline
import os
import pandas as pd
import numpy as np
import config as cfg
236/2:
IDENTIFIER_COLS = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']

def identifier_prefix(cols):
    identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
    return identifier_cols+[x for x in cols if x not in identifier_cols]

opj = os.path.join
236/3:
%matplotlib inline
import os
import pandas as pd
import numpy as np
236/4:
IDENTIFIER_COLS = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']

def identifier_prefix(cols):
    identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
    return identifier_cols+[x for x in cols if x not in identifier_cols]

opj = os.path.join
236/5:
hbi = pd.read_csv('datasets/raw_registry_data/deid_IBD_Registry_BA1951_Harvey_Bradshaw_Questionnaires_2018-07-05-07-57-02.csv', parse_dates=['CONTACT_DATE'])
hbi = hbi[['AUTO_ID', 'CONTACT_DATE', 'CROHNS_SCORE', 'UC_SCORE']]
hbi.AUTO_ID = pd.to_numeric(hbi.AUTO_ID).astype(int)
hbi['RESULT_YEAR'] = hbi.CONTACT_DATE.dt.year
hbi['RESULT_MONTH'] = hbi.CONTACT_DATE.dt.month
del hbi['CONTACT_DATE']

HBI = hbi.groupby(IDENTIFIER_COLS).mean().unstack(-1).reset_index()
HBI.head(30)
236/6:
# hbi = pd.read_csv('datasets/raw_registry_data/deid_IBD_Registry_BA1951_Harvey_Bradshaw_Questionnaires_2018-07-05-07-57-02.csv', parse_dates=['CONTACT_DATE'])
# hbi = hbi[['AUTO_ID', 'CONTACT_DATE', 'CROHNS_SCORE', 'UC_SCORE']]
# hbi.AUTO_ID = pd.to_numeric(hbi.AUTO_ID).astype(int)
# hbi['RESULT_YEAR'] = hbi.CONTACT_DATE.dt.year
# hbi['RESULT_MONTH'] = hbi.CONTACT_DATE.dt.month
# del hbi['CONTACT_DATE']

HBI = hbi.groupby(IDENTIFIER_COLS).mean().unstack().reset_index()
HBI.head(30)
236/7:
# hbi = pd.read_csv('datasets/raw_registry_data/deid_IBD_Registry_BA1951_Harvey_Bradshaw_Questionnaires_2018-07-05-07-57-02.csv', parse_dates=['CONTACT_DATE'])
# hbi = hbi[['AUTO_ID', 'CONTACT_DATE', 'CROHNS_SCORE', 'UC_SCORE']]
# hbi.AUTO_ID = pd.to_numeric(hbi.AUTO_ID).astype(int)
# hbi['RESULT_YEAR'] = hbi.CONTACT_DATE.dt.year
# hbi['RESULT_MONTH'] = hbi.CONTACT_DATE.dt.month
# del hbi['CONTACT_DATE']

HBI = hbi.groupby(IDENTIFIER_COLS).mean().unstack(1).reset_index()
HBI.head(30)
236/8:
# hbi = pd.read_csv('datasets/raw_registry_data/deid_IBD_Registry_BA1951_Harvey_Bradshaw_Questionnaires_2018-07-05-07-57-02.csv', parse_dates=['CONTACT_DATE'])
# hbi = hbi[['AUTO_ID', 'CONTACT_DATE', 'CROHNS_SCORE', 'UC_SCORE']]
# hbi.AUTO_ID = pd.to_numeric(hbi.AUTO_ID).astype(int)
# hbi['RESULT_YEAR'] = hbi.CONTACT_DATE.dt.year
# hbi['RESULT_MONTH'] = hbi.CONTACT_DATE.dt.month
# del hbi['CONTACT_DATE']

HBI = hbi.groupby(IDENTIFIER_COLS).mean()#.unstack().reset_index()
HBI.head(30)
236/9:
# hbi = pd.read_csv('datasets/raw_registry_data/deid_IBD_Registry_BA1951_Harvey_Bradshaw_Questionnaires_2018-07-05-07-57-02.csv', parse_dates=['CONTACT_DATE'])
# hbi = hbi[['AUTO_ID', 'CONTACT_DATE', 'CROHNS_SCORE', 'UC_SCORE']]
# hbi.AUTO_ID = pd.to_numeric(hbi.AUTO_ID).astype(int)
# hbi['RESULT_YEAR'] = hbi.CONTACT_DATE.dt.year
# hbi['RESULT_MONTH'] = hbi.CONTACT_DATE.dt.month
# del hbi['CONTACT_DATE']

HBI = hbi.groupby(IDENTIFIER_COLS).mean().reset_index()
HBI.head(30)
236/10:
# hbi = pd.read_csv('datasets/raw_registry_data/deid_IBD_Registry_BA1951_Harvey_Bradshaw_Questionnaires_2018-07-05-07-57-02.csv', parse_dates=['CONTACT_DATE'])
# hbi = hbi[['AUTO_ID', 'CONTACT_DATE', 'CROHNS_SCORE', 'UC_SCORE']]
# hbi.AUTO_ID = pd.to_numeric(hbi.AUTO_ID).astype(int)
# hbi['RESULT_YEAR'] = hbi.CONTACT_DATE.dt.year
# hbi['RESULT_MONTH'] = hbi.CONTACT_DATE.dt.month
# del hbi['CONTACT_DATE']

HBI = hbi.groupby(IDENTIFIER_COLS).mean().reset_index()
HBI.to_csv('datasets/long_in/hbi.csv', index=False)
239/1:
import pandas as pd
from config import *
import os
from functools import reduce
239/2:
import pandas as pd
import os
from functools import reduce
239/3: IDENTIFIER_COLS = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
239/4:
CT = pd.read_csv('datasets/long_out/noCT.csv')
CT = CT.merge(hbi, how='outer', on=IDE)
239/5:
# charges = pd.read_csv(opj(LONG_IN, 'charges.csv'))
# encounters = pd.read_csv(opj(LONG_IN,'encounters.csv'))
# diag = pd.read_csv(opj(LONG_IN,'gi_diagnostics.csv'))
# diag.columns = [f'DIAG_{x}' if x not in IDENTIFIER_COLS else x for x in diag.columns ]
# labs = pd.read_csv(opj(LONG_IN, 'labs.csv'))
# labs.columns = [f'LAB_{x}' if x not in IDENTIFIER_COLS else x for x in labs.columns]
# meds = pd.read_csv(opj(LONG_IN, 'rx_long.csv'))
# meds.columns = [f'MED_{x}' if x not in IDENTIFIER_COLS else x for x in meds.columns]
hbi = pd.read_csv('datasets/long_in/hbi.csv')
hbi.columns = [f'HBI_{x}' if x not in IDENTIFIER_COLS else x for x in hbi.columns]
hbi.head()
239/6:
CT = pd.read_csv('datasets/long_out/noCT.csv')
CT = CT.merge(hbi, how='outer', on=IDENTIFIER_COLS)
239/7: CT.head(50)
239/8:
cedl = pd.read_csv('datasets/long_out/all.csv')
cedl = cedl.merge(hbi, how='outer', on=IDENTIFIER_COLS)
239/9: cedl.head(50)
239/10:
cedl.to_csv('datasets/long_out/all.csv', index=False)
CT.to_csv('datasets/long_out/noCT.csv', index=False)
242/1:
import utils
import data_utils as du
import os
import pandas as pd
import pickle
import seaborn as sns
from matplotlib import pyplot as plt
import pandas as pd
237/1:
disease = pd.read_csv('datasets/raw_registry_data/deid_disease type and year of diagnosis.csv')
disease.head()
237/2:
%matplotlib inline
import os
import pandas as pd
import numpy as np
237/3:
disease = pd.read_csv('datasets/raw_registry_data/deid_disease type and year of diagnosis.csv')
disease.head()
237/4: disease.DSTYPE.value_counts()
237/5: disease.BOWEL_RESECTION_PRIOR_2009.value_counts()
237/6:
disease = pd.read_csv('datasets/raw_registry_data/deid_disease type and year of diagnosis.csv')
disease.DSTYPE = disease.DSTYPE.replace({'1':'CD', '2':'UC', '-2':'unk', '3':'unk'})
237/7:
# disease.BOWEL_RESECTION_PRIOR_2009.value_counts()
pd.get_dummies(disease.DSTYPE)
237/8:
# disease.BOWEL_RESECTION_PRIOR_2009.value_counts()
# pd.get_dummies(disease.DSTYPE)
disease.AUTO_ID
237/9:
# disease.BOWEL_RESECTION_PRIOR_2009.value_counts()
pd.get_dummies(disease.DSTYPE)
237/10:
# disease.BOWEL_RESECTION_PRIOR_2009.value_counts()
pd.get_dummies(disease.DSTYPE).drop('unk', axis=1)
237/11: disease.head()
237/12:
disease = pd.read_csv('datasets/raw_registry_data/deid_disease type and year of diagnosis.csv')
disease.DSTYPE = disease.DSTYPE.replace({'1':'CD', '2':'UC', '-2':'unk', '3':'unk'})
disease[['DS_CD', 'DS_UC']] = pd.get_dummies(disease.DSTYPE).drop('unk', axis=1)
disease['DS_AGE_DX'] = disease['AGE_OF_DX']
disease['DS_PREV_RESECTION'] = disease['BOWEL_RESECTION_PRIOR_2009']
DS = disease[['AUTO_ID']+[x for x in disease.columns if x[:3]=='DS_']]
DS.head()
237/13: disease.BOWEL_RESECTION_PRIOR_2009.value_counts()
237/14: disease.BOWEL_RESECTION_PRIOR_2009.replace({'(No Data)':0}).value_counts()
237/15: disease.BOWEL_RESECTION_PRIOR_2009.replace({'(No Data)':'0'}).value_counts()
237/16: disease.BOWEL_RESECTION_PRIOR_2009.replace({'(No Data)':'0'}).astype(int).value_counts()
237/17: disease.BOWEL_RESECTION_PRIOR_2009.replace({'(No Data)':'0'}).fillna(0).astype(int).value_counts()
237/18:
disease = pd.read_csv('datasets/raw_registry_data/deid_disease type and year of diagnosis.csv')
disease.DSTYPE = disease.DSTYPE.replace({'1':'CD', '2':'UC', '-2':'unk', '3':'unk'})
disease[['DS_CD', 'DS_UC']] = pd.get_dummies(disease.DSTYPE).drop('unk', axis=1)
disease['DS_AGE_DX'] = disease['AGE_OF_DX']
disease['DS_PREV_RESECTION'] = disease['BOWEL_RESECTION_PRIOR_2009'].replace({'(No Data)':'0'}).fillna(0).astype(int)
DS = disease[['AUTO_ID']+[x for x in disease.columns if x[:3]=='DS_']]
DS.head()
237/19:
disease = pd.read_csv('datasets/raw_registry_data/deid_disease type and year of diagnosis.csv')
disease.DSTYPE = disease.DSTYPE.replace({'1':'CD', '2':'UC', '-2':'unk', '3':'unk'})
disease[['DS_CD', 'DS_UC']] = pd.get_dummies(disease.DSTYPE).drop('unk', axis=1)
disease['DS_AGE_DX'] = disease['AGE_OF_DX']
disease['DS_PREV_RESECTION'] = disease['BOWEL_RESECTION_PRIOR_2009'].replace({'(No Data)':'0'}).fillna(0).astype(int)
DS = disease[['AUTO_ID']+[x for x in disease.columns if x[:3]=='DS_']]
DS.to_csv('datasets/long_in/disease_type.csv', index=False)
237/20: DS.head()
240/1:
# charges = pd.read_csv(opj(LONG_IN, 'charges.csv'))
# encounters = pd.read_csv(opj(LONG_IN,'encounters.csv'))
# diag = pd.read_csv(opj(LONG_IN,'gi_diagnostics.csv'))
# diag.columns = [f'DIAG_{x}' if x not in IDENTIFIER_COLS else x for x in diag.columns ]
# labs = pd.read_csv(opj(LONG_IN, 'labs.csv'))
# labs.columns = [f'LAB_{x}' if x not in IDENTIFIER_COLS else x for x in labs.columns]
# meds = pd.read_csv(opj(LONG_IN, 'rx_long.csv'))
# meds.columns = [f'MED_{x}' if x not in IDENTIFIER_COLS else x for x in meds.columns]
hbi = pd.read_csv('datasets/long_in/hbi.csv')
hbi.columns = [f'HBI_{x}' if x not in IDENTIFIER_COLS else x for x in hbi.columns]
dis = pd.read_csv('datasets/long_in/disease_type.csv')
240/2:
import pandas as pd
import os
from functools import reduce
240/3:
# charges = pd.read_csv(opj(LONG_IN, 'charges.csv'))
# encounters = pd.read_csv(opj(LONG_IN,'encounters.csv'))
# diag = pd.read_csv(opj(LONG_IN,'gi_diagnostics.csv'))
# diag.columns = [f'DIAG_{x}' if x not in IDENTIFIER_COLS else x for x in diag.columns ]
# labs = pd.read_csv(opj(LONG_IN, 'labs.csv'))
# labs.columns = [f'LAB_{x}' if x not in IDENTIFIER_COLS else x for x in labs.columns]
# meds = pd.read_csv(opj(LONG_IN, 'rx_long.csv'))
# meds.columns = [f'MED_{x}' if x not in IDENTIFIER_COLS else x for x in meds.columns]
hbi = pd.read_csv('datasets/long_in/hbi.csv')
hbi.columns = [f'HBI_{x}' if x not in IDENTIFIER_COLS else x for x in hbi.columns]
dis = pd.read_csv('datasets/long_in/disease_type.csv')
240/4: IDENTIFIER_COLS = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
240/5:
# charges = pd.read_csv(opj(LONG_IN, 'charges.csv'))
# encounters = pd.read_csv(opj(LONG_IN,'encounters.csv'))
# diag = pd.read_csv(opj(LONG_IN,'gi_diagnostics.csv'))
# diag.columns = [f'DIAG_{x}' if x not in IDENTIFIER_COLS else x for x in diag.columns ]
# labs = pd.read_csv(opj(LONG_IN, 'labs.csv'))
# labs.columns = [f'LAB_{x}' if x not in IDENTIFIER_COLS else x for x in labs.columns]
# meds = pd.read_csv(opj(LONG_IN, 'rx_long.csv'))
# meds.columns = [f'MED_{x}' if x not in IDENTIFIER_COLS else x for x in meds.columns]
hbi = pd.read_csv('datasets/long_in/hbi.csv')
hbi.columns = [f'HBI_{x}' if x not in IDENTIFIER_COLS else x for x in hbi.columns]
dis = pd.read_csv('datasets/long_in/disease_type.csv')
240/6:
charges = pd.read_csv(opj(LONG_IN, 'charges.csv'))
encounters = pd.read_csv(opj(LONG_IN,'encounters.csv'))
diag = pd.read_csv(opj(LONG_IN,'gi_diagnostics.csv'))
diag.columns = [f'DIAG_{x}' if x not in IDENTIFIER_COLS else x for x in diag.columns ]
labs = pd.read_csv(opj(LONG_IN, 'labs.csv'))
labs.columns = [f'LAB_{x}' if x not in IDENTIFIER_COLS else x for x in labs.columns]
meds = pd.read_csv(opj(LONG_IN, 'rx_long.csv'))
meds.columns = [f'MED_{x}' if x not in IDENTIFIER_COLS else x for x in meds.columns]
hbi = pd.read_csv('datasets/long_in/hbi.csv')
hbi.columns = [f'HBI_{x}' if x not in IDENTIFIER_COLS else x for x in hbi.columns]
dis = pd.read_csv('datasets/long_in/disease_type.csv')
240/7:
charges = pd.read_csv('datasets/long_in/charges.csv'))
encounters = pd.read_csv('datasets/long_in/encounters.csv'))
diag = pd.read_csv('datasets/long_in/gi_diagnostics.csv'))
diag.columns = [f'DIAG_{x}' if x not in IDENTIFIER_COLS else x for x in diag.columns ]
labs = pd.read_csv('datasets/long_in/labs.csv'))
labs.columns = [f'LAB_{x}' if x not in IDENTIFIER_COLS else x for x in labs.columns]
meds = pd.read_csv('datasets/long_in/rx_long.csv'))
meds.columns = [f'MED_{x}' if x not in IDENTIFIER_COLS else x for x in meds.columns]
hbi = pd.read_csv('datasets/long_in/hbi.csv')
hbi.columns = [f'HBI_{x}' if x not in IDENTIFIER_COLS else x for x in hbi.columns]
dis = pd.read_csv('datasets/long_in/disease_type.csv')
240/8:
charges = pd.read_csv('datasets/long_in/charges.csv')
encounters = pd.read_csv('datasets/long_in/encounters.csv')
diag = pd.read_csv('datasets/long_in/gi_diagnostics.csv')
diag.columns = [f'DIAG_{x}' if x not in IDENTIFIER_COLS else x for x in diag.columns ]
labs = pd.read_csv('datasets/long_in/labs.csv')
labs.columns = [f'LAB_{x}' if x not in IDENTIFIER_COLS else x for x in labs.columns]
meds = pd.read_csv('datasets/long_in/rx_long.csv')
meds.columns = [f'MED_{x}' if x not in IDENTIFIER_COLS else x for x in meds.columns]
hbi = pd.read_csv('datasets/long_in/hbi.csv')
hbi.columns = [f'HBI_{x}' if x not in IDENTIFIER_COLS else x for x in hbi.columns]
dis = pd.read_csv('datasets/long_in/disease_type.csv')
240/9: final = reduce(lambda l,r: l.merge(r, how='outer', on=IDENTIFIER_COLS), [charges, encounters, diag, labs, meds, hbi, dis])
240/10: final = reduce(lambda l,r: l.merge(r, how='outer', on=IDENTIFIER_COLS), [charges, encounters, diag, labs, meds, hbi])
240/11: final.head()
240/12:
# final = reduce(lambda l,r: l.merge(r, how='outer', on=IDENTIFIER_COLS), [charges, encounters, diag, labs, meds, hbi])
final = final.merge(dis, how='left', on='AUTO_ID')
240/13: final.head()
240/14:
exclude_patients = pd.read_csv('datasets/clean_registry_data/exclude_patients.csv'))
ct_aid = exclude_patients.AUTO_ID
CT_filtered = final[~final.AUTO_ID.isin(ct_aid)]
240/15:
exclude_patients = pd.read_csv('datasets/clean_registry_data/exclude_patients.csv')
ct_aid = exclude_patients.AUTO_ID
CT_filtered = final[~final.AUTO_ID.isin(ct_aid)]
240/16:
final.to_csv('datasets/long_out/all.csv'), index=False)
CT_filtered.to_csv('datasets/long_out/noCT.csv'), index=False)
240/17:
final.to_csv('datasets/long_out/all.csv', index=False)
CT_filtered.to_csv('datasets/long_out/noCT.csv', index=False)
240/18: final.head()
241/1:
import pandas as pd
import os
from functools import reduce
241/2: IDENTIFIER_COLS = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
241/3:
charges = pd.read_csv('datasets/long_in/charges.csv')
encounters = pd.read_csv('datasets/long_in/encounters.csv')
diag = pd.read_csv('datasets/long_in/gi_diagnostics.csv')
diag.columns = [f'DIAG_{x}' if x not in IDENTIFIER_COLS else x for x in diag.columns ]
labs = pd.read_csv('datasets/long_in/labs.csv')
labs.columns = [f'LAB_{x}' if x not in IDENTIFIER_COLS else x for x in labs.columns]
meds = pd.read_csv('datasets/long_in/rx_long.csv')
meds.columns = [f'MED_{x}' if x not in IDENTIFIER_COLS else x for x in meds.columns]
hbi = pd.read_csv('datasets/long_in/hbi.csv')
hbi.columns = [f'HBI_{x}' if x not in IDENTIFIER_COLS else x for x in hbi.columns]
dis = pd.read_csv('datasets/long_in/disease_type.csv')
241/4:
final = reduce(lambda l,r: l.merge(r, how='outer', on=IDENTIFIER_COLS), [charges, encounters, diag, labs, meds, hbi])
final = final.merge(dis, how='left', on='AUTO_ID')
241/5: final.head()
241/6:
exclude_patients = pd.read_csv('datasets/clean_registry_data/exclude_patients.csv')
ct_aid = exclude_patients.AUTO_ID
CT_filtered = final[~final.AUTO_ID.isin(ct_aid)]
241/7:
final.to_csv('datasets/long_out/all.csv', index=False)
CT_filtered.to_csv('datasets/long_out/noCT.csv', index=False)
241/8:
# final.to_csv('datasets/long_out/all.csv', index=False)
CT_filtered.to_csv('datasets/long_out/noCT.csv', index=False)
242/2:
import utils
import data_utils as du
import os
import pandas as pd
import pickle
import seaborn as sns
from matplotlib import pyplot as plt
import pandas as pd
242/3: inputs = du.cleaned_longitudinal_inputs(load_pkl=False)
243/1:
import utils
import data_utils as du
import os
import pandas as pd
import pickle
import seaborn as sns
from matplotlib import pyplot as plt
import pandas as pd
243/2: inputs = du.cleaned_longitudinal_inputs(load_pkl=False)
243/3: inputs[0].head()
243/4: inputs[0].columns
232/19: import torch
232/20: l = torch.Tensor([1,2,3])
232/21: l.size()
232/22: l.unsqueeze(1)
232/23: l.unsqueeze(1).size()
232/24: l.unsqueeze(1).repeat(1,5)
232/25: l.unsqueeze(1).repeat(0,5)
232/26: l.unsqueeze(1).repeat(2,5)
232/27: l.unsqueeze(1).repeat(1,5)
232/28: l.unsqueeze(1).repeat(1,5).view(15)
232/29: l.unsqueeze(1).repeat(1,5).view(15,-1)
232/30: l = torch.Tensor([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])
232/31: l.unsqueeze(1)
232/32: l.unsqueeze(1).repeat(1,4)
232/33: l.unsqueeze(1).view(4,-1)
232/34: l.view(4,-1)
232/35: y=3
232/36: torch.unsqueeze(y, 1).repeat(1,4).view(4)
232/37: torch.unsqueeze(tensor(3), 1).repeat(1,4).view(4)
232/38: torch.unsqueeze(torch.Tensor(3), 1).repeat(1,4).view(4)
232/39: y=torch.Tensor(3)
232/40: y
232/41: y=torch.Tensor([3])
232/42: y
232/43: torch.unsqueeze(torch.Tensor(3), 1).repeat(1,4).view(4)
232/44: torch.unsqueeze(y, 1).repeat(1,4).view(4)
238/1:
diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
diag['RESULT_YEAR'] = diag.VISIT_DATE.dt.year
diag['RESULT_MONTH'] = diag.VISIT_DATE.dt.month
surgery = diag.copy()

diag = diag[diag.PROC_NAME.str.contains('OSCOPY')]
diag.head()
238/2:
%matplotlib inline
import os
import pandas as pd
import numpy as np
238/3:
IDENTIFIER_COLS = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']

def identifier_prefix(cols):
    identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
    return identifier_cols+[x for x in cols if x not in identifier_cols]

opj = os.path.join
238/4:
diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
diag['RESULT_YEAR'] = diag.VISIT_DATE.dt.year
diag['RESULT_MONTH'] = diag.VISIT_DATE.dt.month
surgery = diag.copy()

diag = diag[diag.PROC_NAME.str.contains('OSCOPY')]
diag.head()
238/5:
diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
surgery = diag.copy()

diag = diag[diag.PROC_NAME.str.contains('OSCOPY')]
diag.sample(30)
238/6:
diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
surgery = diag.copy()

diag = diag[diag.PROC_NAME.str.contains('OSCOPY') & diag.TYPE_OF_SERVICE.str.contains('SURG')]
diag.sample(30)
238/7:
diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
surgery = diag.copy()

diag = diag[diag.PROC_NAME.str.contains('OSCOPY') & diag.TYPE_OF_SERVICE.str.contains('SURG') & (diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY')]
diag.sample(30)
238/8:
# diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
# diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
# diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
# surgery = diag.copy()

# diag = diag[diag.PROC_NAME.str.contains('OSCOPY') & diag.TYPE_OF_SERVICE.str.contains('SURG') & (diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY')]
diag.PROC_NAME.value_counts()
238/9:
# diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
# diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
# diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
# surgery = diag.copy()

# diag = diag[diag.PROC_NAME.str.contains('OSCOPY') & diag.TYPE_OF_SERVICE.str.contains('SURG') & (diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY')]
diag.PROC_NAME.str.split(',').value_counts()
238/10:
# diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
# diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
# diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
# surgery = diag.copy()

# diag = diag[diag.PROC_NAME.str.contains('OSCOPY') & diag.TYPE_OF_SERVICE.str.contains('SURG') & (diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY')]
diag.PROC_NAME.str.split(',', n=1, expand=True).value_counts()
238/11:
# diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
# diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
# diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
# surgery = diag.copy()

# diag = diag[diag.PROC_NAME.str.contains('OSCOPY') & diag.TYPE_OF_SERVICE.str.contains('SURG') & (diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY')]
diag.PROC_NAME.str.split(',', n=1, expand=True)#.value_counts()
238/12:
# diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
# diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
# diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
# surgery = diag.copy()

# diag = diag[diag.PROC_NAME.str.contains('OSCOPY') & diag.TYPE_OF_SERVICE.str.contains('SURG') & (diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY')]
diag.PROC_NAME.str.split(',', n=1, expand=True)[0].value_counts()
238/13: diag.columns
238/14:
diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
surgery = diag.copy()

def onehot_flag(col_name, pat):
    diag['DIAG_'+col_name] = 0
    diag.loc[diag.PROC_NAME.str.contains(pat), col_name] += 1 

col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'), ('ERCP', 'ERCP'), ('EGD', 'ESOPHOGASTRODUODENOSCOPY'), ('EGD', 'UPPER GI ENDO'), ('EGD','UPPER EUS'), ('ANO','ANOSCOPY'), ('EGD', 'ESOPHAGOSCOPY')]

for c,p in col_pat:
    onehot_flag(c, p)

diag[[x for x in diag.columns if x[:4]=='DIAG']]
238/15:
diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
surgery = diag.copy()

def onehot_flag(col_name, pat):
    diag['DIAG_'+col_name] = 0
    diag.loc[diag.PROC_NAME.str.contains(pat), 'DIAG_'+col_name] += 1 

col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'), ('ERCP', 'ERCP'), ('EGD', 'ESOPHOGASTRODUODENOSCOPY'), ('EGD', 'UPPER GI ENDO'), ('EGD','UPPER EUS'), ('ANO','ANOSCOPY'), ('EGD', 'ESOPHAGOSCOPY')]

for c,p in col_pat:
    onehot_flag(c, p)

diag[[x for x in diag.columns if x[:4]=='DIAG']]
238/16:
# diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
# diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
# diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
# surgery = diag.copy()

# def onehot_flag(col_name, pat):
#     diag['DIAG_'+col_name] = 0
#     diag.loc[diag.PROC_NAME.str.contains(pat), 'DIAG_'+col_name] += 1 

# col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'), ('ERCP', 'ERCP'), ('EGD', 'ESOPHOGASTRODUODENOSCOPY'), ('EGD', 'UPPER GI ENDO'), ('EGD','UPPER EUS'), ('ANO','ANOSCOPY'), ('EGD', 'ESOPHAGOSCOPY')]

# for c,p in col_pat:
#     onehot_flag(c, p)

diag[IDENTIFIER_COLS+[x for x in diag.columns if x[:4]=='DIAG']]
238/17:
# diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
# diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
# diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
# surgery = diag.copy()

# def onehot_flag(col_name, pat):
#     diag['DIAG_'+col_name] = 0
#     diag.loc[diag.PROC_NAME.str.contains(pat), 'DIAG_'+col_name] += 1 

# col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'), ('ERCP', 'ERCP'), ('EGD', 'ESOPHOGASTRODUODENOSCOPY'), ('EGD', 'UPPER GI ENDO'), ('EGD','UPPER EUS'), ('ANO','ANOSCOPY'), ('EGD', 'ESOPHAGOSCOPY')]

# for c,p in col_pat:
#     onehot_flag(c, p)

diag[IDENTIFIER_COLS+[x for x in diag.columns if x[:4]=='DIAG']].sum()
246/1:
%matplotlib inline
import os
import pandas as pd
import numpy as np
249/1:
%matplotlib inline
import os
import pandas as pd
import numpy as np
249/2:
IDENTIFIER_COLS = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']

def identifier_prefix(cols):
    identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
    return identifier_cols+[x for x in cols if x not in identifier_cols]

opj = os.path.join
249/3:
diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
surgery = diag.copy()

def onehot_flag(col_name, pat):
    diag['DIAG_'+col_name] = 0
    diag.loc[diag.PROC_NAME.str.contains(pat), 'DIAG_'+col_name] += 1 

col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'), ('ERCP', 'ERCP'), ('EGD', 'ESOPHOGASTRODUODENOSCOPY'), ('EGD', 'UPPER GI ENDO'), ('EGD','UPPER EUS'), ('ANO','ANOSCOPY'), ('EGD', 'ESOPHAGOSCOPY')]

for c,p in col_pat:
    onehot_flag(c, p)

diag[[x for x in diag.columns if x[:4]=='DIAG']].sum()
249/4:
# diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
# diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
# diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
# surgery = diag.copy()

# def onehot_flag(col_name, pat):
#     diag['DIAG_'+col_name] = 0
#     diag.loc[diag.PROC_NAME.str.contains(pat), 'DIAG_'+col_name] += 1 

# col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'), ('ERCP', 'ERCP'), ('EGD', 'ESOPHOGASTRODUODENOSCOPY'), ('EGD', 'UPPER GI ENDO'), ('EGD','UPPER EUS'), ('ANO','ANOSCOPY'), ('EGD', 'ESOPHAGOSCOPY')]

# for c,p in col_pat:
#     onehot_flag(c, p)

diag[IDENTIFIER_COLS+[x for x in diag.columns if x[:4]=='DIAG']].to_csv('datasets/long_in/gi_diagnostics.csv', index=False)
250/1:
%matplotlib inline
import os
import pandas as pd
import numpy as np
250/2:
IDENTIFIER_COLS = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']

def identifier_prefix(cols):
    identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
    return identifier_cols+[x for x in cols if x not in identifier_cols]

opj = os.path.join
250/3:
diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
surgery = diag.copy()

def onehot_flag(col_name, pat):
    diag['DIAG_'+col_name] = 0
    diag.loc[diag.PROC_NAME.str.contains(pat) & diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY', 'DIAG_'+col_name] += 1 

col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'), ('ERCP', 'ERCP'), ('EGD', 'ESOPHOGASTRODUODENOSCOPY'), ('EGD', 'UPPER GI ENDO'), ('EGD','UPPER EUS'), ('ANO','ANOSCOPY'), ('EGD', 'ESOPHAGOSCOPY')]

for c,p in col_pat:
    onehot_flag(c, p)

diag[IDENTIFIER_COLS+[x for x in diag.columns if x[:4]=='DIAG']].to_csv('datasets/long_in/gi_diagnostics.csv', index=False)
250/4:
# diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
# diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
# diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
# surgery = diag.copy()

# def onehot_flag(col_name, pat):
#     diag['DIAG_'+col_name] = 0
#     diag.loc[diag.PROC_NAME.str.contains(pat) & diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY', 'DIAG_'+col_name] += 1 

# col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'), ('ERCP', 'ERCP'), ('EGD', 'ESOPHOGASTRODUODENOSCOPY'), ('EGD', 'UPPER GI ENDO'), ('EGD','UPPER EUS'), ('ANO','ANOSCOPY'), ('EGD', 'ESOPHAGOSCOPY')]

# for c,p in col_pat:
#     onehot_flag(c, p)

diag[IDENTIFIER_COLS+[x for x in diag.columns if x[:4]=='DIAG']].to_csv('datasets/long_in/gi_diagnostics2.csv', index=False)
250/5: diag[diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY'].shape
250/6:

for c,p in col_pat:
    onehot_flag(c, p)
diag[[x for x in diag.columns if x[:4]=='DIAG']].sum()
250/7: diag[diag.PROC_NAME.str.contains('COLONOSCOPY')]['PROC_GROUP_NAME']
250/8: diag[diag.PROC_NAME.str.contains('COLONOSCOPY') & diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY'].shape#['PROC_GROUP_NAME']
250/9: diag[diag.PROC_NAME.str.contains('COLONOSCOPY') & (diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY')].shape#['PROC_GROUP_NAME']
250/10:
def onehot_flag(col_name, pat):
    diag['DIAG_'+col_name] = 0
    diag.loc[diag.PROC_NAME.str.contains(pat) & (diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY'), 'DIAG_'+col_name] += 1 

diag[[x for x in diag.columns if x[:4]=='DIAG']].sum()
250/11:
def onehot_flag(col_name, pat):
    diag['DIAG_'+col_name] = 0
    diag.loc[diag.PROC_NAME.str.contains(pat) & (diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY'), 'DIAG_'+col_name] = 1 

diag[[x for x in diag.columns if x[:4]=='DIAG']].sum()
250/12:
def onehot_flag(col_name, pat):
    diag['DIAG_'+col_name] = 0
    diag.loc[(diag.PROC_NAME.str.contains(pat) & (diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY')), 'DIAG_'+col_name] += 1 

diag[[x for x in diag.columns if x[:4]=='DIAG']].sum()
250/13:
def onehot_flag(col_name, pat):
    diag['DIAG_'+col_name] = 0
    diag.loc[(diag.PROC_NAME.str.contains(pat) & (diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY')), 'DIAG_'+col_name] = 1 

diag[[x for x in diag.columns if x[:4]=='DIAG']].sum()
250/14:
def onehot_flag(col_name, pat):
    diag['DIAG_'+col_name] = 0
    diag.loc[diag.PROC_NAME.str.contains(pat) & (diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY'), 'DIAG_'+col_name] += 1 

for c,p in col_pat:
    onehot_flag(c, p)

diag[[x for x in diag.columns if x[:4]=='DIAG']].sum()
250/15: diag[IDENTIFIER_COLS+[x for x in diag.columns if x[:4]=='DIAG']].to_csv('datasets/long_in/gi_diagnostics2.csv', index=False)
250/16:
diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
surgery = diag.copy()

def onehot_flag(col_name, pat):
    diag['DIAG_'+col_name] = 0
    diag.loc[diag.PROC_NAME.str.contains(pat) & (diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY'), 'DIAG_'+col_name] += 1 

col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'), ('ERCP', 'ERCP'), ('EGD', 'ESOPHOGASTRODUODENOSCOPY'), ('EGD', 'UPPER GI ENDO'), ('EGD','UPPER EUS'), ('ANO','ANOSCOPY'), ('EGD', 'ESOPHAGOSCOPY')]

for c,p in col_pat:
    onehot_flag(c, p)

diag[IDENTIFIER_COLS+[x for x in diag.columns if x[:4]=='DIAG']].sort_values(IDENTIFIER_COLS).drop_duplicates().to_csv('datasets/long_in/gi_diagnostics2.csv', index=False)
250/17:
surg = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
surg['RESULT_YEAR'] = surg.ORIG_SERVICE_DATE.dt.year
surg['RESULT_MONTH'] = surg.ORIG_SERVICE_DATE.dt.month

surg = surg[surg.TYPE_OF_SERVICE=='20-SURGERY MA' & surg.PROC_GROUP_NAME=='GASTROINTESTINAL/DIGESTIVE']
250/18:
surg = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
surg['RESULT_YEAR'] = surg.ORIG_SERVICE_DATE.dt.year
surg['RESULT_MONTH'] = surg.ORIG_SERVICE_DATE.dt.month

surg = surg[(surg.TYPE_OF_SERVICE=='20-SURGERY MA') & (surg.PROC_GROUP_NAME=='GASTROINTESTINAL/DIGESTIVE')]
250/19: surg.sample(50)
250/20: surg.head(50)
250/21: surg.shape
250/22: surg.sort_values(['AUTO_ID', 'ORIG_SERVICE_DATE']).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE']).shape
250/23: surg = surg.sort_values(['AUTO_ID', 'ORIG_SERVICE_DATE']).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE']).shape
250/24: surg.head()
250/25:
surg = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
surg['RESULT_YEAR'] = surg.ORIG_SERVICE_DATE.dt.year
surg['RESULT_MONTH'] = surg.ORIG_SERVICE_DATE.dt.month

surg = surg[(surg.TYPE_OF_SERVICE=='20-SURGERY MA') & (surg.PROC_GROUP_NAME=='GASTROINTESTINAL/DIGESTIVE')].sort_values(['AUTO_ID', 'ORIG_SERVICE_DATE']).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE'])
250/26: surg.head()
250/27:
surg = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
surg['RESULT_YEAR'] = surg.ORIG_SERVICE_DATE.dt.year
surg['RESULT_MONTH'] = surg.ORIG_SERVICE_DATE.dt.month

surg = surg[(surg.TYPE_OF_SERVICE=='20-SURGERY MA') & (surg.PROC_GROUP_NAME=='GASTROINTESTINAL/DIGESTIVE')].sort_values(['AUTO_ID', 'ORIG_SERVICE_DATE']).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE'])

surg['ENC_SURGERY'] = 1
surg[IDEN]
250/28: surg[IDENTIFIER_COLS+['ENC_SURGERY']]
250/29:
diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
diag.drop(diag.index[diag.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)


def onehot_flag(col_name, pat):
    diag['DIAG_'+col_name] = 0
    diag.loc[diag.PROC_NAME.str.contains(pat) & (diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY'), 'DIAG_'+col_name] += 1 

col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'), ('ERCP', 'ERCP'), ('EGD', 'ESOPHOGASTRODUODENOSCOPY'), ('EGD', 'UPPER GI ENDO'), ('EGD','UPPER EUS'), ('ANO','ANOSCOPY'), ('EGD', 'ESOPHAGOSCOPY')]

for c,p in col_pat:
    onehot_flag(c, p)

diag[IDENTIFIER_COLS+[x for x in diag.columns if x[:4]=='DIAG']].sort_values(IDENTIFIER_COLS).drop_duplicates().to_csv('datasets/long_in/gi_diagnostics2.csv', index=False)
250/30:
surg = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
surg['RESULT_YEAR'] = surg.ORIG_SERVICE_DATE.dt.year
surg['RESULT_MONTH'] = surg.ORIG_SERVICE_DATE.dt.month
surg.drop(surg.index[surg.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)

surg = surg[(surg.TYPE_OF_SERVICE=='20-SURGERY MA') & (surg.PROC_GROUP_NAME=='GASTROINTESTINAL/DIGESTIVE')].sort_values(['AUTO_ID', 'ORIG_SERVICE_DATE']).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE'])

surg['ENC_SURGERY'] = 1
surg[IDENTIFIER_COLS+['ENC_SURGERY']].to_csv('datasets/long_in/surgeries.csv')
250/31:
diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
diag.drop(diag.index[diag.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)


def onehot_flag(col_name, pat):
    diag['DIAG_'+col_name] = 0
    diag.loc[diag.PROC_NAME.str.contains(pat) & (diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY'), 'DIAG_'+col_name] += 1 

col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'), ('ERCP', 'ERCP'), ('EGD', 'ESOPHOGASTRODUODENOSCOPY'), ('EGD', 'UPPER GI ENDO'), ('EGD','UPPER EUS'), ('ANO','ANOSCOPY'), ('EGD', 'ESOPHAGOSCOPY')]

for c,p in col_pat:
    onehot_flag(c, p)

diag[IDENTIFIER_COLS+[x for x in diag.columns if x[:4]=='DIAG']].sort_values(IDENTIFIER_COLS).drop_duplicates().to_csv('datasets/long_in/gi_diagnostics2.csv', index=False)
250/32:
diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
diag.drop(diag.index[diag.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
diag.AUTO_ID = diag.AUTO_ID.astype(int)

def onehot_flag(col_name, pat):
    diag['DIAG_'+col_name] = 0
    diag.loc[diag.PROC_NAME.str.contains(pat) & (diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY'), 'DIAG_'+col_name] += 1 

col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'), ('ERCP', 'ERCP'), ('EGD', 'ESOPHOGASTRODUODENOSCOPY'), ('EGD', 'UPPER GI ENDO'), ('EGD','UPPER EUS'), ('ANO','ANOSCOPY'), ('EGD', 'ESOPHAGOSCOPY')]

for c,p in col_pat:
    onehot_flag(c, p)

diag[IDENTIFIER_COLS+[x for x in diag.columns if x[:4]=='DIAG']].sort_values(IDENTIFIER_COLS).drop_duplicates().to_csv('datasets/long_in/gi_diagnostics2.csv', index=False)
251/1:
%matplotlib inline
import os
import pandas as pd
import numpy as np
251/2:
IDENTIFIER_COLS = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']

def identifier_prefix(cols):
    identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
    return identifier_cols+[x for x in cols if x not in identifier_cols]

opj = os.path.join
251/3:
diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
diag.drop(diag.index[diag.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
diag.AUTO_ID = diag.AUTO_ID.astype(int)

def onehot_flag(col_name, pat):
    diag['DIAG_'+col_name] = 0
    diag.loc[diag.PROC_NAME.str.contains(pat) & (diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY'), 'DIAG_'+col_name] += 1 

col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'), ('ERCP', 'ERCP'), ('EGD', 'ESOPHOGASTRODUODENOSCOPY'), ('EGD', 'UPPER GI ENDO'), ('EGD','UPPER EUS'), ('ANO','ANOSCOPY'), ('EGD', 'ESOPHAGOSCOPY')]

for c,p in col_pat:
    onehot_flag(c, p)

diag[IDENTIFIER_COLS+[x for x in diag.columns if x[:4]=='DIAG']].sort_values(IDENTIFIER_COLS).drop_duplicates().to_csv('datasets/long_in/gi_diagnostics2.csv', index=False)
251/4:

diag[IDENTIFIER_COLS+[x for x in diag.columns if x[:4]=='DIAG']].sort_values(IDENTIFIER_COLS).drop_duplicates().to_csv('datasets/long_in/gi_diagnostics2.csv', index=False)
251/5:
surg = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
surg['RESULT_YEAR'] = surg.ORIG_SERVICE_DATE.dt.year
surg['RESULT_MONTH'] = surg.ORIG_SERVICE_DATE.dt.month
surg.drop(surg.index[surg.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
surg.AUTO_ID = surg.AUTO_ID.astype(int)

surg = surg[(surg.TYPE_OF_SERVICE=='20-SURGERY MA') & (surg.PROC_GROUP_NAME=='GASTROINTESTINAL/DIGESTIVE')].sort_values(['AUTO_ID', 'ORIG_SERVICE_DATE']).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE'])

surg['ENC_SURGERY'] = 1
surg[IDENTIFIER_COLS+['ENC_SURGERY']].to_csv('datasets/long_in/surgeries.csv')
251/6: surg[IDENTIFIER_COLS+['ENC_SURGERY']]
252/1:
from functools import reduce


def active_intervals(o,e):    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
    active_during = list(zip(start_dates, end_dates))
    return active_during


def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        if df.empty:
            intervals.append(None)
        else:
            intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    # Get activity one-hot
    drug_columns=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        drug_col = []
        
        if not intvl:            
            drug_col = pd.Series(name=dr)
        else:
            for s,e in intvl:
                drug_col.append(pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1))
            drug_col = pd.concat(drug_col)
        drug_columns.append(drug_col)
        
    return pd.concat(drug_columns, axis=1)
252/2:
from functools import reduce


def active_intervals(o,e):    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
    active_during = list(zip(start_dates, end_dates))
    return active_during


def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN', 'Vitamin_D']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        if df.empty:
            intervals.append(None)
        else:
            intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    # Get activity one-hot
    drug_columns=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        drug_col = []
        
        if not intvl:            
            drug_col = pd.Series(name=dr)
        else:
            for s,e in intvl:
                drug_col.append(pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1))
            drug_col = pd.concat(drug_col)
        drug_columns.append(drug_col)
        
    return pd.concat(drug_columns, axis=1)
252/3:
psych_med_prescrip = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Medications_2018-07-05-09-47-15.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.THERAPEUTIC_CLASS=='PSYCHOTHERAPEUTIC DRUGS']
psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.END_DATE.notnull() & psych_med_prescrip.ORDERING_DATE.notnull()] # filter no end_date rows
psych_med_prescrip['GROUP'] = 'Psych'
psych_med_prescrip = psych_med_prescrip[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
psych_med_prescrip.to_csv(os.path.join(cfg.OUT_DATA_PATH,'psych_rx.csv'))

med_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,'filtered_meds2018.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
med_df = med_df[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
med_df = med_df[med_df.END_DATE.notnull() & med_df.ORDERING_DATE.notnull()] # filter no end_date rows
med_df = pd.concat([med_df, psych_med_prescrip])


# drop redundant rows like (Apr-Dec, Apr-Nov) and (Jan-Sept, Mar-Sept) ==> second row is redundant in both tuples 
med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])
med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'END_DATE', 'GROUP'])
med_df.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))

med_df2 = med_df.groupby('AUTO_ID').apply(active_prescriptions).reset_index()
med_df2['RESULT_YEAR'] = med_df2.level_1.dt.year
med_df2['RESULT_MONTH'] = med_df2.level_1.dt.month
med_df2 = med_df2[identifier_cols+list(med_df.GROUP.unique())]
med_df2.to_csv(os.path.join(cfg.LONG_IN, 'rx_long.csv'), index=False)
252/4:
%matplotlib inline
import os
import pandas as pd
import numpy as np
252/5:
IDENTIFIER_COLS = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']

def identifier_prefix(cols):
    identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
    return identifier_cols+[x for x in cols if x not in identifier_cols]

opj = os.path.join
252/6:
mrn_auto_map = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'MRN_AUTO_mapping.csv'))
mrn_auto_map['PAT_MRN_ID'] = mrn_auto_map['PAT_MRN_ID'].astype(int)

exclude_patients = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'cancer_transplants.csv'))
exclude_patients['PAT_MRN_ID'] = exclude_patients['PAT_MRN_ID'].astype(int)
EXCLUDE = exclude_patients.merge(mrn_auto_map, how='left', on='PAT_MRN_ID')
EXCLUDE.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'exclude_patients.csv'))
252/7:
from functools import reduce


def active_intervals(o,e):    
    start_dates = [o.iloc[0]]
    end_dates=[]
    start_loc, end_loc = 0, 0
    
    for end_tm1, start_t0 in zip(e[:-1],o[1:]):
        if end_tm1<start_t0:
            end_dates.append(end_tm1)
            start_loc=end_loc+1
            start_dates.append(start_t0)
        end_loc+=1
    end_dates.append(e.iloc[-1])
    
    start_dates = list(map(pd.Timestamp, start_dates))
    end_dates = list(map(pd.Timestamp, end_dates))    
    active_during = list(zip(start_dates, end_dates))
    return active_during


def active_prescriptions(med_a):
    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\
       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN', 'Vitamin_D']
    
    # Get contiguous use info
    intervals = []
    for dr in drugs:
        df = med_a[med_a.GROUP==dr]
        if df.empty:
            intervals.append(None)
        else:
            intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))
    # Get activity one-hot
    drug_columns=[]
    for ix,dr in enumerate(drugs):
        intvl = intervals[ix]
        drug_col = []
        
        if not intvl:            
            drug_col = pd.Series(name=dr)
        else:
            for s,e in intvl:
                drug_col.append(pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1))
            drug_col = pd.concat(drug_col)
        drug_columns.append(drug_col)
        
    return pd.concat(drug_columns, axis=1)
252/8:
psych_med_prescrip = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Medications_2018-07-05-09-47-15.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.THERAPEUTIC_CLASS=='PSYCHOTHERAPEUTIC DRUGS']
psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.END_DATE.notnull() & psych_med_prescrip.ORDERING_DATE.notnull()] # filter no end_date rows
psych_med_prescrip['GROUP'] = 'Psych'
psych_med_prescrip = psych_med_prescrip[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
psych_med_prescrip.to_csv(os.path.join(cfg.OUT_DATA_PATH,'psych_rx.csv'))

med_df = pd.read_csv(os.path.join(cfg.IN_DATA_PATH,'filtered_meds2018.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
med_df = med_df[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
med_df = med_df[med_df.END_DATE.notnull() & med_df.ORDERING_DATE.notnull()] # filter no end_date rows
med_df = pd.concat([med_df, psych_med_prescrip])


# drop redundant rows like (Apr-Dec, Apr-Nov) and (Jan-Sept, Mar-Sept) ==> second row is redundant in both tuples 
med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])
med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'END_DATE', 'GROUP'])
med_df.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))

med_df2 = med_df.groupby('AUTO_ID').apply(active_prescriptions).reset_index()
med_df2['RESULT_YEAR'] = med_df2.level_1.dt.year
med_df2['RESULT_MONTH'] = med_df2.level_1.dt.month
med_df2 = med_df2[identifier_cols+list(med_df.GROUP.unique())]
med_df2.to_csv(os.path.join(cfg.LONG_IN, 'rx_long.csv'), index=False)
252/9:
psych_med_prescrip = pd.read_csv(os.path.join('datasets/raw_registry_data/deid_IBD_Registry_BA1951_Medications_2018-07-05-09-47-15.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.THERAPEUTIC_CLASS=='PSYCHOTHERAPEUTIC DRUGS']
psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.END_DATE.notnull() & psych_med_prescrip.ORDERING_DATE.notnull()] # filter no end_date rows
psych_med_prescrip['GROUP'] = 'Psych'
psych_med_prescrip = psych_med_prescrip[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
psych_med_prescrip.to_csv(os.path.join(cfg.OUT_DATA_PATH,'psych_rx.csv'))

med_df = pd.read_csv('datasets/raw_registry_data/filtered_meds2018.csv'), parse_dates=['ORDERING_DATE', 'END_DATE'])
med_df = med_df[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
med_df = med_df[med_df.END_DATE.notnull() & med_df.ORDERING_DATE.notnull()] # filter no end_date rows
med_df = pd.concat([med_df, psych_med_prescrip])


# drop redundant rows like (Apr-Dec, Apr-Nov) and (Jan-Sept, Mar-Sept) ==> second row is redundant in both tuples 
med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])
med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'END_DATE', 'GROUP'])
med_df.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))

med_df2 = med_df.groupby('AUTO_ID').apply(active_prescriptions).reset_index()
med_df2['RESULT_YEAR'] = med_df2.level_1.dt.year
med_df2['RESULT_MONTH'] = med_df2.level_1.dt.month
med_df2 = med_df2[identifier_cols+list(med_df.GROUP.unique())]
med_df2.to_csv(os.path.join(cfg.LONG_IN, 'rx_long.csv'), index=False)
252/10:
psych_med_prescrip = pd.read_csv(os.path.join('datasets/raw_registry_data/deid_IBD_Registry_BA1951_Medications_2018-07-05-09-47-15.csv', parse_dates=['ORDERING_DATE', 'END_DATE'])
psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.THERAPEUTIC_CLASS=='PSYCHOTHERAPEUTIC DRUGS']
psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.END_DATE.notnull() & psych_med_prescrip.ORDERING_DATE.notnull()] # filter no end_date rows
psych_med_prescrip['GROUP'] = 'Psych'
psych_med_prescrip = psych_med_prescrip[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
psych_med_prescrip.to_csv(os.path.join(cfg.OUT_DATA_PATH,'psych_rx.csv'))

med_df = pd.read_csv('datasets/raw_registry_data/filtered_meds2018.csv', parse_dates=['ORDERING_DATE', 'END_DATE'])
med_df = med_df[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
med_df = med_df[med_df.END_DATE.notnull() & med_df.ORDERING_DATE.notnull()] # filter no end_date rows
med_df = pd.concat([med_df, psych_med_prescrip])


# drop redundant rows like (Apr-Dec, Apr-Nov) and (Jan-Sept, Mar-Sept) ==> second row is redundant in both tuples 
med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])
med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'END_DATE', 'GROUP'])
med_df.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))

med_df2 = med_df.groupby('AUTO_ID').apply(active_prescriptions).reset_index()
med_df2['RESULT_YEAR'] = med_df2.level_1.dt.year
med_df2['RESULT_MONTH'] = med_df2.level_1.dt.month
med_df2 = med_df2[identifier_cols+list(med_df.GROUP.unique())]
med_df2.to_csv(os.path.join(cfg.LONG_IN, 'rx_long.csv'), index=False)
252/11:
psych_med_prescrip = pd.read_csv('datasets/raw_registry_data/deid_IBD_Registry_BA1951_Medications_2018-07-05-09-47-15.csv', parse_dates=['ORDERING_DATE', 'END_DATE'])

psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.THERAPEUTIC_CLASS=='PSYCHOTHERAPEUTIC DRUGS']
psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.END_DATE.notnull() & psych_med_prescrip.ORDERING_DATE.notnull()] # filter no end_date rows
psych_med_prescrip['GROUP'] = 'Psych'
psych_med_prescrip = psych_med_prescrip[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
psych_med_prescrip.to_csv(os.path.join(cfg.OUT_DATA_PATH,'psych_rx.csv'))

med_df = pd.read_csv('datasets/raw_registry_data/filtered_meds2018.csv', parse_dates=['ORDERING_DATE', 'END_DATE'])
med_df = med_df[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
med_df = med_df[med_df.END_DATE.notnull() & med_df.ORDERING_DATE.notnull()] # filter no end_date rows
med_df = pd.concat([med_df, psych_med_prescrip])


# drop redundant rows like (Apr-Dec, Apr-Nov) and (Jan-Sept, Mar-Sept) ==> second row is redundant in both tuples 
med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])
med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'END_DATE', 'GROUP'])
med_df.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))

med_df2 = med_df.groupby('AUTO_ID').apply(active_prescriptions).reset_index()
med_df2['RESULT_YEAR'] = med_df2.level_1.dt.year
med_df2['RESULT_MONTH'] = med_df2.level_1.dt.month
med_df2 = med_df2[identifier_cols+list(med_df.GROUP.unique())]
med_df2.to_csv(os.path.join(cfg.LONG_IN, 'rx_long.csv'), index=False)
252/12:
psych_med_prescrip = pd.read_csv('datasets/raw_registry_data/deid_IBD_Registry_BA1951_Medications_2018-07-05-09-47-15.csv', parse_dates=['ORDERING_DATE', 'END_DATE'])

psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.THERAPEUTIC_CLASS=='PSYCHOTHERAPEUTIC DRUGS']
psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.END_DATE.notnull() & psych_med_prescrip.ORDERING_DATE.notnull()] # filter no end_date rows
psych_med_prescrip['GROUP'] = 'Psych'
psych_med_prescrip = psych_med_prescrip[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
# psych_med_prescrip.to_csv(os.path.join(cfg.OUT_DATA_PATH,'psych_rx.csv'))

med_df = pd.read_csv('datasets/raw_registry_data/filtered_meds2018.csv', parse_dates=['ORDERING_DATE', 'END_DATE'])
med_df = med_df[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]
med_df = med_df[med_df.END_DATE.notnull() & med_df.ORDERING_DATE.notnull()] # filter no end_date rows
med_df = pd.concat([med_df, psych_med_prescrip])


# drop redundant rows like (Apr-Dec, Apr-Nov) and (Jan-Sept, Mar-Sept) ==> second row is redundant in both tuples 
med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])
med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'END_DATE', 'GROUP'])
# med_df.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))

med_df2 = med_df.groupby('AUTO_ID').apply(active_prescriptions).reset_index()
med_df2['RESULT_YEAR'] = med_df2.level_1.dt.year
med_df2['RESULT_MONTH'] = med_df2.level_1.dt.month
med_df2 = med_df2[identifier_cols+list(med_df.GROUP.unique())]
med_df2.to_csv(os.path.join(cfg.LONG_IN, 'rx_long.csv'), index=False)
252/13:
med_df2.head()
# med_df2 = med_df2[IDENTIFIER_COLS+list(med_df.GROUP.unique())]
# med_df2.to_csv('datasets/long_in/rx_long.csv'), index=False)
252/14:
med_df2 = med_df2[IDENTIFIER_COLS+list(med_df.GROUP.unique())]
med_df2.to_csv('datasets/long_in/rx_long.csv'), index=False)
252/15:
med_df2 = med_df2[IDENTIFIER_COLS+list(med_df.GROUP.unique())]
med_df2.to_csv('datasets/long_in/rx_long.csv', index=False)
244/1:
import pandas as pd
import os
from functools import reduce
244/2: IDENTIFIER_COLS = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
244/3:
charges = pd.read_csv('datasets/long_in/charges.csv')
encounters = pd.read_csv('datasets/long_in/encounters.csv')
diag = pd.read_csv('datasets/long_in/gi_diagnostics.csv')
# diag.columns = [f'DIAG_{x}' if x not in IDENTIFIER_COLS else x for x in diag.columns ]
labs = pd.read_csv('datasets/long_in/labs.csv')
labs.columns = [f'LAB_{x}' if x not in IDENTIFIER_COLS else x for x in labs.columns]
meds = pd.read_csv('datasets/long_in/rx_long.csv')
meds.columns = [f'MED_{x}' if x not in IDENTIFIER_COLS else x for x in meds.columns]
hbi = pd.read_csv('datasets/long_in/hbi.csv')
hbi.columns = [f'HBI_{x}' if x not in IDENTIFIER_COLS else x for x in hbi.columns]
surg = pd.read_csv('datasets/long_in/surgeries.csv')
dis = pd.read_csv('datasets/long_in/disease_type.csv')
244/4:
final = reduce(lambda l,r: l.merge(r, how='outer', on=IDENTIFIER_COLS), [charges, encounters, diag, labs, meds, hbi, surg])
final = final.merge(dis, how='left', on='AUTO_ID')
244/5: final.head()
245/1:
import pandas as pd
import os
from functools import reduce
245/2: IDENTIFIER_COLS = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
245/3:
charges = pd.read_csv('datasets/long_in/charges.csv')
encounters = pd.read_csv('datasets/long_in/encounters.csv')
diag = pd.read_csv('datasets/long_in/gi_diagnostics.csv')
# diag.columns = [f'DIAG_{x}' if x not in IDENTIFIER_COLS else x for x in diag.columns ]
labs = pd.read_csv('datasets/long_in/labs.csv')
labs.columns = [f'LAB_{x}' if x not in IDENTIFIER_COLS else x for x in labs.columns]
meds = pd.read_csv('datasets/long_in/rx_long.csv')
meds.columns = [f'MED_{x}' if x not in IDENTIFIER_COLS else x for x in meds.columns]
hbi = pd.read_csv('datasets/long_in/hbi.csv')
hbi.columns = [f'HBI_{x}' if x not in IDENTIFIER_COLS else x for x in hbi.columns]
surg = pd.read_csv('datasets/long_in/surgeries.csv')
dis = pd.read_csv('datasets/long_in/disease_type.csv')
245/4:
final = reduce(lambda l,r: l.merge(r, how='outer', on=IDENTIFIER_COLS), [charges, encounters, diag, labs, meds, hbi, surg])
final = final.merge(dis, how='left', on='AUTO_ID')
245/5: final.head()
245/6:
exclude_patients = pd.read_csv('datasets/clean_registry_data/exclude_patients.csv')
ct_aid = exclude_patients.AUTO_ID
CT_filtered = final[~final.AUTO_ID.isin(ct_aid)]
245/7:
final.to_csv('datasets/long_out/all.csv', index=False)
CT_filtered.to_csv('datasets/long_out/noCT.csv', index=False)
245/8: list(final.columns)
247/1:
import utils
import data_utils as du
import os
import pandas as pd
import pickle
import seaborn as sns
from matplotlib import pyplot as plt
import pandas as pd
247/2: inputs = du.cleaned_longitudinal_inputs(load_pkl=False)
232/45: df = pd.read_csv('noCT.csv')
232/46: df.columns
232/47:
cols = ['LAB_albumin_Low', 'LAB_crp_High', 'LAB_eos_High', 'LAB_esr_High', \
            'LAB_hemoglobin_Low', 'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_vitamin_d_Low']cols = ['LAB_albumin_Low', 'LAB_crp_High', 'LAB_eos_High', 'LAB_esr_High', \
            'LAB_hemoglobin_Low', 'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_vitamin_d_Low']
232/48:
cols = ['LAB_albumin_Low', 'LAB_crp_High', 'LAB_eos_High', 'LAB_esr_High', \
            'LAB_hemoglobin_Low', 'LAB_monocytes_High', 'LAB_monocytes_Low', 'LAB_vitamin_d_Low']
232/49: df[cols].head()
232/50: df[cols].sample(10)
232/51: df[cols].sample(10).fillna(-1).values
232/52: df[cols].sample(10).fillna(-1).values
232/53: df[cols].sample(10).fillna(-1).values
232/54:
x=np.array([[-1., -1., -1., -1., -1., -1., -1., -1.],
       [-1., -1., -1., -1., -1., -1., -1., -1.],
       [ 1.,  0.,  0.,  0.,  1.,  0.,  1.,  0.],
       [-1., -1., -1., -1., -1., -1., -1., -1.],
       [-1., -1., -1., -1., -1., -1., -1., -1.],
       [-1., -1., -1., -1., -1., -1., -1., -1.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
       [-1., -1., -1., -1., -1., -1., -1., -1.],
       [-1., -1., -1., -1., -1., -1., -1., -1.]])
232/55: import numpy as np
232/56:
x=np.array([[-1., -1., -1., -1., -1., -1., -1., -1.],
       [-1., -1., -1., -1., -1., -1., -1., -1.],
       [ 1.,  0.,  0.,  0.,  1.,  0.,  1.,  0.],
       [-1., -1., -1., -1., -1., -1., -1., -1.],
       [-1., -1., -1., -1., -1., -1., -1., -1.],
       [-1., -1., -1., -1., -1., -1., -1., -1.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
       [-1., -1., -1., -1., -1., -1., -1., -1.],
       [-1., -1., -1., -1., -1., -1., -1., -1.]])
232/57: np.min(1,x)
232/58: np.min(np.ones_like(x),x)
232/59: np.minimum(np.ones_like(x),x)
232/60: x
232/61: x=df[cols].sample(10).fillna(2).values
232/62: x
232/63: np.minimum(np.ones_like(x),x)
247/3:
import utils
import data_utils as du
import os
import pandas as pd
import pickle
import seaborn as sns
from matplotlib import pyplot as plt
import pandas as pd
247/4: inputs = du.cleaned_longitudinal_inputs(load_pkl=False)
232/64:
def apply_monthly_timestamp_(df): 
    df['tmp_ts'] = (df.RESULT_YEAR.astype(str)+df.RESULT_MONTH.astype(str).str.zfill(2)).astype(int)
    t0_indices = df.groupby('AUTO_ID').tmp_ts.idxmin()     # find earliest row
    t0_df = df.loc[t0_indices][['AUTO_ID','RESULT_YEAR','RESULT_MONTH']]
    t0_df.columns = ['AUTO_ID', 'tmp_minY', 'tmp_minMo']
    # add tmp columns to df for vectorized calculation of delta in months
    timestamped_df = df.merge(t0_df, how='left', on='AUTO_ID')
    timestamped_df['MONTH_TS'] = (timestamped_df['RESULT_YEAR']-timestamped_df['tmp_minY'])*12 + timestamped_df['RESULT_MONTH']-timestamped_df['tmp_minMo']
    timestamped_df = timestamped_df[[x for x in timestamped_df.columns if x[:3]!='tmp']]
    return timestamped_df
232/65: df
232/66: xdf = df.head(30000)
232/67: xdf2 = apply_monthly_timestamp_(xdf)
232/68: xdf2.head()
232/69: xdf2.columns
232/70:
def pad(df):
    df = df.reset_index()
    new_index = pd.MultiIndex.from_product([[df.AUTO_ID.values[0]], list(range(0, df.MONTH_TS.max()+1))], names=['AUTO_ID', 'MONTH_TS'])
    df = df.set_index(['AUTO_ID', 'MONTH_TS']).reindex(new_index).reset_index()
    return df
232/71: padded = timestamped_df.groupby('AUTO_ID').apply(pad)
232/72: padded = xdf2.groupby('AUTO_ID').apply(pad)
232/73: grp = xdf2.groupby('AUTO_ID')
232/74:
for a,g in grp:
    break
232/75: g
232/76: g.reset_index()
232/77: grp = xdf2.groupby('AUTO_ID', as_index=False)
232/78:
for a,g in grp:
    break
232/79: g
248/1:
import utils
import data_utils as du
import os
import pandas as pd
import pickle
import seaborn as sns
from matplotlib import pyplot as plt
import pandas as pd
248/2: inputs = du.cleaned_longitudinal_inputs(load_pkl=False)
232/80: xdf.shape
232/81: xdf.drop_duplicates(['AUTO_ID','MONTH_TS']).shape
232/82: xdf2.drop_duplicates(['AUTO_ID','MONTH_TS']).shape
232/83: df.shape
232/84: df.drop_duplicates(['AUTO_ID','RESULT_YEAR', 'RESULT_MONTH']).shape
256/1:
import pandas as pd
import os
from functools import reduce
256/2: IDENTIFIER_COLS = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
256/3:
charges = pd.read_csv('datasets/long_in/charges.csv')
encounters = pd.read_csv('datasets/long_in/encounters.csv')
diag = pd.read_csv('datasets/long_in/gi_diagnostics.csv')
# diag.columns = [f'DIAG_{x}' if x not in IDENTIFIER_COLS else x for x in diag.columns ]
labs = pd.read_csv('datasets/long_in/labs.csv')
labs.columns = [f'LAB_{x}' if x not in IDENTIFIER_COLS else x for x in labs.columns]
meds = pd.read_csv('datasets/long_in/rx_long.csv')
meds.columns = [f'MED_{x}' if x not in IDENTIFIER_COLS else x for x in meds.columns]
hbi = pd.read_csv('datasets/long_in/hbi.csv')
hbi.columns = [f'HBI_{x}' if x not in IDENTIFIER_COLS else x for x in hbi.columns]
surg = pd.read_csv('datasets/long_in/surgeries.csv')
dis = pd.read_csv('datasets/long_in/disease_type.csv')
256/4:
final = reduce(lambda l,r: l.merge(r, how='outer', on=IDENTIFIER_COLS), [charges, encounters, diag, labs, meds, hbi, surg])
# final = final.merge(dis, how='left', on='AUTO_ID')
256/5: final.shape
256/6: final.drop_duplicates([IDENTIFIER_COLS]).shape
256/7: final.drop_duplicates(IDENTIFIER_COLS).shape
256/8:
for df in [charges, encounters, diag, labs, meds, hbi, surg]:
    print(df.shape[0] - df.drop_duplicates(IDENTIFIER_COLS).shape[0])
254/1:
diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
diag.drop(diag.index[diag.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
diag.AUTO_ID = diag.AUTO_ID.astype(int)

def onehot_flag(col_name, pat):
    diag['DIAG_'+col_name] = 0
    diag.loc[diag.PROC_NAME.str.contains(pat) & (diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY'), 'DIAG_'+col_name] += 1 

col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'), ('ERCP', 'ERCP'), ('EGD', 'ESOPHOGASTRODUODENOSCOPY'), ('EGD', 'UPPER GI ENDO'), ('EGD','UPPER EUS'), ('ANO','ANOSCOPY'), ('EGD', 'ESOPHAGOSCOPY')]

for c,p in col_pat:
    onehot_flag(c, p)

diag = diag[IDENTIFIER_COLS+[x for x in diag.columns if x[:4]=='DIAG']]
diag = diag.groupby(IDENTIFIER_COLS, as_index=False).sum()
diag.sort_values(IDENTIFIER_COLS).to_csv('datasets/long_in/gi_diagnostics2.csv', index=False)
254/2:
%matplotlib inline
import os
import pandas as pd
import numpy as np
254/3:
diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
diag.drop(diag.index[diag.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
diag.AUTO_ID = diag.AUTO_ID.astype(int)

def onehot_flag(col_name, pat):
    diag['DIAG_'+col_name] = 0
    diag.loc[diag.PROC_NAME.str.contains(pat) & (diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY'), 'DIAG_'+col_name] += 1 

col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'), ('ERCP', 'ERCP'), ('EGD', 'ESOPHOGASTRODUODENOSCOPY'), ('EGD', 'UPPER GI ENDO'), ('EGD','UPPER EUS'), ('ANO','ANOSCOPY'), ('EGD', 'ESOPHAGOSCOPY')]

for c,p in col_pat:
    onehot_flag(c, p)

diag = diag[IDENTIFIER_COLS+[x for x in diag.columns if x[:4]=='DIAG']]
diag = diag.groupby(IDENTIFIER_COLS, as_index=False).sum()
diag.sort_values(IDENTIFIER_COLS).to_csv('datasets/long_in/gi_diagnostics2.csv', index=False)
254/4:
IDENTIFIER_COLS = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']

def identifier_prefix(cols):
    identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
    return identifier_cols+[x for x in cols if x not in identifier_cols]

opj = os.path.join
254/5:
diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
diag.drop(diag.index[diag.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
diag.AUTO_ID = diag.AUTO_ID.astype(int)

def onehot_flag(col_name, pat):
    diag['DIAG_'+col_name] = 0
    diag.loc[diag.PROC_NAME.str.contains(pat) & (diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY'), 'DIAG_'+col_name] += 1 

col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'), ('ERCP', 'ERCP'), ('EGD', 'ESOPHOGASTRODUODENOSCOPY'), ('EGD', 'UPPER GI ENDO'), ('EGD','UPPER EUS'), ('ANO','ANOSCOPY'), ('EGD', 'ESOPHAGOSCOPY')]

for c,p in col_pat:
    onehot_flag(c, p)

diag = diag[IDENTIFIER_COLS+[x for x in diag.columns if x[:4]=='DIAG']]
diag = diag.groupby(IDENTIFIER_COLS, as_index=False).sum()
diag.sort_values(IDENTIFIER_COLS).to_csv('datasets/long_in/gi_diagnostics2.csv', index=False)
254/6:
surg = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
surg['RESULT_YEAR'] = surg.ORIG_SERVICE_DATE.dt.year
surg['RESULT_MONTH'] = surg.ORIG_SERVICE_DATE.dt.month
surg.drop(surg.index[surg.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
surg.AUTO_ID = surg.AUTO_ID.astype(int)

surg = surg[(surg.TYPE_OF_SERVICE=='20-SURGERY MA') & (surg.PROC_GROUP_NAME=='GASTROINTESTINAL/DIGESTIVE')].sort_values(['AUTO_ID', 'ORIG_SERVICE_DATE']).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE'])

surg['ENC_SURGERY'] = 1
surg[IDENTIFIER_COLS+['ENC_SURGERY']].groupby(IDENTIFIER_COLS, as_index=False).to_csv('datasets/long_in/surgeries.csv')
254/7:
surg = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
surg['RESULT_YEAR'] = surg.ORIG_SERVICE_DATE.dt.year
surg['RESULT_MONTH'] = surg.ORIG_SERVICE_DATE.dt.month
surg.drop(surg.index[surg.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
surg.AUTO_ID = surg.AUTO_ID.astype(int)

surg = surg[(surg.TYPE_OF_SERVICE=='20-SURGERY MA') & (surg.PROC_GROUP_NAME=='GASTROINTESTINAL/DIGESTIVE')].sort_values(['AUTO_ID', 'ORIG_SERVICE_DATE']).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE'])

surg['ENC_SURGERY'] = 1
surg[IDENTIFIER_COLS+['ENC_SURGERY']].groupby(IDENTIFIER_COLS, as_index=False).sum().to_csv('datasets/long_in/surgeries.csv')
254/8:
surg = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
surg['RESULT_YEAR'] = surg.ORIG_SERVICE_DATE.dt.year
surg['RESULT_MONTH'] = surg.ORIG_SERVICE_DATE.dt.month
surg.drop(surg.index[surg.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
surg.AUTO_ID = surg.AUTO_ID.astype(int)

surg = surg[(surg.TYPE_OF_SERVICE=='20-SURGERY MA') & (surg.PROC_GROUP_NAME=='GASTROINTESTINAL/DIGESTIVE')].sort_values(['AUTO_ID', 'ORIG_SERVICE_DATE']).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE'])

surg['ENC_SURGERY'] = 1
surg[IDENTIFIER_COLS+['ENC_SURGERY']].groupby(IDENTIFIER_COLS, as_index=False).sum().to_csv('datasets/long_in/surgeries.csv', index=False)
256/9:
charges = pd.read_csv('datasets/long_in/charges.csv')
encounters = pd.read_csv('datasets/long_in/encounters.csv')
diag = pd.read_csv('datasets/long_in/gi_diagnostics.csv')
# diag.columns = [f'DIAG_{x}' if x not in IDENTIFIER_COLS else x for x in diag.columns ]
labs = pd.read_csv('datasets/long_in/labs.csv')
labs.columns = [f'LAB_{x}' if x not in IDENTIFIER_COLS else x for x in labs.columns]
meds = pd.read_csv('datasets/long_in/rx_long.csv')
meds.columns = [f'MED_{x}' if x not in IDENTIFIER_COLS else x for x in meds.columns]
hbi = pd.read_csv('datasets/long_in/hbi.csv')
hbi.columns = [f'HBI_{x}' if x not in IDENTIFIER_COLS else x for x in hbi.columns]
surg = pd.read_csv('datasets/long_in/surgeries.csv')
dis = pd.read_csv('datasets/long_in/disease_type.csv')
256/10:
final = reduce(lambda l,r: l.merge(r, how='outer', on=IDENTIFIER_COLS), [charges, encounters, diag, labs, meds, hbi, surg])
# final = final.merge(dis, how='left', on='AUTO_ID')
256/11:
final = reduce(lambda l,r: l.merge(r, how='outer', on=IDENTIFIER_COLS), [charges, encounters, diag, labs, meds, hbi, surg])
final = final.merge(dis, how='left', on='AUTO_ID')
256/12:
for df in [charges, encounters, diag, labs, meds, hbi, surg]:
    print(df.shape[0] - df.drop_duplicates(IDENTIFIER_COLS).shape[0])
256/13:
exclude_patients = pd.read_csv('datasets/clean_registry_data/exclude_patients.csv')
ct_aid = exclude_patients.AUTO_ID
CT_filtered = final[~final.AUTO_ID.isin(ct_aid)]
256/14:
final.to_csv('datasets/long_out/all.csv', index=False)
CT_filtered.to_csv('datasets/long_out/noCT.csv', index=False)
255/1:
%matplotlib inline
import os
import pandas as pd
import numpy as np
255/2:
IDENTIFIER_COLS = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']

def identifier_prefix(cols):
    identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
    return identifier_cols+[x for x in cols if x not in identifier_cols]

opj = os.path.join
255/3:
surg = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
surg.columns
255/4:
surg = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])

surg = surg[surg.AMOUNT>500] # TODO confirm this is good filter to remove trivial non-surgeries

surg['RESULT_YEAR'] = surg.ORIG_SERVICE_DATE.dt.year
surg['RESULT_MONTH'] = surg.ORIG_SERVICE_DATE.dt.month
surg.drop(surg.index[surg.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
surg.AUTO_ID = surg.AUTO_ID.astype(int)

surg = surg[(surg.TYPE_OF_SERVICE=='20-SURGERY MA') & (surg.PROC_GROUP_NAME=='GASTROINTESTINAL/DIGESTIVE')].sort_values(['AUTO_ID', 'ORIG_SERVICE_DATE']).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE'])

surg['ENC_SURGERY'] = 1
surg[IDENTIFIER_COLS+['ENC_SURGERY']].groupby(IDENTIFIER_COLS, as_index=False).sum().to_csv('datasets/long_in/surgeries.csv', index=False)
255/5:
surg = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])

surg = surg[surg.AMOUNT>500] # TODO confirm this is good filter to remove trivial non-surgeries

surg['RESULT_YEAR'] = surg.ORIG_SERVICE_DATE.dt.year
surg['RESULT_MONTH'] = surg.ORIG_SERVICE_DATE.dt.month
surg.drop(surg.index[surg.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
surg.AUTO_ID = surg.AUTO_ID.astype(int)

surg = surg[(surg.TYPE_OF_SERVICE=='20-SURGERY MA') & (surg.PROC_GROUP_NAME=='GASTROINTESTINAL/DIGESTIVE')].sort_values(['AUTO_ID', 'ORIG_SERVICE_DATE']).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE'])

surg['ENC_SURGERY'] = 1
surg[IDENTIFIER_COLS+['ENC_SURGERY']].groupby(IDENTIFIER_COLS, as_index=False).sum().to_csv('datasets/long_in/surgeries.csv', index=False)
256/15:
charges = pd.read_csv('datasets/long_in/charges.csv')
encounters = pd.read_csv('datasets/long_in/encounters.csv')
diag = pd.read_csv('datasets/long_in/gi_diagnostics.csv')
# diag.columns = [f'DIAG_{x}' if x not in IDENTIFIER_COLS else x for x in diag.columns ]
labs = pd.read_csv('datasets/long_in/labs.csv')
labs.columns = [f'LAB_{x}' if x not in IDENTIFIER_COLS else x for x in labs.columns]
meds = pd.read_csv('datasets/long_in/rx_long.csv')
meds.columns = [f'MED_{x}' if x not in IDENTIFIER_COLS else x for x in meds.columns]
hbi = pd.read_csv('datasets/long_in/hbi.csv')
hbi.columns = [f'HBI_{x}' if x not in IDENTIFIER_COLS else x for x in hbi.columns]
surg = pd.read_csv('datasets/long_in/surgeries.csv')
dis = pd.read_csv('datasets/long_in/disease_type.csv')
256/16:
final = reduce(lambda l,r: l.merge(r, how='outer', on=IDENTIFIER_COLS), [charges, encounters, diag, labs, meds, hbi, surg])
final = final.merge(dis, how='left', on='AUTO_ID')
256/17:
exclude_patients = pd.read_csv('datasets/clean_registry_data/exclude_patients.csv')
ct_aid = exclude_patients.AUTO_ID
CT_filtered = final[~final.AUTO_ID.isin(ct_aid)]
256/18:
final.to_csv('datasets/long_out/all.csv', index=False)
CT_filtered.to_csv('datasets/long_out/noCT.csv', index=False)
248/3: inputs = du.cleaned_longitudinal_inputs(load_pkl=False)
232/85: import os
232/86: os.get_cwd()
232/87: os.getcwd()
232/88: df = pd.read_pickle('../../../pickles/monthwise_inputs.pkl')
232/89: df.shape
232/90: df.columns
232/91: df.DELTA_DS_UC.mean()
232/92: df.DELTA_DS_UC.describe()
232/93: df.DS_UC.isnull().sum()
232/94: nullds = df[df.DS_UC.isnull()]
232/95: nullds.AUTO_ID.unique().shape
232/96: nullds.AUTO_ID.unique()
232/97: nullds = df[df.DS_UC.notnull()]
232/98: nullds.AUTO_ID.unique()
232/99: nullds.loc[:15, :58]
232/100: nullds.iloc[:15, :58]
232/101: nullds.iloc[:15, :54]
232/102: nullds = df[df.DS_UC.isnull()]
232/103: nullds.iloc[:15, :54]
232/104: nullds.iloc[:15, :54].to_clipboard()
232/105: nullds.iloc[:15, :53]
257/1:
import pandas as pd
import os
from functools import reduce
257/2: IDENTIFIER_COLS = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
257/3:
charges = pd.read_csv('datasets/long_in/charges.csv')
encounters = pd.read_csv('datasets/long_in/encounters.csv')
diag = pd.read_csv('datasets/long_in/gi_diagnostics.csv')
# diag.columns = [f'DIAG_{x}' if x not in IDENTIFIER_COLS else x for x in diag.columns ]
labs = pd.read_csv('datasets/long_in/labs.csv')
labs.columns = [f'LAB_{x}' if x not in IDENTIFIER_COLS else x for x in labs.columns]
meds = pd.read_csv('datasets/long_in/rx_long.csv')
meds.columns = [f'MED_{x}' if x not in IDENTIFIER_COLS else x for x in meds.columns]
hbi = pd.read_csv('datasets/long_in/hbi.csv')
hbi.columns = [f'HBI_{x}' if x not in IDENTIFIER_COLS else x for x in hbi.columns]
surg = pd.read_csv('datasets/long_in/surgeries.csv')
dis = pd.read_csv('datasets/long_in/disease_type.csv')
257/4:
final = reduce(lambda l,r: l.merge(r, how='outer', on=IDENTIFIER_COLS), [charges, encounters, diag, labs, meds, hbi, surg])
print(final.shape)
final = final.merge(dis, how='left', on='AUTO_ID')
print(final.shape)
257/5:
exclude_patients = pd.read_csv('datasets/clean_registry_data/exclude_patients.csv')
ct_aid = exclude_patients.AUTO_ID
CT_filtered = final[~final.AUTO_ID.isin(ct_aid)]
257/6:
exclude_patients = pd.read_csv('datasets/clean_registry_data/exclude_patients.csv')
ct_aid = exclude_patients.AUTO_ID
CT_filtered = final[~final.AUTO_ID.isin(ct_aid)]
257/7:
final.to_csv('datasets/long_out/all.csv', index=False)
CT_filtered.to_csv('datasets/long_out/noCT.csv', index=False)
258/1:
import utils
import data_utils as du
import os
import pandas as pd
import pickle
import seaborn as sns
from matplotlib import pyplot as plt
import pandas as pd
258/2: inputs = du.cleaned_longitudinal_inputs(load_pkl=False)
265/1: import data_utils as du
265/2: inputs = du.cleaned_longitudinal_inputs(load_pkl=False)
265/3: df = inputs[0]
265/4: df.DS_UC.isnull().sum()
265/5: df.columns[:55]
265/6: import importlib
265/7: importlib.reload(du)
265/8: inputs = du.cleaned_longitudinal_inputs(load_pkl=False)
265/9: inputs[0].DS_UC.isnull().sum()
265/10: import torch
265/11: l = [torch.Tensor([1]), torch.Tensor([3,4,5])]
265/12: l
265/13: torch.cat(l)
265/14: l = [torch.Tensor([1], requires_grad=True), torch.Tensor([3,4,5], requires_grad=True)]
265/15: l = [torch.tensor([1], requires_grad=True), torch.tensor([3,4,5], requires_grad=True)]
265/16: l = [torch.tensor([1.0], requires_grad=True), torch.tensor([3.0,4.0,5.0], requires_grad=True)]
265/17: l
265/18: torch.cat(l)
265/19: torch.cat(l).grad_fn
265/20: type(torch.cat(l).grad_fn)
265/21: l = 0
265/22: l += torch.tensor([1])
265/23: l
265/24: l += torch.tensor([4])
265/25: l
265/26: l.grad_fn
265/27: torch.softmax([1, 0.3, 0.4])
265/28: torch.softmax(torch.tensor([1, 0.3, 0.4]))
265/29: torch.softmax(torch.tensor([1, 0.3, 0.4]). -1)
265/30: torch.softmax(torch.tensor([1, 0.3, 0.4]), -1)
265/31: torch.softmax(torch.tensor([1, 0.3, 0.4]), -1).item()
265/32: torch.softmax(torch.tensor([1, 0.3, 0.4]), -1).data
265/33: a,b,c = torch.softmax(torch.tensor([1, 0.3, 0.4]), -1)
265/34: a
265/35: b
265/36: c
265/37: torch.tensor([1,2,3])*torch.tensor([2,2,2])
265/38: (torch.tensor([1,2,3])*torch.tensor([2,2,2])).sum()
265/39: torch.softmax(torch.tensor([1, 0.2, 0.2]), -1)
265/40: torch.tensor([1,2,3])*torch.tensor([2,0,0])
265/41: (torch.tensor([1,2,3])*torch.tensor([2,2,2])).sum().item()
265/42: len(torch.tensor([1,2,3]))
265/43: b = None
265/44: b = 3 or b
265/45: b
265/46: os.getcwd()
265/47: import o
265/48: import os
265/49: os.getcwd()
265/50: clear
265/51: df = pd.read_pickle('pickles/x_padded_inputs.pkl')
265/52: import pandas as pd
265/53: df = pd.read_pickle('pickles/x_padded_inputs.pkl')
265/54: df.columns
265/55: len(df.columns)
265/56: df.mean()
265/57: df.to_clipboard()
265/58: df[x for x in df.columns if x[:3]=='MED'].mean()
265/59: df[[x for x in df.columns if x[:3]=='MED']].fillna(0).mean()
265/60: import data_utils as du
265/61: imp = du.Imputer()
265/62: df2 = imp.commonsense_imputer(df)
265/63: import importlib
265/64: importlib.reload(du)
265/65: imp = du.Imputer()
265/66: df2 = imp.commonsense_imputer(df)
265/67: df2.mean()
265/68: df2.mean().to_clipboard()
265/69: df.mean().to_clipboard()
265/70: df.ENC_SURGERY.fillna(0).mean()
265/71: df.shape
265/72: df.columns
265/73: df.DS_AGE_DX
265/74: df.DS_AGE_DX.mean()
265/75: clear
265/76: df.DS_AGE_DX = df.DS_AGE_DX.astype(int)
265/77: df.DS_AGE_DX = df.DS_AGE_DX.replace('#VALUE1', np.nan)
265/78: import numpy as np
265/79: df.DS_AGE_DX = df.DS_AGE_DX.replace('#VALUE1', np.nan)
265/80: df.DS_AGE_DX.isnull().sum()
265/81: df.DS_AGE_DX.astype(int)
265/82: df.DS_AGE_DX = df.DS_AGE_DX.replace('#VALUE1 ', np.nan)
265/83: df.DS_AGE_DX.isnull().sum()
265/84: df.DS_AGE_DX = df.DS_AGE_DX.replace('#VALUE!', np.nan)
265/85: df.DS_AGE_DX.isnull().sum()
265/86: df.DS_AGE_DX.mean()
265/87: df.DS_AGE_DX.astype(float).mean()
265/88: df.DS_AGE_DX = df.DS_AGE_DX.fillna(30)
265/89: df.DS_AGE_DX = df.DS_AGE_DX.astype(int)
265/90: df.DS_AGE_DX.mean()
265/91:
disease = pd.read_csv('preprocessing/datasets/raw_registry_data/deid_disease type and year of diagnosis.csv')
disease.DSTYPE = disease.DSTYPE.replace({'1':'CD', '2':'UC', '-2':'unk', '3':'unk'})
disease[['DS_CD', 'DS_UC']] = pd.get_dummies(disease.DSTYPE).drop('unk', axis=1)
disease['DS_AGE_DX'] = disease['AGE_OF_DX'].replace('#VALUE!', np.nan).astype(float).fillna(30).astype(int) # 30 is the average age
disease['DS_PREV_RESECTION'] = disease['BOWEL_RESECTION_PRIOR_2009'].replace({'(No Data)':'0'}).fillna(0).astype(int)
DS = disease[['AUTO_ID']+[x for x in disease.columns if x[:3]=='DS_']]
DS.to_csv('datasets/long_in/disease_type.csv', index=False)
265/92:
disease = pd.read_csv('preprocessing/datasets/raw_registry_data/deid_disease type and year of diagnosis.csv')
disease.DSTYPE = disease.DSTYPE.replace({'1':'CD', '2':'UC', '-2':'unk', '3':'unk'})
disease[['DS_CD', 'DS_UC']] = pd.get_dummies(disease.DSTYPE).drop('unk', axis=1)
disease['DS_AGE_DX'] = disease['AGE_OF_DX'].replace('#VALUE!', np.nan).astype(float).fillna(30).astype(int) # 30 is the average age
disease['DS_PREV_RESECTION'] = disease['BOWEL_RESECTION_PRIOR_2009'].replace({'(No Data)':'0'}).fillna(0).astype(int)
DS = disease[['AUTO_ID']+[x for x in disease.columns if x[:3]=='DS_']]
DS.to_csv('preprocessing/datasets/long_in/disease_type.csv', index=False)
265/93: inputs = du.cleaned_longitudinal_inputs(load_pkl=False)
265/94: inputs = du.cleaned_longitudinal_inputs(load_pkl=False)
265/95: inputs[0].mean()
265/96: inputs[0].mean().to_clipboard()
265/97: inputs[0].quantile(0.5).to_clipboard()
265/98: imp.commonsense_imputer(inputs[0]).mean().to_clipboard()
265/99: imp.commonsense_imputer(inputs[0]).quantile(0.5).to_clipboard()
265/100: inputs[0].HBI_CROHNS_SCORE.mean()
265/101: inputs[0].HBI_UC_SCORE.mean()
267/1:
%matplotlib inline
import os
import pandas as pd
import numpy as np
267/2:
IDENTIFIER_COLS = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']

def identifier_prefix(cols):
    identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
    return identifier_cols+[x for x in cols if x not in identifier_cols]

opj = os.path.join
267/3:
surg = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
surg = surg[surg.AMOUNT>0] 

surg['RESULT_YEAR'] = surg.ORIG_SERVICE_DATE.dt.year
surg['RESULT_MONTH'] = surg.ORIG_SERVICE_DATE.dt.month
surg.drop(surg.index[surg.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
surg.AUTO_ID = surg.AUTO_ID.astype(int)

reqd_surg = surg[(surg.TYPE_OF_SERVICE=='20-SURGERY MA') & (surg.PROC_GROUP_NAME=='GASTROINTESTINAL/DIGESTIVE') & (~surg.PROC_NAME.str.contains('NEEDLE BIOPSY')) & (~surg.PROC_NAME.str.contains('APPENDECTOMY')) & (~surg.PROC_NAME.str.contains('GASTROSTOMY')) & (~surg.PROC_NAME.str.contains('ESOPH')) & (~surg.PROC_NAME.str.contains('CHOLECYSTOSTOMY')) & (~surg.PROC_NAME.str.contains('ANAST')) & (~surg.PROC_NAME.str.contains('TONGUE')) & (~surg.PROC_NAME.str.contains('LIVER')) & (~surg.PROC_NAME.str.contains('PLASTY')) & (~surg.PROC_NAME.str.contains('BILIARY')) & (~surg.PROC_NAME.str.contains('BILE')) & (~surg.PROC_NAME.str.contains('PAROTD')) & (~surg.PROC_NAME.str.contains('EXTRAHEP')) & (~surg.PROC_NAME.str.contains('PANCREA')) & (~surg.PROC_NAME.str.contains('SCLEROTX')) & (~surg.PROC_NAME.str.contains('GUM LESION'))]

surgeries = pd.concat([reqd_surg, 
                surg[(surg.PROC_CODE.astype(int).isin([44205, 44212, 44210, 44211, 44207, 44206, 44204, 44158])],
                surg[surg.PROC_NAME.str.contains('CT ABDOMEN') or surg.PROC_NAME.str.contains('CT PELVIS')],
                surg[surg.PROC_NAME.str.contains('CELIAC PLEXUS')]).sort_values(['AUTO_ID', 'ORIG_SERVICE_DATE'])           .drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE'])

surgeries['ENC_SURGERY'] = 1
surgeries[IDENTIFIER_COLS+['ENC_SURGERY']].groupby(IDENTIFIER_COLS, as_index=False).sum().to_csv('datasets/long_in/surgeries.csv', index=False)
267/4:
surg = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
surg = surg[surg.AMOUNT>0] 

surg['RESULT_YEAR'] = surg.ORIG_SERVICE_DATE.dt.year
surg['RESULT_MONTH'] = surg.ORIG_SERVICE_DATE.dt.month
surg.drop(surg.index[surg.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
surg.AUTO_ID = surg.AUTO_ID.astype(int)

reqd_surg = surg[(surg.TYPE_OF_SERVICE=='20-SURGERY MA') & (surg.PROC_GROUP_NAME=='GASTROINTESTINAL/DIGESTIVE') & (~surg.PROC_NAME.str.contains('NEEDLE BIOPSY')) & (~surg.PROC_NAME.str.contains('APPENDECTOMY')) & (~surg.PROC_NAME.str.contains('GASTROSTOMY')) & (~surg.PROC_NAME.str.contains('ESOPH')) & (~surg.PROC_NAME.str.contains('CHOLECYSTOSTOMY')) & (~surg.PROC_NAME.str.contains('ANAST')) & (~surg.PROC_NAME.str.contains('TONGUE')) & (~surg.PROC_NAME.str.contains('LIVER')) & (~surg.PROC_NAME.str.contains('PLASTY')) & (~surg.PROC_NAME.str.contains('BILIARY')) & (~surg.PROC_NAME.str.contains('BILE')) & (~surg.PROC_NAME.str.contains('PAROTD')) & (~surg.PROC_NAME.str.contains('EXTRAHEP')) & (~surg.PROC_NAME.str.contains('PANCREA')) & (~surg.PROC_NAME.str.contains('SCLEROTX')) & (~surg.PROC_NAME.str.contains('GUM LESION'))]

surgeries = pd.concat([reqd_surg, 
                surg[surg.PROC_CODE.astype(int).isin([44205, 44212, 44210, 44211, 44207, 44206, 44204, 44158])],
                surg[surg.PROC_NAME.str.contains('CT ABDOMEN') or surg.PROC_NAME.str.contains('CT PELVIS')],
                surg[surg.PROC_NAME.str.contains('CELIAC PLEXUS')]).sort_values(['AUTO_ID', 'ORIG_SERVICE_DATE'])           .drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE'])

surgeries['ENC_SURGERY'] = 1
surgeries[IDENTIFIER_COLS+['ENC_SURGERY']].groupby(IDENTIFIER_COLS, as_index=False).sum().to_csv('datasets/long_in/surgeries.csv', index=False)
267/5:
surg = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
surg = surg[surg.AMOUNT>0] 

surg['RESULT_YEAR'] = surg.ORIG_SERVICE_DATE.dt.year
surg['RESULT_MONTH'] = surg.ORIG_SERVICE_DATE.dt.month
surg.drop(surg.index[surg.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
surg.AUTO_ID = surg.AUTO_ID.astype(int)

reqd_surg = surg[(surg.TYPE_OF_SERVICE=='20-SURGERY MA') & (surg.PROC_GROUP_NAME=='GASTROINTESTINAL/DIGESTIVE') & (~surg.PROC_NAME.str.contains('NEEDLE BIOPSY')) & (~surg.PROC_NAME.str.contains('APPENDECTOMY')) & (~surg.PROC_NAME.str.contains('GASTROSTOMY')) & (~surg.PROC_NAME.str.contains('ESOPH')) & (~surg.PROC_NAME.str.contains('CHOLECYSTOSTOMY')) & (~surg.PROC_NAME.str.contains('ANAST')) & (~surg.PROC_NAME.str.contains('TONGUE')) & (~surg.PROC_NAME.str.contains('LIVER')) & (~surg.PROC_NAME.str.contains('PLASTY')) & (~surg.PROC_NAME.str.contains('BILIARY')) & (~surg.PROC_NAME.str.contains('BILE')) & (~surg.PROC_NAME.str.contains('PAROTD')) & (~surg.PROC_NAME.str.contains('EXTRAHEP')) & (~surg.PROC_NAME.str.contains('PANCREA')) & (~surg.PROC_NAME.str.contains('SCLEROTX')) & (~surg.PROC_NAME.str.contains('GUM LESION'))]

surgeries = pd.concat([reqd_surg, 
                surg[surg.PROC_CODE.astype(int).isin([44205, 44212, 44210, 44211, 44207, 44206, 44204, 44158])],
                surg[surg.PROC_NAME.str.contains('CT ABDOMEN') or surg.PROC_NAME.str.contains('CT PELVIS')],
                surg[surg.PROC_NAME.str.contains('CELIAC PLEXUS')])
surgeries = surgeries.sort_values(['AUTO_ID', 'ORIG_SERVICE_DATE']).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE'])

surgeries['ENC_SURGERY'] = 1
surgeries[IDENTIFIER_COLS+['ENC_SURGERY']].groupby(IDENTIFIER_COLS, as_index=False).sum().to_csv('datasets/long_in/surgeries.csv', index=False)
267/6:
surg = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
surg = surg[surg.AMOUNT>0] 

surg['RESULT_YEAR'] = surg.ORIG_SERVICE_DATE.dt.year
surg['RESULT_MONTH'] = surg.ORIG_SERVICE_DATE.dt.month
surg.drop(surg.index[surg.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
surg.AUTO_ID = surg.AUTO_ID.astype(int)

reqd_surg = surg[(surg.TYPE_OF_SERVICE=='20-SURGERY MA') & (surg.PROC_GROUP_NAME=='GASTROINTESTINAL/DIGESTIVE') & (~surg.PROC_NAME.str.contains('NEEDLE BIOPSY')) & (~surg.PROC_NAME.str.contains('APPENDECTOMY')) & (~surg.PROC_NAME.str.contains('GASTROSTOMY')) & (~surg.PROC_NAME.str.contains('ESOPH')) & (~surg.PROC_NAME.str.contains('CHOLECYSTOSTOMY')) & (~surg.PROC_NAME.str.contains('ANAST')) & (~surg.PROC_NAME.str.contains('TONGUE')) & (~surg.PROC_NAME.str.contains('LIVER')) & (~surg.PROC_NAME.str.contains('PLASTY')) & (~surg.PROC_NAME.str.contains('BILIARY')) & (~surg.PROC_NAME.str.contains('BILE')) & (~surg.PROC_NAME.str.contains('PAROTD')) & (~surg.PROC_NAME.str.contains('EXTRAHEP')) & (~surg.PROC_NAME.str.contains('PANCREA')) & (~surg.PROC_NAME.str.contains('SCLEROTX')) & (~surg.PROC_NAME.str.contains('GUM LESION'))]

surgeries = pd.concat([reqd_surg, 
                surg[surg.PROC_CODE.astype(int).isin([44205, 44212, 44210, 44211, 44207, 44206, 44204, 44158])],
                surg[surg.PROC_NAME.str.contains('CT ABDOMEN') or surg.PROC_NAME.str.contains('CT PELVIS')],
                surg[surg.PROC_NAME.str.contains('CELIAC PLEXUS')]])
surgeries = surgeries.sort_values(['AUTO_ID', 'ORIG_SERVICE_DATE']).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE'])

surgeries['ENC_SURGERY'] = 1
surgeries[IDENTIFIER_COLS+['ENC_SURGERY']].groupby(IDENTIFIER_COLS, as_index=False).sum().to_csv('datasets/long_in/surgeries.csv', index=False)
267/7:
surg = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
surg = surg[surg.AMOUNT>0] 

surg['RESULT_YEAR'] = surg.ORIG_SERVICE_DATE.dt.year
surg['RESULT_MONTH'] = surg.ORIG_SERVICE_DATE.dt.month
surg.drop(surg.index[surg.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
surg.AUTO_ID = surg.AUTO_ID.astype(int)

reqd_surg = surg[(surg.TYPE_OF_SERVICE=='20-SURGERY MA') & (surg.PROC_GROUP_NAME=='GASTROINTESTINAL/DIGESTIVE') & (~surg.PROC_NAME.str.contains('NEEDLE BIOPSY')) & (~surg.PROC_NAME.str.contains('APPENDECTOMY')) & (~surg.PROC_NAME.str.contains('GASTROSTOMY')) & (~surg.PROC_NAME.str.contains('ESOPH')) & (~surg.PROC_NAME.str.contains('CHOLECYSTOSTOMY')) & (~surg.PROC_NAME.str.contains('ANAST')) & (~surg.PROC_NAME.str.contains('TONGUE')) & (~surg.PROC_NAME.str.contains('LIVER')) & (~surg.PROC_NAME.str.contains('PLASTY')) & (~surg.PROC_NAME.str.contains('BILIARY')) & (~surg.PROC_NAME.str.contains('BILE')) & (~surg.PROC_NAME.str.contains('PAROTD')) & (~surg.PROC_NAME.str.contains('EXTRAHEP')) & (~surg.PROC_NAME.str.contains('PANCREA')) & (~surg.PROC_NAME.str.contains('SCLEROTX')) & (~surg.PROC_NAME.str.contains('GUM LESION'))]

surgeries = pd.concat([reqd_surg, 
                surg[surg.PROC_CODE.isin(['44205', '44212', '44210', '44211', '44207', '44206', '44204', '44158'])],
                surg[surg.PROC_NAME.str.contains('CT ABDOMEN') or surg.PROC_NAME.str.contains('CT PELVIS')],
                surg[surg.PROC_NAME.str.contains('CELIAC PLEXUS')]])
surgeries = surgeries.sort_values(['AUTO_ID', 'ORIG_SERVICE_DATE']).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE'])

surgeries['ENC_SURGERY'] = 1
surgeries[IDENTIFIER_COLS+['ENC_SURGERY']].groupby(IDENTIFIER_COLS, as_index=False).sum().to_csv('datasets/long_in/surgeries.csv', index=False)
267/8:
surg = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
surg = surg[surg.AMOUNT>0] 

surg['RESULT_YEAR'] = surg.ORIG_SERVICE_DATE.dt.year
surg['RESULT_MONTH'] = surg.ORIG_SERVICE_DATE.dt.month
surg.drop(surg.index[surg.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
surg.AUTO_ID = surg.AUTO_ID.astype(int)

reqd_surg = surg[(surg.TYPE_OF_SERVICE=='20-SURGERY MA') & (surg.PROC_GROUP_NAME=='GASTROINTESTINAL/DIGESTIVE') & (~surg.PROC_NAME.str.contains('NEEDLE BIOPSY')) & (~surg.PROC_NAME.str.contains('APPENDECTOMY')) & (~surg.PROC_NAME.str.contains('GASTROSTOMY')) & (~surg.PROC_NAME.str.contains('ESOPH')) & (~surg.PROC_NAME.str.contains('CHOLECYSTOSTOMY')) & (~surg.PROC_NAME.str.contains('ANAST')) & (~surg.PROC_NAME.str.contains('TONGUE')) & (~surg.PROC_NAME.str.contains('LIVER')) & (~surg.PROC_NAME.str.contains('PLASTY')) & (~surg.PROC_NAME.str.contains('BILIARY')) & (~surg.PROC_NAME.str.contains('BILE')) & (~surg.PROC_NAME.str.contains('PAROTD')) & (~surg.PROC_NAME.str.contains('EXTRAHEP')) & (~surg.PROC_NAME.str.contains('PANCREA')) & (~surg.PROC_NAME.str.contains('SCLEROTX')) & (~surg.PROC_NAME.str.contains('GUM LESION'))]

surgeries = pd.concat([reqd_surg, 
                surg[surg.PROC_CODE.isin(['44205', '44212', '44210', '44211', '44207', '44206', '44204', '44158'])],
                surg[surg.PROC_NAME.str.contains('CT ABDOMEN')],
                surg[surg.PROC_NAME.str.contains('CT PELVIS')],
                surg[surg.PROC_NAME.str.contains('CELIAC PLEXUS')]])
surgeries = surgeries.sort_values(['AUTO_ID', 'ORIG_SERVICE_DATE']).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE'])

surgeries['ENC_SURGERY'] = 1
surgeries[IDENTIFIER_COLS+['ENC_SURGERY']].groupby(IDENTIFIER_COLS, as_index=False).sum().to_csv('datasets/long_in/surgeries.csv', index=False)
267/9:
diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
diag.drop(diag.index[diag.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
diag.AUTO_ID = diag.AUTO_ID.astype(int)

def onehot_flag(col_name, pat):
    diag['DIAG_'+col_name] = 0
    diag.loc[diag.PROC_NAME.str.contains(pat) & (diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY'), 'DIAG_'+col_name] += 1 

col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'), ('ANO','ANOSCOPY')]

for c,p in col_pat:
    onehot_flag(c, p)

diag = diag[IDENTIFIER_COLS+[x for x in diag.columns if x[:4]=='DIAG']]
diag['DIAG_CT_ABPEL'] = np.where(np.logical_or(diag.PROC_NAME.str.contains('CT ABDOMEN'), diag.PROC_NAME.str.contains('CT PELVIS')), 1, 0)
diag = diag.groupby(IDENTIFIER_COLS, as_index=False).sum()
diag.sort_values(IDENTIFIER_COLS).to_csv('datasets/long_in/gi_diagnostics2.csv', index=False)











surg = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
surg = surg[surg.AMOUNT>0] 

surg['RESULT_YEAR'] = surg.ORIG_SERVICE_DATE.dt.year
surg['RESULT_MONTH'] = surg.ORIG_SERVICE_DATE.dt.month
surg.drop(surg.index[surg.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
surg.AUTO_ID = surg.AUTO_ID.astype(int)

reqd_surg = surg[(surg.TYPE_OF_SERVICE=='20-SURGERY MA') & (surg.PROC_GROUP_NAME=='GASTROINTESTINAL/DIGESTIVE') & (~surg.PROC_NAME.str.contains('NEEDLE BIOPSY')) & (~surg.PROC_NAME.str.contains('APPENDECTOMY')) & (~surg.PROC_NAME.str.contains('GASTROSTOMY')) & (~surg.PROC_NAME.str.contains('ESOPH')) & (~surg.PROC_NAME.str.contains('CHOLECYSTOSTOMY')) & (~surg.PROC_NAME.str.contains('ANAST')) & (~surg.PROC_NAME.str.contains('TONGUE')) & (~surg.PROC_NAME.str.contains('LIVER')) & (~surg.PROC_NAME.str.contains('PLASTY')) & (~surg.PROC_NAME.str.contains('BILIARY')) & (~surg.PROC_NAME.str.contains('BILE')) & (~surg.PROC_NAME.str.contains('PAROTD')) & (~surg.PROC_NAME.str.contains('EXTRAHEP')) & (~surg.PROC_NAME.str.contains('PANCREA')) & (~surg.PROC_NAME.str.contains('SCLEROTX')) & (~surg.PROC_NAME.str.contains('GUM LESION'))]

surgeries = pd.concat([reqd_surg, 
            surg[surg.PROC_CODE.isin(['44205', '44212', '44210', '44211', '44207', '44206', '44204', '44158'])], # COLECTOMIES
            surg[surg.PROC_NAME.str.contains('CELIAC PLEXUS')]])

surgeries = surgeries.sort_values(['AUTO_ID', 'ORIG_SERVICE_DATE']).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE'])
surgeries['ENC_PK_INJ'] = np.where(surgeries.PROC_NAME.str.contains('CELIAC PLEXUS'), 1, 0)
surgeries['ENC_SURGERY'] = (1 - surgeries['ENC_PK_INJ']).abs()

surgeries[IDENTIFIER_COLS+['ENC_SURGERY', 'ENC_PK_INJ']].groupby(IDENTIFIER_COLS, as_index=False).sum().to_csv('datasets/long_in/surgeries.csv', index=False)
267/10:
diag = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year
diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month
diag.drop(diag.index[diag.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
diag.AUTO_ID = diag.AUTO_ID.astype(int)

def onehot_flag(col_name, pat):
    diag['DIAG_'+col_name] = 0
    diag.loc[diag.PROC_NAME.str.contains(pat) & (diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY'), 'DIAG_'+col_name] += 1 

col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'), ('ANO','ANOSCOPY')]

for c,p in col_pat:
    onehot_flag(c, p)

diag['DIAG_CT_ABPEL'] = np.where(np.logical_or(diag.PROC_NAME.str.contains('CT ABDOMEN'), diag.PROC_NAME.str.contains('CT PELVIS')), 1, 0)
diag = diag[IDENTIFIER_COLS+[x for x in diag.columns if x[:4]=='DIAG']]
diag = diag.groupby(IDENTIFIER_COLS, as_index=False).sum()
diag.sort_values(IDENTIFIER_COLS).to_csv('datasets/long_in/gi_diagnostics2.csv', index=False)











surg = pd.read_csv('datasets/raw_registry_data/deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])
surg = surg[surg.AMOUNT>0] 

surg['RESULT_YEAR'] = surg.ORIG_SERVICE_DATE.dt.year
surg['RESULT_MONTH'] = surg.ORIG_SERVICE_DATE.dt.month
surg.drop(surg.index[surg.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)
surg.AUTO_ID = surg.AUTO_ID.astype(int)

reqd_surg = surg[(surg.TYPE_OF_SERVICE=='20-SURGERY MA') & (surg.PROC_GROUP_NAME=='GASTROINTESTINAL/DIGESTIVE') & (~surg.PROC_NAME.str.contains('NEEDLE BIOPSY')) & (~surg.PROC_NAME.str.contains('APPENDECTOMY')) & (~surg.PROC_NAME.str.contains('GASTROSTOMY')) & (~surg.PROC_NAME.str.contains('ESOPH')) & (~surg.PROC_NAME.str.contains('CHOLECYSTOSTOMY')) & (~surg.PROC_NAME.str.contains('ANAST')) & (~surg.PROC_NAME.str.contains('TONGUE')) & (~surg.PROC_NAME.str.contains('LIVER')) & (~surg.PROC_NAME.str.contains('PLASTY')) & (~surg.PROC_NAME.str.contains('BILIARY')) & (~surg.PROC_NAME.str.contains('BILE')) & (~surg.PROC_NAME.str.contains('PAROTD')) & (~surg.PROC_NAME.str.contains('EXTRAHEP')) & (~surg.PROC_NAME.str.contains('PANCREA')) & (~surg.PROC_NAME.str.contains('SCLEROTX')) & (~surg.PROC_NAME.str.contains('GUM LESION'))]

surgeries = pd.concat([reqd_surg, 
            surg[surg.PROC_CODE.isin(['44205', '44212', '44210', '44211', '44207', '44206', '44204', '44158'])], # COLECTOMIES
            surg[surg.PROC_NAME.str.contains('CELIAC PLEXUS')]])

surgeries = surgeries.sort_values(['AUTO_ID', 'ORIG_SERVICE_DATE']).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE'])
surgeries['ENC_PK_INJ'] = np.where(surgeries.PROC_NAME.str.contains('CELIAC PLEXUS'), 1, 0)
surgeries['ENC_SURGERY'] = (1 - surgeries['ENC_PK_INJ']).abs()

surgeries[IDENTIFIER_COLS+['ENC_SURGERY', 'ENC_PK_INJ']].groupby(IDENTIFIER_COLS, as_index=False).sum().to_csv('datasets/long_in/surgeries.csv', index=False)
269/1:
import pandas as pd
import os
from functools import reduce
269/2: IDENTIFIER_COLS = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
269/3:
charges = pd.read_csv('datasets/long_in/charges.csv')
encounters = pd.read_csv('datasets/long_in/encounters.csv')
diag = pd.read_csv('datasets/long_in/gi_diagnostics.csv')
# diag.columns = [f'DIAG_{x}' if x not in IDENTIFIER_COLS else x for x in diag.columns ]
labs = pd.read_csv('datasets/long_in/labs.csv')
labs.columns = [f'LAB_{x}' if x not in IDENTIFIER_COLS else x for x in labs.columns]
meds = pd.read_csv('datasets/long_in/rx_long.csv')
meds.columns = [f'MED_{x}' if x not in IDENTIFIER_COLS else x for x in meds.columns]
hbi = pd.read_csv('datasets/long_in/hbi.csv')
hbi.columns = [f'HBI_{x}' if x not in IDENTIFIER_COLS else x for x in hbi.columns]
surg = pd.read_csv('datasets/long_in/surgeries.csv')
dis = pd.read_csv('datasets/long_in/disease_type.csv')
269/4:
exclude_patients = pd.read_csv('datasets/clean_registry_data/exclude_patients.csv')
ct_aid = exclude_patients.AUTO_ID
CT_filtered = final[~final.AUTO_ID.isin(ct_aid)]
269/5:
final.to_csv('datasets/long_out/all.csv', index=False)
CT_filtered.to_csv('datasets/long_out/noCT.csv', index=False)
269/6:
final = reduce(lambda l,r: l.merge(r, how='outer', on=IDENTIFIER_COLS), [charges, encounters, diag, labs, meds, hbi, surg])
print(final.shape)
final = final.merge(dis, how='left', on='AUTO_ID')
print(final.shape)
269/7:
import pandas as pd
import os
from functools import reduce
269/8: IDENTIFIER_COLS = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']
269/9:
charges = pd.read_csv('datasets/long_in/charges.csv')
encounters = pd.read_csv('datasets/long_in/encounters.csv')
diag = pd.read_csv('datasets/long_in/gi_diagnostics.csv')
# diag.columns = [f'DIAG_{x}' if x not in IDENTIFIER_COLS else x for x in diag.columns ]
labs = pd.read_csv('datasets/long_in/labs.csv')
labs.columns = [f'LAB_{x}' if x not in IDENTIFIER_COLS else x for x in labs.columns]
meds = pd.read_csv('datasets/long_in/rx_long.csv')
meds.columns = [f'MED_{x}' if x not in IDENTIFIER_COLS else x for x in meds.columns]
hbi = pd.read_csv('datasets/long_in/hbi.csv')
hbi.columns = [f'HBI_{x}' if x not in IDENTIFIER_COLS else x for x in hbi.columns]
surg = pd.read_csv('datasets/long_in/surgeries.csv')
dis = pd.read_csv('datasets/long_in/disease_type.csv')
269/10:
final = reduce(lambda l,r: l.merge(r, how='outer', on=IDENTIFIER_COLS), [charges, encounters, diag, labs, meds, hbi, surg])
print(final.shape)
final = final.merge(dis, how='left', on='AUTO_ID')
print(final.shape)
269/11:
exclude_patients = pd.read_csv('datasets/clean_registry_data/exclude_patients.csv')
ct_aid = exclude_patients.AUTO_ID
CT_filtered = final[~final.AUTO_ID.isin(ct_aid)]
269/12:
final.to_csv('datasets/long_out/all.csv', index=False)
CT_filtered.to_csv('datasets/long_out/noCT.csv', index=False)
273/1:
import utils
import data_utils as du
import os
import pandas as pd
import pickle
import seaborn as sns
from matplotlib import pyplot as plt
import pandas as pd
273/2: inputs = du.cleaned_longitudinal_inputs(load_pkl=False)
   1: import pandas as pd
   2: df = pd.read_pickle('pickles/monthwise_inputs.pkl')
   3: df.shape
   4: df = pd.read_pickle('pickles/x_padded_inputs.pkl')
   5: df.shape
   6: df.mean()
   7: %history
   8: %history -g -f ipy.hist
