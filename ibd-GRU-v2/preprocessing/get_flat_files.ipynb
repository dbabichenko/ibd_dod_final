{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process raw deid data into flat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrn_auto_map = pd.read_csv('MRN_Auto_Mapping.csv')\n",
    "mrn_auto_map['PAT_MRN_ID'] = mrn_auto_map['PAT_MRN_ID'].astype(int)\n",
    "\n",
    "exclude_patients = pd.read_csv('cancer_transplants.csv')\n",
    "exclude_patients['PAT_MRN_ID'] = exclude_patients['PAT_MRN_ID'].astype(int)\n",
    "\n",
    "EXCLUDE = exclude_patients.merge(mrn_auto_map, how='left', on='PAT_MRN_ID')\n",
    "# EXCLUDE.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'exclude_patients.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']\n",
    "\n",
    "def identifier_prefix(cols):\n",
    "    identifier_cols = ['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH']\n",
    "    return identifier_cols+[x for x in cols if x not in identifier_cols]\n",
    "\n",
    "opj = os.path.join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBD Medications\n",
    "- Get intervals of prescription validity for each group of drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "\n",
    "def active_intervals(o,e):    \n",
    "    start_dates = [o.iloc[0]]\n",
    "    end_dates=[]\n",
    "    start_loc, end_loc = 0, 0\n",
    "    \n",
    "    for end_tm1, start_t0 in zip(e[:-1],o[1:]):\n",
    "        if end_tm1<start_t0:\n",
    "            end_dates.append(end_tm1)\n",
    "            start_loc=end_loc+1\n",
    "            start_dates.append(start_t0)\n",
    "        end_loc+=1\n",
    "    end_dates.append(e.iloc[-1])\n",
    "    \n",
    "    start_dates = list(map(pd.Timestamp, start_dates))\n",
    "    end_dates = list(map(pd.Timestamp, end_dates))    \n",
    "    active_during = list(zip(start_dates, end_dates))\n",
    "    return active_during\n",
    "\n",
    "\n",
    "def active_prescriptions(med_a):\n",
    "    drugs = ['5_ASA', 'Systemic_steroids', 'Immunomodulators', 'Psych',\\\n",
    "       'ANTI_TNF', 'ANTI_IL12', 'ANTI_INTEGRIN', 'Vitamin_D']\n",
    "    \n",
    "    # Get contiguous use info\n",
    "    intervals = []\n",
    "    for dr in drugs:\n",
    "        df = med_a[med_a.GROUP==dr]\n",
    "        if df.empty:\n",
    "            intervals.append(None)\n",
    "        else:\n",
    "            intervals.append(active_intervals(df.ORDERING_DATE, df.END_DATE))\n",
    "    # Get activity one-hot\n",
    "    drug_columns=[]\n",
    "    for ix,dr in enumerate(drugs):\n",
    "        intvl = intervals[ix]\n",
    "        drug_col = []\n",
    "        \n",
    "        if not intvl:            \n",
    "            drug_col = pd.Series(name=dr)\n",
    "        else:\n",
    "            for s,e in intvl:\n",
    "                drug_col.append(pd.Series(index=pd.date_range(s,e, freq='M'), name=dr).fillna(1))\n",
    "            drug_col = pd.concat(drug_col)\n",
    "        drug_columns.append(drug_col)\n",
    "        \n",
    "    return pd.concat(drug_columns, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "psych_med_prescrip = pd.read_csv('deid_IBD_Registry_BA1951_Medications_2018-07-05-09-47-15.csv', parse_dates=['ORDERING_DATE', 'END_DATE'])\n",
    "\n",
    "psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.THERAPEUTIC_CLASS=='PSYCHOTHERAPEUTIC DRUGS']\n",
    "psych_med_prescrip = psych_med_prescrip[psych_med_prescrip.END_DATE.notnull() & psych_med_prescrip.ORDERING_DATE.notnull()] # filter no end_date rows\n",
    "psych_med_prescrip['GROUP'] = 'Psych'\n",
    "psych_med_prescrip = psych_med_prescrip[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]\n",
    "\n",
    "\n",
    "med_df = pd.read_csv('filtered_meds2018.csv', parse_dates=['ORDERING_DATE', 'END_DATE'])\n",
    "med_df = med_df[['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP']]\n",
    "med_df = med_df[med_df.END_DATE.notnull() & med_df.ORDERING_DATE.notnull()] # filter no end_date rows\n",
    "med_df = pd.concat([med_df, psych_med_prescrip])\n",
    "\n",
    "\n",
    "# drop redundant rows like (Apr-Dec, Apr-Nov) and (Jan-Sept, Mar-Sept) ==> second row is redundant in both tuples \n",
    "med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'ORDERING_DATE', 'GROUP'])\n",
    "med_df = med_df.sort_values(['AUTO_ID', 'ORDERING_DATE', 'END_DATE', 'GROUP'], ascending=[True, True, False, True]).drop_duplicates(['AUTO_ID', 'END_DATE', 'GROUP'])\n",
    "# med_df.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'all_rx.csv'))\n",
    "\n",
    "med_df2 = med_df.groupby('AUTO_ID').apply(active_prescriptions).reset_index()\n",
    "med_df2['RESULT_YEAR'] = med_df2.level_1.dt.year\n",
    "med_df2['RESULT_MONTH'] = med_df2.level_1.dt.month\n",
    "med_df2 = med_df2[identifier_cols+list(med_df.GROUP.unique())]\n",
    "med_df2.to_csv('flatfiles/rx_long.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labs\n",
    "- No imputation\n",
    "- Get range, numeric reading, and High/Normal/Low flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = pd.read_csv('filtered_labs.csv', parse_dates=['ORDER_DATE','RESULT_DATE'])\n",
    "labs = labs[['AUTO_ID', 'ORDER_DATE', 'RESULT_DATE', 'ORD_NUM_VALUE', 'REF_LOW', 'REF_HIGH', 'REF_NORMAL_VALS', 'REF_UNIT',\n",
    "       'RESULT_FLAG', 'GROUP']]\n",
    "\n",
    "# drop invalid rows\n",
    "labs = labs.drop(labs.index[np.logical_or(labs['ORD_NUM_VALUE'].isnull(), labs['ORD_NUM_VALUE']>=9999999)]) # Drop rows with invalid readings\n",
    "\n",
    "# === clean messy readings data ====\n",
    "# copy REF_LOW values with <,- to REF_NORMAL_VALS. WILL PROCESS THEM \n",
    "labs['REF_NORMAL_VALS'] = np.where(labs['REF_LOW'].str.contains(r'[<-]')==True, labs['REF_LOW'], labs['REF_NORMAL_VALS'])\n",
    "# replace invalid values with nan\n",
    "labs['REF_HIGH'] = np.where(labs['REF_HIGH'].str.contains(r'[a-zA-Z<]')==True, np.nan, labs['REF_HIGH'])\n",
    "labs['REF_LOW'] = np.where(labs['REF_LOW'].str.contains(r'[a-zA-Z<\\-]')==True, np.nan, labs['REF_LOW'])\n",
    "# drop garbage rows\n",
    "labs.drop(labs[labs['REF_NORMAL_VALS'].str.contains(r'[a-zA-Z]')==True].index, inplace=True)\n",
    "# extract LOW, HIGH by splitting REF_NORMAL_VALS\n",
    "labs[['NEW_LOW','NEW_HIGH']] = labs[labs['REF_NORMAL_VALS'].str.contains('-')==True]['REF_NORMAL_VALS'].str.split('-').apply(pd.Series, 1) # solve cases where REF_LOW is like 0-10\n",
    "labs['NEW_LOW'] = np.where(np.logical_and(labs['REF_LOW'].isnull(), labs['REF_NORMAL_VALS'].str.contains(r'[<=]')==True), -999, labs.NEW_LOW) # solve cases where REF_LOW is like '<TAU', anything below TAU is normal => x is normal if -999 < x < TAU. \n",
    "labs['NEW_HIGH'] = np.where(labs['REF_NORMAL_VALS'].str.contains(r'[<=]'), labs['REF_NORMAL_VALS'].str.replace(r'[<=]',''), labs.NEW_HIGH) # NEW_LOW = -999, NEW_HIGH = TAU\n",
    "# put REF_X into NEW_X\n",
    "labs['NEW_LOW'] = np.where(labs.NEW_LOW.isnull(), labs['REF_LOW'], labs.NEW_LOW)\n",
    "labs['NEW_HIGH'] = np.where(labs.NEW_HIGH.isnull(), labs['REF_HIGH'], labs.NEW_HIGH)\n",
    "labs['NEW_HIGH'] = labs['NEW_HIGH'].astype(float)\n",
    "labs['NEW_LOW'] = labs['NEW_LOW'].astype(float)    \n",
    "labs['REF_UNIT'] = labs['REF_UNIT'].str.lower()\n",
    "\n",
    "labs['RESULT_FLAG'] = 'Normal'\n",
    "labs['RESULT_FLAG'] = np.where(labs['ORD_NUM_VALUE'] < labs.NEW_LOW, 'Low', labs['RESULT_FLAG'])\n",
    "labs['RESULT_FLAG'] = np.where(labs['ORD_NUM_VALUE'] > labs.NEW_HIGH, 'High', labs['RESULT_FLAG'])\n",
    "labs['RESULT_YEAR'] = labs.RESULT_DATE.dt.year\n",
    "labs['RESULT_MONTH'] = labs.RESULT_DATE.dt.month\n",
    "\n",
    "# Get total counts per month for each lab result type\n",
    "labs['newcol'] = labs['GROUP']+'_'+ labs['RESULT_FLAG']\n",
    "labs = labs[['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'newcol']]\n",
    "labs['chk'] = 1\n",
    "labs=labs.groupby(identifier_cols+['newcol']).chk.sum().unstack(-1).reset_index()\n",
    "labs.to_csv('flatfiles/labs.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get record of various diagnostic tests (related to GI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DEPRECATED - using old file\n",
    "\"\"\"\n",
    "\n",
    "# diagnostic = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'deid_IBD_Registry_BA1951_Rad_Diagnostic_Tests_2018-07-05-12-19-00.csv'), parse_dates=['VISIT_DATE'])\n",
    "# diag = diagnostic[diagnostic.ORDER_TYPE=='GI PROCEDURES']\n",
    "\n",
    "# def onehot_flag(col_name, pat):\n",
    "#     diag[col_name] = 0\n",
    "#     diag.loc[diag.DESCRIPTION.str.contains(pat), col_name] += 1 \n",
    "\n",
    "# col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'), ('ERCP', 'ERCP'), ('EGD', 'ESOPHOGASTRODUODENOSCOPY'), ('EGD', 'UPPER GI ENDO'), ('EGD','UPPER EUS'), ('ANO','ANOSCOPY'), ('EGD', 'ESOPHAGOSCOPY')]\n",
    "\n",
    "# for c,p in col_pat:\n",
    "#     onehot_flag(c, p)\n",
    "\n",
    "# \"\"\"\n",
    "# DON't KNOW WHAT THIS IS FOR\n",
    "# diag['GI_PROCEDURE'] = 0\n",
    "# diag.loc[~diag[[c for c,_ in col_pat]].any(axis=1), 'GI_PROCEDURE'] = 1\n",
    "# \"\"\"\n",
    "\n",
    "# diag['RESULT_YEAR'] = diag.VISIT_DATE.dt.year\n",
    "# diag['RESULT_MONTH'] = diag.VISIT_DATE.dt.month\n",
    "# diag = diag[['AUTO_ID','RESULT_YEAR', 'RESULT_MONTH']+[c for c,_ in col_pat].drop_duplicates()\n",
    "# diag=diag.groupby(identifier_cols).sum().reset_index()\n",
    "# diag.to_csv(opj(cfg.LONG_IN, 'gi_diagnostics.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternate Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = pd.read_csv('deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])\n",
    "diag['RESULT_YEAR'] = diag.ORIG_SERVICE_DATE.dt.year\n",
    "diag['RESULT_MONTH'] = diag.ORIG_SERVICE_DATE.dt.month\n",
    "diag.drop(diag.index[diag.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)\n",
    "diag.AUTO_ID = diag.AUTO_ID.astype(int)\n",
    "\n",
    "def onehot_flag(col_name, pat):\n",
    "    diag['DIAG_'+col_name] = 0\n",
    "    diag.loc[diag.PROC_NAME.str.contains(pat) & (diag.PROC_GROUP_NAME=='DIAGNOSTIC RADIOLOGY'), 'DIAG_'+col_name] += 1 \n",
    "\n",
    "col_pat = [('COLONOSCOPY', 'COLONOSCOP'), ('ENDOSCOPY', 'ENDOSCOP'), ('SIGMOIDOSCOPY', 'SIGMOIDOSCO'), ('ILEOSCOPY', 'ILEOSCOP'), ('ANO','ANOSCOPY')]\n",
    "\n",
    "for c,p in col_pat:\n",
    "    onehot_flag(c, p)\n",
    "\n",
    "diag['DIAG_CT_ABPEL'] = np.where(np.logical_or(diag.PROC_NAME.str.contains('CT ABDOMEN'), diag.PROC_NAME.str.contains('CT PELVIS')), 1, 0)\n",
    "diag = diag[IDENTIFIER_COLS+[x for x in diag.columns if x[:4]=='DIAG']]\n",
    "diag = diag.groupby(identifier_cols, as_index=False).sum()\n",
    "diag.sort_values(identifier_cols).to_csv('flatfiles/gi_diagnostics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surgeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surg = pd.read_csv('deid_R3_84_BINION_PROCEDURE_DATA_2018_11_05.csv', parse_dates=['ORIG_SERVICE_DATE'])\n",
    "surg = surg[surg.AMOUNT>0] \n",
    "\n",
    "surg['RESULT_YEAR'] = surg.ORIG_SERVICE_DATE.dt.year\n",
    "surg['RESULT_MONTH'] = surg.ORIG_SERVICE_DATE.dt.month\n",
    "surg.drop(surg.index[surg.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)\n",
    "surg.AUTO_ID = surg.AUTO_ID.astype(int)\n",
    "\n",
    "reqd_surg = surg[(surg.TYPE_OF_SERVICE=='20-SURGERY MA') & (surg.PROC_GROUP_NAME=='GASTROINTESTINAL/DIGESTIVE') & (~surg.PROC_NAME.str.contains('NEEDLE BIOPSY')) & (~surg.PROC_NAME.str.contains('APPENDECTOMY')) & (~surg.PROC_NAME.str.contains('GASTROSTOMY')) & (~surg.PROC_NAME.str.contains('ESOPH')) & (~surg.PROC_NAME.str.contains('CHOLECYSTOSTOMY')) & (~surg.PROC_NAME.str.contains('ANAST')) & (~surg.PROC_NAME.str.contains('TONGUE')) & (~surg.PROC_NAME.str.contains('LIVER')) & (~surg.PROC_NAME.str.contains('PLASTY')) & (~surg.PROC_NAME.str.contains('BILIARY')) & (~surg.PROC_NAME.str.contains('BILE')) & (~surg.PROC_NAME.str.contains('PAROTD')) & (~surg.PROC_NAME.str.contains('EXTRAHEP')) & (~surg.PROC_NAME.str.contains('PANCREA')) & (~surg.PROC_NAME.str.contains('SCLEROTX')) & (~surg.PROC_NAME.str.contains('GUM LESION'))]\n",
    "\n",
    "surgeries = pd.concat([reqd_surg, \n",
    "            surg[surg.PROC_CODE.isin(['44205', '44212', '44210', '44211', '44207', '44206', '44204', '44158'])], # COLECTOMIES\n",
    "            surg[surg.PROC_NAME.str.contains('CELIAC PLEXUS')]])\n",
    "\n",
    "surgeries = surgeries.sort_values(['AUTO_ID', 'ORIG_SERVICE_DATE']).drop_duplicates(['AUTO_ID', 'ORIG_SERVICE_DATE'])\n",
    "surgeries['ENC_PK_INJ'] = np.where(surgeries.PROC_NAME.str.contains('CELIAC PLEXUS'), 1, 0)\n",
    "surgeries['ENC_SURGERY'] = (1 - surgeries['ENC_PK_INJ']).abs()\n",
    "\n",
    "surgeries[IDENTIFIER_COLS+['ENC_SURGERY', 'ENC_PK_INJ']].groupby(IDENTIFIER_COLS, as_index=False).sum().to_csv('flatfiles/surgeries.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get encounter information\n",
    "- Flag for related/unrelated based on department/office (dermatology, ortho, opto, ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "enc_df = pd.read_csv(\"deid_IBD_Registry_BA1951_Office_Phone_Email_Encs_2018-07-05-09-43-22.csv\", parse_dates=['CONTACT_DATE']).drop('Unnamed: 0', axis=1)\n",
    "enc_df.ENC_TYPE_NAME = enc_df.ENC_TYPE_NAME.replace({'Telephone':'TEL', 'Office Visit':'OFF', 'Patient Email':'TEL', 'Procedure Visit':'PROC', 'New Patient Visit':'OFF', 'Consult':'OFF'})\n",
    "enc_df=enc_df[enc_df['DEPT_NAME'].notnull()]\n",
    "\n",
    "\n",
    "# related/unrelated\n",
    "enc_df['unrelated'] = 0\n",
    "bad_dept = re.compile(r'DERM |ORTHO |OPTOMETRY |ENT MERCY')\n",
    "enc_df['unrelated'] = enc_df.DEPT_NAME.str.match(bad_dept).astype(int)\n",
    "\n",
    "ENC = enc_df[['AUTO_ID', 'ENC_TYPE_NAME', 'CONTACT_DATE', 'unrelated']]\n",
    "ENC = pd.get_dummies(ENC, columns=['ENC_TYPE_NAME'], prefix='ENC')\n",
    "ENC['RESULT_YEAR'] = ENC.CONTACT_DATE.dt.year\n",
    "ENC['RESULT_MONTH'] = ENC.CONTACT_DATE.dt.month\n",
    "\n",
    "\n",
    "\n",
    "ENC = ENC[identifier_cols+[x for x in ENC.columns if x not in identifier_cols]]\n",
    "ENC['unrelated'] = ENC['unrelated'].replace({0:'Related', 1:'Unrelated'})\n",
    "ENC = ENC.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'unrelated']).sum()\n",
    "ENC = ENC.unstack(-1).reset_index().fillna(0)\n",
    "ENC.columns = ENC.columns.map('_'.join).str.strip('_')\n",
    "ENC.to_csv('flatfiles/encounters.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIBDQ - not using for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sibdq = pd.read_csv(os.path.join(IN_DATA_PATH, 'deid_IBD_Registry_BA1951_SIBDQ_Pain_Questionnaires_2018-07-05-10-02-02.csv'), parse_dates=['CONTACT_DATE']).drop(['Unnamed: 0', 'ENC_TYPE'], axis=1).dropna(how='all')\n",
    "# sibdq.drop(sibdq.index[sibdq.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)\n",
    "# sibdq.AUTO_ID = pd.to_numeric(sibdq.AUTO_ID).astype(int)\n",
    "# sibdq['PAIN_TODAY'] = sibdq['PAIN_TODAY'].fillna('No')\n",
    "\n",
    "# #compute missing total scores\n",
    "# q_cols = [x for x in sibdq.columns if x.startswith('Q')]\n",
    "# # parse responses as integers\n",
    "# sibdq[q_cols] = sibdq[q_cols].fillna('0').applymap(lambda st: int(st[0]))\n",
    "# # fill empty totals\n",
    "# null_scores_ix = sibdq[sibdq.SIBDQ_TOTAL_SCORE.isnull()].index\n",
    "# sibdq.loc[null_scores_ix, 'SIBDQ_TOTAL_SCORE'] = sibdq.loc[null_scores_ix][q_cols].sum(axis=1).astype(int)\n",
    "# sibdq.SIBDQ_TOTAL_SCORE = sibdq.SIBDQ_TOTAL_SCORE.astype(int)\n",
    "# sibdq['PAIN_TODAY'] = sibdq['PAIN_TODAY'].replace({'Yes':1, 'No':0})\n",
    "# sibdq.to_csv(os.path.join(OUT_DATA_PATH, 'sibdq.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbi = pd.read_csv('deid_IBD_Registry_BA1951_Harvey_Bradshaw_Questionnaires_2018-07-05-07-57-02.csv', parse_dates=['CONTACT_DATE'])\n",
    "hbi = hbi[['AUTO_ID', 'CONTACT_DATE', 'CROHNS_SCORE', 'UC_SCORE']]\n",
    "hbi.AUTO_ID = pd.to_numeric(hbi.AUTO_ID).astype(int)\n",
    "hbi['RESULT_YEAR'] = hbi.CONTACT_DATE.dt.year\n",
    "hbi['RESULT_MONTH'] = hbi.CONTACT_DATE.dt.month\n",
    "del hbi['CONTACT_DATE']\n",
    "\n",
    "HBI = hbi.groupby(IDENTIFIER_COLS).mean().reset_index()\n",
    "HBI.to_csv('flatfiles/hbi.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disease Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease = pd.read_csv('deid_disease type and year of diagnosis.csv')\n",
    "disease.DSTYPE = disease.DSTYPE.replace({'1':'CD', '2':'UC', '-2':'unk', '3':'unk'})\n",
    "disease[['DS_CD', 'DS_UC']] = pd.get_dummies(disease.DSTYPE).drop('unk', axis=1)\n",
    "disease['DS_AGE_DX'] = disease['AGE_OF_DX'].replace('#VALUE!', np.nan).astype(float).fillna(30).astype(int) # 30 is the average age\n",
    "disease['DS_PREV_RESECTION'] = disease['BOWEL_RESECTION_PRIOR_2009'].replace({'(No Data)':'0'}).fillna(0).astype(int)\n",
    "DS = disease[['AUTO_ID']+[x for x in disease.columns if x[:3]=='DS_']]\n",
    "DS.to_csv('flatfiles/disease_type.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inflation_data():\n",
    "    #http://www.usinflationcalculator.com/inflation/current-inflation-rates/\n",
    "    #I took it from the December column as it is explained on the website (last 12 months inflation)\n",
    "    #year,inflation\n",
    "    inflationrates={2005:3.4,\n",
    "                    2006:2.5,\n",
    "                    2007:4.1,\n",
    "                    2008:0.1,\n",
    "                    2009:2.7,\n",
    "                    2010:1.5,\n",
    "                    2011:3.0,\n",
    "                    2012:1.7,\n",
    "                    2013:1.5,\n",
    "                    2014:0.8,\n",
    "                    2015:0.7,\n",
    "                    2016:2.1,\n",
    "                    2017:2.1 } #last update: last charges made in 2018, so need to take into account only till 2017\n",
    "    for (year,rate) in inflationrates.items():\n",
    "        inflationrates[year]=rate*0.01+1\n",
    "    \n",
    "    #calculate coefficients    \n",
    "    lastyear=np.max(list(inflationrates.keys()))\n",
    "    inflation_coeff={lastyear+1:1.0}\n",
    "    for year in range(lastyear, 2005, -1):\n",
    "        inflation_coeff[year]=inflation_coeff[year+1]*inflationrates[year]\n",
    "    return inflation_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'ibd_related_ip_charges.csv' does not exist: b'ibd_related_ip_charges.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-fbd48c371167>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# unrelated_ip CHARGES -- from Claudia\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0munrelated_ip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ibd_related_ip_charges.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ADMISSION_DATE'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'DISCHARGE_DATE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0munrelated_ip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PAT_MRN_ID'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munrelated_ip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PAT_MRN_ID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0munrelated_ip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'related'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munrelated_ip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'related'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\utils\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\utils\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\utils\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\utils\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\utils\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'ibd_related_ip_charges.csv' does not exist: b'ibd_related_ip_charges.csv'"
     ]
    }
   ],
   "source": [
    "inflation_coeff = get_inflation_data()\n",
    "\n",
    "# unrelated_ip CHARGES -- from Claudia\n",
    "unrelated_ip = pd.read_csv('ibd_related_ip_charges.csv', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE'])\n",
    "unrelated_ip['PAT_MRN_ID'] = unrelated_ip['PAT_MRN_ID'].astype(int)\n",
    "unrelated_ip['related'] = unrelated_ip['related'].replace({2:1})\n",
    "unrelated_ip = unrelated_ip.merge(mrn_auto_map[['AUTO_ID', 'PAT_MRN_ID']], on='PAT_MRN_ID')\n",
    "\n",
    "\n",
    "#INPATIENT\n",
    "inpatient_charges_txn = pd.read_csv(\"deid_IP_Charges_Aug_2018_12_6_18.csv\",thousands=r',', parse_dates=['ADMISSION_DATE', 'DISCHARGE_DATE']).drop('Unnamed: 0', axis=1)\n",
    "inpatient_charges_txn.drop(inpatient_charges_txn.index[inpatient_charges_txn.AUTO_ID=='NO_MATCH'], axis=0, inplace=True)\n",
    "inpatient_charges_txn.index=inpatient_charges_txn.index.astype(int)\n",
    "#adjust for inflation\n",
    "inpatient_charges_txn['INPATIENT_CHARGES']=inpatient_charges_txn.apply(lambda row: row['TOTAL_CHARGES']*inflation_coeff[row['DISCHARGE_DATE'].year], axis=1)\n",
    "inpatient_charges_txn.AUTO_ID = inpatient_charges_txn.AUTO_ID.astype(int)\n",
    "\n",
    "# inpatients must have >0 DAYS_ADMITTED\n",
    "inpatient_charges_txn['DAYS_ADMITTED'] = (inpatient_charges_txn['DISCHARGE_DATE'] - inpatient_charges_txn['ADMISSION_DATE']).dt.days\n",
    "inpatient_charges_txn.loc[inpatient_charges.DAYS_ADMITTED==0, 'DAYS_ADMITTED'] = 1\n",
    "\n",
    "\n",
    "inpatient_charges = inpatient_charges_txn[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'INPATIENT_CHARGES']]\n",
    "inpatient_charges['INPATIENT'] = 1\n",
    "inpatient_charges.columns = ['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT', 'INPATIENT']\n",
    "\n",
    "unrelated_ip = unrelated_ip[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'related']]\n",
    "inpatient_charges = inpatient_charges.merge(unrelated_ip, how='left',  on=['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE'])\n",
    "inpatient_charges['unrelated'] = (inpatient_charges['related'].replace({2:0}) - 1).abs().fillna(0)\n",
    "inpatient_charges.drop(['related'], inplace=True, axis=1)\n",
    "inpatient_charges.to_csv('flatfiles/inpatient_charges.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # OUTPATIENT\n",
    "outpatient = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, \"deid_OP_charges_2018.csv\"), parse_dates=['ORIG_SERVICE_DATE'])\n",
    "outpatient = outpatient[outpatient['AMOUNT']>0]\n",
    "outpatient.drop(outpatient[outpatient.AUTO_ID=='NO_MATCH'].index, inplace=True)\n",
    "outpatient = outpatient[outpatient.DX.notnull() & outpatient.DX!='']\n",
    "outpatient.AUTO_ID=outpatient.AUTO_ID.astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TAGGING UNRELATED PROCEDURES ((422943, 9), (383395, 9), (357001, 10))\n",
    "# =======================================================================\n",
    "\n",
    "# Method 0: No filter\n",
    "# outpatient_charges_transactions0 = outpatient\n",
    "\n",
    "# Method 1: USe Claudia-provided procedures_to_exclude.csv as mask\n",
    "bad_proc = pd.read_csv(os.path.join(cfg.IN_DATA_PATH, 'procedures_to_exclude.csv'))\n",
    "outpatient_charges_transactions1 = outpatient.merge(bad_proc, how='left', on='PROC_NAME').drop_duplicates()\n",
    " \n",
    "\n",
    "# Method 2: Use Suraj-approximated ICD-classes filter in bad_code_ranges\n",
    "# def separate_dx_codes(opc):\n",
    "#     opc1 = opc[opc.DX.notnull() & opc.DX.str.contains(',')]\n",
    "#     opc2 = opc[~opc.index.isin(opc1.index)]\n",
    "#     dx = opc1.DX.str.split(', ').apply(pd.Series).stack()\n",
    "#     dx = dx.droplevel(-1)\n",
    "#     dx.name = 'DX'\n",
    "#     del opc1['DX']\n",
    "#     opc1 = opc1.join(dx)\n",
    "#     opc1 = opc1[opc2.columns]\n",
    "#     opc = pd.concat([opc1, opc2]).reset_index()\n",
    "#     return opc\n",
    "# outpatient = separate_dx_codes(outpatient)\n",
    "# opc = outpatient[outpatient.DX!='']\n",
    "# bad_code_ranges = [(630, 780), (460, 520), (280, 290)]\n",
    "# opc_num = opc[~(opc.DX.str.startswith('V') | opc.DX.str.startswith('E'))]\n",
    "\n",
    "# Method 3: Specify unrelated PROC_GROUP_NAME\n",
    "bad_proc_grp = ['FEMALE GENITAL & REPRODUCTIVE', 'INTEGUMENTARY/DERMATOLOGY', 'MUSCLE/SKELETAL SYSTEM',\\\n",
    "            'NERVOUS SYSTEM & NEURO STUDIES', 'MATERNITY CARE & DELIVERY', 'EAR, NOSE & THROAT', 'DENTAL', 'DME',\\\n",
    "           'EYE & OCULAR', 'MALE GENITAL', 'OTHER', 'PHYSICAL MEDICINE & REHAB', 'PULMONARY', 'SUPPLIES', 'URINARY/RENAL SYSTEM']\n",
    "\n",
    "# =======================================================================\n",
    "\n",
    "outpatient_charges_transactions = outpatient_charges_transactions1\n",
    "outpatient_charges_transactions['AMOUNT']=outpatient_charges_transactions.apply(lambda row: row['AMOUNT']*inflation_coeff[ row['ORIG_SERVICE_DATE'].year ], axis=1) #adjust by inflation\n",
    "outpatient_charges_transactions.loc[outpatient_charges_transactions.PROC_GROUP_NAME.isin(bad_proc_grp), 'unrelated'] = 1\n",
    "outpatient_charges_transactions['unrelated'] = outpatient_charges_transactions['unrelated'].fillna(0)\n",
    "outpatient_charges_transactions.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges_txn.csv'))\n",
    "\n",
    "outpatient_charges_transactions['ADMISSION_DATE'] = outpatient_charges_transactions['ORIG_SERVICE_DATE']\n",
    "outpatient_charges_transactions['DISCHARGE_DATE'] = outpatient_charges_transactions['ORIG_SERVICE_DATE']\n",
    "outpatient_charges_transactions['DAYS_ADMITTED'] = 0\n",
    "outpatient_charges_transactions['INPATIENT'] = 0\n",
    "op = outpatient_charges_transactions[['AUTO_ID', 'ADMISSION_DATE', 'DISCHARGE_DATE', 'DAYS_ADMITTED', 'AMOUNT', 'unrelated', 'INPATIENT']]\n",
    "op.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'outpatient_charges.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tot = pd.concat([inpatient_charges, op], ignore_index=True).sort_values(['AUTO_ID', 'DISCHARGE_DATE'])\n",
    "tot['RESULT_YEAR'] = tot.DISCHARGE_DATE.dt.year\n",
    "tot['RESULT_MONTH'] = tot.DISCHARGE_DATE.dt.month\n",
    "tot['CUMSUM'] = tot.groupby(['AUTO_ID', 'unrelated']).AMOUNT.cumsum()\n",
    "tot['unrelated'] = tot['unrelated'].replace({0:'RELATED', 1:'UNRELATED'})\n",
    "tot['INPATIENT'] = tot['INPATIENT'].replace({0:'OP', 1:'IP'})\n",
    "tot.to_csv(os.path.join(cfg.OUT_DATA_PATH, 'total_charges.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregates = tot.groupby(['AUTO_ID', 'RESULT_YEAR', 'RESULT_MONTH', 'INPATIENT', 'unrelated']).AMOUNT.sum()\n",
    "agg = aggregates.unstack([-1, -2]).reset_index().fillna(0)\n",
    "agg.columns = agg.columns.map('_'.join).str.strip('_')\n",
    "agg.to_csv(os.path.join(cfg.LONG_IN, 'charges.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUTO_ID</th>\n",
       "      <th>RESULT_YEAR</th>\n",
       "      <th>RESULT_MONTH</th>\n",
       "      <th>RELATED_OP</th>\n",
       "      <th>UNRELATED_OP</th>\n",
       "      <th>RELATED_IP</th>\n",
       "      <th>UNRELATED_IP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>8</td>\n",
       "      <td>138.383465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>9</td>\n",
       "      <td>79.746403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1050.556892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>184.989366</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>3</td>\n",
       "      <td>173.570269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87114</td>\n",
       "      <td>3140</td>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>134.782210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87115</td>\n",
       "      <td>3140</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>105.163000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87116</td>\n",
       "      <td>3140</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87117</td>\n",
       "      <td>3141</td>\n",
       "      <td>2017</td>\n",
       "      <td>9</td>\n",
       "      <td>456.397210</td>\n",
       "      <td>86.785000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87118</td>\n",
       "      <td>3141</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>220.536000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87119 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       AUTO_ID  RESULT_YEAR  RESULT_MONTH   RELATED_OP  UNRELATED_OP  \\\n",
       "0            0         2009             8   138.383465      0.000000   \n",
       "1            0         2009             9    79.746403      0.000000   \n",
       "2            0         2010             1  1050.556892      0.000000   \n",
       "3            0         2010             2     0.000000    184.989366   \n",
       "4            0         2010             3   173.570269      0.000000   \n",
       "...        ...          ...           ...          ...           ...   \n",
       "87114     3140         2017             3   134.782210      0.000000   \n",
       "87115     3140         2017             6   105.163000      0.000000   \n",
       "87116     3140         2018             8   103.000000      0.000000   \n",
       "87117     3141         2017             9   456.397210     86.785000   \n",
       "87118     3141         2017            11   220.536000      0.000000   \n",
       "\n",
       "       RELATED_IP  UNRELATED_IP  \n",
       "0             0.0           0.0  \n",
       "1             0.0           0.0  \n",
       "2             0.0           0.0  \n",
       "3             0.0           0.0  \n",
       "4             0.0           0.0  \n",
       "...           ...           ...  \n",
       "87114         0.0           0.0  \n",
       "87115         0.0           0.0  \n",
       "87116         0.0           0.0  \n",
       "87117         0.0           0.0  \n",
       "87118         0.0           0.0  \n",
       "\n",
       "[87119 rows x 7 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "500.766px",
    "left": "187px",
    "right": "20px",
    "top": "63px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
